{"ID": 1, "Question": "### Background\n\nA monopolist sells an infinitely durable good to a market with a constant inflow of new consumers each period. A fraction `α` are high-valuation (per-period utility `b₁`), and `1-α` are low-valuation (per-period utility `b₂`). The monopolist cannot commit to a future price path, creating a time-consistency problem: a plan that is optimal today may be repudiated by the monopolist's future self, who will face a large accumulated stock of low-valuation consumers.\n\n### Data / Model Specification\n\nThe monopolist and consumers share the same discount factor, `ρ=β`. The monopolist's discounted profit, as viewed from the start of a cycle (`j=1`) for a cycle of length `n`, is denoted `π(n,1)`. The profitability of extending a cycle from `n` to `n+1` periods is governed by:\n\n  \n\\text{sign}[\\pi(n+1,1) - \\pi(n,1)] = \\text{sign}(\\alpha b_{1} - b_{2}) \\quad \\text{for all } n>0 \\quad \\text{(Eq. (1))}\n \n\nIn the no-commitment case, the subgame perfect equilibrium cycle length `n*` is the smallest positive integer such that `f(n*) ≥ 0`, where `f(j) = π(j,j) - π(j+1,j)` represents the marginal gain to the monopolist at period `j` of ending the cycle immediately versus continuing.\n\n### Question\n\nSelect all statements that correctly describe the monopolist's strategy and the resulting equilibrium under the paper's assumptions.", "Answer": ["B", "D"], "qid": "1", "question": "### Background\n\nA monopolist sells an infinitely durable good to a market with a constant inflow of new consumers each period. A fraction `α` are high-valuation (per-period utility `b₁`), and `1-α` are low-valuation (per-period utility `b₂`). The monopolist cannot commit to a future price path, creating a time-consistency problem: a plan that is optimal today may be repudiated by the monopolist's future self, who will face a large accumulated stock of low-valuation consumers.\n\n### Data / Model Specification\n\nThe monopolist and consumers share the same discount factor, `ρ=β`. The monopolist's discounted profit, as viewed from the start of a cycle (`j=1`) for a cycle of length `n`, is denoted `π(n,1)`. The profitability of extending a cycle from `n` to `n+1` periods is governed by:\n\n  \n\\text{sign}[\\pi(n+1,1) - \\pi(n,1)] = \\text{sign}(\\alpha b_{1} - b_{2}) \\quad \\text{for all } n>0 \\quad \\text{(Eq. (1))}\n \n\nIn the no-commitment case, the subgame perfect equilibrium cycle length `n*` is the smallest positive integer such that `f(n*) ≥ 0`, where `f(j) = π(j,j) - π(j+1,j)` represents the marginal gain to the monopolist at period `j` of ending the cycle immediately versus continuing.\n\n### Question\n\nSelect all statements that correctly describe the monopolist's strategy and the resulting equilibrium under the paper's assumptions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA monopolist sells an infinitely d...", "model_answer": "This is a detailed response to the question about ### Background\n\nA monopolist sells an infinitely d...", "error": null}}
{"ID": 2, "Question": "### Background\n\nThis problem investigates the robustness of the paper's cyclic pricing model by examining its behavior in the limit as the time between price changes shrinks to zero. This transitions the model from discrete to continuous time. Let `T` be the calendar time between price changes and `L=nT` be the total calendar time of a cycle of `n` periods.\n\n### Data / Model Specification\n\nThe equilibrium cycle length in calendar time, `L`, is the root of a function `f(L,T)`. As the period length `T` approaches zero, this function has a well-defined limit, `f(L,0)`:\n\n  \nf(L,0) = -B(1-\\beta^{L})^{-2} \\left[ |\\log(\\beta)| L - (1-\\beta^{L}) \\right] + |\\log(\\beta)| L \\quad \\text{(Eq. (1))}\n \n\nHere, `B` is a parameter such that `0 < B < 1`. The paper establishes that `f(L,0)` is continuous, upward sloping in `L`, and becomes positive for large `L`.\n\n### Question\n\nSelect all statements that correctly describe the model's properties in the continuous-time limit (as the period length `T` approaches zero).", "Answer": ["B", "C"], "qid": "2", "question": "### Background\n\nThis problem investigates the robustness of the paper's cyclic pricing model by examining its behavior in the limit as the time between price changes shrinks to zero. This transitions the model from discrete to continuous time. Let `T` be the calendar time between price changes and `L=nT` be the total calendar time of a cycle of `n` periods.\n\n### Data / Model Specification\n\nThe equilibrium cycle length in calendar time, `L`, is the root of a function `f(L,T)`. As the period length `T` approaches zero, this function has a well-defined limit, `f(L,0)`:\n\n  \nf(L,0) = -B(1-\\beta^{L})^{-2} \\left[ |\\log(\\beta)| L - (1-\\beta^{L}) \\right] + |\\log(\\beta)| L \\quad \\text{(Eq. (1))}\n \n\nHere, `B` is a parameter such that `0 < B < 1`. The paper establishes that `f(L,0)` is continuous, upward sloping in `L`, and becomes positive for large `L`.\n\n### Question\n\nSelect all statements that correctly describe the model's properties in the continuous-time limit (as the period length `T` approaches zero).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the robu...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the robu...", "error": null}}
{"ID": 3, "Question": "### Background\n\nThis problem focuses on the microfoundations of the cyclic pricing model. A monopolist sells an infinitely durable good to infinitely-lived consumers who discount the future with factor `β`. The monopolist sets prices over a known cycle of length `n`, which ends with a sale at price `p_n = V₂`, where `V₂` is the reservation price of low-valuation consumers.\n\n### Data / Model Specification\n\nThe reservation price `V_i` for a consumer with per-period utility flow `b_i` is the discounted sum of all future flows, `V_i = b_i / (1-β)`. To induce a high-valuation consumer entering at period `j` to purchase immediately rather than wait `h` periods, the monopolist must satisfy the no-arbitrage condition:\n\n  \nV_{1} - p_{j} \\ge \\beta^{h}(V_{1} - p_{j+h}) \\quad \\text{for } h=1, ..., n-j \\quad \\text{(Eq. (1))}\n \n\nThe profit-maximizing price `p_j` is set by making the consumer indifferent between buying now and waiting for the most attractive future option, which is the final sale at period `n`.\n\n### Question\n\nConsider the derivation of the price path `p_j` and a counterfactual scenario where all consumers incur a small, non-monetary transaction cost `c > 0` at the time of purchase. Select all correct statements.", "Answer": ["C", "D"], "qid": "3", "question": "### Background\n\nThis problem focuses on the microfoundations of the cyclic pricing model. A monopolist sells an infinitely durable good to infinitely-lived consumers who discount the future with factor `β`. The monopolist sets prices over a known cycle of length `n`, which ends with a sale at price `p_n = V₂`, where `V₂` is the reservation price of low-valuation consumers.\n\n### Data / Model Specification\n\nThe reservation price `V_i` for a consumer with per-period utility flow `b_i` is the discounted sum of all future flows, `V_i = b_i / (1-β)`. To induce a high-valuation consumer entering at period `j` to purchase immediately rather than wait `h` periods, the monopolist must satisfy the no-arbitrage condition:\n\n  \nV_{1} - p_{j} \\ge \\beta^{h}(V_{1} - p_{j+h}) \\quad \\text{for } h=1, ..., n-j \\quad \\text{(Eq. (1))}\n \n\nThe profit-maximizing price `p_j` is set by making the consumer indifferent between buying now and waiting for the most attractive future option, which is the final sale at period `n`.\n\n### Question\n\nConsider the derivation of the price path `p_j` and a counterfactual scenario where all consumers incur a small, non-monetary transaction cost `c > 0` at the time of purchase. Select all correct statements.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem focuses on the microf...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem focuses on the microf...", "error": null}}
{"ID": 4, "Question": "### Background\n\n**Research Question.** This problem requires the interpretation and quantification of the main empirical findings regarding tacit collusion in retail gasoline markets.\n\n**Setting / Institutional Environment.** The analysis uses a two-stage least squares (2SLS) estimation of a dynamic price model on a monthly panel of 43 U.S. cities. The goal is to estimate the causal effect of expected future market conditions on current retail margins.\n\n**Variables & Parameters.**\n- `MARGIN_{it}`: Retail-terminal margin (units: cents per gallon).\n- `EXPNVOLUME_{it+1}`: Expected next-period normalized volume (dimensionless).\n- `EXPTERMINAL_{it+1}`: Expected next-period terminal price (units: cents per gallon).\n- Unit of observation: City-month panel (`i` indexes city, `t` indexes month).\n\n---\n\n### Data / Model Specification\n\nThe core estimating equation is a dynamic model for the retail margin, including controls for current conditions and lagged price adjustments.\n  \nMARGIN_{it} = ... + \\alpha_{2}EXPNVOLUME_{it+1} + \\alpha_{4}EXPTERMINAL_{it+1} + ... + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \nKey results from the 2SLS estimation of this model and relevant sample means are provided below.\n\n**Table 1: 2SLS Estimation Results for Key Variables**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :--- | :--- |\n| `EXPNVOLUME_{t+1}` | 4.158 | 1.533 |\n| `EXPTERMINAL_{t+1}` | -0.063 | 0.022 |\n\n**Table 2: Descriptive Statistics**\n\n| Variable | Mean |\n| :--- | :--- |\n| `MARGIN` | 10.99 |\n| `NVOLUME` | 1.004 |\n| `TERMINAL` | 61.82 |\n\n---\n\nBased on the provided data, which of the following statements are valid quantitative or qualitative inferences?\n", "Answer": ["B", "C"], "qid": "4", "question": "### Background\n\n**Research Question.** This problem requires the interpretation and quantification of the main empirical findings regarding tacit collusion in retail gasoline markets.\n\n**Setting / Institutional Environment.** The analysis uses a two-stage least squares (2SLS) estimation of a dynamic price model on a monthly panel of 43 U.S. cities. The goal is to estimate the causal effect of expected future market conditions on current retail margins.\n\n**Variables & Parameters.**\n- `MARGIN_{it}`: Retail-terminal margin (units: cents per gallon).\n- `EXPNVOLUME_{it+1}`: Expected next-period normalized volume (dimensionless).\n- `EXPTERMINAL_{it+1}`: Expected next-period terminal price (units: cents per gallon).\n- Unit of observation: City-month panel (`i` indexes city, `t` indexes month).\n\n---\n\n### Data / Model Specification\n\nThe core estimating equation is a dynamic model for the retail margin, including controls for current conditions and lagged price adjustments.\n  \nMARGIN_{it} = ... + \\alpha_{2}EXPNVOLUME_{it+1} + \\alpha_{4}EXPTERMINAL_{it+1} + ... + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \nKey results from the 2SLS estimation of this model and relevant sample means are provided below.\n\n**Table 1: 2SLS Estimation Results for Key Variables**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :--- | :--- |\n| `EXPNVOLUME_{t+1}` | 4.158 | 1.533 |\n| `EXPTERMINAL_{t+1}` | -0.063 | 0.022 |\n\n**Table 2: Descriptive Statistics**\n\n| Variable | Mean |\n| :--- | :--- |\n| `MARGIN` | 10.99 |\n| `NVOLUME` | 1.004 |\n| `TERMINAL` | 61.82 |\n\n---\n\nBased on the provided data, which of the following statements are valid quantitative or qualitative inferences?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 5, "Question": "### Background\n\n**Research Question.** This problem explores the core theoretical mechanism from supergame theory used to test for tacit collusion. The central idea is that to sustain cooperation, firms must adjust current prices in response to expected future market conditions.\n\n**Setting / Institutional Environment.** The analysis is grounded in repeated game models where firms weigh the immediate gains from deviating from a collusive strategy against the future losses from a subsequent punishment phase (e.g., a price war).\n\n**Variables & Parameters.**\n- `π^C(k_t)`: Per-firm profit in period `t` under a collusive strategy, conditional on market state `k_t` (e.g., demand and cost).\n- `π^D(k_t)`: Per-firm profit in period `t` from optimally deviating (e.g., undercutting the price), conditional on market state `k_t`.\n- `π^P(k_t)`: Per-firm profit in period `t` during a punishment phase (e.g., reversion to a non-cooperative Nash equilibrium), conditional on market state `k_t`.\n- `δ`: The firm's discount factor, `0 < δ < 1`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental condition for sustaining a tacitly collusive agreement in any period `t` is that the one-time gain from defection must be less than or equal to the discounted present value of the future losses incurred during the punishment phase. This is known as the incentive compatibility (IC) constraint for collusion:\n  \nπ^D(k_t) - π^C(k_t) ≤ ∑_{s=1}^{∞} δ^s [E_t(π^C(k_{t+s})) - E_t(π^P(k_{t+s}))] \\quad \\text{(Eq. (1))}\n \n\n---\n\nAccording to the logic of the incentive compatibility constraint in Eq. (1), which of the following statements correctly describe the mechanics of tacit collusion?\n", "Answer": ["A", "D"], "qid": "5", "question": "### Background\n\n**Research Question.** This problem explores the core theoretical mechanism from supergame theory used to test for tacit collusion. The central idea is that to sustain cooperation, firms must adjust current prices in response to expected future market conditions.\n\n**Setting / Institutional Environment.** The analysis is grounded in repeated game models where firms weigh the immediate gains from deviating from a collusive strategy against the future losses from a subsequent punishment phase (e.g., a price war).\n\n**Variables & Parameters.**\n- `π^C(k_t)`: Per-firm profit in period `t` under a collusive strategy, conditional on market state `k_t` (e.g., demand and cost).\n- `π^D(k_t)`: Per-firm profit in period `t` from optimally deviating (e.g., undercutting the price), conditional on market state `k_t`.\n- `π^P(k_t)`: Per-firm profit in period `t` during a punishment phase (e.g., reversion to a non-cooperative Nash equilibrium), conditional on market state `k_t`.\n- `δ`: The firm's discount factor, `0 < δ < 1`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental condition for sustaining a tacitly collusive agreement in any period `t` is that the one-time gain from defection must be less than or equal to the discounted present value of the future losses incurred during the punishment phase. This is known as the incentive compatibility (IC) constraint for collusion:\n  \nπ^D(k_t) - π^C(k_t) ≤ ∑_{s=1}^{∞} δ^s [E_t(π^C(k_{t+s})) - E_t(π^P(k_{t+s}))] \\quad \\text{(Eq. (1))}\n \n\n---\n\nAccording to the logic of the incentive compatibility constraint in Eq. (1), which of the following statements correctly describe the mechanics of tacit collusion?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 6, "Question": "### Background\n\n**Research Question.** This problem examines the empirical strategy for estimating the causal effect of expected future demand and cost on current retail gasoline margins, as a test for tacitly collusive behavior.\n\n**Setting / Institutional Environment.** The analysis uses a monthly panel of data for 43 U.S. cities. The core idea is to regress the current retail margin on measures of current and expected future market conditions.\n\n**Variables & Parameters.**\n- `MARGIN_{it}`: Retail price minus terminal (wholesale) price in city `i` at time `t`.\n- `NVOLUME_{it}`: Normalized gasoline volume in city `i` at time `t`.\n- `TERMINAL_{it}`: Terminal price in city `i` at time `t`.\n- `EXPTERMINAL_{it+1}`: Expected next-period terminal price.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is specified as:\n  \nMARGIN_{it} = \\alpha_{1}NVOLUME_{it} + \\alpha_{2}EXPNVOLUME_{it+1} + \\alpha_{3}TERMINAL_{it} + \\alpha_{4}EXPTERMINAL_{it+1} + ... + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \nAll estimations include city and time fixed effects and are estimated via two-stage least squares (2SLS) to address endogeneity.\n\n---\n\nWhich of the following statements describe valid identification challenges or strategies related to Eq. (1)?\n", "Answer": ["A", "D"], "qid": "6", "question": "### Background\n\n**Research Question.** This problem examines the empirical strategy for estimating the causal effect of expected future demand and cost on current retail gasoline margins, as a test for tacitly collusive behavior.\n\n**Setting / Institutional Environment.** The analysis uses a monthly panel of data for 43 U.S. cities. The core idea is to regress the current retail margin on measures of current and expected future market conditions.\n\n**Variables & Parameters.**\n- `MARGIN_{it}`: Retail price minus terminal (wholesale) price in city `i` at time `t`.\n- `NVOLUME_{it}`: Normalized gasoline volume in city `i` at time `t`.\n- `TERMINAL_{it}`: Terminal price in city `i` at time `t`.\n- `EXPTERMINAL_{it+1}`: Expected next-period terminal price.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is specified as:\n  \nMARGIN_{it} = \\alpha_{1}NVOLUME_{it} + \\alpha_{2}EXPNVOLUME_{it+1} + \\alpha_{3}TERMINAL_{it} + \\alpha_{4}EXPTERMINAL_{it+1} + ... + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \nAll estimations include city and time fixed effects and are estimated via two-stage least squares (2SLS) to address endogeneity.\n\n---\n\nWhich of the following statements describe valid identification challenges or strategies related to Eq. (1)?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 7, "Question": "### Background\n\n**Research Question.** This problem examines the equilibrium determination of technology capital stocks and the resulting specialization patterns, focusing on why smaller or later-opening countries might optimally choose to produce no technology capital of their own.\n\n**Setting.** In a multi-country world, each country `i` endogenously chooses its stock of technology capital, `M_i`. The equilibrium is characterized by a no-arbitrage condition: the total return on `M_i` must equal its user cost. This framework is used to analyze a scenario where one country unilaterally opens to a world of `I` closed, identical countries.\n\n### Data / Model Specification\n\nThe total world return on country `i`'s technology capital is the sum of its marginal products in all countries `j` where it can be used:\n  \nr_i(M) = \\sum_{j=1}^I \\frac{\\partial F_j}{\\partial M_i}\n \nThe steady-state equilibrium for `M` is defined by the complementary slackness conditions:\n  \nr_i(M) \\le \\rho + \\delta_m, \\quad \\text{with equality if } M_i > 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\rho + \\delta_m` is the user cost of technology capital.\n\nFor a set of identical countries, the steady-state output can be expressed as:\n  \nY_i = \\psi (AN)_i \\left( M_i + \\omega_i \\sum_{j \\ne i} M_j \\right)^{\\phi / (1 - \\alpha(1-\\phi))}\n \nwhere `psi` is a constant incorporating underlying parameters.\n\n### Question\n\nConsider a world of `I+1` identical countries. One country (`o`) unilaterally becomes fully open (`omega_o=1`), while the other `I` countries (`c`) remain closed (`omega_c=0`). The open country finds it optimal to set its own technology capital to zero (`M_o=0`) and rely on FDI from the `I` closed countries. Based on the model, select all statements that correctly describe this equilibrium.", "Answer": ["B", "C"], "qid": "7", "question": "### Background\n\n**Research Question.** This problem examines the equilibrium determination of technology capital stocks and the resulting specialization patterns, focusing on why smaller or later-opening countries might optimally choose to produce no technology capital of their own.\n\n**Setting.** In a multi-country world, each country `i` endogenously chooses its stock of technology capital, `M_i`. The equilibrium is characterized by a no-arbitrage condition: the total return on `M_i` must equal its user cost. This framework is used to analyze a scenario where one country unilaterally opens to a world of `I` closed, identical countries.\n\n### Data / Model Specification\n\nThe total world return on country `i`'s technology capital is the sum of its marginal products in all countries `j` where it can be used:\n  \nr_i(M) = \\sum_{j=1}^I \\frac{\\partial F_j}{\\partial M_i}\n \nThe steady-state equilibrium for `M` is defined by the complementary slackness conditions:\n  \nr_i(M) \\le \\rho + \\delta_m, \\quad \\text{with equality if } M_i > 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\rho + \\delta_m` is the user cost of technology capital.\n\nFor a set of identical countries, the steady-state output can be expressed as:\n  \nY_i = \\psi (AN)_i \\left( M_i + \\omega_i \\sum_{j \\ne i} M_j \\right)^{\\phi / (1 - \\alpha(1-\\phi))}\n \nwhere `psi` is a constant incorporating underlying parameters.\n\n### Question\n\nConsider a world of `I+1` identical countries. One country (`o`) unilaterally becomes fully open (`omega_o=1`), while the other `I` countries (`c`) remain closed (`omega_c=0`). The open country finds it optimal to set its own technology capital to zero (`M_o=0`) and rely on FDI from the `I` closed countries. Based on the model, select all statements that correctly describe this equilibrium.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 8, "Question": "### Background\n\n**Research Question.** This problem analyzes the full dynamic consequences of economic opening, focusing on the crucial distinction between a country's true economic output and its measured GDP, and the role of endogenous TFP growth in amplifying long-run gains.\n\n**Setting.** The model's transition analysis reveals that when a small country opens to a large economic bloc, its short-run performance as measured by official statistics can be misleading. This is due to the accounting treatment of intangible investment. Furthermore, the total gains from opening are significantly larger if openness itself fosters productivity growth.\n\n### Data / Model Specification\n\nThe economy-wide resource constraint defines the allocation of true output `Y_i`:\n  \nY_i = C_i + X_{ik} + X_{im}\n \nwhere `C_i` is consumption, `X_{ik}` is investment in tangible (plant-specific) capital, and `X_{im}` is investment in intangible (technology) capital.\n\nStandard national accounting treats intangible investment `X_{im}` as an intermediate business expense, so it is excluded from measured GDP, `Y_i^{\\text{meas}}`:\n  \nY_i^{\\text{meas}} = Y_i - X_{im} = C_i + X_{ik}\n \nIn an extension, the model considers that opening can cause a country's TFP to rise. The paper reports that for a small country, the direct long-run consumption gain (holding TFP fixed) is 30%, while the total gain (with TFP diffusion) is 50%.\n\n### Question\n\nAccording to the paper's transition analysis for a small country opening up, select all statements that are **INCORRECT** or **NOT supported** by the model's predictions.", "Answer": ["A", "B", "D"], "qid": "8", "question": "### Background\n\n**Research Question.** This problem analyzes the full dynamic consequences of economic opening, focusing on the crucial distinction between a country's true economic output and its measured GDP, and the role of endogenous TFP growth in amplifying long-run gains.\n\n**Setting.** The model's transition analysis reveals that when a small country opens to a large economic bloc, its short-run performance as measured by official statistics can be misleading. This is due to the accounting treatment of intangible investment. Furthermore, the total gains from opening are significantly larger if openness itself fosters productivity growth.\n\n### Data / Model Specification\n\nThe economy-wide resource constraint defines the allocation of true output `Y_i`:\n  \nY_i = C_i + X_{ik} + X_{im}\n \nwhere `C_i` is consumption, `X_{ik}` is investment in tangible (plant-specific) capital, and `X_{im}` is investment in intangible (technology) capital.\n\nStandard national accounting treats intangible investment `X_{im}` as an intermediate business expense, so it is excluded from measured GDP, `Y_i^{\\text{meas}}`:\n  \nY_i^{\\text{meas}} = Y_i - X_{im} = C_i + X_{ik}\n \nIn an extension, the model considers that opening can cause a country's TFP to rise. The paper reports that for a small country, the direct long-run consumption gain (holding TFP fixed) is 30%, while the total gain (with TFP diffusion) is 50%.\n\n### Question\n\nAccording to the paper's transition analysis for a small country opening up, select all statements that are **INCORRECT** or **NOT supported** by the model's predictions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 9, "Question": "### Background\n\n**Research Question.** This problem requires understanding the key properties of the country-level aggregate production function, which is derived from plant-level microfoundations.\n\n**Setting.** A country `i` consists of `N_i` production locations. Output can be produced at these locations by domestic firms or by foreign firms. The country optimally allocates its aggregate composite input, `Z_i`, across all active plants to maximize total output.\n\n### Data / Model Specification\n\nAt a single location, output from a domestic-owned plant (`y_d`) and a foreign-owned plant (`y_f`) are given by:\n  \ny_d = A_i z_d^{1-\\phi}\ny_f = \\sigma_i A_i z_f^{1-\\phi}\n \nwhere `z` is the composite input and `\\phi \\in (0,1)` governs diminishing returns at the plant level.\n\nThis micro-setup leads to the derived country aggregate production function:\n  \nY_i = A_i N_i^{\\phi} \\left( M_i + \\omega_i \\sum_{j \\ne i} M_j \\right)^{\\phi} (K_i^{\\alpha} L_i^{1-\\alpha})^{1-\\phi}\n \nwhere `{K_i, L_i, {M_j}}` are the productive inputs.\n\n### Question\n\nBased on the model's microfoundations and the resulting aggregate production function, select all statements that are correct.", "Answer": ["A", "C"], "qid": "9", "question": "### Background\n\n**Research Question.** This problem requires understanding the key properties of the country-level aggregate production function, which is derived from plant-level microfoundations.\n\n**Setting.** A country `i` consists of `N_i` production locations. Output can be produced at these locations by domestic firms or by foreign firms. The country optimally allocates its aggregate composite input, `Z_i`, across all active plants to maximize total output.\n\n### Data / Model Specification\n\nAt a single location, output from a domestic-owned plant (`y_d`) and a foreign-owned plant (`y_f`) are given by:\n  \ny_d = A_i z_d^{1-\\phi}\ny_f = \\sigma_i A_i z_f^{1-\\phi}\n \nwhere `z` is the composite input and `\\phi \\in (0,1)` governs diminishing returns at the plant level.\n\nThis micro-setup leads to the derived country aggregate production function:\n  \nY_i = A_i N_i^{\\phi} \\left( M_i + \\omega_i \\sum_{j \\ne i} M_j \\right)^{\\phi} (K_i^{\\alpha} L_i^{1-\\alpha})^{1-\\phi}\n \nwhere `{K_i, L_i, {M_j}}` are the productive inputs.\n\n### Question\n\nBased on the model's microfoundations and the resulting aggregate production function, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 10, "Question": "### Background\n\n**Research Question.** A central challenge in empirical studies of social networks is identifying the nature of strategic interactions. An observed positive correlation between an agent's network connectivity (degree) and their propensity to take an action (e.g., vaccinate) could arise from fundamentally different strategic environments. This problem explores this observational equivalence and the risk of policy failure when the underlying mechanism is misspecified.\n\n**Setting / Institutional Environment.** A policymaker observes that individuals with a higher degree `$k$` are more likely to vaccinate (`$x=1$`). The policymaker is considering an intervention but is uncertain about the true model governing behavior. Two models are consistent with the data.\n\n### Data / Model Specification\n\nAn agent's payoff is given by `$\\Pi = f(x,s) - c(x)$`, where `$x \\in \\{0,1\\}$` is the agent's action, `$s$` is a statistic aggregating their neighbors' actions, and `$c$` is a cost. The probability that a random neighbor vaccinates is `$p_{\\sigma}$`.\n\n**Model A: Strategic Substitutes (Weakest-Link Game)**\n- An agent's payoff is `$\\Pi = 1-c$` if they vaccinate (`$x=1$`).\n- The payoff is `$\\Pi = s$` if they do not vaccinate (`$x=0$`), where `$s = \\min(\\text{neighbors' actions})$`.\n- This is a game of strategic substitutes, where another's contribution reduces one's own incentive to contribute.\n\n**Model B: Strategic Complements (Conformity Game)**\n- An agent's payoff is `$\\Pi = s-c$` if they vaccinate (`$x=1$`).\n- The payoff is `$\\Pi = 0$` if they do not vaccinate (`$x=0$`), where `$s = \\max(\\text{neighbors' actions})$`.\n- This is a game of strategic complements, where another's action increases one's own incentive to act.\n\n**Theoretical Primitives:**\n- A statistic `$s$` is **FOSD degree-increasing** if the distribution of `$s$` for a degree-`(k+1)` agent first-order stochastically dominates that for a degree-`$k$` agent. The 'maximum' is an example.\n- A statistic `$s$` is **FOSD degree-decreasing** if the distribution for a degree-`$k$` agent first-order stochastically dominates that for a degree-`(k+1)` agent. The 'minimum' is an example.\n\n### Question\n\nBased on the provided models and theoretical framework, select all statements that are correct.", "Answer": ["C", "D"], "qid": "10", "question": "### Background\n\n**Research Question.** A central challenge in empirical studies of social networks is identifying the nature of strategic interactions. An observed positive correlation between an agent's network connectivity (degree) and their propensity to take an action (e.g., vaccinate) could arise from fundamentally different strategic environments. This problem explores this observational equivalence and the risk of policy failure when the underlying mechanism is misspecified.\n\n**Setting / Institutional Environment.** A policymaker observes that individuals with a higher degree `$k$` are more likely to vaccinate (`$x=1$`). The policymaker is considering an intervention but is uncertain about the true model governing behavior. Two models are consistent with the data.\n\n### Data / Model Specification\n\nAn agent's payoff is given by `$\\Pi = f(x,s) - c(x)$`, where `$x \\in \\{0,1\\}$` is the agent's action, `$s$` is a statistic aggregating their neighbors' actions, and `$c$` is a cost. The probability that a random neighbor vaccinates is `$p_{\\sigma}$`.\n\n**Model A: Strategic Substitutes (Weakest-Link Game)**\n- An agent's payoff is `$\\Pi = 1-c$` if they vaccinate (`$x=1$`).\n- The payoff is `$\\Pi = s$` if they do not vaccinate (`$x=0$`), where `$s = \\min(\\text{neighbors' actions})$`.\n- This is a game of strategic substitutes, where another's contribution reduces one's own incentive to contribute.\n\n**Model B: Strategic Complements (Conformity Game)**\n- An agent's payoff is `$\\Pi = s-c$` if they vaccinate (`$x=1$`).\n- The payoff is `$\\Pi = 0$` if they do not vaccinate (`$x=0$`), where `$s = \\max(\\text{neighbors' actions})$`.\n- This is a game of strategic complements, where another's action increases one's own incentive to act.\n\n**Theoretical Primitives:**\n- A statistic `$s$` is **FOSD degree-increasing** if the distribution of `$s$` for a degree-`(k+1)` agent first-order stochastically dominates that for a degree-`$k$` agent. The 'maximum' is an example.\n- A statistic `$s$` is **FOSD degree-decreasing** if the distribution for a degree-`$k$` agent first-order stochastically dominates that for a degree-`(k+1)` agent. The 'minimum' is an example.\n\n### Question\n\nBased on the provided models and theoretical framework, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** A central c...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** A central c...", "error": null}}
{"ID": 11, "Question": "### Background\n\n**Research Question.** In many empirical models of peer effects, an agent's payoff is assumed to depend on the *average* action of their neighbors. This problem explores the novel theoretical finding that for such statistics, the nature of the equilibrium relationship between degree and action depends not on standard strategic complementarity/substitutability, but on higher-order properties of the payoff function and the stability of the resulting equilibria.\n\n**Setting / Institutional Environment.** Consider a symmetric Bayesian network game where the relevant statistic `$s$` is the average of neighbors' actions. Agents choose an action `$x \\in \\{0,1\\}$` to maximize their payoff.\n\n### Data / Model Specification\n\nThe 'average' statistic `$s$` has two key properties under degree independence:\n1.  **Degree-stable:** The expected value `$E_k(s)` is constant for all degrees `$k$`.\n2.  **Satisfies SOSD:** The distribution of `$s$` undergoes a mean-preserving contraction as `$k$` increases, meaning its variance `$\\text{Var}_k(s)$` is strictly decreasing in `$k$`.\n\nAn agent's payoff is `$\\Pi = f(x,s) - c(x)$`. The key theoretical results are:\n- **Proposition 1 (part 2a):** If `$s$` is degree-stable and satisfies SOSD, the equilibrium strategy `$\\sigma^*_k$` is FOSD increasing if `$f_{xss} < 0$` and FOSD decreasing if `$f_{xss} > 0$`.\n- **Proposition 2:** If `$s$` is degree-stable, satisfies SOSD, and `$f_{xss} \\neq 0$`, then an 'all equal' equilibrium in *mixed strategies* cannot exist.\n\nFor an agent to be willing to play a mixed strategy with payoff function `$f(x,s) = x(-\\alpha s^2 - \\beta s + 1)$` and cost `$c(x)=c \\cdot x$`, the indifference condition is:\n  \n\\alpha\\mathrm{Var}_{k}(s)+\\alpha{(E_{k}(s))}^{2}+\\beta E_{k}(s)=1-c \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the provided model specifications and theoretical results for the 'average' statistic, select all statements that are correct.", "Answer": ["A", "D"], "qid": "11", "question": "### Background\n\n**Research Question.** In many empirical models of peer effects, an agent's payoff is assumed to depend on the *average* action of their neighbors. This problem explores the novel theoretical finding that for such statistics, the nature of the equilibrium relationship between degree and action depends not on standard strategic complementarity/substitutability, but on higher-order properties of the payoff function and the stability of the resulting equilibria.\n\n**Setting / Institutional Environment.** Consider a symmetric Bayesian network game where the relevant statistic `$s$` is the average of neighbors' actions. Agents choose an action `$x \\in \\{0,1\\}$` to maximize their payoff.\n\n### Data / Model Specification\n\nThe 'average' statistic `$s$` has two key properties under degree independence:\n1.  **Degree-stable:** The expected value `$E_k(s)` is constant for all degrees `$k$`.\n2.  **Satisfies SOSD:** The distribution of `$s$` undergoes a mean-preserving contraction as `$k$` increases, meaning its variance `$\\text{Var}_k(s)$` is strictly decreasing in `$k$`.\n\nAn agent's payoff is `$\\Pi = f(x,s) - c(x)$`. The key theoretical results are:\n- **Proposition 1 (part 2a):** If `$s$` is degree-stable and satisfies SOSD, the equilibrium strategy `$\\sigma^*_k$` is FOSD increasing if `$f_{xss} < 0$` and FOSD decreasing if `$f_{xss} > 0$`.\n- **Proposition 2:** If `$s$` is degree-stable, satisfies SOSD, and `$f_{xss} \\neq 0$`, then an 'all equal' equilibrium in *mixed strategies* cannot exist.\n\nFor an agent to be willing to play a mixed strategy with payoff function `$f(x,s) = x(-\\alpha s^2 - \\beta s + 1)$` and cost `$c(x)=c \\cdot x$`, the indifference condition is:\n  \n\\alpha\\mathrm{Var}_{k}(s)+\\alpha{(E_{k}(s))}^{2}+\\beta E_{k}(s)=1-c \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the provided model specifications and theoretical results for the 'average' statistic, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** In many emp...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** In many emp...", "error": null}}
{"ID": 12, "Question": "### Background\n\n**Research Question.** This problem analyzes how rational, forward-looking voters trade off the desire to influence the current election's outcome against the desire to communicate private information that influences future elections.\n\n**Setting / Institutional Environment.** The setting is a two-period voting game. In period 1, candidates A and C compete. In period 2, the winner of period 1 competes against a new candidate, B. A subset of voters, called \"type-1,\" prefer A to C, but their preference between A and B depends on an unknown state of the world, `s ∈ {s_A, s_B}`. Specifically, they prefer A in state `s_A` and B in state `s_B`. These voters receive private signals about the true state. A \"type-B\" voter is a type-1 voter whose signal indicates state `s_B` is more likely. By voting for C in period 1 (an act of \"communicative voting\"), type-B voters can lower A's vote share, signaling to others that `s_B` is the likely state and thus helping to coordinate votes for B in period 2.\n\n**Variables & Parameters.**\n- `γ`: The probability that a type-B voter votes for C in period 1 (dimensionless).\n- `α_s(γ)`: The expected vote share for candidate A in state `s`, which depends on `γ`.\n- `P_n^{d-m}`: The probability of a voter being pivotal for decision-making (i.e., changing the winner of the period-1 election).\n- `P_n^{com}`: The probability of a voter being pivotal for communication (i.e., changing the collective inference about the state `s` for the period-2 election).\n\n### Data / Model Specification\n\nIn large electorates (`n → +∞`), the probabilities of being pivotal for decision-making (`P_n^{d-m}`) and communication (`P_n^{com}`) both converge to zero. The equilibrium voting strategy is determined by their relative rates of convergence. These rates are captured by `r^{d-m}` and `r^{com}` in the following asymptotic approximations:\n\n  \n\\lim_{n \\to +\\infty} \\frac{\\log(P_n^{d-m})}{n} = \\log(r^{d-m}(\\beta)) \\quad \\text{(Eq. (1))}\n \n\n  \n\\lim_{n \\to +\\infty} \\frac{\\log(P_n^{com})}{n} = \\log(r^{com}(\\beta, \\beta')) \\quad \\text{(Eq. (2))}\n \n\nThe functional forms for these rates are:\n\n  \nr^{d-m}(\\beta) = 2(\\beta(1-\\beta))^{1/2} \\quad \\text{(Eq. (3))}\n \n\nA stable equilibrium level of communicative voting, `γ^*`, occurs when the rates of convergence are equalized: `r_{com}(γ^*) = r_{d-m}(γ^*)`, where `r_{com}(γ) = r^{com}(α_A(γ), α_B(γ))` and `r_{d-m}(γ) = \\max[r^{d-m}(α_A(γ)), r^{d-m}(α_B(γ))]`.\n\n### Question\n\nBased on the model, which of the following statements correctly describe the logic of communicative voting and the factors influencing its equilibrium level?\n", "Answer": ["B", "C", "D"], "qid": "12", "question": "### Background\n\n**Research Question.** This problem analyzes how rational, forward-looking voters trade off the desire to influence the current election's outcome against the desire to communicate private information that influences future elections.\n\n**Setting / Institutional Environment.** The setting is a two-period voting game. In period 1, candidates A and C compete. In period 2, the winner of period 1 competes against a new candidate, B. A subset of voters, called \"type-1,\" prefer A to C, but their preference between A and B depends on an unknown state of the world, `s ∈ {s_A, s_B}`. Specifically, they prefer A in state `s_A` and B in state `s_B`. These voters receive private signals about the true state. A \"type-B\" voter is a type-1 voter whose signal indicates state `s_B` is more likely. By voting for C in period 1 (an act of \"communicative voting\"), type-B voters can lower A's vote share, signaling to others that `s_B` is the likely state and thus helping to coordinate votes for B in period 2.\n\n**Variables & Parameters.**\n- `γ`: The probability that a type-B voter votes for C in period 1 (dimensionless).\n- `α_s(γ)`: The expected vote share for candidate A in state `s`, which depends on `γ`.\n- `P_n^{d-m}`: The probability of a voter being pivotal for decision-making (i.e., changing the winner of the period-1 election).\n- `P_n^{com}`: The probability of a voter being pivotal for communication (i.e., changing the collective inference about the state `s` for the period-2 election).\n\n### Data / Model Specification\n\nIn large electorates (`n → +∞`), the probabilities of being pivotal for decision-making (`P_n^{d-m}`) and communication (`P_n^{com}`) both converge to zero. The equilibrium voting strategy is determined by their relative rates of convergence. These rates are captured by `r^{d-m}` and `r^{com}` in the following asymptotic approximations:\n\n  \n\\lim_{n \\to +\\infty} \\frac{\\log(P_n^{d-m})}{n} = \\log(r^{d-m}(\\beta)) \\quad \\text{(Eq. (1))}\n \n\n  \n\\lim_{n \\to +\\infty} \\frac{\\log(P_n^{com})}{n} = \\log(r^{com}(\\beta, \\beta')) \\quad \\text{(Eq. (2))}\n \n\nThe functional forms for these rates are:\n\n  \nr^{d-m}(\\beta) = 2(\\beta(1-\\beta))^{1/2} \\quad \\text{(Eq. (3))}\n \n\nA stable equilibrium level of communicative voting, `γ^*`, occurs when the rates of convergence are equalized: `r_{com}(γ^*) = r_{d-m}(γ^*)`, where `r_{com}(γ) = r^{com}(α_A(γ), α_B(γ))` and `r_{d-m}(γ) = \\max[r^{d-m}(α_A(γ)), r^{d-m}(α_B(γ))]`.\n\n### Question\n\nBased on the model, which of the following statements correctly describe the logic of communicative voting and the factors influencing its equilibrium level?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 13, "Question": "### Background\n\n**Research Question.** This problem develops a rational choice explanation for why voters cast ballots for third-party candidates with no chance of winning, providing a dynamic alternative to Duverger's Law.\n\n**Setting / Institutional Environment.** The setting is a two-period, three-candidate (A, B, C) election under plurality rule. A majority of voters (type-1) prefer A and B to C, but are split between A and B. A minority (type-2) supports C. In period 1, type-B voters (those who prefer B to A) face a choice: vote strategically for A to defeat C, or vote communicatively for their true preference B. A vote for B, while unable to win the current election, provides information to other type-1 voters about B's underlying support, which is valuable for coordinating in the period 2 election.\n\n### Data / Model Specification\n\nThe paper's model of communicative voting in a three-candidate race (Proposition 6) shows that it can be rational for voters to support a third candidate (e.g., B) even if that candidate has no chance of winning the current election. This occurs because the vote serves a communicative purpose for future elections. However, this can lead to an inefficient outcome in period 1 if candidate C (the least preferred candidate of the majority) wins because the majority splits its vote between A and B.\n\n### Question\n\nConsider the implications of this model for electoral system design. Which of the following statements accurately describe the model's findings or logical extensions regarding the efficiency of different voting systems?\n", "Answer": ["B", "C", "D"], "qid": "13", "question": "### Background\n\n**Research Question.** This problem develops a rational choice explanation for why voters cast ballots for third-party candidates with no chance of winning, providing a dynamic alternative to Duverger's Law.\n\n**Setting / Institutional Environment.** The setting is a two-period, three-candidate (A, B, C) election under plurality rule. A majority of voters (type-1) prefer A and B to C, but are split between A and B. A minority (type-2) supports C. In period 1, type-B voters (those who prefer B to A) face a choice: vote strategically for A to defeat C, or vote communicatively for their true preference B. A vote for B, while unable to win the current election, provides information to other type-1 voters about B's underlying support, which is valuable for coordinating in the period 2 election.\n\n### Data / Model Specification\n\nThe paper's model of communicative voting in a three-candidate race (Proposition 6) shows that it can be rational for voters to support a third candidate (e.g., B) even if that candidate has no chance of winning the current election. This occurs because the vote serves a communicative purpose for future elections. However, this can lead to an inefficient outcome in period 1 if candidate C (the least preferred candidate of the majority) wins because the majority splits its vote between A and B.\n\n### Question\n\nConsider the implications of this model for electoral system design. Which of the following statements accurately describe the model's findings or logical extensions regarding the efficiency of different voting systems?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 14, "Question": "### Background\n\n**Research Question.** This problem investigates the large-sample properties of the Conditional Mean Estimator (CME), establishing its asymptotic equivalence to the Maximum Likelihood Estimator (MLE) and its limiting distribution for inference.\n\n**Setting / Institutional Environment.** We are in a large-sample asymptotic framework where both the data sample size, $n$, and the number of MCMC draws, $m$, go to infinity. The goal is to understand the behavior of the CME, a Bayesian estimator, and relate it to its frequentist counterpart, the MLE.\n\n**Variables & Parameters.**\n- $\\theta_0$: The true, unknown $d$-dimensional parameter vector.\n- $\\widehat{\\theta}_n$: The Maximum Likelihood Estimator of $\\theta$.\n- $\\widehat{\\theta}_{mn}$: The Conditional Mean Estimator (CME) based on a sample of size $n$ and $m$ MCMC draws.\n- $\\Delta_n(\\widehat{\\theta}_n)$: The observed Fisher information, defined as $-l_n^{(2)}(\\widehat{\\theta}_n)$, where $l_n^{(2)}$ is the second derivative of the log-likelihood.\n- $\\pi(\\theta)$: The prior density for $\\theta$.\n- $\\pi^{(1)}(\\theta_0)$: The gradient (first derivative) of the prior density, evaluated at the true parameter $\\theta_0$.\n\n---\n\n### Data / Model Specification\n\nTheorem 2 of the paper provides the key asymptotic results for the CME, $\\widehat{\\theta}_{mn}$:\n\n  \n\\Delta_{n}^{1/2}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\widehat{\\theta}_{n}\\right) = o_{P}(1) \\quad \\text{(Eq. (1))}\n \n\nUnder stronger assumptions, this can be refined to:\n\n  \n\\varDelta_{n}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\widehat{\\theta}_{n}\\right) \\rightarrow_P \\frac{\\pi^{(1)}(\\theta_{0})}{\\pi\\left(\\theta_{0}\\right)} \\quad \\text{(Eq. (2))}\n \n\nThe limiting distribution of the CME, which allows for statistical inference, is given by:\n\n  \n\\varDelta_{n}^{1/2}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\theta_{0}\\right) \\rightarrow_{D} N(0,I_{d}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the provided asymptotic results, select all statements that are correct.", "Answer": ["C", "D"], "qid": "14", "question": "### Background\n\n**Research Question.** This problem investigates the large-sample properties of the Conditional Mean Estimator (CME), establishing its asymptotic equivalence to the Maximum Likelihood Estimator (MLE) and its limiting distribution for inference.\n\n**Setting / Institutional Environment.** We are in a large-sample asymptotic framework where both the data sample size, $n$, and the number of MCMC draws, $m$, go to infinity. The goal is to understand the behavior of the CME, a Bayesian estimator, and relate it to its frequentist counterpart, the MLE.\n\n**Variables & Parameters.**\n- $\\theta_0$: The true, unknown $d$-dimensional parameter vector.\n- $\\widehat{\\theta}_n$: The Maximum Likelihood Estimator of $\\theta$.\n- $\\widehat{\\theta}_{mn}$: The Conditional Mean Estimator (CME) based on a sample of size $n$ and $m$ MCMC draws.\n- $\\Delta_n(\\widehat{\\theta}_n)$: The observed Fisher information, defined as $-l_n^{(2)}(\\widehat{\\theta}_n)$, where $l_n^{(2)}$ is the second derivative of the log-likelihood.\n- $\\pi(\\theta)$: The prior density for $\\theta$.\n- $\\pi^{(1)}(\\theta_0)$: The gradient (first derivative) of the prior density, evaluated at the true parameter $\\theta_0$.\n\n---\n\n### Data / Model Specification\n\nTheorem 2 of the paper provides the key asymptotic results for the CME, $\\widehat{\\theta}_{mn}$:\n\n  \n\\Delta_{n}^{1/2}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\widehat{\\theta}_{n}\\right) = o_{P}(1) \\quad \\text{(Eq. (1))}\n \n\nUnder stronger assumptions, this can be refined to:\n\n  \n\\varDelta_{n}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\widehat{\\theta}_{n}\\right) \\rightarrow_P \\frac{\\pi^{(1)}(\\theta_{0})}{\\pi\\left(\\theta_{0}\\right)} \\quad \\text{(Eq. (2))}\n \n\nThe limiting distribution of the CME, which allows for statistical inference, is given by:\n\n  \n\\varDelta_{n}^{1/2}(\\widehat{\\theta}_{n})\\left(\\widehat{\\theta}_{m n}-\\theta_{0}\\right) \\rightarrow_{D} N(0,I_{d}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the provided asymptotic results, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 15, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical and practical foundations of Bayesian point estimation, starting from the definition of the posterior distribution and culminating in the challenge of statistical inference with its numerical approximation.\n\n**Setting / Institutional Environment.** In a standard Bayesian framework, after combining a prior belief $\\pi(\\theta)$ with data evidence contained in summary statistics $T_n$, all information about an unknown parameter vector $\\theta$ is summarized by the posterior distribution. A key task is to choose a single value (a point estimate) to represent this distribution. In practice, this estimate is often computed via simulation.\n\n**Variables & Parameters.**\n- $\\theta$: A $d$-dimensional vector of unknown parameters.\n- $T_n$: A vector of summary statistics from a data sample of size $n$.\n- $l_n(\\theta)$: The log-likelihood function.\n- $\\pi(\\theta)$: The prior probability density function of $\\theta$.\n- $m$: The number of draws from the posterior distribution used for numerical approximation.\n- $\\{\\theta_{jn}\\}_{j=1}^m$: A sequence of $m$ draws from the posterior density $f_n(\\theta|T_n)$, often from an MCMC algorithm.\n\n---\n\n### Data / Model Specification\n\nThe posterior density of $\\theta$ given $T_n$ is defined as:\n\n  \nf_{n}(\\theta|T_{n})=\\frac{e^{l_{n}(\\theta)}\\pi(\\theta)}{\\int e^{l_{n}(\\theta)}\\pi(\\theta)d\\theta} \\quad \\text{(Eq. (1))}\n \n\nThe posterior mean is the expectation of $\\theta$ with respect to this density:\n\n  \n\\mathbb{E}[\\theta|T_{n}]=\\int\\theta f_{n}(\\theta|T_{n})d\\theta \\quad \\text{(Eq. (2))}\n \n\nWhen Eq. (2) is intractable, it is approximated by the Conditional Mean Estimator (CME):\n\n  \n\\widehat{\\theta}_{m n}=\\frac{1}{m}\\sum_{j=1}^{m}\\theta_{j n} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nIn the context of Bayesian estimation described, select all statements that are INCORRECT.", "Answer": ["B", "D"], "qid": "15", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical and practical foundations of Bayesian point estimation, starting from the definition of the posterior distribution and culminating in the challenge of statistical inference with its numerical approximation.\n\n**Setting / Institutional Environment.** In a standard Bayesian framework, after combining a prior belief $\\pi(\\theta)$ with data evidence contained in summary statistics $T_n$, all information about an unknown parameter vector $\\theta$ is summarized by the posterior distribution. A key task is to choose a single value (a point estimate) to represent this distribution. In practice, this estimate is often computed via simulation.\n\n**Variables & Parameters.**\n- $\\theta$: A $d$-dimensional vector of unknown parameters.\n- $T_n$: A vector of summary statistics from a data sample of size $n$.\n- $l_n(\\theta)$: The log-likelihood function.\n- $\\pi(\\theta)$: The prior probability density function of $\\theta$.\n- $m$: The number of draws from the posterior distribution used for numerical approximation.\n- $\\{\\theta_{jn}\\}_{j=1}^m$: A sequence of $m$ draws from the posterior density $f_n(\\theta|T_n)$, often from an MCMC algorithm.\n\n---\n\n### Data / Model Specification\n\nThe posterior density of $\\theta$ given $T_n$ is defined as:\n\n  \nf_{n}(\\theta|T_{n})=\\frac{e^{l_{n}(\\theta)}\\pi(\\theta)}{\\int e^{l_{n}(\\theta)}\\pi(\\theta)d\\theta} \\quad \\text{(Eq. (1))}\n \n\nThe posterior mean is the expectation of $\\theta$ with respect to this density:\n\n  \n\\mathbb{E}[\\theta|T_{n}]=\\int\\theta f_{n}(\\theta|T_{n})d\\theta \\quad \\text{(Eq. (2))}\n \n\nWhen Eq. (2) is intractable, it is approximated by the Conditional Mean Estimator (CME):\n\n  \n\\widehat{\\theta}_{m n}=\\frac{1}{m}\\sum_{j=1}^{m}\\theta_{j n} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nIn the context of Bayesian estimation described, select all statements that are INCORRECT.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 16, "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** The equilibrium safe rate, `R_0*`, is determined by the market clearing condition, which equates the aggregate demand for investment to the total supply of savings `w`. A marginal entrepreneur type, `p*`, separates the non-monitoring (`p ≤ p*`) and monitoring (`p > p*`) sectors. The failure probability for all monitoring banks is equal to `p*`.\n\n### Question\n\nAccording to the model, which of the following effects occur in the new equilibrium after an exogenous increase in the aggregate supply of savings `w`? Select all that apply.", "Answer": ["B", "C"], "qid": "16", "question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** The equilibrium safe rate, `R_0*`, is determined by the market clearing condition, which equates the aggregate demand for investment to the total supply of savings `w`. A marginal entrepreneur type, `p*`, separates the non-monitoring (`p ≤ p*`) and monitoring (`p > p*`) sectors. The failure probability for all monitoring banks is equal to `p*`.\n\n### Question\n\nAccording to the model, which of the following effects occur in the new equilibrium after an exogenous increase in the aggregate supply of savings `w`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the core mec...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the core mec...", "error": null}}
{"ID": 17, "Question": "### Background\n\nAn analysis of Venezuelan manufacturing plants splits the sample into 'small' (≤ 49 employees) and 'large' (> 49 employees) to explore how the effects of direct foreign investment (DFI) differ by firm size. A 'within' (plant fixed effects) estimator is used to control for the possibility that foreign investors 'cherry-pick' already productive plants.\n\n### Data / Model Specification\n\nThe following table presents the key coefficients from the 'within' estimation, which are robust to plant-level selection bias.\n\n**Table 1: Impact of Foreign Ownership by Plant Size ('Within' Estimator Results)**\n\n| Variable | Small Plants (≤ 49 employees) | Large Plants (> 49 employees) |\n|:---|:---:|:---:|\n| `Plant_DFI` | 0.100 | -0.018 |\n| | (0.055) | (0.049) |\n| `Sector_DFI` | -0.340 | -0.214 |\n| | (0.074) | (0.111) |\n\n*Notes: Standard errors in parentheses. `Plant_DFI` measures the 'own-plant' effect of receiving foreign equity. `Sector_DFI` measures the 'spillover' effect from foreign presence in the sector on domestically-owned firms.*\n\n---\n\nBased on the results in Table 1, which of the following conclusions are supported by the evidence? (Select all that apply)", "Answer": ["A", "C", "D"], "qid": "17", "question": "### Background\n\nAn analysis of Venezuelan manufacturing plants splits the sample into 'small' (≤ 49 employees) and 'large' (> 49 employees) to explore how the effects of direct foreign investment (DFI) differ by firm size. A 'within' (plant fixed effects) estimator is used to control for the possibility that foreign investors 'cherry-pick' already productive plants.\n\n### Data / Model Specification\n\nThe following table presents the key coefficients from the 'within' estimation, which are robust to plant-level selection bias.\n\n**Table 1: Impact of Foreign Ownership by Plant Size ('Within' Estimator Results)**\n\n| Variable | Small Plants (≤ 49 employees) | Large Plants (> 49 employees) |\n|:---|:---:|:---:|\n| `Plant_DFI` | 0.100 | -0.018 |\n| | (0.055) | (0.049) |\n| `Sector_DFI` | -0.340 | -0.214 |\n| | (0.074) | (0.111) |\n\n*Notes: Standard errors in parentheses. `Plant_DFI` measures the 'own-plant' effect of receiving foreign equity. `Sector_DFI` measures the 'spillover' effect from foreign presence in the sector on domestically-owned firms.*\n\n---\n\nBased on the results in Table 1, which of the following conclusions are supported by the evidence? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of Venezuelan manufact...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of Venezuelan manufact...", "error": null}}
{"ID": 18, "Question": "### Background\n\nA government development agency wants to use a limited budget to subsidize a 100% foreign takeover of a single, currently domestic plant to maximize the net productivity of the *entire sector*. The sector contains one large domestic plant and ten small domestic plants.\n\n### Data / Model Specification\n\nThe agency uses the 'within' coefficients from a study on DFI, which are robust to selection bias.\n\n**Table 1: Causal Effects of DFI by Plant Size ('Within' Estimator)**\n\n| Effect | Small Plants (≤ 49 employees) | Large Plants (> 49 employees) |\n|:---|:---:|:---:|\n| Own-Plant Effect (`Plant_DFI`) | 0.100 | -0.018 |\n| Spillover Effect (`Sector_DFI`) | -0.340 | -0.214 |\n\n*Assumption: All plants have equal employment shares, so the takeover of any single plant increases the sector-level DFI (`Sector_DFI`) by 1/11.*\n\n---\n\nAn analyst calculates the net change in total sectoral log output for two policy choices: (1) subsidizing the takeover of one small plant, and (2) subsidizing the takeover of the large plant. Which of the following statements about this policy scenario are correct? (Select all that apply)", "Answer": ["B", "C", "D"], "qid": "18", "question": "### Background\n\nA government development agency wants to use a limited budget to subsidize a 100% foreign takeover of a single, currently domestic plant to maximize the net productivity of the *entire sector*. The sector contains one large domestic plant and ten small domestic plants.\n\n### Data / Model Specification\n\nThe agency uses the 'within' coefficients from a study on DFI, which are robust to selection bias.\n\n**Table 1: Causal Effects of DFI by Plant Size ('Within' Estimator)**\n\n| Effect | Small Plants (≤ 49 employees) | Large Plants (> 49 employees) |\n|:---|:---:|:---:|\n| Own-Plant Effect (`Plant_DFI`) | 0.100 | -0.018 |\n| Spillover Effect (`Sector_DFI`) | -0.340 | -0.214 |\n\n*Assumption: All plants have equal employment shares, so the takeover of any single plant increases the sector-level DFI (`Sector_DFI`) by 1/11.*\n\n---\n\nAn analyst calculates the net change in total sectoral log output for two policy choices: (1) subsidizing the takeover of one small plant, and (2) subsidizing the takeover of the large plant. Which of the following statements about this policy scenario are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA government development agency wa...", "model_answer": "This is a detailed response to the question about ### Background\n\nA government development agency wa...", "error": null}}
{"ID": 19, "Question": "### Background\n\nAn econometrician is studying technology spillovers from direct foreign investment (DFI). A key challenge is that foreign investors may be attracted to inherently more productive industries. This selection process can bias the estimated spillover effect.\n\n### Data / Model Specification\n\nThe true data generating process for a domestic plant's log output (`Y`) is:\n\n  \nY_{ijt} = \\beta_0 + \\beta_2 Sector\\_DFI_{jt} + \\alpha_j + \\varepsilon_{ijt}\n \n\nwhere `Sector_DFI` is the foreign presence in sector `j`, and `αⱼ` is an unobserved, time-invariant component of productivity specific to industry `j`. A higher `αⱼ` means a more productive industry.\n\nTwo models are estimated:\n1.  **Model with Dummies:** Includes industry fixed effects, providing an unbiased estimate of the true spillover effect, `β₂`.\n2.  **Model without Dummies:** Omits industry fixed effects, mimicking earlier cross-section studies. It yields a potentially biased estimate, `b₂`.\n\nThe results are:\n\n| Variable | Model with Dummies (Unbiased) | Model without Dummies (Biased) |\n|:---|:---:|:---:|\n| `Sector_DFI` | -0.267 | 0.058 |\n\n---\n\nGiven the theoretical framework and the empirical results, which of the following statements are correct? (Select all that apply)", "Answer": ["B", "C", "D"], "qid": "19", "question": "### Background\n\nAn econometrician is studying technology spillovers from direct foreign investment (DFI). A key challenge is that foreign investors may be attracted to inherently more productive industries. This selection process can bias the estimated spillover effect.\n\n### Data / Model Specification\n\nThe true data generating process for a domestic plant's log output (`Y`) is:\n\n  \nY_{ijt} = \\beta_0 + \\beta_2 Sector\\_DFI_{jt} + \\alpha_j + \\varepsilon_{ijt}\n \n\nwhere `Sector_DFI` is the foreign presence in sector `j`, and `αⱼ` is an unobserved, time-invariant component of productivity specific to industry `j`. A higher `αⱼ` means a more productive industry.\n\nTwo models are estimated:\n1.  **Model with Dummies:** Includes industry fixed effects, providing an unbiased estimate of the true spillover effect, `β₂`.\n2.  **Model without Dummies:** Omits industry fixed effects, mimicking earlier cross-section studies. It yields a potentially biased estimate, `b₂`.\n\nThe results are:\n\n| Variable | Model with Dummies (Unbiased) | Model without Dummies (Biased) |\n|:---|:---:|:---:|\n| `Sector_DFI` | -0.267 | 0.058 |\n\n---\n\nGiven the theoretical framework and the empirical results, which of the following statements are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn econometrician is studying tech...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn econometrician is studying tech...", "error": null}}
{"ID": 20, "Question": "### Background\n\nA researcher models the log output (`Y`) of a plant as a function of its own foreign equity share (`DFI_Plant`) and the foreign equity share in its sector (`DFI_Sector`). The goal is to distinguish the direct effects of receiving investment from the spillover effects of competition.\n\n### Data / Model Specification\n\nThe estimated log-linear production function is:\n\n  \nY_{ijt} = C + \\beta_{1} DFI\\_Plant_{ijt} + \\beta_{2} DFI\\_Sector_{jt} + \\beta_{3} (DFI\\_Plant_{ijt} \\cdot DFI\\_Sector_{jt}) + ...\n \n\n*Eq. (1)*\n\n---\n\nBased on the structure of Eq. (1), which of the following interpretations or derivations are **INCORRECT**? (Select all that apply)", "Answer": ["B", "D"], "qid": "20", "question": "### Background\n\nA researcher models the log output (`Y`) of a plant as a function of its own foreign equity share (`DFI_Plant`) and the foreign equity share in its sector (`DFI_Sector`). The goal is to distinguish the direct effects of receiving investment from the spillover effects of competition.\n\n### Data / Model Specification\n\nThe estimated log-linear production function is:\n\n  \nY_{ijt} = C + \\beta_{1} DFI\\_Plant_{ijt} + \\beta_{2} DFI\\_Sector_{jt} + \\beta_{3} (DFI\\_Plant_{ijt} \\cdot DFI\\_Sector_{jt}) + ...\n \n\n*Eq. (1)*\n\n---\n\nBased on the structure of Eq. (1), which of the following interpretations or derivations are **INCORRECT**? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA researcher models the log output...", "model_answer": "This is a detailed response to the question about ### Background\n\nA researcher models the log output...", "error": null}}
{"ID": 21, "Question": "### Background\n\n**Research Question.** This problem develops the theoretical foundation for the paper's central hypothesis: that community college enrollment is countercyclical. It uses a lifetime income maximization framework to model educational choice and explores how this choice is affected by economic recessions.\n\n**Setting / Institutional Environment.** An 18-year-old high school graduate `i` with innate ability `a_i` chooses one of three educational paths (`ED`) to maximize the expected present value (`PV`) of lifetime income. Higher ability is assumed to yield a higher economic return to education.\n\n**Variables & Parameters.**\n- `ED`: Educational path chosen. `ED=12` (work immediately), `ED=14` (2-year community college), `ED=16` (4-year college).\n- `PV_{ED}`: Expected present value of lifetime income for path `ED`.\n- `a_i`: Innate ability of individual `i`.\n\n---\n\n### Data / Model Specification\n\nAn individual `i` chooses `ED ∈ {12, 14, 16}` to maximize `PV_{ED}`. The choice between working immediately (`ED=12`) and attending community college (`ED=14`) depends on a comparison of their present values. The model implies that there is a threshold ability level, `a*`, such that individuals with `a_i > a*` choose to enroll in community college.\n\n---\n\n### Question\n\nAccording to the lifetime income maximization model presented, which of the following statements correctly describe the primary mechanism through which an economic recession increases the demand for community college enrollment? Select all that apply.", "Answer": ["A", "B"], "qid": "21", "question": "### Background\n\n**Research Question.** This problem develops the theoretical foundation for the paper's central hypothesis: that community college enrollment is countercyclical. It uses a lifetime income maximization framework to model educational choice and explores how this choice is affected by economic recessions.\n\n**Setting / Institutional Environment.** An 18-year-old high school graduate `i` with innate ability `a_i` chooses one of three educational paths (`ED`) to maximize the expected present value (`PV`) of lifetime income. Higher ability is assumed to yield a higher economic return to education.\n\n**Variables & Parameters.**\n- `ED`: Educational path chosen. `ED=12` (work immediately), `ED=14` (2-year community college), `ED=16` (4-year college).\n- `PV_{ED}`: Expected present value of lifetime income for path `ED`.\n- `a_i`: Innate ability of individual `i`.\n\n---\n\n### Data / Model Specification\n\nAn individual `i` chooses `ED ∈ {12, 14, 16}` to maximize `PV_{ED}`. The choice between working immediately (`ED=12`) and attending community college (`ED=14`) depends on a comparison of their present values. The model implies that there is a threshold ability level, `a*`, such that individuals with `a_i > a*` choose to enroll in community college.\n\n---\n\n### Question\n\nAccording to the lifetime income maximization model presented, which of the following statements correctly describe the primary mechanism through which an economic recession increases the demand for community college enrollment? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 22, "Question": "### Background\n\n**Research Question.** This problem develops the theoretical foundation for the paper's central hypothesis: that community college enrollment is countercyclical. It uses a lifetime income maximization framework to model educational choice and explores how this choice is affected by economic recessions and institutional supply constraints.\n\n**Setting / Institutional Environment.** An 18-year-old high school graduate `i` with innate ability `a_i` chooses one of three educational paths (`ED`) to maximize the expected present value (`PV`) of lifetime income. Higher ability is assumed to yield a higher economic return to education.\n\n**Variables & Parameters.**\n- `ED`: Educational path chosen. `ED=12` (work immediately), `ED=14` (2-year community college), `ED=16` (4-year college).\n- `PV_{ED}`: Expected present value of lifetime income for path `ED`.\n- `a_i`: Innate ability of individual `i`.\n\n---\n\n### Data / Model Specification\n\nAn individual `i` chooses `ED ∈ {12, 14, 16}` to maximize `PV_{ED}`. The choice between working immediately (`ED=12`) and attending community college (`ED=14`) depends on a comparison of their present values. The model implies that there is a threshold ability level, `a*`, such that individuals with `a_i > a*` choose to enroll in community college. During a recession, `a*` falls, increasing demand.\n\n---\n\n### Question\n\nNow, consider the policy context discussed in the paper: during a recession, demand for community college surges, but state funding cuts impose a binding enrollment cap, `N_max`, which is less than the number of potential applicants. Assuming the college admits the highest-ability applicants, which of the following are correct statements about the resulting market equilibrium? Select all that apply.", "Answer": ["A", "C"], "qid": "22", "question": "### Background\n\n**Research Question.** This problem develops the theoretical foundation for the paper's central hypothesis: that community college enrollment is countercyclical. It uses a lifetime income maximization framework to model educational choice and explores how this choice is affected by economic recessions and institutional supply constraints.\n\n**Setting / Institutional Environment.** An 18-year-old high school graduate `i` with innate ability `a_i` chooses one of three educational paths (`ED`) to maximize the expected present value (`PV`) of lifetime income. Higher ability is assumed to yield a higher economic return to education.\n\n**Variables & Parameters.**\n- `ED`: Educational path chosen. `ED=12` (work immediately), `ED=14` (2-year community college), `ED=16` (4-year college).\n- `PV_{ED}`: Expected present value of lifetime income for path `ED`.\n- `a_i`: Innate ability of individual `i`.\n\n---\n\n### Data / Model Specification\n\nAn individual `i` chooses `ED ∈ {12, 14, 16}` to maximize `PV_{ED}`. The choice between working immediately (`ED=12`) and attending community college (`ED=14`) depends on a comparison of their present values. The model implies that there is a threshold ability level, `a*`, such that individuals with `a_i > a*` choose to enroll in community college. During a recession, `a*` falls, increasing demand.\n\n---\n\n### Question\n\nNow, consider the policy context discussed in the paper: during a recession, demand for community college surges, but state funding cuts impose a binding enrollment cap, `N_max`, which is less than the number of potential applicants. Assuming the college admits the highest-ability applicants, which of the following are correct statements about the resulting market equilibrium? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 23, "Question": "### Background\n\nA monopolist produces a good that can be stored as inventory. The firm's technology is characterized by the cost of adjusting production (via a convex production cost function) and the cost of adjusting inventories (via a convex inventory carrying cost function). The firm faces demand shocks with varying degrees of persistence.\n\n### Data / Model Specification\n\nThe firm's optimal responses of production (`y_0`) and price (`p_0`) to an expected demand shock (`ε_0`) are functions of the response of the shadow value of inventories, `λ_0`.\n\n  \n\\frac{\\partial y_{0}}{\\partial\\varepsilon_{0}}=c\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} \n\\quad \\text{(Eq. (1))}\n \n  \n\\frac{\\partial p_{0}}{\\partial\\varepsilon_{0}}=\\frac{1}{2}\\left(1+\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}}\\right) \n\\quad \\text{(Eq. (2))}\n \n\nThe key to the model's comparative statics is the expression for the response of the shadow value of inventories:\n\n  \n\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} = \\left(\\frac{d}{c+d}\\right)\\left(\\frac{z_{2}-(1+r)}{z_{2}-\\rho}\\right) \n\\quad \\text{(Eq. (3))}\n \n\nWhere:\n- `c` is a parameter of the quadratic production cost function. A larger `c` implies a flatter marginal cost curve.\n- `d` is a parameter from the linear demand curve.\n- `ρ` is the persistence of demand shocks (`0 ≤ ρ ≤ 1`). A lower `ρ` means shocks are more transitory.\n- `r` is the interest rate.\n- `z_2` is the unstable root of the model's characteristic equation. It is an increasing function of `b`, the convexity of the inventory cost function. A lower `b` means output is more \"inventoriable\".\n\n### Question\n\nAccording to the model, which of the following statements correctly describe the conditions that lead to \"sticky\" prices and production (i.e., smaller responses to demand shocks)? Select all that apply.", "Answer": ["B", "C"], "qid": "23", "question": "### Background\n\nA monopolist produces a good that can be stored as inventory. The firm's technology is characterized by the cost of adjusting production (via a convex production cost function) and the cost of adjusting inventories (via a convex inventory carrying cost function). The firm faces demand shocks with varying degrees of persistence.\n\n### Data / Model Specification\n\nThe firm's optimal responses of production (`y_0`) and price (`p_0`) to an expected demand shock (`ε_0`) are functions of the response of the shadow value of inventories, `λ_0`.\n\n  \n\\frac{\\partial y_{0}}{\\partial\\varepsilon_{0}}=c\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} \n\\quad \\text{(Eq. (1))}\n \n  \n\\frac{\\partial p_{0}}{\\partial\\varepsilon_{0}}=\\frac{1}{2}\\left(1+\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}}\\right) \n\\quad \\text{(Eq. (2))}\n \n\nThe key to the model's comparative statics is the expression for the response of the shadow value of inventories:\n\n  \n\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} = \\left(\\frac{d}{c+d}\\right)\\left(\\frac{z_{2}-(1+r)}{z_{2}-\\rho}\\right) \n\\quad \\text{(Eq. (3))}\n \n\nWhere:\n- `c` is a parameter of the quadratic production cost function. A larger `c` implies a flatter marginal cost curve.\n- `d` is a parameter from the linear demand curve.\n- `ρ` is the persistence of demand shocks (`0 ≤ ρ ≤ 1`). A lower `ρ` means shocks are more transitory.\n- `r` is the interest rate.\n- `z_2` is the unstable root of the model's characteristic equation. It is an increasing function of `b`, the convexity of the inventory cost function. A lower `b` means output is more \"inventoriable\".\n\n### Question\n\nAccording to the model, which of the following statements correctly describe the conditions that lead to \"sticky\" prices and production (i.e., smaller responses to demand shocks)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA monopolist produces a good that ...", "model_answer": "This is a detailed response to the question about ### Background\n\nA monopolist produces a good that ...", "error": null}}
{"ID": 24, "Question": "### Background\n\nA monopolist produces a good that can be stored as inventory. The firm's technology is characterized by the cost of adjusting production (via a convex production cost function) and the cost of adjusting inventories (via a convex inventory carrying cost function). The firm faces demand shocks with varying degrees of persistence.\n\n### Data / Model Specification\n\nThe firm's optimal responses of production (`y_0`) and price (`p_0`) to an expected demand shock (`ε_0`) are functions of the response of the shadow value of inventories, `λ_0`.\n\n  \n\\frac{\\partial y_{0}}{\\partial\\varepsilon_{0}}=c\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} \n\\quad \\text{(Eq. (1))}\n \n  \n\\frac{\\partial p_{0}}{\\partial\\varepsilon_{0}}=\\frac{1}{2}\\left(1+\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}}\\right) \n\\quad \\text{(Eq. (2))}\n \n\nThe key to the model's comparative statics is the expression for the response of the shadow value of inventories:\n\n  \n\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} = \\left(\\frac{d}{c+d}\\right)\\left(\\frac{z_{2}-(1+r)}{z_{2}-\\rho}\\right) \n\\quad \\text{(Eq. (3))}\n \n\nWhere:\n- `c` and `d` are positive parameters from the cost and demand functions.\n- `ρ` is the persistence of demand shocks (`0 ≤ ρ ≤ 1`).\n- `r` is the interest rate.\n- `z_2` is the unstable root of the model's characteristic equation, and it is known that `z_2 > 1+r`.\n\n### Question\n\nConsider two industries, Durable Goods (low inventory cost convexity, `b_D`) and Perishable Goods (high inventory cost convexity, `b_P`). An economy-wide, temporary positive demand shock (`ε_0 > 0`) occurs. Based on the model, which of the following outcomes are expected? Select all that apply.", "Answer": ["B", "D"], "qid": "24", "question": "### Background\n\nA monopolist produces a good that can be stored as inventory. The firm's technology is characterized by the cost of adjusting production (via a convex production cost function) and the cost of adjusting inventories (via a convex inventory carrying cost function). The firm faces demand shocks with varying degrees of persistence.\n\n### Data / Model Specification\n\nThe firm's optimal responses of production (`y_0`) and price (`p_0`) to an expected demand shock (`ε_0`) are functions of the response of the shadow value of inventories, `λ_0`.\n\n  \n\\frac{\\partial y_{0}}{\\partial\\varepsilon_{0}}=c\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} \n\\quad \\text{(Eq. (1))}\n \n  \n\\frac{\\partial p_{0}}{\\partial\\varepsilon_{0}}=\\frac{1}{2}\\left(1+\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}}\\right) \n\\quad \\text{(Eq. (2))}\n \n\nThe key to the model's comparative statics is the expression for the response of the shadow value of inventories:\n\n  \n\\frac{\\partial\\lambda_{0}}{\\partial\\varepsilon_{0}} = \\left(\\frac{d}{c+d}\\right)\\left(\\frac{z_{2}-(1+r)}{z_{2}-\\rho}\\right) \n\\quad \\text{(Eq. (3))}\n \n\nWhere:\n- `c` and `d` are positive parameters from the cost and demand functions.\n- `ρ` is the persistence of demand shocks (`0 ≤ ρ ≤ 1`).\n- `r` is the interest rate.\n- `z_2` is the unstable root of the model's characteristic equation, and it is known that `z_2 > 1+r`.\n\n### Question\n\nConsider two industries, Durable Goods (low inventory cost convexity, `b_D`) and Perishable Goods (high inventory cost convexity, `b_P`). An economy-wide, temporary positive demand shock (`ε_0 > 0`) occurs. Based on the model, which of the following outcomes are expected? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA monopolist produces a good that ...", "model_answer": "This is a detailed response to the question about ### Background\n\nA monopolist produces a good that ...", "error": null}}
{"ID": 25, "Question": "### Background\n\nThe firm's optimal plan is characterized by a system of linear difference equations. The solution requires finding a unique, non-explosive path for the state variables. This is achieved by imposing a stability condition that restricts the solution to the \"stable arm\" of the system's saddle-point dynamics.\n\n### Data / Model Specification\n\nThe solution for the initial shadow value of inventories, `λ_0`, is a function of the initial inventory disequilibrium (`n-bar - n_0`) and the initial expected demand shock (`ε_0`):\n\n  \n\\lambda_{0}=\\left(\\frac{1-z_{1}}{c+d}\\right)\\left[\\bar{n}-n_{0}+\\frac{d}{1-\\theta\\rho}\\varepsilon_{0}\\right] \n\\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `λ_0`: Deviation of the shadow value of inventories from its steady state.\n- `n_0`: Initial deviation of inventories from the long-run steady state `n-bar`.\n- `ε_0`: Initial expected demand shock.\n- `z_1`: The stable root (`0 < z_1 < 1`) of the system's characteristic equation.\n- `b`: Convexity of the inventory cost function. As `b` approaches 0, `z_1` approaches 1.\n\n### Question\n\nBased on the solution for the shadow value of inventories (`λ_0`) in Eq. (1), which of the following statements are valid interpretations or implications of the model? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "25", "question": "### Background\n\nThe firm's optimal plan is characterized by a system of linear difference equations. The solution requires finding a unique, non-explosive path for the state variables. This is achieved by imposing a stability condition that restricts the solution to the \"stable arm\" of the system's saddle-point dynamics.\n\n### Data / Model Specification\n\nThe solution for the initial shadow value of inventories, `λ_0`, is a function of the initial inventory disequilibrium (`n-bar - n_0`) and the initial expected demand shock (`ε_0`):\n\n  \n\\lambda_{0}=\\left(\\frac{1-z_{1}}{c+d}\\right)\\left[\\bar{n}-n_{0}+\\frac{d}{1-\\theta\\rho}\\varepsilon_{0}\\right] \n\\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `λ_0`: Deviation of the shadow value of inventories from its steady state.\n- `n_0`: Initial deviation of inventories from the long-run steady state `n-bar`.\n- `ε_0`: Initial expected demand shock.\n- `z_1`: The stable root (`0 < z_1 < 1`) of the system's characteristic equation.\n- `b`: Convexity of the inventory cost function. As `b` approaches 0, `z_1` approaches 1.\n\n### Question\n\nBased on the solution for the shadow value of inventories (`λ_0`) in Eq. (1), which of the following statements are valid interpretations or implications of the model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe firm's optimal plan is charact...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe firm's optimal plan is charact...", "error": null}}
{"ID": 26, "Question": "### Background\n\n**Research Question.** This problem analyzes the economic efficiency of policies for long-term care, focusing on the distortion created by subsidizing formal care while ignoring the opportunity cost of informal care.\n\n**Setting / Institutional Environment.** A family must provide long-term care for a parent with disabilities. This care can be sourced from the formal market (e.g., hiring a nurse) or provided informally by an adult child. The adult child faces an opportunity cost, in the form of forgone wages, if they choose to provide informal care. This paper's empirical findings show that this opportunity cost is large and causally linked to caregiving.\n\n**Variables & Parameters.**\n*   `P_F`: The market price per hour of formal care.\n*   `w_i`: The hourly wage (opportunity cost) of the adult child `i`.\n*   `s`: The fraction of formal care costs subsidized by the government, `0 ≤ s < 1`.\n\n---\n\n### Data / Model Specification\n\nAssume a family needs to provide one unit of care, which can be met by one hour of formal care or one hour of informal care. A cost-minimizing family will choose the cheaper option. An efficient allocation of resources requires that care be provided by the source with the lower social cost.\n\nSuppose the market price of formal care (`P_F`) is $20 per hour and the government introduces a policy that subsidizes 25% of this cost (`s = 0.25`). Based on the model, select all of the following statements that are correct.", "Answer": ["A", "C"], "qid": "26", "question": "### Background\n\n**Research Question.** This problem analyzes the economic efficiency of policies for long-term care, focusing on the distortion created by subsidizing formal care while ignoring the opportunity cost of informal care.\n\n**Setting / Institutional Environment.** A family must provide long-term care for a parent with disabilities. This care can be sourced from the formal market (e.g., hiring a nurse) or provided informally by an adult child. The adult child faces an opportunity cost, in the form of forgone wages, if they choose to provide informal care. This paper's empirical findings show that this opportunity cost is large and causally linked to caregiving.\n\n**Variables & Parameters.**\n*   `P_F`: The market price per hour of formal care.\n*   `w_i`: The hourly wage (opportunity cost) of the adult child `i`.\n*   `s`: The fraction of formal care costs subsidized by the government, `0 ≤ s < 1`.\n\n---\n\n### Data / Model Specification\n\nAssume a family needs to provide one unit of care, which can be met by one hour of formal care or one hour of informal care. A cost-minimizing family will choose the cheaper option. An efficient allocation of resources requires that care be provided by the source with the lower social cost.\n\nSuppose the market price of formal care (`P_F`) is $20 per hour and the government introduces a policy that subsidizes 25% of this cost (`s = 0.25`). Based on the model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 27, "Question": "### Background\n\n**Research Question.** This problem analyzes the economic efficiency of policies for long-term care, focusing on the distortion created by subsidizing formal care while ignoring the opportunity cost of informal care.\n\n**Setting / Institutional Environment.** A family must provide long-term care for a parent with disabilities. This care can be sourced from the formal market (e.g., hiring a nurse) or provided informally by an adult child. The adult child faces an opportunity cost, in the form of forgone wages, if they choose to provide informal care. This paper's empirical findings show that this opportunity cost is large and causally linked to caregiving.\n\n**Variables & Parameters.**\n*   `P_F`: The market price per hour of formal care.\n*   `w_i`: The hourly wage (opportunity cost) of the adult child `i`.\n*   `s`: The fraction of formal care costs subsidized by the government, `0 ≤ s < 1`.\n\n---\n\n### Data / Model Specification\n\nAssume a family needs to provide one unit of care, which can be met by one hour of formal care or one hour of informal care. A cost-minimizing family will choose the cheaper option. An efficient allocation of resources requires that care be provided by the source with the lower social cost.\n\nThe paper's author discusses alternative policies to the formal care subsidy. Select all statements below that correctly describe these alternatives and their associated challenges.", "Answer": ["C", "D"], "qid": "27", "question": "### Background\n\n**Research Question.** This problem analyzes the economic efficiency of policies for long-term care, focusing on the distortion created by subsidizing formal care while ignoring the opportunity cost of informal care.\n\n**Setting / Institutional Environment.** A family must provide long-term care for a parent with disabilities. This care can be sourced from the formal market (e.g., hiring a nurse) or provided informally by an adult child. The adult child faces an opportunity cost, in the form of forgone wages, if they choose to provide informal care. This paper's empirical findings show that this opportunity cost is large and causally linked to caregiving.\n\n**Variables & Parameters.**\n*   `P_F`: The market price per hour of formal care.\n*   `w_i`: The hourly wage (opportunity cost) of the adult child `i`.\n*   `s`: The fraction of formal care costs subsidized by the government, `0 ≤ s < 1`.\n\n---\n\n### Data / Model Specification\n\nAssume a family needs to provide one unit of care, which can be met by one hour of formal care or one hour of informal care. A cost-minimizing family will choose the cheaper option. An efficient allocation of resources requires that care be provided by the source with the lower social cost.\n\nThe paper's author discusses alternative policies to the formal care subsidy. Select all statements below that correctly describe these alternatives and their associated challenges.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 28, "Question": "### Background\n\n**Research Question.** This problem addresses the econometric challenges of modeling a limited dependent variable, specifically weekly work hours, and deriving marginal effects from a non-linear model.\n\n**Setting / Institutional Environment.** The data is a cross-sectional survey of individuals. A significant fraction of the sample reports zero weekly work hours. This feature of the data requires a modeling approach that can accommodate a mass point at zero and distinguish between the decision to work at all and the decision of how many hours to work.\n\n**Variables & Parameters.**\n*   `H_i`: Observed weekly hours worked by individual `i`, `H_i ≥ 0`.\n*   `D_i`: An indicator variable such that `D_i = 1` if `H_i > 0` and `D_i = 0` if `H_i = 0`.\n*   `X_i`: A vector of explanatory variables.\n*   `α`, `β`: Parameter vectors for the first and second parts of the model.\n\n---\n\n### Data / Model Specification\n\nThe study employs a two-part model to estimate the determinants of work hours (`H_i`):\n\n1.  **Part 1 (Participation Decision):** A probit model for the binary outcome of working positive hours.\n      \n    P(D_i = 1 | X_i) = \\Phi(X_i'\\alpha) \\quad \\text{(Eq. (1))}\n     \n    where `Φ(·)` is the standard normal CDF.\n\n2.  **Part 2 (Hours Decision):** A linear regression model for the level of hours, conditional on working.\n      \n    E[H_i | D_i = 1, X_i] = X_i'\\beta \\quad \\text{(Eq. (2))}\n     \n\nThe unconditional expected value of hours is `E[H_i | X_i] = \\Phi(X_i'\\alpha) \\cdot (X_i'\\beta)`. The marginal effect of a continuous variable `X_{ik}` is `∂E[H_i | X_i] / ∂X_{ik} = \\phi(X_i'\\alpha) \\alpha_k \\cdot (X_i'\\beta) + \\Phi(X_i'\\alpha) \\cdot \\beta_k`.\n\nSuppose for a given individual, the model yields the following values: `X_i'α = 0.847`, `X_i'β = 40`, `α_k = 0.5`, and `β_k = -2`. You are given that `Φ(0.847) ≈ 0.80` and `φ(0.847) ≈ 0.28`. Select all correct statements about the marginal effect of the variable `X_{ik}` on this individual's expected weekly hours.", "Answer": ["A", "C"], "qid": "28", "question": "### Background\n\n**Research Question.** This problem addresses the econometric challenges of modeling a limited dependent variable, specifically weekly work hours, and deriving marginal effects from a non-linear model.\n\n**Setting / Institutional Environment.** The data is a cross-sectional survey of individuals. A significant fraction of the sample reports zero weekly work hours. This feature of the data requires a modeling approach that can accommodate a mass point at zero and distinguish between the decision to work at all and the decision of how many hours to work.\n\n**Variables & Parameters.**\n*   `H_i`: Observed weekly hours worked by individual `i`, `H_i ≥ 0`.\n*   `D_i`: An indicator variable such that `D_i = 1` if `H_i > 0` and `D_i = 0` if `H_i = 0`.\n*   `X_i`: A vector of explanatory variables.\n*   `α`, `β`: Parameter vectors for the first and second parts of the model.\n\n---\n\n### Data / Model Specification\n\nThe study employs a two-part model to estimate the determinants of work hours (`H_i`):\n\n1.  **Part 1 (Participation Decision):** A probit model for the binary outcome of working positive hours.\n      \n    P(D_i = 1 | X_i) = \\Phi(X_i'\\alpha) \\quad \\text{(Eq. (1))}\n     \n    where `Φ(·)` is the standard normal CDF.\n\n2.  **Part 2 (Hours Decision):** A linear regression model for the level of hours, conditional on working.\n      \n    E[H_i | D_i = 1, X_i] = X_i'\\beta \\quad \\text{(Eq. (2))}\n     \n\nThe unconditional expected value of hours is `E[H_i | X_i] = \\Phi(X_i'\\alpha) \\cdot (X_i'\\beta)`. The marginal effect of a continuous variable `X_{ik}` is `∂E[H_i | X_i] / ∂X_{ik} = \\phi(X_i'\\alpha) \\alpha_k \\cdot (X_i'\\beta) + \\Phi(X_i'\\alpha) \\cdot \\beta_k`.\n\nSuppose for a given individual, the model yields the following values: `X_i'α = 0.847`, `X_i'β = 40`, `α_k = 0.5`, and `β_k = -2`. You are given that `Φ(0.847) ≈ 0.80` and `φ(0.847) ≈ 0.28`. Select all correct statements about the marginal effect of the variable `X_{ik}` on this individual's expected weekly hours.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 29, "Question": "### Background\n\n**Research Question.** This problem addresses the econometric challenges of modeling a limited dependent variable, specifically weekly work hours, and deriving marginal effects from a non-linear model.\n\n**Setting / Institutional Environment.** The data is a cross-sectional survey of individuals. A significant fraction of the sample reports zero weekly work hours. This feature of the data requires a modeling approach that can accommodate a mass point at zero and distinguish between the decision to work at all and the decision of how many hours to work.\n\n**Variables & Parameters.**\n*   `H_i`: Observed weekly hours worked by individual `i`, `H_i ≥ 0`.\n*   `D_i`: An indicator variable such that `D_i = 1` if `H_i > 0` and `D_i = 0` if `H_i = 0`.\n*   `X_i`: A vector of explanatory variables.\n*   `α`, `β`: Parameter vectors for the first and second parts of the model.\n\n---\n\n### Data / Model Specification\n\nThe study employs a two-part model to estimate the determinants of work hours (`H_i`):\n\n1.  **Part 1 (Participation Decision):** A probit model for the binary outcome of working positive hours.\n      \n    P(D_i = 1 | X_i) = \\Phi(X_i'\\alpha) \\quad \\text{(Eq. (1))}\n     \n    where `Φ(·)` is the standard normal CDF.\n\n2.  **Part 2 (Hours Decision):** A linear regression model for the level of hours, conditional on working.\n      \n    E[H_i | D_i = 1, X_i] = X_i'\\beta \\quad \\text{(Eq. (2))}\n     \n\nAn analyst is choosing between a Tobit and a two-part model for weekly work hours. The Tobit model imposes the restriction that a variable must affect the participation decision and the hours decision in the same direction. Select all empirical findings below that would provide strong evidence *against* this restriction, thus favoring the more flexible two-part model.", "Answer": ["B", "C"], "qid": "29", "question": "### Background\n\n**Research Question.** This problem addresses the econometric challenges of modeling a limited dependent variable, specifically weekly work hours, and deriving marginal effects from a non-linear model.\n\n**Setting / Institutional Environment.** The data is a cross-sectional survey of individuals. A significant fraction of the sample reports zero weekly work hours. This feature of the data requires a modeling approach that can accommodate a mass point at zero and distinguish between the decision to work at all and the decision of how many hours to work.\n\n**Variables & Parameters.**\n*   `H_i`: Observed weekly hours worked by individual `i`, `H_i ≥ 0`.\n*   `D_i`: An indicator variable such that `D_i = 1` if `H_i > 0` and `D_i = 0` if `H_i = 0`.\n*   `X_i`: A vector of explanatory variables.\n*   `α`, `β`: Parameter vectors for the first and second parts of the model.\n\n---\n\n### Data / Model Specification\n\nThe study employs a two-part model to estimate the determinants of work hours (`H_i`):\n\n1.  **Part 1 (Participation Decision):** A probit model for the binary outcome of working positive hours.\n      \n    P(D_i = 1 | X_i) = \\Phi(X_i'\\alpha) \\quad \\text{(Eq. (1))}\n     \n    where `Φ(·)` is the standard normal CDF.\n\n2.  **Part 2 (Hours Decision):** A linear regression model for the level of hours, conditional on working.\n      \n    E[H_i | D_i = 1, X_i] = X_i'\\beta \\quad \\text{(Eq. (2))}\n     \n\nAn analyst is choosing between a Tobit and a two-part model for weekly work hours. The Tobit model imposes the restriction that a variable must affect the participation decision and the hours decision in the same direction. Select all empirical findings below that would provide strong evidence *against* this restriction, thus favoring the more flexible two-part model.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 30, "Question": "### Background\n\nThe paper's main empirical model is a quantile regression where the key explanatory variables (permanent income and income risk) are themselves estimates generated from a first-stage regression on panel data.\n\nThe authors state that the standard errors of the parameter estimates are calculated using bootstrapping techniques.\n\n### Question\n\nIn this specific context, which of the following statistical problems make the use of bootstrapping essential for valid inference? Select all that apply.", "Answer": ["A", "B"], "qid": "30", "question": "### Background\n\nThe paper's main empirical model is a quantile regression where the key explanatory variables (permanent income and income risk) are themselves estimates generated from a first-stage regression on panel data.\n\nThe authors state that the standard errors of the parameter estimates are calculated using bootstrapping techniques.\n\n### Question\n\nIn this specific context, which of the following statistical problems make the use of bootstrapping essential for valid inference? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's main empirical model i...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's main empirical model i...", "error": null}}
{"ID": 31, "Question": "### Background\n\n**Research Question.** This problem analyzes the long-run comparative statics of two alternative models of a dual labor market: the Entry Job Competition (EJC) model, where a market-clearing wage premium exists for new hires, and the Entry Job Rationing (EJR) model, where it does not. The analysis focuses on how the economy responds to changes in union power and the accessibility of education.\n\n**Setting / Institutional Environment.** The economy can be in one of three long-run states:\n1.  **EJC Model:** A competitive market for new entrants determines an education premium `x*` that equates the supply of and demand for educated workers.\n2.  **EJR Model (Supply Constrained):** Demand for educated workers exceeds supply. Firms' growth is constrained by the availability of new talent. All educated workers find primary sector jobs.\n3.  **EJR Model (Demand Constrained):** Supply of educated workers exceeds demand. Firms' growth is determined by their profitability, and some educated workers are rationed out of primary sector jobs.\n\n**Variables & Parameters.**\n- `m`: Trade unions' relative bargaining power.\n- `h(α)` or `c(α)`: Factors determining the accessibility of education (a favorable change means more people get educated for any given return).\n- `x*`: The long-run equilibrium education premium in the EJC model.\n- `w_1*`, `w_2*`: Long-run real wages in the primary and secondary sectors.\n- `r*`: Long-run net rate of quasi-rent on capital.\n- `p*`: Long-run proportion of a cohort that gets educated.\n- `θ*`: Long-run proportion of educated workers who find a primary sector job.\n- `k*`: Long-run capital-general labor ratio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the long-run effects of an increase in union bargaining power (`m`) and an improvement in the accessibility of education on key economic variables across the different models and regimes. A `+` indicates an increase, a `-` indicates a decrease, and a `0` indicates no change.\n\n**Table 1: EJC Model**\n| Cause of Change | Permanent Wage Differential | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | 0 | 0 | 0 | 0 |\n| Accessibility of Education | - | + | 0 | + |\n\n**Table 2: EJR Model - Supply Constrained Regime**\n| Cause of Change | Wage Differential `w_1* - w_2*` | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | + | + | - | + |\n| Accessibility of Education | - | + | - | + |\n\n**Table 3: EJR Model - Demand Constrained Regime**\n| Cause of Change | Wage Differential `w_1* - w_2*` | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*θ*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | + | - | 0 | - |\n| Accessibility of Education | 0 | 0 | 0 | 0 |\n\n---\n\nAccording to the models and results presented, which of the following statements accurately describe the long-run effects of policy or environmental changes?", "Answer": ["A", "B", "D"], "qid": "31", "question": "### Background\n\n**Research Question.** This problem analyzes the long-run comparative statics of two alternative models of a dual labor market: the Entry Job Competition (EJC) model, where a market-clearing wage premium exists for new hires, and the Entry Job Rationing (EJR) model, where it does not. The analysis focuses on how the economy responds to changes in union power and the accessibility of education.\n\n**Setting / Institutional Environment.** The economy can be in one of three long-run states:\n1.  **EJC Model:** A competitive market for new entrants determines an education premium `x*` that equates the supply of and demand for educated workers.\n2.  **EJR Model (Supply Constrained):** Demand for educated workers exceeds supply. Firms' growth is constrained by the availability of new talent. All educated workers find primary sector jobs.\n3.  **EJR Model (Demand Constrained):** Supply of educated workers exceeds demand. Firms' growth is determined by their profitability, and some educated workers are rationed out of primary sector jobs.\n\n**Variables & Parameters.**\n- `m`: Trade unions' relative bargaining power.\n- `h(α)` or `c(α)`: Factors determining the accessibility of education (a favorable change means more people get educated for any given return).\n- `x*`: The long-run equilibrium education premium in the EJC model.\n- `w_1*`, `w_2*`: Long-run real wages in the primary and secondary sectors.\n- `r*`: Long-run net rate of quasi-rent on capital.\n- `p*`: Long-run proportion of a cohort that gets educated.\n- `θ*`: Long-run proportion of educated workers who find a primary sector job.\n- `k*`: Long-run capital-general labor ratio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the long-run effects of an increase in union bargaining power (`m`) and an improvement in the accessibility of education on key economic variables across the different models and regimes. A `+` indicates an increase, a `-` indicates a decrease, and a `0` indicates no change.\n\n**Table 1: EJC Model**\n| Cause of Change | Permanent Wage Differential | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | 0 | 0 | 0 | 0 |\n| Accessibility of Education | - | + | 0 | + |\n\n**Table 2: EJR Model - Supply Constrained Regime**\n| Cause of Change | Wage Differential `w_1* - w_2*` | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | + | + | - | + |\n| Accessibility of Education | - | + | - | + |\n\n**Table 3: EJR Model - Demand Constrained Regime**\n| Cause of Change | Wage Differential `w_1* - w_2*` | Secondary Wage `w_2*` | Net Quasi-Rent `r*` | Primary Employment Share `p*θ*` |\n| :--- | :---: | :---: | :---: | :---: |\n| Union Power (`m`) | + | - | 0 | - |\n| Accessibility of Education | 0 | 0 | 0 | 0 |\n\n---\n\nAccording to the models and results presented, which of the following statements accurately describe the long-run effects of policy or environmental changes?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 32, "Question": "### Background\n\n**Research Question.** This problem analyzes a firm's optimal investment strategy when growth requires not only physical investment but also costly recruitment of skilled labor from a competitive market.\n\n**Setting / Institutional Environment.** In the Entry Job Competition (EJC) model, a firm's growth is constrained by its ability to hire newly educated workers for its primary labor force. To do so, it must pay a market-determined education premium, `x(t)`, for each new hire. This premium is an additional cost of growth, on top of physical investment costs.\n\n**Variables & Parameters.**\n- `V(t)`: The value of the firm at time `t`.\n- `r(t)`: Net rate of quasi-rent per unit of capital (profit flow before growth costs).\n- `g(τ)`: Firm's growth rate at time `τ`.\n- `φ(g)`: Physical investment and adjustment cost per unit of capital, with `φ''(g) > 0`.\n- `x(t)`: The education premium per new hire, taken as given by the firm.\n- `a`: The fixed number of primary workers per unit of capital.\n- `K(τ)`: The firm's capital stock at time `τ`.\n- `ρ`: The constant market rate of discount.\n\n---\n\n### Data / Model Specification\n\nThe firm chooses a growth path `g(τ)` for `τ ≥ t` to maximize its value `V(t)`:\n\n  \nV(t) = \\int_{t}^{\\infty} \\{ r(t) - [\\phi(g(\\tau)) + a \\cdot x(t) \\cdot g(\\tau)] \\} K(\\tau) e^{-\\rho(\\tau-t)} d\\tau \\quad \\text{(Eq. (1))}\n \n\nsubject to the capital accumulation equation `\\dot{K}(\\tau) = g(\\tau)K(\\tau)` and an initial stock `K(t)`. The firm operates under static expectations, treating the current rent `r(t)` and premium `x(t)` as permanent.\n\nThe optimal growth rate `g` is constant for all `τ ≥ t` and satisfies the condition:\n\n  \n\\frac{r(t) - (\\phi(g) + a \\cdot x(t) \\cdot g)}{\\rho - g} = \\phi'(g) + a \\cdot x(t) \\quad \\text{(Eq. (2))}\n \n\n---\n\nBased on the firm's optimization problem, which of the following statements are valid interpretations or consequences of the model?\n", "Answer": ["A", "C", "D"], "qid": "32", "question": "### Background\n\n**Research Question.** This problem analyzes a firm's optimal investment strategy when growth requires not only physical investment but also costly recruitment of skilled labor from a competitive market.\n\n**Setting / Institutional Environment.** In the Entry Job Competition (EJC) model, a firm's growth is constrained by its ability to hire newly educated workers for its primary labor force. To do so, it must pay a market-determined education premium, `x(t)`, for each new hire. This premium is an additional cost of growth, on top of physical investment costs.\n\n**Variables & Parameters.**\n- `V(t)`: The value of the firm at time `t`.\n- `r(t)`: Net rate of quasi-rent per unit of capital (profit flow before growth costs).\n- `g(τ)`: Firm's growth rate at time `τ`.\n- `φ(g)`: Physical investment and adjustment cost per unit of capital, with `φ''(g) > 0`.\n- `x(t)`: The education premium per new hire, taken as given by the firm.\n- `a`: The fixed number of primary workers per unit of capital.\n- `K(τ)`: The firm's capital stock at time `τ`.\n- `ρ`: The constant market rate of discount.\n\n---\n\n### Data / Model Specification\n\nThe firm chooses a growth path `g(τ)` for `τ ≥ t` to maximize its value `V(t)`:\n\n  \nV(t) = \\int_{t}^{\\infty} \\{ r(t) - [\\phi(g(\\tau)) + a \\cdot x(t) \\cdot g(\\tau)] \\} K(\\tau) e^{-\\rho(\\tau-t)} d\\tau \\quad \\text{(Eq. (1))}\n \n\nsubject to the capital accumulation equation `\\dot{K}(\\tau) = g(\\tau)K(\\tau)` and an initial stock `K(t)`. The firm operates under static expectations, treating the current rent `r(t)` and premium `x(t)` as permanent.\n\nThe optimal growth rate `g` is constant for all `τ ≥ t` and satisfies the condition:\n\n  \n\\frac{r(t) - (\\phi(g) + a \\cdot x(t) \\cdot g)}{\\rho - g} = \\phi'(g) + a \\cdot x(t) \\quad \\text{(Eq. (2))}\n \n\n---\n\nBased on the firm's optimization problem, which of the following statements are valid interpretations or consequences of the model?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 33, "Question": "### Background\n\n**Research Question.** This problem examines how wages are determined in a non-competitive primary labor market through a formal bargaining process between a firm and a union.\n\n**Setting / Institutional Environment.** The wage for organization-specific (primary) labor, `w_1`, is not set by market clearing but by collective bargaining. The bargaining occurs over the quasi-rent generated by the firm's fixed factors. The secondary market wage, `w_2`, serves as the workers' outside option, forming the floor of the bargaining range. The ceiling is the maximum wage the firm can sustain without ceasing to grow.\n\n**Variables & Parameters.**\n- `w_1(t)`: Real wage in the primary market.\n- `w_2(t)`: Real wage in the secondary market (workers' outside option).\n- `w_1^{\\max}(t)`: Maximum sustainable wage (firm's reservation point).\n- `m`: A positive parameter representing the trade union's relative bargaining power.\n- `r_c(t)`: Gross organizational quasi-rent per unit of capital.\n- `r^{\\min}(t)`: Minimum net rate of quasi-rent required by the firm to achieve zero growth.\n- `a`: The fixed number of primary workers per unit of capital.\n- `r(t)`: The net rate of quasi-rent per unit of capital accruing to shareholders.\n\n---\n\n### Data / Model Specification\n\nThe outcome of the wage bargain is modeled as a weighted average of the upper and lower bounds of the bargaining range:\n\n  \nw_1(t) = \\frac{m}{1+m}w_1^{\\max}(t) + \\frac{1}{1+m}w_2(t) \\quad \\text{(Eq. (1))}\n \n\nThe maximum wage is determined by the firm's viability constraint:\n\n  \nw_1^{\\max}(t) = \\frac{r_c(t) - r^{\\min}(t)}{a} \\quad \\text{(Eq. (2))}\n \n\nThe net return to capital is defined as the residual after paying primary workers:\n\n  \nr(t) = r_c(t) - a \\cdot w_1(t) \\quad \\text{(Eq. (3))}\n \n\n---\n\nConsider a scenario where an exogenous shock increases the minimum required return on capital, `r^{\\min}(t)`, while holding `r_c(t)` and `w_2(t)` constant. Based on the bargaining model, which of the following outcomes are **INCORRECT** descriptions of how the burden of this shock is distributed?\n", "Answer": ["A", "B", "C"], "qid": "33", "question": "### Background\n\n**Research Question.** This problem examines how wages are determined in a non-competitive primary labor market through a formal bargaining process between a firm and a union.\n\n**Setting / Institutional Environment.** The wage for organization-specific (primary) labor, `w_1`, is not set by market clearing but by collective bargaining. The bargaining occurs over the quasi-rent generated by the firm's fixed factors. The secondary market wage, `w_2`, serves as the workers' outside option, forming the floor of the bargaining range. The ceiling is the maximum wage the firm can sustain without ceasing to grow.\n\n**Variables & Parameters.**\n- `w_1(t)`: Real wage in the primary market.\n- `w_2(t)`: Real wage in the secondary market (workers' outside option).\n- `w_1^{\\max}(t)`: Maximum sustainable wage (firm's reservation point).\n- `m`: A positive parameter representing the trade union's relative bargaining power.\n- `r_c(t)`: Gross organizational quasi-rent per unit of capital.\n- `r^{\\min}(t)`: Minimum net rate of quasi-rent required by the firm to achieve zero growth.\n- `a`: The fixed number of primary workers per unit of capital.\n- `r(t)`: The net rate of quasi-rent per unit of capital accruing to shareholders.\n\n---\n\n### Data / Model Specification\n\nThe outcome of the wage bargain is modeled as a weighted average of the upper and lower bounds of the bargaining range:\n\n  \nw_1(t) = \\frac{m}{1+m}w_1^{\\max}(t) + \\frac{1}{1+m}w_2(t) \\quad \\text{(Eq. (1))}\n \n\nThe maximum wage is determined by the firm's viability constraint:\n\n  \nw_1^{\\max}(t) = \\frac{r_c(t) - r^{\\min}(t)}{a} \\quad \\text{(Eq. (2))}\n \n\nThe net return to capital is defined as the residual after paying primary workers:\n\n  \nr(t) = r_c(t) - a \\cdot w_1(t) \\quad \\text{(Eq. (3))}\n \n\n---\n\nConsider a scenario where an exogenous shock increases the minimum required return on capital, `r^{\\min}(t)`, while holding `r_c(t)` and `w_2(t)` constant. Based on the bargaining model, which of the following outcomes are **INCORRECT** descriptions of how the burden of this shock is distributed?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 34, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's central theoretical contribution: how political stability interacts with policymaker heterogeneity to determine the price of sovereign debt and the likelihood of default.\n\n**Setting.** A small open economy issues one-period bonds to risk-neutral lenders. The government is run by one of two policymaker types: patient (`H`) or impatient (`L`), with `β_H > β_L`. The types have different default thresholds: for a given income level, there are 'intermediate' debt levels that a patient type would repay but an impatient type would default on. At the end of each period, the incumbent policymaker is replaced by the other type with a constant probability `π`.\n\n**Variables & Parameters.**\n*   `b'`: End-of-period net bond position (assets); `-b'` is debt issuance.\n*   `q_j(b', y)`: The price of a bond issued by a type-`j` policymaker.\n*   `π`: The probability of political turnover; `1-π` is a measure of political stability.\n*   `r`: The risk-free interest rate.\n*   `d_j(b', y')`: Default decision rule for type `j` (1 if default, 0 if repay).\n\n---\n\n### Data / Model Specification\n\nThe bond price is determined by the lenders' zero-profit condition, which states that the price equals the discounted expected repayment probability:\n  \nq_{j}(b',y)=\\frac{1}{1+r} \\left( 1 - E[\\text{default on } b'] \\right)\n \n**(Eq. 1)**\n\nThe expectation accounts for the probability `π` that the other policymaker type (`-j`) will be in power next period to make the repayment decision.\n\n---\n\n### Question\n\nThe paper's central mechanism links political stability (`1-π`) to default risk via the bond price schedule. Select all statements that correctly describe this mechanism.", "Answer": ["A", "C"], "qid": "34", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's central theoretical contribution: how political stability interacts with policymaker heterogeneity to determine the price of sovereign debt and the likelihood of default.\n\n**Setting.** A small open economy issues one-period bonds to risk-neutral lenders. The government is run by one of two policymaker types: patient (`H`) or impatient (`L`), with `β_H > β_L`. The types have different default thresholds: for a given income level, there are 'intermediate' debt levels that a patient type would repay but an impatient type would default on. At the end of each period, the incumbent policymaker is replaced by the other type with a constant probability `π`.\n\n**Variables & Parameters.**\n*   `b'`: End-of-period net bond position (assets); `-b'` is debt issuance.\n*   `q_j(b', y)`: The price of a bond issued by a type-`j` policymaker.\n*   `π`: The probability of political turnover; `1-π` is a measure of political stability.\n*   `r`: The risk-free interest rate.\n*   `d_j(b', y')`: Default decision rule for type `j` (1 if default, 0 if repay).\n\n---\n\n### Data / Model Specification\n\nThe bond price is determined by the lenders' zero-profit condition, which states that the price equals the discounted expected repayment probability:\n  \nq_{j}(b',y)=\\frac{1}{1+r} \\left( 1 - E[\\text{default on } b'] \\right)\n \n**(Eq. 1)**\n\nThe expectation accounts for the probability `π` that the other policymaker type (`-j`) will be in power next period to make the repayment decision.\n\n---\n\n### Question\n\nThe paper's central mechanism links political stability (`1-π`) to default risk via the bond price schedule. Select all statements that correctly describe this mechanism.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 35, "Question": "### Background\n\n**Research Question.** This problem analyzes the model's key empirical predictions that distinguish 'political defaults' from standard 'economic defaults'.\n\n**Setting.** The model generates two types of defaults. 'Economic defaults' are driven by severe negative output shocks. 'Political defaults' are triggered by a change in power from a patient (`H`) to an impatient (`L`) policymaker, who then defaults on a large debt stock inherited from their predecessor.\n\n**Variables & Parameters.**\n*   `y`: The stochastic endowment (output).\n*   `-b'`: The volume of debt issuance.\n*   Sovereign Spread: The yield on sovereign bonds over the risk-free rate.\n*   `q_j(b', y)`: The bond price schedule faced by a type-`j` policymaker.\n\n---\n\n### Data / Model Specification\n\nThe model makes two distinctive predictions about the empirical footprint of political defaults:\n1.  **Weak Output Correlation:** Unlike economic defaults, political defaults are not necessarily preceded by poor concurrent economic conditions. Simulations show that with high political stability, 38% of defaults occur when output is above its long-run mean, matching historical data.\n2.  **Post-Default Dynamics:** After a political default, post-default debt issuance levels and equilibrium spreads are *lower* than their pre-default levels.\n\n---\n\n### Question\n\nThe model predicts several distinctive empirical features of 'political defaults' that differ from purely 'economic defaults'. Select all statements that are **INCORRECT** descriptions of the mechanisms or outcomes related to political defaults in the model.", "Answer": ["A", "C"], "qid": "35", "question": "### Background\n\n**Research Question.** This problem analyzes the model's key empirical predictions that distinguish 'political defaults' from standard 'economic defaults'.\n\n**Setting.** The model generates two types of defaults. 'Economic defaults' are driven by severe negative output shocks. 'Political defaults' are triggered by a change in power from a patient (`H`) to an impatient (`L`) policymaker, who then defaults on a large debt stock inherited from their predecessor.\n\n**Variables & Parameters.**\n*   `y`: The stochastic endowment (output).\n*   `-b'`: The volume of debt issuance.\n*   Sovereign Spread: The yield on sovereign bonds over the risk-free rate.\n*   `q_j(b', y)`: The bond price schedule faced by a type-`j` policymaker.\n\n---\n\n### Data / Model Specification\n\nThe model makes two distinctive predictions about the empirical footprint of political defaults:\n1.  **Weak Output Correlation:** Unlike economic defaults, political defaults are not necessarily preceded by poor concurrent economic conditions. Simulations show that with high political stability, 38% of defaults occur when output is above its long-run mean, matching historical data.\n2.  **Post-Default Dynamics:** After a political default, post-default debt issuance levels and equilibrium spreads are *lower* than their pre-default levels.\n\n---\n\n### Question\n\nThe model predicts several distinctive empirical features of 'political defaults' that differ from purely 'economic defaults'. Select all statements that are **INCORRECT** descriptions of the mechanisms or outcomes related to political defaults in the model.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 36, "Question": "### Background\n\n**Research Question.** This problem examines the structure of the policymaker's dynamic optimization problem, which is the foundation of the sovereign default model with political turnover.\n\n**Setting.** A policymaker of a given type `j` (patient or impatient) makes decisions to maximize the expected present value of utility. In each period, the policymaker first decides whether to default on existing debt. Then, they choose a new level of borrowing/saving. The type of policymaker in power may change next period with probability `π`.\n\n**Variables & Parameters.**\n*   `V_j(b,y,h)`: Value function for a type-`j` policymaker who is in power.\n*   `W_j(b,y,h)`: Value function for a type-`j` policymaker who is *not* in power.\n*   `b, y, h`: State variables for debt, endowment, and past default history.\n*   `β_j`: Discount factor for a type-`j` policymaker.\n*   `π`: Probability of political turnover.\n\n---\n\n### Data / Model Specification\n\nThe policymaker's decision problem is characterized by a set of Bellman equations. The overall value for a type-`j` policymaker in power is the maximum of the value of repaying and the value of defaulting:\n  \nV_{j}(b,y,h)=\\max\\{V_{j1}(y,h),V_{j0}(b,y,h)\\}\n \n**(Eq. 1)**\n\nThe value of repaying (`d=0`) is:\n  \nV_{j0}(b,y,h) = \\max_{b'}\\left\\{u(c_0) + \\beta_{j}\\left[(1-\\pi)E[V_{j}(b',y',0)] + \\pi E[W_{j}(b',y',0)]\\right]\\right\\}\n \n**(Eq. 2)**\nwhere `c_0` is consumption under repayment. The policymaker types are distinguished by their discount factors, as shown in Table 1.\n\n**Table 1: Discount Factor Parameters**\n| Parameter | Symbol | Value |\n| :--- | :--- | :--- |\n| Higher Discount Factor | `β_H` | 0.9 |\n| Lower Discount Factor | `β_L` | 0.6 |\n\n\n---\n\n### Question\n\nBased on the policymaker's dynamic optimization problem described by Eq. (1) and Eq. (2), select all statements that are correct.", "Answer": ["A", "D"], "qid": "36", "question": "### Background\n\n**Research Question.** This problem examines the structure of the policymaker's dynamic optimization problem, which is the foundation of the sovereign default model with political turnover.\n\n**Setting.** A policymaker of a given type `j` (patient or impatient) makes decisions to maximize the expected present value of utility. In each period, the policymaker first decides whether to default on existing debt. Then, they choose a new level of borrowing/saving. The type of policymaker in power may change next period with probability `π`.\n\n**Variables & Parameters.**\n*   `V_j(b,y,h)`: Value function for a type-`j` policymaker who is in power.\n*   `W_j(b,y,h)`: Value function for a type-`j` policymaker who is *not* in power.\n*   `b, y, h`: State variables for debt, endowment, and past default history.\n*   `β_j`: Discount factor for a type-`j` policymaker.\n*   `π`: Probability of political turnover.\n\n---\n\n### Data / Model Specification\n\nThe policymaker's decision problem is characterized by a set of Bellman equations. The overall value for a type-`j` policymaker in power is the maximum of the value of repaying and the value of defaulting:\n  \nV_{j}(b,y,h)=\\max\\{V_{j1}(y,h),V_{j0}(b,y,h)\\}\n \n**(Eq. 1)**\n\nThe value of repaying (`d=0`) is:\n  \nV_{j0}(b,y,h) = \\max_{b'}\\left\\{u(c_0) + \\beta_{j}\\left[(1-\\pi)E[V_{j}(b',y',0)] + \\pi E[W_{j}(b',y',0)]\\right]\\right\\}\n \n**(Eq. 2)**\nwhere `c_0` is consumption under repayment. The policymaker types are distinguished by their discount factors, as shown in Table 1.\n\n**Table 1: Discount Factor Parameters**\n| Parameter | Symbol | Value |\n| :--- | :--- | :--- |\n| Higher Discount Factor | `β_H` | 0.9 |\n| Lower Discount Factor | `β_L` | 0.6 |\n\n\n---\n\n### Question\n\nBased on the policymaker's dynamic optimization problem described by Eq. (1) and Eq. (2), select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 37, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical core of the paper's model of stagflation, analyzing how different assumptions about expectations formation alter the trade-off between unemployment and inflation.\n\n**Setting.** The analysis is based on a macroeconomic model for a national economy, built from a wage-setting curve (an expectations-augmented Phillips curve) and a price-setting curve derived from a labor demand function.\n\n**Variables & Parameters.**\n- `w`, `p`: Logarithm of hourly wages and the price level, respectively.\n- `\\dot{w}`, `\\dot{p}`: First difference (annual growth rate) of the corresponding log variable.\n- `\\dot{p}^e`: Expected rate of price inflation.\n- `U`: Unemployment rate.\n- `U_0`: Baseline unemployment rate corresponding to zero labor market slack.\n- `\\dot{x}^e`: Target rate of growth of real wages sought in wage settlements when slack is zero.\n- `x`: Trend level of log value-added per worker (productivity).\n- `\\dot{x}`: Growth rate of trend productivity.\n- `p_m`: Log of import prices relative to domestic prices.\n- `\\dot{p}_m`: Growth rate of relative import prices.\n- `\\beta, \\mu, \\gamma`: Positive structural parameters of the model.\n\n### Data / Model Specification\n\nThe model consists of a wage equation and a price-change equation:\n\n  \n\\dot{w} = \\dot{p}^e - \\beta(U - U_0) + \\dot{x}^e \\quad \\text{(Eq. (1))}\n \n\n  \n\\dot{p} = \\dot{w} - \\dot{x} + \\mu\\dot{p}_m - \\gamma\\dot{U} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nAssuming price expectations are correct (`\\dot{p} = \\dot{p}^e`) and unemployment is stable (`\\dot{U} = 0`), select all of the following statements that are mathematically correct and consistent with the paper's framework for the medium-term equilibrium (NAIRU).\n", "Answer": ["A", "B"], "qid": "37", "question": "### Background\n\n**Research Question.** This problem explores the theoretical core of the paper's model of stagflation, analyzing how different assumptions about expectations formation alter the trade-off between unemployment and inflation.\n\n**Setting.** The analysis is based on a macroeconomic model for a national economy, built from a wage-setting curve (an expectations-augmented Phillips curve) and a price-setting curve derived from a labor demand function.\n\n**Variables & Parameters.**\n- `w`, `p`: Logarithm of hourly wages and the price level, respectively.\n- `\\dot{w}`, `\\dot{p}`: First difference (annual growth rate) of the corresponding log variable.\n- `\\dot{p}^e`: Expected rate of price inflation.\n- `U`: Unemployment rate.\n- `U_0`: Baseline unemployment rate corresponding to zero labor market slack.\n- `\\dot{x}^e`: Target rate of growth of real wages sought in wage settlements when slack is zero.\n- `x`: Trend level of log value-added per worker (productivity).\n- `\\dot{x}`: Growth rate of trend productivity.\n- `p_m`: Log of import prices relative to domestic prices.\n- `\\dot{p}_m`: Growth rate of relative import prices.\n- `\\beta, \\mu, \\gamma`: Positive structural parameters of the model.\n\n### Data / Model Specification\n\nThe model consists of a wage equation and a price-change equation:\n\n  \n\\dot{w} = \\dot{p}^e - \\beta(U - U_0) + \\dot{x}^e \\quad \\text{(Eq. (1))}\n \n\n  \n\\dot{p} = \\dot{w} - \\dot{x} + \\mu\\dot{p}_m - \\gamma\\dot{U} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nAssuming price expectations are correct (`\\dot{p} = \\dot{p}^e`) and unemployment is stable (`\\dot{U} = 0`), select all of the following statements that are mathematically correct and consistent with the paper's framework for the medium-term equilibrium (NAIRU).\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 38, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical core of the paper's model of stagflation, analyzing how different assumptions about expectations formation alter the trade-off between unemployment and inflation.\n\n**Setting.** The analysis is based on a macroeconomic model for a national economy, built from a wage-setting curve (an expectations-augmented Phillips curve) and a price-setting curve derived from a labor demand function.\n\n**Variables & Parameters.**\n- `w`, `p`: Logarithm of hourly wages and the price level, respectively.\n- `\\dot{w}`, `\\dot{p}`: First difference (annual growth rate) of the corresponding log variable.\n- `\\dot{p}^e`: Expected rate of price inflation.\n- `U`: Unemployment rate.\n- `U_0`: Baseline unemployment rate corresponding to zero labor market slack.\n- `\\dot{x}^e`: Target rate of growth of real wages sought in wage settlements when slack is zero.\n- `x`: Trend level of log value-added per worker (productivity).\n- `\\dot{x}`: Growth rate of trend productivity.\n- `p_m`: Log of import prices relative to domestic prices.\n- `\\dot{p}_m`: Growth rate of relative import prices.\n- `\\beta, \\mu, \\gamma`: Positive structural parameters of the model.\n\n### Data / Model Specification\n\nThe model consists of a wage equation and a price-change equation:\n\n  \n\\dot{w} = \\dot{p}^e - \\beta(U - U_0) + \\dot{x}^e \\quad \\text{(Eq. (1))}\n \n\n  \n\\dot{p} = \\dot{w} - \\dot{x} + \\mu\\dot{p}_m - \\gamma\\dot{U} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nAssuming wage setters have adaptive expectations (`\\dot{p}^e = \\dot{p}_{-1}`), select all statements that correctly describe the short-run dynamics of inflation and the associated policy trade-offs.\n", "Answer": ["A", "B"], "qid": "38", "question": "### Background\n\n**Research Question.** This problem explores the theoretical core of the paper's model of stagflation, analyzing how different assumptions about expectations formation alter the trade-off between unemployment and inflation.\n\n**Setting.** The analysis is based on a macroeconomic model for a national economy, built from a wage-setting curve (an expectations-augmented Phillips curve) and a price-setting curve derived from a labor demand function.\n\n**Variables & Parameters.**\n- `w`, `p`: Logarithm of hourly wages and the price level, respectively.\n- `\\dot{w}`, `\\dot{p}`: First difference (annual growth rate) of the corresponding log variable.\n- `\\dot{p}^e`: Expected rate of price inflation.\n- `U`: Unemployment rate.\n- `U_0`: Baseline unemployment rate corresponding to zero labor market slack.\n- `\\dot{x}^e`: Target rate of growth of real wages sought in wage settlements when slack is zero.\n- `x`: Trend level of log value-added per worker (productivity).\n- `\\dot{x}`: Growth rate of trend productivity.\n- `p_m`: Log of import prices relative to domestic prices.\n- `\\dot{p}_m`: Growth rate of relative import prices.\n- `\\beta, \\mu, \\gamma`: Positive structural parameters of the model.\n\n### Data / Model Specification\n\nThe model consists of a wage equation and a price-change equation:\n\n  \n\\dot{w} = \\dot{p}^e - \\beta(U - U_0) + \\dot{x}^e \\quad \\text{(Eq. (1))}\n \n\n  \n\\dot{p} = \\dot{w} - \\dot{x} + \\mu\\dot{p}_m - \\gamma\\dot{U} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nAssuming wage setters have adaptive expectations (`\\dot{p}^e = \\dot{p}_{-1}`), select all statements that correctly describe the short-run dynamics of inflation and the associated policy trade-offs.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 39, "Question": "### Background\n\n**Research Question.** This problem assesses the long-run properties of the paper's wage-setting model. The simple model used for the main analysis implies that a permanent fall in productivity growth leads to a permanent rise in unemployment. This motivates a more complex \"fully adaptive\" model where real wage aspirations adjust over time.\n\n**Setting.** The authors estimate a fully adaptive wage equation and use it to simulate the dynamic path of unemployment following a shock.\n\n### Data / Model Specification\n\nThe authors simulate the dynamic impact on unemployment of a one-time, permanent 1% fall in the growth rate of real wages that occurs at time `t=0`. The results are in Table 1.\n\n**Table 1: Simulated Unemployment Effect of a 1% Permanent Fall in Real Wage Growth**\n(Effect on `U - U_0` in percentage points)\n\n| Year `t` after shock | Unemployment Effect `100(U - U_0)` |\n| :--- | :--- |\n| 1 | 0.703 |\n| 2 | 0.714 |\n| 3 | 0.685 |\n| 5 | 0.578 |\n| 7 | 0.463 |\n| 10 | 0.319 |\n| 20 | 0.086 |\n\n### Question\n\nBased on the fully adaptive wage model and the simulation results in Table 1, select all statements that are **INCORRECT** or **NOT supported** by the authors' analysis.\n", "Answer": ["A", "B", "C"], "qid": "39", "question": "### Background\n\n**Research Question.** This problem assesses the long-run properties of the paper's wage-setting model. The simple model used for the main analysis implies that a permanent fall in productivity growth leads to a permanent rise in unemployment. This motivates a more complex \"fully adaptive\" model where real wage aspirations adjust over time.\n\n**Setting.** The authors estimate a fully adaptive wage equation and use it to simulate the dynamic path of unemployment following a shock.\n\n### Data / Model Specification\n\nThe authors simulate the dynamic impact on unemployment of a one-time, permanent 1% fall in the growth rate of real wages that occurs at time `t=0`. The results are in Table 1.\n\n**Table 1: Simulated Unemployment Effect of a 1% Permanent Fall in Real Wage Growth**\n(Effect on `U - U_0` in percentage points)\n\n| Year `t` after shock | Unemployment Effect `100(U - U_0)` |\n| :--- | :--- |\n| 1 | 0.703 |\n| 2 | 0.714 |\n| 3 | 0.685 |\n| 5 | 0.578 |\n| 7 | 0.463 |\n| 10 | 0.319 |\n| 20 | 0.086 |\n\n### Question\n\nBased on the fully adaptive wage model and the simulation results in Table 1, select all statements that are **INCORRECT** or **NOT supported** by the authors' analysis.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 40, "Question": "### Background\n\nIn the paper's theoretical framework, a wife's non-participation in the labor market is a corner solution. The household's behavior is then described by a 'rationed' cost function, which depends on her reservation wage (`w_f^*`) rather than the market wage (`w_f`). The reservation wage is the shadow price of her time.\n\n### The Question\n\nAssuming preferences between male and female non-market time are *not* separable, which of the following statements are correct implications of this framework? (Select all that apply)", "Answer": ["C", "D"], "qid": "40", "question": "### Background\n\nIn the paper's theoretical framework, a wife's non-participation in the labor market is a corner solution. The household's behavior is then described by a 'rationed' cost function, which depends on her reservation wage (`w_f^*`) rather than the market wage (`w_f`). The reservation wage is the shadow price of her time.\n\n### The Question\n\nAssuming preferences between male and female non-market time are *not* separable, which of the following statements are correct implications of this framework? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn the paper's theoretical framewo...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn the paper's theoretical framewo...", "error": null}}
{"ID": 41, "Question": "### Background\n\nThis problem characterizes the optimal agreement between a buyer and a Preferred Supplier (PS) when the buyer has full commitment power to design the trading mechanism. The objective of the agreement is to maximize the buyer-PS coalition's expected joint surplus.\n\nThe model considers a buyer and `n` Competing Suppliers (CS). Each supplier's cost `c_i` is an independent draw from a distribution `G(c)` with density `g(c)` on `[0,1]`. The PS's cost `c_p` is her private information.\n\n### Data / Model Specification\n\nTo make the optimal mechanism incentive-compatible, the paper proposes a revelation mechanism. A PS with true cost `c_p` reports a cost `z` to the buyer. Her expected utility from this report is:\n\n  \nU(z; c_p) = \\underbrace{\\int_{z}^{1} [1 - G(r(x))]^n dx}_{\\text{Payment } P(z)} + \\underbrace{(z - c_p) \\cdot [1 - G(r(z))]^n}_{\\text{Expected Production Profit}}\n \n\nwhere `r(x)` is the optimal reserve price set in a subsequent auction among CSs, defined by `r + G(r)/g(r) = x`. The paper assumes `G(c)/g(c)` is increasing, which ensures `r'(x) > 0`.\n\n### Question\n\nSelect all statements that are mathematically correct regarding the PS's incentive compatibility problem of choosing a report `z` to maximize `U(z; c_p)`.", "Answer": ["C", "D"], "qid": "41", "question": "### Background\n\nThis problem characterizes the optimal agreement between a buyer and a Preferred Supplier (PS) when the buyer has full commitment power to design the trading mechanism. The objective of the agreement is to maximize the buyer-PS coalition's expected joint surplus.\n\nThe model considers a buyer and `n` Competing Suppliers (CS). Each supplier's cost `c_i` is an independent draw from a distribution `G(c)` with density `g(c)` on `[0,1]`. The PS's cost `c_p` is her private information.\n\n### Data / Model Specification\n\nTo make the optimal mechanism incentive-compatible, the paper proposes a revelation mechanism. A PS with true cost `c_p` reports a cost `z` to the buyer. Her expected utility from this report is:\n\n  \nU(z; c_p) = \\underbrace{\\int_{z}^{1} [1 - G(r(x))]^n dx}_{\\text{Payment } P(z)} + \\underbrace{(z - c_p) \\cdot [1 - G(r(z))]^n}_{\\text{Expected Production Profit}}\n \n\nwhere `r(x)` is the optimal reserve price set in a subsequent auction among CSs, defined by `r + G(r)/g(r) = x`. The paper assumes `G(c)/g(c)` is increasing, which ensures `r'(x) > 0`.\n\n### Question\n\nSelect all statements that are mathematically correct regarding the PS's incentive compatibility problem of choosing a report `z` to maximize `U(z; c_p)`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem characterizes the opt...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem characterizes the opt...", "error": null}}
{"ID": 42, "Question": "### Background\n\nThis problem characterizes the optimal agreement between a buyer and a Preferred Supplier (PS) when the buyer has full commitment power. The agreement involves the PS reporting her cost `c_p`, which is then used to set an optimal reserve price `r(c_p)` for an auction among `n` Competing Suppliers (CS).\n\n### Data / Model Specification\n\nStandard auction theory states that to maximize surplus from `n` bidders when the auctioneer's valuation is `x`, the optimal reserve price `r(x)` is defined by:\n\n  \nr + \\frac{G(r)}{g(r)} = x \\quad \\text{(Eq. (1))}\n \n\nIn the absence of an agreement, the buyer's optimal mechanism is a first-price auction with a reserve price `r(v)`, where `v` is the buyer's own valuation.\n\n### Question\n\nConsider the specific case of a uniform cost distribution `G(c)=c` on `[0,1]`, `n=1` competing supplier, and a buyer value `v=1`. Select all of the following conclusions that are correct.", "Answer": ["B", "C"], "qid": "42", "question": "### Background\n\nThis problem characterizes the optimal agreement between a buyer and a Preferred Supplier (PS) when the buyer has full commitment power. The agreement involves the PS reporting her cost `c_p`, which is then used to set an optimal reserve price `r(c_p)` for an auction among `n` Competing Suppliers (CS).\n\n### Data / Model Specification\n\nStandard auction theory states that to maximize surplus from `n` bidders when the auctioneer's valuation is `x`, the optimal reserve price `r(x)` is defined by:\n\n  \nr + \\frac{G(r)}{g(r)} = x \\quad \\text{(Eq. (1))}\n \n\nIn the absence of an agreement, the buyer's optimal mechanism is a first-price auction with a reserve price `r(v)`, where `v` is the buyer's own valuation.\n\n### Question\n\nConsider the specific case of a uniform cost distribution `G(c)=c` on `[0,1]`, `n=1` competing supplier, and a buyer value `v=1`. Select all of the following conclusions that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem characterizes the opt...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem characterizes the opt...", "error": null}}
{"ID": 43, "Question": "### Background\n\nThis problem investigates the equilibrium bidding behavior in a first-price procurement auction where one supplier is granted a Right of First Refusal (ROFR). This 'preference auction' (PA) is compared to a standard first-price auction (FPA).\n\n### Data / Model Specification\n\nIn a PA, `n` Competing Suppliers (CS) submit bids. A CS with true cost `c` chooses a bid `b(z)` (corresponding to a pretended cost `z`) to maximize expected profit:\n\n  \n\\max_{z} \\Pi_{k}[b(z);c] = (b(z)-c) \\cdot [1-G(b(z))] \\cdot [1-G(z)]^{n-1} \\quad \\text{(Eq. (1))}\n \n\nThe paper shows that if the inverse hazard rate `[1-G(c)]/g(c)` is decreasing and convex, CSs bid more aggressively (lower their bids) in a PA compared to an FPA.\n\n### Question\n\nSelect all statements that correctly describe the strategic environment faced by a Competing Supplier (CS) and the resulting bidding behavior.", "Answer": ["B", "D"], "qid": "43", "question": "### Background\n\nThis problem investigates the equilibrium bidding behavior in a first-price procurement auction where one supplier is granted a Right of First Refusal (ROFR). This 'preference auction' (PA) is compared to a standard first-price auction (FPA).\n\n### Data / Model Specification\n\nIn a PA, `n` Competing Suppliers (CS) submit bids. A CS with true cost `c` chooses a bid `b(z)` (corresponding to a pretended cost `z`) to maximize expected profit:\n\n  \n\\max_{z} \\Pi_{k}[b(z);c] = (b(z)-c) \\cdot [1-G(b(z))] \\cdot [1-G(z)]^{n-1} \\quad \\text{(Eq. (1))}\n \n\nThe paper shows that if the inverse hazard rate `[1-G(c)]/g(c)` is decreasing and convex, CSs bid more aggressively (lower their bids) in a PA compared to an FPA.\n\n### Question\n\nSelect all statements that correctly describe the strategic environment faced by a Competing Supplier (CS) and the resulting bidding behavior.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the equi...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the equi...", "error": null}}
{"ID": 44, "Question": "### Background\n\nThis problem investigates the welfare implications in a first-price procurement auction where one supplier is granted a Right of First Refusal (ROFR). The analysis compares this 'preference auction' (PA) to a standard first-price auction (FPA).\n\n### Data / Model Specification\n\nThe paper's Proposition 2 states that if the inverse hazard rate `[1-G(c)]/g(c)` is decreasing and convex, the expected joint surplus of the buyer and the Preferred Supplier (PS) is higher in the PA than the FPA. This gain for the buyer-PS coalition is achieved because the Competing Suppliers (CSs) are induced to bid more aggressively.\n\n### Question\n\nAccording to the paper's analysis, select all correct statements about the welfare implications of granting a ROFR in a first-price auction.", "Answer": ["B", "D"], "qid": "44", "question": "### Background\n\nThis problem investigates the welfare implications in a first-price procurement auction where one supplier is granted a Right of First Refusal (ROFR). The analysis compares this 'preference auction' (PA) to a standard first-price auction (FPA).\n\n### Data / Model Specification\n\nThe paper's Proposition 2 states that if the inverse hazard rate `[1-G(c)]/g(c)` is decreasing and convex, the expected joint surplus of the buyer and the Preferred Supplier (PS) is higher in the PA than the FPA. This gain for the buyer-PS coalition is achieved because the Competing Suppliers (CSs) are induced to bid more aggressively.\n\n### Question\n\nAccording to the paper's analysis, select all correct statements about the welfare implications of granting a ROFR in a first-price auction.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the welf...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the welf...", "error": null}}
{"ID": 45, "Question": "### Background\nThe stability of a generalized Cournot oligopoly model with `n` firms, adjustment speed `K`, demand parameter `b`, and marginal cost slope `d` is determined by the following condition:\n\n  \nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n \n\nFor the system to be stable, this inequality must be satisfied.\n\n### Question\nAccording to the stability condition provided, which of the following changes are **stabilizing** factors (i.e., they relax the condition, making stability more likely)? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "45", "question": "### Background\nThe stability of a generalized Cournot oligopoly model with `n` firms, adjustment speed `K`, demand parameter `b`, and marginal cost slope `d` is determined by the following condition:\n\n  \nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n \n\nFor the system to be stable, this inequality must be satisfied.\n\n### Question\nAccording to the stability condition provided, which of the following changes are **stabilizing** factors (i.e., they relax the condition, making stability more likely)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\nThe stability of a generalized Cour...", "model_answer": "This is a detailed response to the question about ### Background\nThe stability of a generalized Cour...", "error": null}}
{"ID": 46, "Question": "### Background\n\nTwo banks compete for High-ability (H) and Low-ability (L) traders, whose types are private information. Banks offer contracts defined by a total compensation (`u^j`) and a bonus ratio (`r^j = w_1^j/w_0^j`) that governs project risk. The first-best, profit-maximizing bonus ratio is `R*`. However, if the market for H-traders is very competitive, banks must offer them a high utility, `u^{H*}`. This high utility might tempt L-traders to mimic H-traders, violating the L-type's incentive compatibility (IC) constraint.\n\n### Data / Model Specification\n\nThe model establishes two equilibrium regimes based on a key threshold condition. The first-best utilities under symmetric information are:\n\n  \nu^{L*} = \\pi^{L}(R^{*}) - \\tau^{L} \\quad \\text{(Eq. 1)}\n \n\n  \nu^{H*} = \\pi^{H}(R^{*}) - \\tau^{H} \\quad \\text{(Eq. 2)}\n \n\nwhere `\\pi^j(R*)` is the maximum gross profit from a type-`j` trader and `\\tau^j` is a parameter inversely related to competition intensity (lower `\\tau^j` means more competition).\n\nAn L-type mimicking an H-type with contract `(u^H, r^H)` gets a payoff of `u^H f(r^H)`, where `f(r^H) \\le 1` measures the L-type's relative inefficiency. The switch to an excessive-risk regime occurs if the L-type's IC constraint is violated at the first-best contract offer, i.e., if `f(R^{*}) u^{H*} > u^{L*}`.\n\n### Question\n\nAssume the system is in the excessive-risk regime, where banks must distort the H-type contract by setting `r^H > R*` to satisfy the L-type's incentive compatibility constraint. According to the model, which of the following statements accurately describe the mechanisms and consequences of this situation? Select all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "46", "question": "### Background\n\nTwo banks compete for High-ability (H) and Low-ability (L) traders, whose types are private information. Banks offer contracts defined by a total compensation (`u^j`) and a bonus ratio (`r^j = w_1^j/w_0^j`) that governs project risk. The first-best, profit-maximizing bonus ratio is `R*`. However, if the market for H-traders is very competitive, banks must offer them a high utility, `u^{H*}`. This high utility might tempt L-traders to mimic H-traders, violating the L-type's incentive compatibility (IC) constraint.\n\n### Data / Model Specification\n\nThe model establishes two equilibrium regimes based on a key threshold condition. The first-best utilities under symmetric information are:\n\n  \nu^{L*} = \\pi^{L}(R^{*}) - \\tau^{L} \\quad \\text{(Eq. 1)}\n \n\n  \nu^{H*} = \\pi^{H}(R^{*}) - \\tau^{H} \\quad \\text{(Eq. 2)}\n \n\nwhere `\\pi^j(R*)` is the maximum gross profit from a type-`j` trader and `\\tau^j` is a parameter inversely related to competition intensity (lower `\\tau^j` means more competition).\n\nAn L-type mimicking an H-type with contract `(u^H, r^H)` gets a payoff of `u^H f(r^H)`, where `f(r^H) \\le 1` measures the L-type's relative inefficiency. The switch to an excessive-risk regime occurs if the L-type's IC constraint is violated at the first-best contract offer, i.e., if `f(R^{*}) u^{H*} > u^{L*}`.\n\n### Question\n\nAssume the system is in the excessive-risk regime, where banks must distort the H-type contract by setting `r^H > R*` to satisfy the L-type's incentive compatibility constraint. According to the model, which of the following statements accurately describe the mechanisms and consequences of this situation? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTwo banks compete for High-ability...", "model_answer": "This is a detailed response to the question about ### Background\n\nTwo banks compete for High-ability...", "error": null}}
{"ID": 47, "Question": "### Background\n\nTwo banks compete for High-ability (H) and Low-ability (L) traders. The system is in an equilibrium where banks induce excessive risk-taking (`r^H > R*`) from H-traders as a screening device. A regulator considers two policies to mitigate this risk: a direct cap on the bonus ratio (`\\bar{R}`) and an indirect increase in bank capital requirements (modeled as an increase in the loss parameter `x_{-1}`).\n\n### Data / Model Specification\n\n**Policy 1: Bonus Cap.** A binding cap `\\bar{R}` is imposed, so `r^H = \\bar{R}`. Total welfare is the sum of gross profits:\n\n  \nW = \\phi\\pi^{L}(R^{\\ast}) + (1-\\phi)\\pi^{H}(r^{H}) \\quad \\text{(Eq. 1)}\n \n\nGross profit `\\pi^H(r^H)` is maximized at `r^H = R*`.\n\n**Policy 2: Capital Requirements.** An increase in bank equity `E` is equivalent to increasing the loss parameter `x_{-1}`. The bank's gross profit from a type-`j` trader is `\\pi^j = q_1^j x_1 + q_0^j x_0 - q_{-1}^j x_{-1}`. The partial derivative of profit with respect to this loss is:\n\n  \n\\frac{\\partial \\pi^j}{\\partial x_{-1}} = -q_{-1}^j \\quad \\text{(Eq. 2)}\n \n\nUnder the simplifying assumption that H-type projects are inelastic to the bonus ratio (`\\partial \\pi^H / \\partial r^H = 0`), the equilibrium condition is:\n\n  \n\\left(\\pi^{H}(r^{H}, x_{-1})-\\tau^{H}\\right)f(r^{H}) = \\pi^{L}(R^{*}, x_{-1})-\\tau^{L} \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the model's analysis of these two regulatory policies, which of the following statements are valid conclusions? Select all that apply.", "Answer": ["B", "C", "D"], "qid": "47", "question": "### Background\n\nTwo banks compete for High-ability (H) and Low-ability (L) traders. The system is in an equilibrium where banks induce excessive risk-taking (`r^H > R*`) from H-traders as a screening device. A regulator considers two policies to mitigate this risk: a direct cap on the bonus ratio (`\\bar{R}`) and an indirect increase in bank capital requirements (modeled as an increase in the loss parameter `x_{-1}`).\n\n### Data / Model Specification\n\n**Policy 1: Bonus Cap.** A binding cap `\\bar{R}` is imposed, so `r^H = \\bar{R}`. Total welfare is the sum of gross profits:\n\n  \nW = \\phi\\pi^{L}(R^{\\ast}) + (1-\\phi)\\pi^{H}(r^{H}) \\quad \\text{(Eq. 1)}\n \n\nGross profit `\\pi^H(r^H)` is maximized at `r^H = R*`.\n\n**Policy 2: Capital Requirements.** An increase in bank equity `E` is equivalent to increasing the loss parameter `x_{-1}`. The bank's gross profit from a type-`j` trader is `\\pi^j = q_1^j x_1 + q_0^j x_0 - q_{-1}^j x_{-1}`. The partial derivative of profit with respect to this loss is:\n\n  \n\\frac{\\partial \\pi^j}{\\partial x_{-1}} = -q_{-1}^j \\quad \\text{(Eq. 2)}\n \n\nUnder the simplifying assumption that H-type projects are inelastic to the bonus ratio (`\\partial \\pi^H / \\partial r^H = 0`), the equilibrium condition is:\n\n  \n\\left(\\pi^{H}(r^{H}, x_{-1})-\\tau^{H}\\right)f(r^{H}) = \\pi^{L}(R^{*}, x_{-1})-\\tau^{L} \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the model's analysis of these two regulatory policies, which of the following statements are valid conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTwo banks compete for High-ability...", "model_answer": "This is a detailed response to the question about ### Background\n\nTwo banks compete for High-ability...", "error": null}}
{"ID": 48, "Question": "### Background\n\n**Research Question:** How can experimental designs ensure validity when the underlying behavioral assumptions of rationality are violated? This question explores the methodological foundation of the Conditional Information Lottery (CIL) design.\n\n**Setting / Institutional Environment:** The validity of the CIL design rests on the 'isolation hypothesis'—the assumption that subjects treat each of a series of tasks as if it were the only real one. This hypothesis is guaranteed if subjects' preferences satisfy the independence axiom of Expected Utility Theory (EUT).\n\n---\n\n### Data / Model Specification\n\n**The Isolation Hypothesis:** The rational argument for the isolation hypothesis is represented by the following act/event matrix for a given Task X:\n\n**Table 1. Act/Event Matrix for Task X**\n\n| | Event 1: Task X is Fictional | Event 2: Task X is Real |\n| :--- | :--- | :--- |\n| **Action 1:** Treat task X as real | No consequence | Preferred outcome |\n| **Action 2:** Treat task X as fictional | No consequence | Less preferred outcome |\n\n**A Threat to the Isolation Hypothesis:** Violations of the independence axiom can threaten the isolation hypothesis. Consider a choice between two pairs of prospects:\n\nPair 1: A choice between prospect `J` and prospect `K`.\n  \nJ: (x, p; 0, 1-p) \\quad \\text{vs.} \\quad K: (y, q; 0, 1-q)\n \n\nPair 2: A choice between prospect `M` and prospect `N`.\n  \nM: (x, \\lambda p; 0, 1 - \\lambda p) \\quad \\text{vs.} \\quad N: (y, \\lambda q; 0, 1 - \\lambda q)\n \n(Eq. (1))\n\nwhere `y` is preferred to `x`, `p > q`, and `0 < λ < 1`. The 'common ratio effect' is the empirical finding that subjects often exhibit the preference pattern `J ≻ K` but `N ≻ M`, which violates EUT.\n\n---\n\n### Question\n\nSelect all statements that accurately describe the methodological foundations and potential challenges of the Conditional Information Lottery (CIL) design, based on the provided information.", "Answer": ["A", "C"], "qid": "48", "question": "### Background\n\n**Research Question:** How can experimental designs ensure validity when the underlying behavioral assumptions of rationality are violated? This question explores the methodological foundation of the Conditional Information Lottery (CIL) design.\n\n**Setting / Institutional Environment:** The validity of the CIL design rests on the 'isolation hypothesis'—the assumption that subjects treat each of a series of tasks as if it were the only real one. This hypothesis is guaranteed if subjects' preferences satisfy the independence axiom of Expected Utility Theory (EUT).\n\n---\n\n### Data / Model Specification\n\n**The Isolation Hypothesis:** The rational argument for the isolation hypothesis is represented by the following act/event matrix for a given Task X:\n\n**Table 1. Act/Event Matrix for Task X**\n\n| | Event 1: Task X is Fictional | Event 2: Task X is Real |\n| :--- | :--- | :--- |\n| **Action 1:** Treat task X as real | No consequence | Preferred outcome |\n| **Action 2:** Treat task X as fictional | No consequence | Less preferred outcome |\n\n**A Threat to the Isolation Hypothesis:** Violations of the independence axiom can threaten the isolation hypothesis. Consider a choice between two pairs of prospects:\n\nPair 1: A choice between prospect `J` and prospect `K`.\n  \nJ: (x, p; 0, 1-p) \\quad \\text{vs.} \\quad K: (y, q; 0, 1-q)\n \n\nPair 2: A choice between prospect `M` and prospect `N`.\n  \nM: (x, \\lambda p; 0, 1 - \\lambda p) \\quad \\text{vs.} \\quad N: (y, \\lambda q; 0, 1 - \\lambda q)\n \n(Eq. (1))\n\nwhere `y` is preferred to `x`, `p > q`, and `0 < λ < 1`. The 'common ratio effect' is the empirical finding that subjects often exhibit the preference pattern `J ≻ K` but `N ≻ M`, which violates EUT.\n\n---\n\n### Question\n\nSelect all statements that accurately describe the methodological foundations and potential challenges of the Conditional Information Lottery (CIL) design, based on the provided information.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** How can exp...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** How can exp...", "error": null}}
{"ID": 49, "Question": "### Background\n\n**Research Question:** In a sequential public goods game, how does a player's position in the sequence of moves causally affect their contribution level, and what behavioral mechanisms drive this effect?\n\n**Setting / Institutional Environment:** Subjects play a sequential Voluntary Contribution Mechanism (VCM) game in groups of seven. The experiment reveals two key findings: (1) overall cooperation is higher in simultaneous-move games than in sequential-move games, and (2) within sequential games, there is a 'diminution effect' where contributions are highest for the first mover and decline for later positions.\n\n**Variables & Parameters.**\n- **First Mover:** The player in position 1, who acts with no information about others' choices. Mean contribution was 4.9 tokens.\n- **Subsequent Movers:** Players in positions 2-7. Their overall mean contribution was 2.1 tokens.\n\n---\n\n### Data / Model Specification\n\nThe payoff function for individual `i` in a group of `n=7` is given by:\n  \nC_{i} = 10 - w_{i} + \\frac{2 \\sum_{h=1}^{n} w_{h}}{n}\n \n(Eq. (1))\n\nwhere `w_i` is player `i`'s contribution from a 10-token endowment.\n\n---\n\n### Question\n\nBased on the provided model and experimental findings, select all statements that correctly analyze the behavior of first movers in the sequential game.", "Answer": ["B", "C"], "qid": "49", "question": "### Background\n\n**Research Question:** In a sequential public goods game, how does a player's position in the sequence of moves causally affect their contribution level, and what behavioral mechanisms drive this effect?\n\n**Setting / Institutional Environment:** Subjects play a sequential Voluntary Contribution Mechanism (VCM) game in groups of seven. The experiment reveals two key findings: (1) overall cooperation is higher in simultaneous-move games than in sequential-move games, and (2) within sequential games, there is a 'diminution effect' where contributions are highest for the first mover and decline for later positions.\n\n**Variables & Parameters.**\n- **First Mover:** The player in position 1, who acts with no information about others' choices. Mean contribution was 4.9 tokens.\n- **Subsequent Movers:** Players in positions 2-7. Their overall mean contribution was 2.1 tokens.\n\n---\n\n### Data / Model Specification\n\nThe payoff function for individual `i` in a group of `n=7` is given by:\n  \nC_{i} = 10 - w_{i} + \\frac{2 \\sum_{h=1}^{n} w_{h}}{n}\n \n(Eq. (1))\n\nwhere `w_i` is player `i`'s contribution from a 10-token endowment.\n\n---\n\n### Question\n\nBased on the provided model and experimental findings, select all statements that correctly analyze the behavior of first movers in the sequential game.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** In a sequen...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** In a sequen...", "error": null}}
{"ID": 50, "Question": "### Background\n\n**Research Question:** How do individual incentives conflict with group welfare in a public goods game with a Voluntary Contribution Mechanism (VCM)?\n\n**Setting / Institutional Environment:** Subjects participate in a VCM public good experiment. Each subject is part of a group of `n=7` and must decide how to allocate a 10-token endowment between a private account (which they keep) and a public account (which benefits all group members).\n\n---\n\n### Data / Model Specification\n\nThe payoff function for individual `i` is given by:\n  \nC_{i} = 10 - w_{i} + \\frac{2 \\sum_{h=1}^{n} w_{h}}{n}\n \n(Eq. (1))\n\nwhere `w_i` is individual `i`'s contribution to the public good (`w_i ∈ [0, 10]`) and `n=7`.\n\n---\n\n### Question\n\nBased on a formal analysis of the payoff function provided in Eq. (1) for a group of `n=7`, select all statements that are mathematically correct.", "Answer": ["A", "D"], "qid": "50", "question": "### Background\n\n**Research Question:** How do individual incentives conflict with group welfare in a public goods game with a Voluntary Contribution Mechanism (VCM)?\n\n**Setting / Institutional Environment:** Subjects participate in a VCM public good experiment. Each subject is part of a group of `n=7` and must decide how to allocate a 10-token endowment between a private account (which they keep) and a public account (which benefits all group members).\n\n---\n\n### Data / Model Specification\n\nThe payoff function for individual `i` is given by:\n  \nC_{i} = 10 - w_{i} + \\frac{2 \\sum_{h=1}^{n} w_{h}}{n}\n \n(Eq. (1))\n\nwhere `w_i` is individual `i`'s contribution to the public good (`w_i ∈ [0, 10]`) and `n=7`.\n\n---\n\n### Question\n\nBased on a formal analysis of the payoff function provided in Eq. (1) for a group of `n=7`, select all statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** How do indi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** How do indi...", "error": null}}
{"ID": 51, "Question": "### Background\n\n**Research Question.** This problem interrogates the causal relationship between the size of a country's SME sector and its economic growth by comparing Ordinary Least Squares (OLS) and Instrumental Variable (IV) estimates from a cross-country growth regression for the 1990s.\n\n**Variables & Parameters.**\n- `GDP per capita growth`: The dependent variable, averaged over 1990-2000.\n- `SME250`: The key independent variable, representing the share of SME employment in manufacturing.\n- Unit of observation: Country `i`.\n\n---\n\n### Data / Model Specification\n\nThe estimated growth regression is:\n  \n\\text{GDP per capita growth}_i = \\beta_0 + \\beta_1 \\mathrm{SME250}_{i} + \\text{Controls}_i + \\varepsilon_{i} \n \n\n**Table 1: SME Employment and Growth Regressions**\n(Corresponds to Table 5 in the source)\n\n| | (1) OLS | (3) IV |\n| :--- | :---: | :---: |\n| **Outliers** | **Full Sample** | **Full Sample** |\n| `SME250` | 2.197*** | 1.863* |\n| | (0.687) | (1.047) |\n| **IV Diagnostics** | | |\n| F-Test (p-value) | | 0.000 |\n| OIR Test (p-value) | | 0.118 |\n| Observations | 45 | 45 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01, * p<0.10. F-test is for the joint significance of excluded instruments in the first stage. OIR is the Hansen test of overidentifying restrictions.* \n\n---\n\nBased on the regression results and the paper's discussion, which of the following statements are valid interpretations of the findings?\n\nSelect all that apply.", "Answer": ["A", "B", "D"], "qid": "51", "question": "### Background\n\n**Research Question.** This problem interrogates the causal relationship between the size of a country's SME sector and its economic growth by comparing Ordinary Least Squares (OLS) and Instrumental Variable (IV) estimates from a cross-country growth regression for the 1990s.\n\n**Variables & Parameters.**\n- `GDP per capita growth`: The dependent variable, averaged over 1990-2000.\n- `SME250`: The key independent variable, representing the share of SME employment in manufacturing.\n- Unit of observation: Country `i`.\n\n---\n\n### Data / Model Specification\n\nThe estimated growth regression is:\n  \n\\text{GDP per capita growth}_i = \\beta_0 + \\beta_1 \\mathrm{SME250}_{i} + \\text{Controls}_i + \\varepsilon_{i} \n \n\n**Table 1: SME Employment and Growth Regressions**\n(Corresponds to Table 5 in the source)\n\n| | (1) OLS | (3) IV |\n| :--- | :---: | :---: |\n| **Outliers** | **Full Sample** | **Full Sample** |\n| `SME250` | 2.197*** | 1.863* |\n| | (0.687) | (1.047) |\n| **IV Diagnostics** | | |\n| F-Test (p-value) | | 0.000 |\n| OIR Test (p-value) | | 0.118 |\n| Observations | 45 | 45 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01, * p<0.10. F-test is for the joint significance of excluded instruments in the first stage. OIR is the Hansen test of overidentifying restrictions.* \n\n---\n\nBased on the regression results and the paper's discussion, which of the following statements are valid interpretations of the findings?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 52, "Question": "### Background\n\n**Research Question.** This problem assesses whether the size of a country's SME sector has a differential impact on the income growth of the poorest quintile of its population, beyond any effect operating through overall economic growth.\n\n**Variables & Parameters.**\n- `g_p`: Average annual growth rate of real GDP per capita for the lowest income quintile.\n- `g_y`: Average annual growth rate of real GDP per capita for the entire population.\n- `SME250`: The share of SME employment in manufacturing.\n\n---\n\n### Data / Model Specification\n\nThe model for the income growth of the poor is:\n  \n g_p = \\alpha y_{i,l,1990} + \\beta g_y + \\gamma \\mathrm{SME250}_{i} + \\varepsilon_{i} \n \n\n**Table 1: Regression Results for Income Growth of the Poor**\n(Corresponds to Table 8, Column 1 in the source)\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| GDP per capita Growth (`g_y`) | 1.169*** | (0.336) |\n| SME250 | 0.006 | (0.023) |\n\nThe paper's Instrumental Variable (IV) analysis of the SME-growth relationship finds that the causal effect of `SME250` on `g_y` is statistically insignificant.\n\n---\n\nBased on the model and results, which of the following conclusions about the total causal effect of `SME250` on the income growth of the poor (`g_p`) are supported by the paper's findings?\n\nSelect all that apply.", "Answer": ["A", "C", "D"], "qid": "52", "question": "### Background\n\n**Research Question.** This problem assesses whether the size of a country's SME sector has a differential impact on the income growth of the poorest quintile of its population, beyond any effect operating through overall economic growth.\n\n**Variables & Parameters.**\n- `g_p`: Average annual growth rate of real GDP per capita for the lowest income quintile.\n- `g_y`: Average annual growth rate of real GDP per capita for the entire population.\n- `SME250`: The share of SME employment in manufacturing.\n\n---\n\n### Data / Model Specification\n\nThe model for the income growth of the poor is:\n  \n g_p = \\alpha y_{i,l,1990} + \\beta g_y + \\gamma \\mathrm{SME250}_{i} + \\varepsilon_{i} \n \n\n**Table 1: Regression Results for Income Growth of the Poor**\n(Corresponds to Table 8, Column 1 in the source)\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| GDP per capita Growth (`g_y`) | 1.169*** | (0.336) |\n| SME250 | 0.006 | (0.023) |\n\nThe paper's Instrumental Variable (IV) analysis of the SME-growth relationship finds that the causal effect of `SME250` on `g_y` is statistically insignificant.\n\n---\n\nBased on the model and results, which of the following conclusions about the total causal effect of `SME250` on the income growth of the poor (`g_p`) are supported by the paper's findings?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 53, "Question": "### Background\n\n**Research Question.** To identify the causal effect of the SME sector on economic growth, the paper employs an instrumental variables (IV) strategy. The validity of this strategy rests on two key assumptions: instrument relevance and the exclusion restriction.\n\n**Variables & Parameters.**\n- `SME250`: The endogenous regressor (share of SME employment).\n- `Growth`: The outcome variable (GDP per capita growth).\n- `Z`: A vector of instrumental variables, including `Ethnic fractionalization` and regional/transition dummies.\n- `ε`: The error term in the growth regression.\n\n---\n\n### Data / Model Specification\n\nThe two key assumptions for IV validity are:\n1.  **Relevance:** `Cov(Z, SME250) ≠ 0`\n2.  **Exclusion Restriction:** `Cov(Z, ε) = 0` (The instruments affect growth *only* through their effect on `SME250` and other controls).\n\nThe paper argues that the instruments are valid because they are deep-rooted national characteristics that shape the business environment, which in turn influences firm size distribution.\n\n---\n\nWhich of the following statements represent valid critiques or considerations regarding the IV strategy used in the paper?\n\nSelect all that apply.", "Answer": ["A", "C", "D"], "qid": "53", "question": "### Background\n\n**Research Question.** To identify the causal effect of the SME sector on economic growth, the paper employs an instrumental variables (IV) strategy. The validity of this strategy rests on two key assumptions: instrument relevance and the exclusion restriction.\n\n**Variables & Parameters.**\n- `SME250`: The endogenous regressor (share of SME employment).\n- `Growth`: The outcome variable (GDP per capita growth).\n- `Z`: A vector of instrumental variables, including `Ethnic fractionalization` and regional/transition dummies.\n- `ε`: The error term in the growth regression.\n\n---\n\n### Data / Model Specification\n\nThe two key assumptions for IV validity are:\n1.  **Relevance:** `Cov(Z, SME250) ≠ 0`\n2.  **Exclusion Restriction:** `Cov(Z, ε) = 0` (The instruments affect growth *only* through their effect on `SME250` and other controls).\n\nThe paper argues that the instruments are valid because they are deep-rooted national characteristics that shape the business environment, which in turn influences firm size distribution.\n\n---\n\nWhich of the following statements represent valid critiques or considerations regarding the IV strategy used in the paper?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** To identify...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** To identify...", "error": null}}
{"ID": 54, "Question": "### Background\n\n**Research Question.** This problem investigates how uncertainty in knowledge spillovers affects a household's optimal investment decision, with a focus on how this effect is mediated by preferences.\n\n**Setting.** The analysis is based on a Romer-style endogenous growth model with learning-by-doing and knowledge spillovers. A representative agent with a Constant Elasticity of Substitution (CES) utility function chooses consumption and investment to maximize expected lifetime utility. The productivity of knowledge spillovers is stochastic.\n\n### Data / Model Specification\n\nThe agent's preferences are represented by the CES utility function:\n  \nU(c) = \\frac{c^{1-\\theta}}{1-\\theta} \\quad \\text{(Eq. 1)}\n \nwhere `θ ≥ 0` is the coefficient of relative risk aversion.\n\nThe optimal investment policy is linear in output, `x_t = S \\cdot y_t`, where the investment share `S` is:\n  \nS = \\left\\{ \\delta\\beta\\eta^{1-\\theta} E\\left[ (\\gamma_t L)^{(1-\\theta)(1-\\beta)} \\right] \\right\\}^{1/\\theta} \\quad \\text{(Eq. 2)}\n \nwhere `γ` is a random productivity shock, `δ` is the discount factor, `L` is the constant labor force, and `β ∈ (0,1)`.\n\n### Question\n\nAccording to the model, which of the following statements correctly describe how an increase in the volatility of the productivity shock `γ` (i.e., a mean-preserving spread) affects the optimal investment share `S`?", "Answer": ["A", "D"], "qid": "54", "question": "### Background\n\n**Research Question.** This problem investigates how uncertainty in knowledge spillovers affects a household's optimal investment decision, with a focus on how this effect is mediated by preferences.\n\n**Setting.** The analysis is based on a Romer-style endogenous growth model with learning-by-doing and knowledge spillovers. A representative agent with a Constant Elasticity of Substitution (CES) utility function chooses consumption and investment to maximize expected lifetime utility. The productivity of knowledge spillovers is stochastic.\n\n### Data / Model Specification\n\nThe agent's preferences are represented by the CES utility function:\n  \nU(c) = \\frac{c^{1-\\theta}}{1-\\theta} \\quad \\text{(Eq. 1)}\n \nwhere `θ ≥ 0` is the coefficient of relative risk aversion.\n\nThe optimal investment policy is linear in output, `x_t = S \\cdot y_t`, where the investment share `S` is:\n  \nS = \\left\\{ \\delta\\beta\\eta^{1-\\theta} E\\left[ (\\gamma_t L)^{(1-\\theta)(1-\\beta)} \\right] \\right\\}^{1/\\theta} \\quad \\text{(Eq. 2)}\n \nwhere `γ` is a random productivity shock, `δ` is the discount factor, `L` is the constant labor force, and `β ∈ (0,1)`.\n\n### Question\n\nAccording to the model, which of the following statements correctly describe how an increase in the volatility of the productivity shock `γ` (i.e., a mean-preserving spread) affects the optimal investment share `S`?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 55, "Question": "### Background\n\n**Research Question.** This problem traces the causal chain from increased uncertainty in human capital accumulation to its effect on agent behavior and the long-run growth rate of the economy.\n\n**Setting.** The analysis is based on a discrete-time Lucas-style model of endogenous growth. The representative agent has logarithmic utility, `U(c) = ln(c)`, and faces uncertainty about the productivity of time spent accumulating human capital.\n\n### Data / Model Specification\n\nThe technology for goods production and the law of motion for human capital are:\n  \ny_{t+1} = \\eta x_t^\\beta (s_t h_t)^{1-\\beta} \\quad \\text{(Eq. 1)}\n \n  \nh_{t+1} = [1+\\gamma_t(1-s_t)-\\zeta]h_t \\quad \\text{(Eq. 2)}\n \nwhere `s_t` is the fraction of time in production, `h_t` is human capital, `x_t` is physical investment, `γ_t` is a random productivity shock, and `ζ` is the depreciation rate.\n\nThe agent's optimal fraction of time devoted to production, `s̄`, is constant and implicitly defined by the first-order condition:\n  \n\\frac{1}{\\delta} = E\\left[\\frac{1+\\gamma-\\zeta}{1+\\gamma(1-\\bar{s})-\\zeta}\\right] \\quad \\text{(Eq. 3)}\n \nwhere `δ` is the discount factor. The optimal investment policy is `x_t = δβy_t`.\n\n### Question\n\nAccording to the model, which of the following statements are correct consequences of an increase in the volatility of the human capital productivity shock `γ` (i.e., a mean-preserving spread)?", "Answer": ["B", "D"], "qid": "55", "question": "### Background\n\n**Research Question.** This problem traces the causal chain from increased uncertainty in human capital accumulation to its effect on agent behavior and the long-run growth rate of the economy.\n\n**Setting.** The analysis is based on a discrete-time Lucas-style model of endogenous growth. The representative agent has logarithmic utility, `U(c) = ln(c)`, and faces uncertainty about the productivity of time spent accumulating human capital.\n\n### Data / Model Specification\n\nThe technology for goods production and the law of motion for human capital are:\n  \ny_{t+1} = \\eta x_t^\\beta (s_t h_t)^{1-\\beta} \\quad \\text{(Eq. 1)}\n \n  \nh_{t+1} = [1+\\gamma_t(1-s_t)-\\zeta]h_t \\quad \\text{(Eq. 2)}\n \nwhere `s_t` is the fraction of time in production, `h_t` is human capital, `x_t` is physical investment, `γ_t` is a random productivity shock, and `ζ` is the depreciation rate.\n\nThe agent's optimal fraction of time devoted to production, `s̄`, is constant and implicitly defined by the first-order condition:\n  \n\\frac{1}{\\delta} = E\\left[\\frac{1+\\gamma-\\zeta}{1+\\gamma(1-\\bar{s})-\\zeta}\\right] \\quad \\text{(Eq. 3)}\n \nwhere `δ` is the discount factor. The optimal investment policy is `x_t = δβy_t`.\n\n### Question\n\nAccording to the model, which of the following statements are correct consequences of an increase in the volatility of the human capital productivity shock `γ` (i.e., a mean-preserving spread)?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 56, "Question": "### Background\n\nIn a duopoly market for a homogeneous good, consumers endogenously sort into active searchers (fraction `q`) and passive waiters. Active searchers pay a cost `s` per price quote and use a reservation price `r`. Passive waiters receive information from their social network of `k` friends and discount future consumption by `δ`. Firms engage in mixed-strategy pricing.\n\n### Data / Model Specification\n\nThe consumer's indifference condition between searching and waiting, which defines the equilibrium fraction of searchers `q`, is given by:\n\n  \nv-E[p]-s = \\delta\\bigl(1-(1-q)^{k}\\bigr)(v-E[p]) +\\delta\\biggl(1+(1-q)^{k}-2\\Bigl(1-\\frac{q}{2}\\Bigr)^{k}\\biggr)(E[p]-E[\\operatorname*{min}\\{p_{1},p_{2}\\}]) \\quad \\text{(Eq. (1))}\n \n\nwhere `v` is willingness to pay, `E[p]` is the expected price, and `E[min{p1,p2}]` is the expected minimum price.\n\nTheorem 1 of the paper establishes that as the search cost `s` approaches zero, the equilibrium fraction of active searchers `q*` approaches 1, and prices converge to the monopoly level `v`.\n\n### Question\n\nGiven the model's framework and the result that `q* → 1` as `s → 0`, which of the following statements accurately describe the causal mechanisms that lead to monopoly pricing (`E[p] → v`) in this limit? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "56", "question": "### Background\n\nIn a duopoly market for a homogeneous good, consumers endogenously sort into active searchers (fraction `q`) and passive waiters. Active searchers pay a cost `s` per price quote and use a reservation price `r`. Passive waiters receive information from their social network of `k` friends and discount future consumption by `δ`. Firms engage in mixed-strategy pricing.\n\n### Data / Model Specification\n\nThe consumer's indifference condition between searching and waiting, which defines the equilibrium fraction of searchers `q`, is given by:\n\n  \nv-E[p]-s = \\delta\\bigl(1-(1-q)^{k}\\bigr)(v-E[p]) +\\delta\\biggl(1+(1-q)^{k}-2\\Bigl(1-\\frac{q}{2}\\Bigr)^{k}\\biggr)(E[p]-E[\\operatorname*{min}\\{p_{1},p_{2}\\}]) \\quad \\text{(Eq. (1))}\n \n\nwhere `v` is willingness to pay, `E[p]` is the expected price, and `E[min{p1,p2}]` is the expected minimum price.\n\nTheorem 1 of the paper establishes that as the search cost `s` approaches zero, the equilibrium fraction of active searchers `q*` approaches 1, and prices converge to the monopoly level `v`.\n\n### Question\n\nGiven the model's framework and the result that `q* → 1` as `s → 0`, which of the following statements accurately describe the causal mechanisms that lead to monopoly pricing (`E[p] → v`) in this limit? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn a duopoly market for a homogene...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn a duopoly market for a homogene...", "error": null}}
{"ID": 57, "Question": "### Background\n\nIn a duopoly market, consumers are situated in social networks where each person has `k` links. Consumers endogenously choose to be active searchers (fraction `q`) or passive waiters. Firms' incentive to lower prices depends on the ratio (`η`) of non-price-comparing to price-comparing consumers. Price dispersion persists as long as `η` is a finite, positive number.\n\n### Data / Model Specification\n\nThe ratio `η` is given by:\n\n  \n\\eta = \\frac{q/2 + \\delta\\mu_1}{\\delta\\mu_2}\n \n\nwhere `δ` is the discount factor for passive consumers, and `μ_1` and `μ_2` are the fractions of passive consumers who see only one price or both prices, respectively. These fractions are functions of `q` and `k`:\n\n- `μ_1 = (1-q)[(1-q/2)^k - (1-q)^k]`\n- `μ_2 = (1-q)[1 + (1-q)^k - 2(1-q/2)^k]`\n\nAn equilibrium with positive sales requires that the fraction of searchers `q` is between 0 and 1, which is sustained by a positive search cost, `s > 0`.\n\n### Question\n\nConsider the limiting case where networks become infinitely dense (`k → ∞`) while the equilibrium fraction of searchers `q` remains fixed in the interval `(0, 1)`. Which of the following statements are valid conclusions for this scenario? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "57", "question": "### Background\n\nIn a duopoly market, consumers are situated in social networks where each person has `k` links. Consumers endogenously choose to be active searchers (fraction `q`) or passive waiters. Firms' incentive to lower prices depends on the ratio (`η`) of non-price-comparing to price-comparing consumers. Price dispersion persists as long as `η` is a finite, positive number.\n\n### Data / Model Specification\n\nThe ratio `η` is given by:\n\n  \n\\eta = \\frac{q/2 + \\delta\\mu_1}{\\delta\\mu_2}\n \n\nwhere `δ` is the discount factor for passive consumers, and `μ_1` and `μ_2` are the fractions of passive consumers who see only one price or both prices, respectively. These fractions are functions of `q` and `k`:\n\n- `μ_1 = (1-q)[(1-q/2)^k - (1-q)^k]`\n- `μ_2 = (1-q)[1 + (1-q)^k - 2(1-q/2)^k]`\n\nAn equilibrium with positive sales requires that the fraction of searchers `q` is between 0 and 1, which is sustained by a positive search cost, `s > 0`.\n\n### Question\n\nConsider the limiting case where networks become infinitely dense (`k → ∞`) while the equilibrium fraction of searchers `q` remains fixed in the interval `(0, 1)`. Which of the following statements are valid conclusions for this scenario? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn a duopoly market, consumers are...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn a duopoly market, consumers are...", "error": null}}
{"ID": 58, "Question": "### Background\n\nConsider a market with consumers organized in \"star networks.\" Each network has one \"core\" consumer with `k` links and `k` \"periphery\" consumers with one link each. All consumers have the same search cost `s`, willingness to pay `v`, and discount factor `δ`.\n\n### Data / Model Specification\n\nThe payoff from actively searching is `v - E[p] - s` for any consumer. The payoff from passively waiting depends on network position. For a periphery consumer, the waiting payoff is `δq_c(v-E[p])`, as they only get information if the core searches (with probability `q_c`). For the core consumer, the waiting payoff is higher because they can potentially receive information from all `k` periphery members.\n\nAn equilibrium where `0 < q_c = q_p < 1` (where `q_c` and `q_p` are the search probabilities for core and periphery consumers, respectively) is proposed.\n\n### Question\n\nWhich of the following statements are valid reasons why an equilibrium where both core and periphery consumers randomize their search decision (`0 < q_c = q_p < 1`) is IMPOSSIBLE in this model? Select all that apply.", "Answer": ["A", "B"], "qid": "58", "question": "### Background\n\nConsider a market with consumers organized in \"star networks.\" Each network has one \"core\" consumer with `k` links and `k` \"periphery\" consumers with one link each. All consumers have the same search cost `s`, willingness to pay `v`, and discount factor `δ`.\n\n### Data / Model Specification\n\nThe payoff from actively searching is `v - E[p] - s` for any consumer. The payoff from passively waiting depends on network position. For a periphery consumer, the waiting payoff is `δq_c(v-E[p])`, as they only get information if the core searches (with probability `q_c`). For the core consumer, the waiting payoff is higher because they can potentially receive information from all `k` periphery members.\n\nAn equilibrium where `0 < q_c = q_p < 1` (where `q_c` and `q_p` are the search probabilities for core and periphery consumers, respectively) is proposed.\n\n### Question\n\nWhich of the following statements are valid reasons why an equilibrium where both core and periphery consumers randomize their search decision (`0 < q_c = q_p < 1`) is IMPOSSIBLE in this model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nConsider a market with consumers o...", "model_answer": "This is a detailed response to the question about ### Background\n\nConsider a market with consumers o...", "error": null}}
{"ID": 59, "Question": "### Background\n\n**Research Question.** This problem explores the conceptual role of knowing the true Data Generating Process (DGP) in labor economics, specifically how it allows a researcher to distinguish between genuine discrimination and econometric specification error.\n\n**Setting / Institutional Environment.** The setting is a school district where teacher salaries are determined by a rigid, publicly known, two-dimensional grid. A teacher's salary is a deterministic, non-linear function of their contractually-defined experience units ($x^*$) and education units ($s^*$). Demographic characteristics like race and gender play no formal role in the salary formula.\n\n---\n\n### Data / Model Specification\n\nThe true Data Generating Process (DGP) for salary is the known contractual grid:\n\n  \n\\text{Salary}_i = g(x_i^*, s_i^*) \\quad \\text{(Eq. (1))}\n \n\nThis is a deterministic function. In contrast, an econometrician using survey data estimates a statistical model that includes demographics ($D_i$) and proxies for human capital ($H_i$, representing survey-based experience and education):\n\n  \n\\text{Salary}_i = f(H_i, D_i) + \\varepsilon_i \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Stylized San Francisco Salary Schedule ($)**\n| Experience ($x^*$) | Education ($s^*=1$) | Education ($s^*=3$) |\n| :--- | :--- | :--- |\n| 1 | 20,265 | 20,265 |\n| 5 | 20,265 | 24,055 |\n| 10 | 24,055 | 29,730 |\n| 15 | | 33,135 |\n\n---\n\n### Question\n\nIn the institutional context described, an econometrician estimates Eq. (2) and finds a statistically significant, negative coefficient on a `Black` dummy variable. According to the paper's central argument, which of the following statements are valid interpretations of this finding? Select all that apply.", "Answer": ["C", "D"], "qid": "59", "question": "### Background\n\n**Research Question.** This problem explores the conceptual role of knowing the true Data Generating Process (DGP) in labor economics, specifically how it allows a researcher to distinguish between genuine discrimination and econometric specification error.\n\n**Setting / Institutional Environment.** The setting is a school district where teacher salaries are determined by a rigid, publicly known, two-dimensional grid. A teacher's salary is a deterministic, non-linear function of their contractually-defined experience units ($x^*$) and education units ($s^*$). Demographic characteristics like race and gender play no formal role in the salary formula.\n\n---\n\n### Data / Model Specification\n\nThe true Data Generating Process (DGP) for salary is the known contractual grid:\n\n  \n\\text{Salary}_i = g(x_i^*, s_i^*) \\quad \\text{(Eq. (1))}\n \n\nThis is a deterministic function. In contrast, an econometrician using survey data estimates a statistical model that includes demographics ($D_i$) and proxies for human capital ($H_i$, representing survey-based experience and education):\n\n  \n\\text{Salary}_i = f(H_i, D_i) + \\varepsilon_i \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Stylized San Francisco Salary Schedule ($)**\n| Experience ($x^*$) | Education ($s^*=1$) | Education ($s^*=3$) |\n| :--- | :--- | :--- |\n| 1 | 20,265 | 20,265 |\n| 5 | 20,265 | 24,055 |\n| 10 | 24,055 | 29,730 |\n| 15 | | 33,135 |\n\n---\n\n### Question\n\nIn the institutional context described, an econometrician estimates Eq. (2) and finds a statistically significant, negative coefficient on a `Black` dummy variable. According to the paper's central argument, which of the following statements are valid interpretations of this finding? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 60, "Question": "### Background\n\n**Research Question.** This problem explores how non-random measurement and proxy errors can create the appearance of wage discrimination.\n\n**Setting / Institutional Environment.** In a school district, teacher salaries are determined by a contractual grid based on true experience ($x^*$) and education ($s^*$). An econometrician estimates the following model using survey data:\n\n  \n\\text{Salary}_i = \\beta_0 + \\beta_1 H_i + \\gamma_M \\text{Male}_i + \\gamma_B \\text{Black}_i + \\varepsilon_i\n \n\nwhere $H_i$ represents survey-based measures of human capital (experience $x$ and education $s$).\n\n**Key Empirical Findings.** Subsequent analysis in the paper reveals two key patterns of reporting error:\n1.  **Experience Reporting:** Female teachers are significantly more likely than male teachers to have a reported experience level ($x$) that is greater than their true contractual experience level ($x^*$).\n2.  **Education Proxy:** For a given reported education level ($s$), Black teachers are significantly less likely than white teachers to be in the highest contractually-rewarded education category ($s^*$).\n\n---\n\n### Question\n\nGiven the key empirical findings on reporting errors, and assuming the true return to human capital is positive (β₁ > 0), which of the following statements correctly describe the econometric bias in the estimated coefficients for `Male` (γ_M) and `Black` (γ_B)? Select all that apply.", "Answer": ["B", "C"], "qid": "60", "question": "### Background\n\n**Research Question.** This problem explores how non-random measurement and proxy errors can create the appearance of wage discrimination.\n\n**Setting / Institutional Environment.** In a school district, teacher salaries are determined by a contractual grid based on true experience ($x^*$) and education ($s^*$). An econometrician estimates the following model using survey data:\n\n  \n\\text{Salary}_i = \\beta_0 + \\beta_1 H_i + \\gamma_M \\text{Male}_i + \\gamma_B \\text{Black}_i + \\varepsilon_i\n \n\nwhere $H_i$ represents survey-based measures of human capital (experience $x$ and education $s$).\n\n**Key Empirical Findings.** Subsequent analysis in the paper reveals two key patterns of reporting error:\n1.  **Experience Reporting:** Female teachers are significantly more likely than male teachers to have a reported experience level ($x$) that is greater than their true contractual experience level ($x^*$).\n2.  **Education Proxy:** For a given reported education level ($s$), Black teachers are significantly less likely than white teachers to be in the highest contractually-rewarded education category ($s^*$).\n\n---\n\n### Question\n\nGiven the key empirical findings on reporting errors, and assuming the true return to human capital is positive (β₁ > 0), which of the following statements correctly describe the econometric bias in the estimated coefficients for `Male` (γ_M) and `Black` (γ_B)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 61, "Question": "### Background\n\n**Research Question.** This problem examines the core identification strategy of a model designed to estimate the prevalence of three unobserved wage-setting regimes—fully flexible, downwardly real rigid, and downwardly nominal rigid—using only the aggregate distribution of observed wage changes.\n\n**Setting and Sample.** The analysis uses a large panel of individual wage changes for job stayers in West Germany. The observed distribution of wage changes is not normal; it exhibits distinctive features, including a prominent spike at exactly zero and a visible asymmetry around the mode (fewer observations just below the mode than above it).\n\n### Data / Model Specification\n\nThe model assumes that the observed distribution of wage changes is a mixture of three components, all derived from a single underlying, unobserved 'notional' wage change distribution, which is assumed to be normal conditional on worker characteristics.\n1.  **Fully Flexible Regime:** The observed wage change equals the notional wage change.\n2.  **Nominal Rigidity Regime:** The observed wage change is censored from below at zero. If the notional change is negative, the observed change is zero.\n3.  **Real Rigidity Regime:** The observed wage change is censored from below at a positive threshold `r`. If the notional change is less than `r`, the observed change is `r`.\n\nThe key identification challenge is to estimate the population shares of these three unobserved regimes and the parameters of the notional distribution from the shape of the observed distribution.\n\n### Question\n\nAccording to the paper's identification strategy, which of the following statements correctly link features of the observed wage change distribution to the model's components? Select all that apply.", "Answer": ["A", "D"], "qid": "61", "question": "### Background\n\n**Research Question.** This problem examines the core identification strategy of a model designed to estimate the prevalence of three unobserved wage-setting regimes—fully flexible, downwardly real rigid, and downwardly nominal rigid—using only the aggregate distribution of observed wage changes.\n\n**Setting and Sample.** The analysis uses a large panel of individual wage changes for job stayers in West Germany. The observed distribution of wage changes is not normal; it exhibits distinctive features, including a prominent spike at exactly zero and a visible asymmetry around the mode (fewer observations just below the mode than above it).\n\n### Data / Model Specification\n\nThe model assumes that the observed distribution of wage changes is a mixture of three components, all derived from a single underlying, unobserved 'notional' wage change distribution, which is assumed to be normal conditional on worker characteristics.\n1.  **Fully Flexible Regime:** The observed wage change equals the notional wage change.\n2.  **Nominal Rigidity Regime:** The observed wage change is censored from below at zero. If the notional change is negative, the observed change is zero.\n3.  **Real Rigidity Regime:** The observed wage change is censored from below at a positive threshold `r`. If the notional change is less than `r`, the observed change is `r`.\n\nThe key identification challenge is to estimate the population shares of these three unobserved regimes and the parameters of the notional distribution from the shape of the observed distribution.\n\n### Question\n\nAccording to the paper's identification strategy, which of the following statements correctly link features of the observed wage change distribution to the model's components? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 62, "Question": "### Background\n\n**Research Question.** This problem examines the core identification strategy of a model designed to estimate the prevalence of three unobserved wage-setting regimes—fully flexible, downwardly real rigid, and downwardly nominal rigid—using only the aggregate distribution of observed wage changes.\n\n**Setting and Sample.** The analysis uses a large panel of individual wage changes for job stayers in West Germany. The observed distribution of wage changes is not normal; it exhibits distinctive features, including a prominent spike at exactly zero and a visible asymmetry around the mode (fewer observations just below the mode than above it).\n\n### Data / Model Specification\n\nThe model assumes that the observed distribution of wage changes is a mixture of three components, all derived from a single underlying, unobserved 'notional' wage change distribution, which is assumed to be normal conditional on worker characteristics.\n1.  **Fully Flexible Regime:** The observed wage change equals the notional wage change.\n2.  **Nominal Rigidity Regime:** The observed wage change is censored from below at zero. If the notional change is negative, the observed change is zero.\n3.  **Real Rigidity Regime:** The observed wage change is censored from below at a positive threshold `r`. If the notional change is less than `r`, the observed change is `r`.\n\nThe key identification challenge is to estimate the population shares of these three unobserved regimes and the parameters of the notional distribution from the shape of the observed distribution.\n\n### Question\n\nBased on the paper's discussion of its methodology and potential limitations, select all statements that are correct.", "Answer": ["C", "D"], "qid": "62", "question": "### Background\n\n**Research Question.** This problem examines the core identification strategy of a model designed to estimate the prevalence of three unobserved wage-setting regimes—fully flexible, downwardly real rigid, and downwardly nominal rigid—using only the aggregate distribution of observed wage changes.\n\n**Setting and Sample.** The analysis uses a large panel of individual wage changes for job stayers in West Germany. The observed distribution of wage changes is not normal; it exhibits distinctive features, including a prominent spike at exactly zero and a visible asymmetry around the mode (fewer observations just below the mode than above it).\n\n### Data / Model Specification\n\nThe model assumes that the observed distribution of wage changes is a mixture of three components, all derived from a single underlying, unobserved 'notional' wage change distribution, which is assumed to be normal conditional on worker characteristics.\n1.  **Fully Flexible Regime:** The observed wage change equals the notional wage change.\n2.  **Nominal Rigidity Regime:** The observed wage change is censored from below at zero. If the notional change is negative, the observed change is zero.\n3.  **Real Rigidity Regime:** The observed wage change is censored from below at a positive threshold `r`. If the notional change is less than `r`, the observed change is `r`.\n\nThe key identification challenge is to estimate the population shares of these three unobserved regimes and the parameters of the notional distribution from the shape of the observed distribution.\n\n### Question\n\nBased on the paper's discussion of its methodology and potential limitations, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 63, "Question": "### Background\n\n**Research Question.** This problem investigates the 'masquerading shocks' phenomenon in sign-identified Structural Vector Autoregressions (SVARs), where combinations of non-policy shocks can mimic the signature of a monetary policy shock, leading to flawed causal inference.\n\n**Setting.** The economy is described by a static, three-equation New Keynesian model. An econometrician attempts to identify a contractionary monetary policy shock by defining it as any shock that causes inflation to fall (or not rise) and the nominal interest rate to rise (or not fall) on impact. The response of output is left unrestricted.\n\n### Data / Model Specification\n\nThe model's closed-form solution for output (`y`), inflation (`π`), and the interest rate (`i`) in response to true demand (`ε^d`), supply (`ε^s`), and monetary policy (`ε^m`) shocks is:\n\n  \n\\begin{pmatrix} y_t \\\\ π_t \\\\ i_t \\end{pmatrix} = \\frac{1}{1+φ_y+φ_πκ} \\begin{pmatrix} σ^d & φ_πσ^s & -σ^m \\\\ κσ^d & -(1+φ_y)σ^s & -κσ^m \\\\ (φ_y+φ_πκ)σ^d & -φ_πσ^s & σ^m \\end{pmatrix} \\begin{pmatrix} ε_t^d \\\\ ε_t^s \\\\ ε_t^m \\end{pmatrix}\n \n\nAn SVAR-identified 'monetary policy shock', `e_t^m`, is a linear combination of the true shocks, `e_t^m = p_{md}ε_t^d + p_{ms}ε_t^s + p_{mm}ε_t^m`, where `p_m = (p_{md}, p_{ms}, p_{mm})'` is a unit-length vector of weights. The impact responses of inflation (`dπ`) and the interest rate (`di`) to `e_t^m` must satisfy the sign restrictions:\n\n  \ndπ = \\frac{1}{1+φ_y+φ_πκ} [p_{md}κσ^d - p_{ms}(1+φ_y)σ^s - p_{mm}κσ^m] \\le 0 \\quad \\text{(Eq. (1))}\n \n  \ndi = \\frac{1}{1+φ_y+φ_πκ} [p_{md}(φ_y+φ_πκ)σ^d - p_{ms}φ_πσ^s + p_{mm}σ^m] \\ge 0 \\quad \\text{(Eq. (2))}\n \n\nAll model parameters (`κ, φ_y, φ_π, σ^d, σ^s, σ^m`) are positive, and `φ_π > 1`.\n\n### Question\n\nConsider a 'pure masquerading shock' where the identified shock `e_t^m` is a combination of only a positive demand shock (`p_{md} > 0`) and a positive supply shock (`p_{ms} > 0`), with zero weight on the true monetary policy shock (`p_{mm} = 0`). According to the model and sign restrictions, which of the following statements are necessarily true for such a shock to be included in the identified set? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "63", "question": "### Background\n\n**Research Question.** This problem investigates the 'masquerading shocks' phenomenon in sign-identified Structural Vector Autoregressions (SVARs), where combinations of non-policy shocks can mimic the signature of a monetary policy shock, leading to flawed causal inference.\n\n**Setting.** The economy is described by a static, three-equation New Keynesian model. An econometrician attempts to identify a contractionary monetary policy shock by defining it as any shock that causes inflation to fall (or not rise) and the nominal interest rate to rise (or not fall) on impact. The response of output is left unrestricted.\n\n### Data / Model Specification\n\nThe model's closed-form solution for output (`y`), inflation (`π`), and the interest rate (`i`) in response to true demand (`ε^d`), supply (`ε^s`), and monetary policy (`ε^m`) shocks is:\n\n  \n\\begin{pmatrix} y_t \\\\ π_t \\\\ i_t \\end{pmatrix} = \\frac{1}{1+φ_y+φ_πκ} \\begin{pmatrix} σ^d & φ_πσ^s & -σ^m \\\\ κσ^d & -(1+φ_y)σ^s & -κσ^m \\\\ (φ_y+φ_πκ)σ^d & -φ_πσ^s & σ^m \\end{pmatrix} \\begin{pmatrix} ε_t^d \\\\ ε_t^s \\\\ ε_t^m \\end{pmatrix}\n \n\nAn SVAR-identified 'monetary policy shock', `e_t^m`, is a linear combination of the true shocks, `e_t^m = p_{md}ε_t^d + p_{ms}ε_t^s + p_{mm}ε_t^m`, where `p_m = (p_{md}, p_{ms}, p_{mm})'` is a unit-length vector of weights. The impact responses of inflation (`dπ`) and the interest rate (`di`) to `e_t^m` must satisfy the sign restrictions:\n\n  \ndπ = \\frac{1}{1+φ_y+φ_πκ} [p_{md}κσ^d - p_{ms}(1+φ_y)σ^s - p_{mm}κσ^m] \\le 0 \\quad \\text{(Eq. (1))}\n \n  \ndi = \\frac{1}{1+φ_y+φ_πκ} [p_{md}(φ_y+φ_πκ)σ^d - p_{ms}φ_πσ^s + p_{mm}σ^m] \\ge 0 \\quad \\text{(Eq. (2))}\n \n\nAll model parameters (`κ, φ_y, φ_π, σ^d, σ^s, σ^m`) are positive, and `φ_π > 1`.\n\n### Question\n\nConsider a 'pure masquerading shock' where the identified shock `e_t^m` is a combination of only a positive demand shock (`p_{md} > 0`) and a positive supply shock (`p_{ms} > 0`), with zero weight on the true monetary policy shock (`p_{mm} = 0`). According to the model and sign restrictions, which of the following statements are necessarily true for such a shock to be included in the identified set? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 64, "Question": "### Background\n\n**Research Question.** This problem investigates methods to resolve the 'masquerading shocks' problem by imposing additional, theory-consistent restrictions on the SVAR identification.\n\n**Setting.** The economy is described by the same static, three-equation New Keynesian model as before. A researcher augments the initial sign restrictions on inflation and interest rates with a new restriction on the implied Taylor rule.\n\n### Data / Model Specification\n\nThe model's closed-form solution and the definition of the identified shock `e_t^m` remain the same. The paper provides the formula for the implied coefficient on output in the misidentified Taylor rule, `\tilde{φ}_y`:\n\n  \n\\tilde{φ}_y = \\frac{p_{mm}φ_y - p_{md} - p_{ms}κ}{p_{md} + p_{mm}} \\quad \\text{(Eq. (1))}\n \n\nAll model parameters (`κ, φ_y, φ_π, σ^d, σ^s, σ^m`) are positive, and `φ_π > 1`. The true Taylor rule has a non-negative coefficient on output, `φ_y ≥ 0`.\n\n### Question\n\nA researcher proposes a new identification scheme that augments the original sign restrictions with the single, theory-consistent restriction that the implied Taylor rule coefficient on output must be non-negative, `\tilde{φ}_y ≥ 0`. Which of the following statements about the properties and power of this new restriction are correct? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "64", "question": "### Background\n\n**Research Question.** This problem investigates methods to resolve the 'masquerading shocks' problem by imposing additional, theory-consistent restrictions on the SVAR identification.\n\n**Setting.** The economy is described by the same static, three-equation New Keynesian model as before. A researcher augments the initial sign restrictions on inflation and interest rates with a new restriction on the implied Taylor rule.\n\n### Data / Model Specification\n\nThe model's closed-form solution and the definition of the identified shock `e_t^m` remain the same. The paper provides the formula for the implied coefficient on output in the misidentified Taylor rule, `\tilde{φ}_y`:\n\n  \n\\tilde{φ}_y = \\frac{p_{mm}φ_y - p_{md} - p_{ms}κ}{p_{md} + p_{mm}} \\quad \\text{(Eq. (1))}\n \n\nAll model parameters (`κ, φ_y, φ_π, σ^d, σ^s, σ^m`) are positive, and `φ_π > 1`. The true Taylor rule has a non-negative coefficient on output, `φ_y ≥ 0`.\n\n### Question\n\nA researcher proposes a new identification scheme that augments the original sign restrictions with the single, theory-consistent restriction that the implied Taylor rule coefficient on output must be non-negative, `\tilde{φ}_y ≥ 0`. Which of the following statements about the properties and power of this new restriction are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 65, "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental limits of SVAR identification imposed by the information content of observable data, a concept known as (non-)invertibility.\n\n**Setting.** Data are generated by a structural model with true shocks `ε_t`. An econometrician uses an SVAR to identify shocks `e_t`. The quality of identification is measured by the weight of an identified shock on a contemporaneous true shock.\n\n### Data / Model Specification\n\n**Proposition 1** from the paper establishes a theoretical upper bound on the weight `P_0(k,j)` of the `k`-th identified shock on the `j`-th contemporaneous true shock:\n\n  \nP_0(k,j) \\le \\sqrt{R_j^2} \\quad \\text{(Eq. (1))}\n \n\nwhere `R_j^2` is the population R-squared from a regression of the true shock `ε_{j,t}` on the infinite history of observables, `{x_τ}_{τ ≤ t}`.\n\nFor an SVAR with a valid external instrument (SVAR-IV), this bound is attained for the monetary policy shock `m`:\n\n  \nP_0(k,m) = \\sqrt{R_m^2} \\quad \\text{(Eq. (2))}\n \n\nFurthermore, the impact impulse response estimates from SVAR-IV are biased, with the probability limit of the estimator (`\\hat{β}_{IV}`) related to the true impact (`β_{true}`) by:\n\n  \n\\text{plim } \\hat{β}_{IV} = \\frac{1}{\\sqrt{R_m^2}} β_{true} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nSuppose the true impact of a contractionary monetary policy shock on output is `β_{true} = -0.4%`. An econometrician uses a 3-variable VAR (`y, π, i`) and a valid external instrument (SVAR-IV). The information content of this VAR is `R_m^2 = 0.64`. Which of the following statements are correct implications of this scenario? Select all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "65", "question": "### Background\n\n**Research Question.** This problem investigates the fundamental limits of SVAR identification imposed by the information content of observable data, a concept known as (non-)invertibility.\n\n**Setting.** Data are generated by a structural model with true shocks `ε_t`. An econometrician uses an SVAR to identify shocks `e_t`. The quality of identification is measured by the weight of an identified shock on a contemporaneous true shock.\n\n### Data / Model Specification\n\n**Proposition 1** from the paper establishes a theoretical upper bound on the weight `P_0(k,j)` of the `k`-th identified shock on the `j`-th contemporaneous true shock:\n\n  \nP_0(k,j) \\le \\sqrt{R_j^2} \\quad \\text{(Eq. (1))}\n \n\nwhere `R_j^2` is the population R-squared from a regression of the true shock `ε_{j,t}` on the infinite history of observables, `{x_τ}_{τ ≤ t}`.\n\nFor an SVAR with a valid external instrument (SVAR-IV), this bound is attained for the monetary policy shock `m`:\n\n  \nP_0(k,m) = \\sqrt{R_m^2} \\quad \\text{(Eq. (2))}\n \n\nFurthermore, the impact impulse response estimates from SVAR-IV are biased, with the probability limit of the estimator (`\\hat{β}_{IV}`) related to the true impact (`β_{true}`) by:\n\n  \n\\text{plim } \\hat{β}_{IV} = \\frac{1}{\\sqrt{R_m^2}} β_{true} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nSuppose the true impact of a contractionary monetary policy shock on output is `β_{true} = -0.4%`. An econometrician uses a 3-variable VAR (`y, π, i`) and a valid external instrument (SVAR-IV). The information content of this VAR is `R_m^2 = 0.64`. Which of the following statements are correct implications of this scenario? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 66, "Question": "### Background\n\n**Research Question.** This problem analyzes the core policy implications of a dynamic model of influencer-follower relationships. It investigates the effects of a simple tax on advertising revenue, contrasts it with different forms of mandatory disclosure regulation, and evaluates a novel \"opt-in\" policy alternative.\n\n**Setting / Institutional Environment.** In a continuous-time principal-agent model, a follower designs a contract to incentivize an influencer. The relationship's value is summarized by a state variable `d`, the expected future duration of following. The optimal contract exhibits a \"reap and sow\" dynamic: for low `d` (sow phase), the influencer is incentivized to provide good advice (`a=0`); for high `d` (reap phase), the influencer is allowed to monetize their reputation (`a=1`). Policy interventions can alter the profitability of advertising, affecting both the temptation to shirk and the value of the ultimate reward.\n\n### Data / Model Specification\n\nThe influencer's choice `a` between providing good advice (`a=0`) and advertising (`a=1`) is governed by an incentive compatibility (IC) constraint. To induce `a=0`, the promised increase in the influencer's continuation value, `W(d^+) - W(d)`, must outweigh the normalized payoff from advertising, which is 1.\n\nWe consider three policy environments:\n1.  **Taxation:** Advertising revenue `λa` is taxed, so the influencer receives `xλa`, where `x ∈ (0, 1]`.\n2.  **Mandatory Disclosure:** The influencer can use disclosed ads (`a_r`) with return `r` or undisclosed ads (`a_u`) with return `u`. A \"weak\" policy has `u > r` (undisclosed is more profitable), while a \"strong\" policy has `u < r`.\n3.  **Opt-In Disclosure:** Influencers can choose to be subject to the disclosure regime. Followers demand they opt-in during the \"sow\" phase (`d ≤ d̂`) and allow them to opt-out during the \"reap\" phase (`d > d̂`).\n\n### Question\n\nBased on the model's analysis, select all statements that are correct characterizations of the policy effects.", "Answer": ["A", "B"], "qid": "66", "question": "### Background\n\n**Research Question.** This problem analyzes the core policy implications of a dynamic model of influencer-follower relationships. It investigates the effects of a simple tax on advertising revenue, contrasts it with different forms of mandatory disclosure regulation, and evaluates a novel \"opt-in\" policy alternative.\n\n**Setting / Institutional Environment.** In a continuous-time principal-agent model, a follower designs a contract to incentivize an influencer. The relationship's value is summarized by a state variable `d`, the expected future duration of following. The optimal contract exhibits a \"reap and sow\" dynamic: for low `d` (sow phase), the influencer is incentivized to provide good advice (`a=0`); for high `d` (reap phase), the influencer is allowed to monetize their reputation (`a=1`). Policy interventions can alter the profitability of advertising, affecting both the temptation to shirk and the value of the ultimate reward.\n\n### Data / Model Specification\n\nThe influencer's choice `a` between providing good advice (`a=0`) and advertising (`a=1`) is governed by an incentive compatibility (IC) constraint. To induce `a=0`, the promised increase in the influencer's continuation value, `W(d^+) - W(d)`, must outweigh the normalized payoff from advertising, which is 1.\n\nWe consider three policy environments:\n1.  **Taxation:** Advertising revenue `λa` is taxed, so the influencer receives `xλa`, where `x ∈ (0, 1]`.\n2.  **Mandatory Disclosure:** The influencer can use disclosed ads (`a_r`) with return `r` or undisclosed ads (`a_u`) with return `u`. A \"weak\" policy has `u > r` (undisclosed is more profitable), while a \"strong\" policy has `u < r`.\n3.  **Opt-In Disclosure:** Influencers can choose to be subject to the disclosure regime. Followers demand they opt-in during the \"sow\" phase (`d ≤ d̂`) and allow them to opt-out during the \"reap\" phase (`d > d̂`).\n\n### Question\n\nBased on the model's analysis, select all statements that are correct characterizations of the policy effects.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 67, "Question": "### Background\n\n**Research Question.** This problem characterizes the optimal dynamic contract in an influencer-follower relationship, focusing on the \"reap and sow\" dynamics that emerge from the underlying incentive problem.\n\n**Setting / Institutional Environment.** In a continuous-time principal-agent model, the state of the relationship is its expected discounted future duration, `d`. The follower (principal) designs a contract to maximize her own value, `V(d)`, subject to providing the influencer (agent) with value `W(d)`.\n\n### Data / Model Specification\n\nThe total surplus of the relationship is `TS(d) = s + (λ-s)d`. The follower's value is `V(d)` and the influencer's is `W(d) = TS(d) - V(d)`. To incentivize good advice (`a=0`), the follower must promise a future state `d^+` such that the influencer's gain in continuation value satisfies the incentive compatibility (IC) constraint:\n\n  \nW(d^+) - W(d) \\geq 1 \n \n\nThe solution (Proposition 1) is a threshold policy where `a(d)=0` for `d ≤ d̂` (\"Sow Phase\") and `a(d)=1` for `d > d̂` (\"Reap Phase\"). Correspondingly, the follower's value function `V(d)` is strictly concave for `d ≤ d̂` and linear for `d > d̂`.\n\n### Question\n\nSelect all statements that correctly describe the properties and economic intuition of the optimal contract.", "Answer": ["A", "C"], "qid": "67", "question": "### Background\n\n**Research Question.** This problem characterizes the optimal dynamic contract in an influencer-follower relationship, focusing on the \"reap and sow\" dynamics that emerge from the underlying incentive problem.\n\n**Setting / Institutional Environment.** In a continuous-time principal-agent model, the state of the relationship is its expected discounted future duration, `d`. The follower (principal) designs a contract to maximize her own value, `V(d)`, subject to providing the influencer (agent) with value `W(d)`.\n\n### Data / Model Specification\n\nThe total surplus of the relationship is `TS(d) = s + (λ-s)d`. The follower's value is `V(d)` and the influencer's is `W(d) = TS(d) - V(d)`. To incentivize good advice (`a=0`), the follower must promise a future state `d^+` such that the influencer's gain in continuation value satisfies the incentive compatibility (IC) constraint:\n\n  \nW(d^+) - W(d) \\geq 1 \n \n\nThe solution (Proposition 1) is a threshold policy where `a(d)=0` for `d ≤ d̂` (\"Sow Phase\") and `a(d)=1` for `d > d̂` (\"Reap Phase\"). Correspondingly, the follower's value function `V(d)` is strictly concave for `d ≤ d̂` and linear for `d > d̂`.\n\n### Question\n\nSelect all statements that correctly describe the properties and economic intuition of the optimal contract.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 68, "Question": "### Background\n\n**Research Question.** This problem deconstructs the fundamental building blocks of the dynamic principal-agent model, focusing on the incentive constraint that defines the core economic trade-off for the influencer.\n\n**Setting / Institutional Environment.** An influencer privately chooses an ad intensity `a ∈ [0, 1]`. The choice creates a trade-off between an immediate flow payoff from advertising and an expected future reward from providing good advice. The future reward comes from an increase in the influencer's continuation value, `W(d)`, promised by the follower.\n\n### Data / Model Specification\n\nIn the baseline model, the influencer's objective is to choose `a` to maximize their total expected payoff, which is the sum of the flow payoff from ads and the expected change in continuation value:\n\n  \n\\lambda a + (1-a)\\lambda(W(d^+) - W(d)) \n \n\nwhere `λ` is a scaling parameter, `a` is ad intensity, and `W(d^+) - W(d)` is the promised increase in value if good advice is provided. This leads to the baseline incentive compatibility (IC) constraint to induce `a=0`: `W(d^+) - W(d) ≥ 1`.\n\nNow, consider a modification to the model: providing good advice is costly for the influencer, imposing a direct flow cost of `c(a) = k(1-a)` where `k > 0`. The flow payoff from ads remains `λa`.\n\n### Question\n\nUnder this new assumption of costly effort, select all of the following statements that are correct.", "Answer": ["B", "D"], "qid": "68", "question": "### Background\n\n**Research Question.** This problem deconstructs the fundamental building blocks of the dynamic principal-agent model, focusing on the incentive constraint that defines the core economic trade-off for the influencer.\n\n**Setting / Institutional Environment.** An influencer privately chooses an ad intensity `a ∈ [0, 1]`. The choice creates a trade-off between an immediate flow payoff from advertising and an expected future reward from providing good advice. The future reward comes from an increase in the influencer's continuation value, `W(d)`, promised by the follower.\n\n### Data / Model Specification\n\nIn the baseline model, the influencer's objective is to choose `a` to maximize their total expected payoff, which is the sum of the flow payoff from ads and the expected change in continuation value:\n\n  \n\\lambda a + (1-a)\\lambda(W(d^+) - W(d)) \n \n\nwhere `λ` is a scaling parameter, `a` is ad intensity, and `W(d^+) - W(d)` is the promised increase in value if good advice is provided. This leads to the baseline incentive compatibility (IC) constraint to induce `a=0`: `W(d^+) - W(d) ≥ 1`.\n\nNow, consider a modification to the model: providing good advice is costly for the influencer, imposing a direct flow cost of `c(a) = k(1-a)` where `k > 0`. The flow payoff from ads remains `λa`.\n\n### Question\n\nUnder this new assumption of costly effort, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 69, "Question": "### Background\n\nAn analysis of welfare time limits compares Ordinary Least Squares (OLS) and Instrumental Variables (IV) estimates of the parameters governing the behavioral \"banking\" effect (`α`) and the mechanical \"enforcement\" effect (`δ`). The key identification challenge is endogeneity arising from unobserved individual characteristics (e.g., barriers to work) that are correlated with both past and present welfare use.\n\n**Variables & Parameters.**\n- `α`: The \"banking\" parameter, representing the average effect on welfare participation for individuals with a binding time limit (remaining eligibility `S` is less than their eligibility horizon `H`, but greater than zero).\n- `δ`: The \"enforcement\" parameter, representing the average effect on welfare participation for individuals who have exhausted their benefits (`S/H ≤ 0`).\n\n---\n\n### Data / Model Specification\n\nThe following table presents OLS and the paper's preferred IV estimates for the key parameters from a linear probability model of welfare participation.\n\n**Table 1: OLS and IV Estimates of Time-Limit Effects**\n\n| Variable | (1) OLS | (5) IV |\n| :--- | :--- | :--- |\n| `I(0 < S/H < 1)` (Banking, `α`) | 0.064*** | -0.040** |\n| `I(S/H <= 0)` (Enforcement, `δ`) | 0.385*** | -0.863** |\n\n*Notes: From Table 4 in the paper. Standard errors omitted. *** p<0.01, ** p<0.05.*\n\n---\n\nBased on the theoretical framework and empirical results presented, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "69", "question": "### Background\n\nAn analysis of welfare time limits compares Ordinary Least Squares (OLS) and Instrumental Variables (IV) estimates of the parameters governing the behavioral \"banking\" effect (`α`) and the mechanical \"enforcement\" effect (`δ`). The key identification challenge is endogeneity arising from unobserved individual characteristics (e.g., barriers to work) that are correlated with both past and present welfare use.\n\n**Variables & Parameters.**\n- `α`: The \"banking\" parameter, representing the average effect on welfare participation for individuals with a binding time limit (remaining eligibility `S` is less than their eligibility horizon `H`, but greater than zero).\n- `δ`: The \"enforcement\" parameter, representing the average effect on welfare participation for individuals who have exhausted their benefits (`S/H ≤ 0`).\n\n---\n\n### Data / Model Specification\n\nThe following table presents OLS and the paper's preferred IV estimates for the key parameters from a linear probability model of welfare participation.\n\n**Table 1: OLS and IV Estimates of Time-Limit Effects**\n\n| Variable | (1) OLS | (5) IV |\n| :--- | :--- | :--- |\n| `I(0 < S/H < 1)` (Banking, `α`) | 0.064*** | -0.040** |\n| `I(S/H <= 0)` (Enforcement, `δ`) | 0.385*** | -0.863** |\n\n*Notes: From Table 4 in the paper. Standard errors omitted. *** p<0.01, ** p<0.05.*\n\n---\n\nBased on the theoretical framework and empirical results presented, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of welfare time limits...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of welfare time limits...", "error": null}}
{"ID": 70, "Question": "### Background\n\nThe paper investigates whether the mechanical enforcement of welfare time limits varies according to the specific design of state-level policies. The analysis compares the effect of benefit exhaustion (`S ≤ 0`) in two groups of states: \"Termination\" limit states, which drop the entire family from the rolls, and \"Reduction\" limit states, which only terminate the adult's portion of the benefit while children continue to receive assistance.\n\n**Variables & Parameters.**\n- `δ`: The \"enforcement\" parameter, representing the effect on welfare participation of being in the `S/H ≤ 0` range.\n\n---\n\n### Data / Model Specification\n\nThe following table presents IV estimates of the enforcement effect (`δ`), estimated separately for states with different time-limit policies.\n\n**Table 1: IV Estimates of the Enforcement Effect by Policy Type**\n\n| Sub-sample | Coefficient on `I(S <= 0)` (`δ`) |\n| :--- | :--- |\n| (1) Termination Limits | -1.229** |\n| (2) Reduction Limits | -0.102 |\n\n*Notes: From Table 5 in the paper. Standard errors omitted. ** p<0.05. The coefficient for Reduction Limits is not statistically significant.*\n\n---\n\nBased on the institutional context and the results in Table 1, which of the following conclusions are supported by the evidence? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "70", "question": "### Background\n\nThe paper investigates whether the mechanical enforcement of welfare time limits varies according to the specific design of state-level policies. The analysis compares the effect of benefit exhaustion (`S ≤ 0`) in two groups of states: \"Termination\" limit states, which drop the entire family from the rolls, and \"Reduction\" limit states, which only terminate the adult's portion of the benefit while children continue to receive assistance.\n\n**Variables & Parameters.**\n- `δ`: The \"enforcement\" parameter, representing the effect on welfare participation of being in the `S/H ≤ 0` range.\n\n---\n\n### Data / Model Specification\n\nThe following table presents IV estimates of the enforcement effect (`δ`), estimated separately for states with different time-limit policies.\n\n**Table 1: IV Estimates of the Enforcement Effect by Policy Type**\n\n| Sub-sample | Coefficient on `I(S <= 0)` (`δ`) |\n| :--- | :--- |\n| (1) Termination Limits | -1.229** |\n| (2) Reduction Limits | -0.102 |\n\n*Notes: From Table 5 in the paper. Standard errors omitted. ** p<0.05. The coefficient for Reduction Limits is not statistically significant.*\n\n---\n\nBased on the institutional context and the results in Table 1, which of the following conclusions are supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper investigates whether the...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper investigates whether the...", "error": null}}
{"ID": 71, "Question": "### Background\n\nThe paper's primary identification challenge is that the actual stock of remaining benefits, `$S_{its}$`, is endogenous because it depends on past welfare use, which is correlated with unobserved determinants of current use. The strategy is to instrument for `$S_{its}$` using a simulated stock, `$Z_{its}$`, constructed from arguably exogenous components.\n\n---\n\n### Data / Model Specification\n\nThe main estimating equation is a linear probability model for welfare participation `$p_{its}$`:\n\n  \np_{i t s}=X_{i t}{\\Lambda}+V_{t s}\\Phi+L\\left({\\frac{S_{i t s}}{H_{i t}}}\\right)\\pi+\\mu_{s}+\\psi_{s}f(t)+\\nu_{y}+\\varepsilon_{i t s} \\quad \\text{(Eq. (1))}\n \n\nTo address the endogeneity of `$L(S_{its}/H_{it})$`, the strategy uses instruments based on simulated remaining benefits, `$Z_{its}$`, defined as:\n\n  \nZ_{i t s}=N_{s}-k_{j}*E_{i t s} \\quad \\text{(Eq. (2))}\n \n\nwhere `$N_s$` is the state-specific initial benefit endowment, `$E_{its}$` is the individual-specific exposure to time limits, and `$k_j$` is the pre-reform average welfare use for sociodemographic group `j`.\n\n---\n\nAccording to the paper's methodology, which of the following are valid statements regarding the endogeneity problem and the instrumental variable (IV) strategy? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "71", "question": "### Background\n\nThe paper's primary identification challenge is that the actual stock of remaining benefits, `$S_{its}$`, is endogenous because it depends on past welfare use, which is correlated with unobserved determinants of current use. The strategy is to instrument for `$S_{its}$` using a simulated stock, `$Z_{its}$`, constructed from arguably exogenous components.\n\n---\n\n### Data / Model Specification\n\nThe main estimating equation is a linear probability model for welfare participation `$p_{its}$`:\n\n  \np_{i t s}=X_{i t}{\\Lambda}+V_{t s}\\Phi+L\\left({\\frac{S_{i t s}}{H_{i t}}}\\right)\\pi+\\mu_{s}+\\psi_{s}f(t)+\\nu_{y}+\\varepsilon_{i t s} \\quad \\text{(Eq. (1))}\n \n\nTo address the endogeneity of `$L(S_{its}/H_{it})$`, the strategy uses instruments based on simulated remaining benefits, `$Z_{its}$`, defined as:\n\n  \nZ_{i t s}=N_{s}-k_{j}*E_{i t s} \\quad \\text{(Eq. (2))}\n \n\nwhere `$N_s$` is the state-specific initial benefit endowment, `$E_{its}$` is the individual-specific exposure to time limits, and `$k_j$` is the pre-reform average welfare use for sociodemographic group `j`.\n\n---\n\nAccording to the paper's methodology, which of the following are valid statements regarding the endogeneity problem and the instrumental variable (IV) strategy? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's primary identification...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's primary identification...", "error": null}}
{"ID": 72, "Question": "### Background\n\nA linear probability model is used to estimate the welfare participation (`$p_{its}$`) of single mothers. The key explanatory variables are a set of indicators and continuous terms based on the ratio of a woman's remaining welfare eligibility (`$S_{its}$`) to her eligibility horizon (`$H_{it}$`).\n\n---\n\n### Data / Model Specification\n\nThe empirical specification for welfare participation is:\n\n  \np_{i t s}=X_{i t}\\lambda+V_{t s}\\phi+I(t\\geq\\bar{T}_{s})\\bigg\\{\\alpha I\\bigg(0<\\frac{S_{i t s}}{H_{i t}}<1\\bigg)+\\beta\\frac{S_{i t s}}{H_{i t}}I\\bigg(0<\\frac{S_{i t s}}{H_{i t}}<1\\bigg) +\\gamma I\\bigg(\\frac{S_{i t s}}{H_{i t}}\\geq1\\bigg)+\\delta I\\bigg(\\frac{S_{i t s}}{H_{i t}}\\leq0\\bigg)\\bigg\\}+\\varepsilon_{i t s} \\quad \\text{(Eq. (1))}\n \n\nThe baseline group consists of individuals in periods before time limits were implemented.\n\n---\n\nBased on the theoretical hypotheses outlined in the paper (Banking, Unconstrained, Enforcement), which of the following statements about the parameters in Eq. (1) are theoretically predicted to be true? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "72", "question": "### Background\n\nA linear probability model is used to estimate the welfare participation (`$p_{its}$`) of single mothers. The key explanatory variables are a set of indicators and continuous terms based on the ratio of a woman's remaining welfare eligibility (`$S_{its}$`) to her eligibility horizon (`$H_{it}$`).\n\n---\n\n### Data / Model Specification\n\nThe empirical specification for welfare participation is:\n\n  \np_{i t s}=X_{i t}\\lambda+V_{t s}\\phi+I(t\\geq\\bar{T}_{s})\\bigg\\{\\alpha I\\bigg(0<\\frac{S_{i t s}}{H_{i t}}<1\\bigg)+\\beta\\frac{S_{i t s}}{H_{i t}}I\\bigg(0<\\frac{S_{i t s}}{H_{i t}}<1\\bigg) +\\gamma I\\bigg(\\frac{S_{i t s}}{H_{i t}}\\geq1\\bigg)+\\delta I\\bigg(\\frac{S_{i t s}}{H_{i t}}\\leq0\\bigg)\\bigg\\}+\\varepsilon_{i t s} \\quad \\text{(Eq. (1))}\n \n\nThe baseline group consists of individuals in periods before time limits were implemented.\n\n---\n\nBased on the theoretical hypotheses outlined in the paper (Banking, Unconstrained, Enforcement), which of the following statements about the parameters in Eq. (1) are theoretically predicted to be true? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA linear probability model is used...", "model_answer": "This is a detailed response to the question about ### Background\n\nA linear probability model is used...", "error": null}}
{"ID": 73, "Question": "### Background\n\n**Research Question.** This problem addresses the central positive result of the Shapiro-Stiglitz model: the derivation of an aggregate labor market equilibrium characterized by involuntary unemployment. It requires linking the micro-foundations of a worker's effort choice to the macro-level determination of wages and employment.\n\n**Setting / Institutional Environment.** The model describes a continuous-time labor market in a steady-state equilibrium. In this steady state, the flow of workers from employment to unemployment must equal the flow from unemployment back to employment. This condition endogenously determines the job acquisition rate for an unemployed worker. Firms have a production technology and demand labor, but must pay a wage high enough to satisfy the No-Shirking Condition (NSC) which itself depends on aggregate labor market conditions.\n\n**Variables & Parameters.**\n- `w`: Wage (real value).\n- `L`: Aggregate employment (number of workers).\n- `N`: Total labor supply, a fixed number (number of workers).\n- `F(L)`: Aggregate production function, with `F'(L) > 0` and `F''(L) < 0`.\n- `a`: The job acquisition rate per unit time for an unemployed worker (dimensionless rate).\n- `b`: Exogenous job separation rate (dimensionless rate).\n- `e`: Disutility of effort (real value, `e>0`).\n- `\\overline{w}`: Unemployment benefits (real value).\n- `q`: Probability per unit time of detecting a shirker (dimensionless rate).\n- `r`: Worker's discount rate (dimensionless rate, `r>0`).\n\n---\n\n### Data / Model Specification\n\nThe aggregate No-Shirking Condition (NSC), which gives the minimum wage `\\hat{w}` required to prevent shirking at a given level of aggregate employment `L`, is:\n\n  \n\\hat{w}(L) = \\overline{w} + e + \\frac{e}{q} \\left( \\frac{bN}{N-L} + r \\right)\n\\quad \\text{(Eq. (1))}\n \n\nThe aggregate labor demand condition is given by:\n\n  \nw = F'(L) \n\\quad \\text{(Eq. (2))}\n \n\nEquilibrium `(L*, w*)` occurs where the labor demand curve intersects the aggregate NSC curve.\n\n---\n\n### Question\n\nBased on the model, select all of the following statements that are correct descriptions of the aggregate labor market equilibrium.", "Answer": ["C", "D"], "qid": "73", "question": "### Background\n\n**Research Question.** This problem addresses the central positive result of the Shapiro-Stiglitz model: the derivation of an aggregate labor market equilibrium characterized by involuntary unemployment. It requires linking the micro-foundations of a worker's effort choice to the macro-level determination of wages and employment.\n\n**Setting / Institutional Environment.** The model describes a continuous-time labor market in a steady-state equilibrium. In this steady state, the flow of workers from employment to unemployment must equal the flow from unemployment back to employment. This condition endogenously determines the job acquisition rate for an unemployed worker. Firms have a production technology and demand labor, but must pay a wage high enough to satisfy the No-Shirking Condition (NSC) which itself depends on aggregate labor market conditions.\n\n**Variables & Parameters.**\n- `w`: Wage (real value).\n- `L`: Aggregate employment (number of workers).\n- `N`: Total labor supply, a fixed number (number of workers).\n- `F(L)`: Aggregate production function, with `F'(L) > 0` and `F''(L) < 0`.\n- `a`: The job acquisition rate per unit time for an unemployed worker (dimensionless rate).\n- `b`: Exogenous job separation rate (dimensionless rate).\n- `e`: Disutility of effort (real value, `e>0`).\n- `\\overline{w}`: Unemployment benefits (real value).\n- `q`: Probability per unit time of detecting a shirker (dimensionless rate).\n- `r`: Worker's discount rate (dimensionless rate, `r>0`).\n\n---\n\n### Data / Model Specification\n\nThe aggregate No-Shirking Condition (NSC), which gives the minimum wage `\\hat{w}` required to prevent shirking at a given level of aggregate employment `L`, is:\n\n  \n\\hat{w}(L) = \\overline{w} + e + \\frac{e}{q} \\left( \\frac{bN}{N-L} + r \\right)\n\\quad \\text{(Eq. (1))}\n \n\nThe aggregate labor demand condition is given by:\n\n  \nw = F'(L) \n\\quad \\text{(Eq. (2))}\n \n\nEquilibrium `(L*, w*)` occurs where the labor demand curve intersects the aggregate NSC curve.\n\n---\n\n### Question\n\nBased on the model, select all of the following statements that are correct descriptions of the aggregate labor market equilibrium.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 74, "Question": "### Background\n\n**Research Question.** This problem analyzes the comparative statics of the Shapiro-Stiglitz model, specifically how the labor market equilibrium responds to an aggregate productivity shock.\n\n**Setting / Institutional Environment.** The model describes a continuous-time labor market in a steady-state equilibrium. Firms must pay a wage high enough to satisfy the upward-sloping aggregate No-Shirking Condition (NSC), and they hire workers up to the point where the marginal product of labor equals this wage.\n\n**Variables & Parameters.**\n- `w`: Wage (real value).\n- `L`: Aggregate employment (number of workers).\n- `N`: Total labor supply, a fixed number.\n- `F(L)`: Aggregate production function, with `F'(L) > 0` and `F''(L) < 0`.\n- `\\theta`: A Hicks-neutral technology parameter, initially `\\theta=1`.\n- `\\hat{w}(L)`: The aggregate NSC wage, an upward-sloping function of `L`.\n\n---\n\n### Data / Model Specification\n\nThe market equilibrium `(L*, w*)` is determined by the simultaneous satisfaction of the aggregate NSC and the labor demand condition. Following a productivity shock, the new equilibrium condition is:\n\n  \n\\theta F'(L) = \\hat{w}(L)\n\\quad \\text{(Eq. (1))}\n \n\nwhere `\\hat{w}(L) = \\overline{w} + e + \\frac{e}{q} ( \\frac{bN}{N-L} + r )`.\n\n---\n\n### Question\n\nConsider a positive, Hicks-neutral productivity shock that changes the production function to `\\theta F(L)` (where `\\theta > 1`). Select all of the following statements that correctly describe the shock's effect on the equilibrium.", "Answer": ["B", "D"], "qid": "74", "question": "### Background\n\n**Research Question.** This problem analyzes the comparative statics of the Shapiro-Stiglitz model, specifically how the labor market equilibrium responds to an aggregate productivity shock.\n\n**Setting / Institutional Environment.** The model describes a continuous-time labor market in a steady-state equilibrium. Firms must pay a wage high enough to satisfy the upward-sloping aggregate No-Shirking Condition (NSC), and they hire workers up to the point where the marginal product of labor equals this wage.\n\n**Variables & Parameters.**\n- `w`: Wage (real value).\n- `L`: Aggregate employment (number of workers).\n- `N`: Total labor supply, a fixed number.\n- `F(L)`: Aggregate production function, with `F'(L) > 0` and `F''(L) < 0`.\n- `\\theta`: A Hicks-neutral technology parameter, initially `\\theta=1`.\n- `\\hat{w}(L)`: The aggregate NSC wage, an upward-sloping function of `L`.\n\n---\n\n### Data / Model Specification\n\nThe market equilibrium `(L*, w*)` is determined by the simultaneous satisfaction of the aggregate NSC and the labor demand condition. Following a productivity shock, the new equilibrium condition is:\n\n  \n\\theta F'(L) = \\hat{w}(L)\n\\quad \\text{(Eq. (1))}\n \n\nwhere `\\hat{w}(L) = \\overline{w} + e + \\frac{e}{q} ( \\frac{bN}{N-L} + r )`.\n\n---\n\n### Question\n\nConsider a positive, Hicks-neutral productivity shock that changes the production function to `\\theta F(L)` (where `\\theta > 1`). Select all of the following statements that correctly describe the shock's effect on the equilibrium.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 75, "Question": "### Background\n\n**Research Question.** This problem investigates the normative properties of the efficiency wage equilibrium, comparing the decentralized market outcome to a social planner's solution to determine if the market generates a socially efficient level of unemployment.\n\n**Setting / Institutional Environment.** We compare the market equilibrium with the solution chosen by a social planner who maximizes total net national product. We consider a baseline model with exogenous monitoring and an extension where monitoring is endogenous and costly.\n\n**Variables & Parameters.**\n- `L*`, `w*`: The market equilibrium employment and wage.\n- `L**`, `w**`: The socially optimal employment and wage.\n- `F(L)`: Aggregate production function, assumed to be concave (`F'' < 0`).\n- `\\hat{w}(L, q)`: The aggregate no-shirking wage.\n\n---\n\n### Data / Model Specification\n\n- **Market Equilibrium:** Determined by `F'(L*) = \\hat{w}(L*, q)`.\n- **Social Planner's Optimum (Baseline):** Determined by `F(L**)/L** = \\hat{w}(L**, q)`. For a concave production function, `F(L)/L > F'(L)` for `L>0`.\n- **Extension:** In a case with endogenous monitoring and constant returns to scale (`F(L)=L`), the paper shows the welfare conclusion reverses.\n\n---\n\n### Question\n\nRegarding the normative properties of the market equilibrium, select all of the following statements that are correct according to the paper's analysis.", "Answer": ["B", "C"], "qid": "75", "question": "### Background\n\n**Research Question.** This problem investigates the normative properties of the efficiency wage equilibrium, comparing the decentralized market outcome to a social planner's solution to determine if the market generates a socially efficient level of unemployment.\n\n**Setting / Institutional Environment.** We compare the market equilibrium with the solution chosen by a social planner who maximizes total net national product. We consider a baseline model with exogenous monitoring and an extension where monitoring is endogenous and costly.\n\n**Variables & Parameters.**\n- `L*`, `w*`: The market equilibrium employment and wage.\n- `L**`, `w**`: The socially optimal employment and wage.\n- `F(L)`: Aggregate production function, assumed to be concave (`F'' < 0`).\n- `\\hat{w}(L, q)`: The aggregate no-shirking wage.\n\n---\n\n### Data / Model Specification\n\n- **Market Equilibrium:** Determined by `F'(L*) = \\hat{w}(L*, q)`.\n- **Social Planner's Optimum (Baseline):** Determined by `F(L**)/L** = \\hat{w}(L**, q)`. For a concave production function, `F(L)/L > F'(L)` for `L>0`.\n- **Extension:** In a case with endogenous monitoring and constant returns to scale (`F(L)=L`), the paper shows the welfare conclusion reverses.\n\n---\n\n### Question\n\nRegarding the normative properties of the market equilibrium, select all of the following statements that are correct according to the paper's analysis.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 76, "Question": "### Background\n\n**Research Question.** This problem requires analyzing the central behavioral constraint of the Shapiro-Stiglitz model: the No-Shirking Condition (NSC), which specifies the minimum wage `\\hat{w}` a firm must pay to induce a rational worker to exert effort.\n\n**Setting / Institutional Environment.** An employed worker chooses an effort level. If they shirk, they face a probability `q` of being caught and fired. The NSC defines the wage that makes a worker indifferent between shirking and not shirking.\n\n**Variables & Parameters.**\n- `\\hat{w}`: The minimum required wage (the NSC).\n- `V_u`: Expected lifetime utility of an unemployed individual.\n- `e`: Disutility of effort (`e>0`).\n- `r`: Worker's discount rate (`r>0`).\n- `b`: Exogenous separation rate.\n- `q`: Probability per unit time of detecting a shirker.\n\n---\n\n### Data / Model Specification\n\nThe No-Shirking Condition (NSC) can be expressed as a function of the model's parameters. The required wage `\\hat{w}` is given by:\n\n  \n\\hat{w} = rV_u + \\frac{e(r+b+q)}{q} = rV_u + e + \\frac{e(r+b)}{q}\n\\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nA labor market has two types of firms paying efficiency wages to identical workers: high-tech firms with a high detection probability `q_H` and low-tech firms with a lower detection probability `q_L < q_H`. Based on the properties of the No-Shirking Condition (NSC) in Eq. (1), select all of the following statements that are correct.", "Answer": ["C", "D"], "qid": "76", "question": "### Background\n\n**Research Question.** This problem requires analyzing the central behavioral constraint of the Shapiro-Stiglitz model: the No-Shirking Condition (NSC), which specifies the minimum wage `\\hat{w}` a firm must pay to induce a rational worker to exert effort.\n\n**Setting / Institutional Environment.** An employed worker chooses an effort level. If they shirk, they face a probability `q` of being caught and fired. The NSC defines the wage that makes a worker indifferent between shirking and not shirking.\n\n**Variables & Parameters.**\n- `\\hat{w}`: The minimum required wage (the NSC).\n- `V_u`: Expected lifetime utility of an unemployed individual.\n- `e`: Disutility of effort (`e>0`).\n- `r`: Worker's discount rate (`r>0`).\n- `b`: Exogenous separation rate.\n- `q`: Probability per unit time of detecting a shirker.\n\n---\n\n### Data / Model Specification\n\nThe No-Shirking Condition (NSC) can be expressed as a function of the model's parameters. The required wage `\\hat{w}` is given by:\n\n  \n\\hat{w} = rV_u + \\frac{e(r+b+q)}{q} = rV_u + e + \\frac{e(r+b)}{q}\n\\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nA labor market has two types of firms paying efficiency wages to identical workers: high-tech firms with a high detection probability `q_H` and low-tech firms with a lower detection probability `q_L < q_H`. Based on the properties of the No-Shirking Condition (NSC) in Eq. (1), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 77, "Question": "### Background\n\n**Research Question.** This problem explores the trade-off between model complexity and forecasting performance by comparing a dynamic structural model with a simple static model in the context of food demand.\n\n**Setting and Sample.** The analysis uses Swedish annual time-series data for a 4-commodity grouping of food items from 1950-1970. Model performance is evaluated based on in-sample fit (1950-70) and out-of-sample predictive performance for 1971 and 1972, using information inaccuracy as the metric (lower is better).\n\n### Data / Model Specification\n\nThe Linear Expenditure System with Habit Formation (LESH-pq) incorporates dynamics via lagged consumption:\n\n  \np_{i t}q_{i t}=\\alpha_{i}p_{i t}q_{i,t-1}+\\beta_{i}\\left(y_{t}-\\sum_{k}\\alpha_{k}p_{k t}q_{k,t-1}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe simple Constant Elasticity of Demand System (CEDS-ln q) is a static model:\n\n  \n\\ln(q_{i t})=\\alpha_{i}+e_{i}\\ln(y_{t}/p_{t})+E_{i i}\\ln(p_{i t}/p_{t})+\\varepsilon_{i t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Average Information Inaccuracies (4 Food Commodities)**\n\n| Model     | In-Sample Fit (I50-70) | Predictive Performance (I71-72) |\n|-----------|------------------------|---------------------------------| \n| LESH-pq   | 153                    | 389                             |\n| CEDS-ln q | 245                    | 217                             |\n\n*Note: Lower values indicate better performance.* \n\n### Question\n\nBased on the model specifications and the results in Table 1, which of the following statements are valid interpretations of the models' performance? Select all that apply.", "Answer": ["A", "D"], "qid": "77", "question": "### Background\n\n**Research Question.** This problem explores the trade-off between model complexity and forecasting performance by comparing a dynamic structural model with a simple static model in the context of food demand.\n\n**Setting and Sample.** The analysis uses Swedish annual time-series data for a 4-commodity grouping of food items from 1950-1970. Model performance is evaluated based on in-sample fit (1950-70) and out-of-sample predictive performance for 1971 and 1972, using information inaccuracy as the metric (lower is better).\n\n### Data / Model Specification\n\nThe Linear Expenditure System with Habit Formation (LESH-pq) incorporates dynamics via lagged consumption:\n\n  \np_{i t}q_{i t}=\\alpha_{i}p_{i t}q_{i,t-1}+\\beta_{i}\\left(y_{t}-\\sum_{k}\\alpha_{k}p_{k t}q_{k,t-1}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe simple Constant Elasticity of Demand System (CEDS-ln q) is a static model:\n\n  \n\\ln(q_{i t})=\\alpha_{i}+e_{i}\\ln(y_{t}/p_{t})+E_{i i}\\ln(p_{i t}/p_{t})+\\varepsilon_{i t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Average Information Inaccuracies (4 Food Commodities)**\n\n| Model     | In-Sample Fit (I50-70) | Predictive Performance (I71-72) |\n|-----------|------------------------|---------------------------------| \n| LESH-pq   | 153                    | 389                             |\n| CEDS-ln q | 245                    | 217                             |\n\n*Note: Lower values indicate better performance.* \n\n### Question\n\nBased on the model specifications and the results in Table 1, which of the following statements are valid interpretations of the models' performance? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 78, "Question": "### Background\n\n**Research Question.** This problem investigates the drivers of aggregate consumption patterns by comparing the performance of demand models that incorporate different forms of dynamic behavior.\n\n**Setting and Sample.** The analysis uses a broad dataset including all goods and services, aggregated into four categories: I. Food, etc., II. Housing, III. Clothing, IV. Other. The data are Swedish annual time series from 1950-1970. Model performance is evaluated on both in-sample fit and out-of-sample prediction for 1971-1972.\n\n### Data / Model Specification\n\nThe LESH-pq model captures dynamics through a backward-looking habit-formation mechanism:\n\n  \np_{i t}q_{i t}=\\alpha_{i}p_{i t}q_{i,t-1}+\\beta_{i}\\left(y_{t}-\\sum_{k}\\alpha_{k}p_{k t}q_{k,t-1}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe RDI-w*Dq model captures dynamics through an autonomous trend term, `κ_i`:\n\n  \nw_{i t}^{*}D q_{i t}=\\kappa_{i}+\\mu_{i}D q_{t}+\\sum_{j}\\pi_{i j}D p_{j t}+\\varepsilon_{i t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Average Information Inaccuracies (All Goods, 4 Commodities)**\n\n| Model      | In-Sample Fit (Is50.70) | Predictive Performance (I71-72) |\n|------------|-------------------------|---------------------------------| \n| LESH-pq    | 179                     | 41                              |\n| RDI-w*Dq   | 153                     | 32                              |\n\n*Note: Lower values indicate better performance.* \n\n### Question\n\nSuppose a government is considering a large, permanent, revenue-neutral tax reform in 1973: a new tax on \"Other Goods and Services\" (IV) offset by a subsidy for \"Food, Beverages and Tobacco\" (I). Based on the models' structures, which of the following are valid considerations when choosing a model to forecast the **long-run** consumption shares after this policy shock? Select all that apply.", "Answer": ["A", "D"], "qid": "78", "question": "### Background\n\n**Research Question.** This problem investigates the drivers of aggregate consumption patterns by comparing the performance of demand models that incorporate different forms of dynamic behavior.\n\n**Setting and Sample.** The analysis uses a broad dataset including all goods and services, aggregated into four categories: I. Food, etc., II. Housing, III. Clothing, IV. Other. The data are Swedish annual time series from 1950-1970. Model performance is evaluated on both in-sample fit and out-of-sample prediction for 1971-1972.\n\n### Data / Model Specification\n\nThe LESH-pq model captures dynamics through a backward-looking habit-formation mechanism:\n\n  \np_{i t}q_{i t}=\\alpha_{i}p_{i t}q_{i,t-1}+\\beta_{i}\\left(y_{t}-\\sum_{k}\\alpha_{k}p_{k t}q_{k,t-1}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe RDI-w*Dq model captures dynamics through an autonomous trend term, `κ_i`:\n\n  \nw_{i t}^{*}D q_{i t}=\\kappa_{i}+\\mu_{i}D q_{t}+\\sum_{j}\\pi_{i j}D p_{j t}+\\varepsilon_{i t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Average Information Inaccuracies (All Goods, 4 Commodities)**\n\n| Model      | In-Sample Fit (Is50.70) | Predictive Performance (I71-72) |\n|------------|-------------------------|---------------------------------| \n| LESH-pq    | 179                     | 41                              |\n| RDI-w*Dq   | 153                     | 32                              |\n\n*Note: Lower values indicate better performance.* \n\n### Question\n\nSuppose a government is considering a large, permanent, revenue-neutral tax reform in 1973: a new tax on \"Other Goods and Services\" (IV) offset by a subsidy for \"Food, Beverages and Tobacco\" (I). Based on the models' structures, which of the following are valid considerations when choosing a model to forecast the **long-run** consumption shares after this policy shock? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 79, "Question": "### Background\n\n**Research Question.** This problem analyzes the structure and theoretical underpinnings of the classic Linear Expenditure System (LES), focusing on the restrictive implications of its underlying utility function.\n\n### Data / Model Specification\n\nThe Linear Expenditure System (LES-w) is specified in share form as:\n\n  \nw_{i t}=c_{i}p_{i t}/y_{t}+\\beta_{i}\\left(1-\\sum_{k}c_{k}p_{k t}/y_{t}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe model imposes the adding-up constraint `\\sum_{i}\\beta_{i}=1`. The paper notes that the LES is derived from an additive utility function, which implies that \"there are no specific substitution effects and that the own-price elasticity is approximately proportional to the income elasticity, as shown in Deaton.\"\n\n### Question\n\nBased on the structure of the LES model in Eq. (1) and its theoretical underpinnings, which of the following statements are valid interpretations or implications? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "79", "question": "### Background\n\n**Research Question.** This problem analyzes the structure and theoretical underpinnings of the classic Linear Expenditure System (LES), focusing on the restrictive implications of its underlying utility function.\n\n### Data / Model Specification\n\nThe Linear Expenditure System (LES-w) is specified in share form as:\n\n  \nw_{i t}=c_{i}p_{i t}/y_{t}+\\beta_{i}\\left(1-\\sum_{k}c_{k}p_{k t}/y_{t}\\right)+\\varepsilon_{i t} \\quad \\text{(Eq. (1))}\n \n\nThe model imposes the adding-up constraint `\\sum_{i}\\beta_{i}=1`. The paper notes that the LES is derived from an additive utility function, which implies that \"there are no specific substitution effects and that the own-price elasticity is approximately proportional to the income elasticity, as shown in Deaton.\"\n\n### Question\n\nBased on the structure of the LES model in Eq. (1) and its theoretical underpinnings, which of the following statements are valid interpretations or implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 80, "Question": "### Background\n\n**Research Question.** This problem analyzes how a firm's need to retain skilled labor for the future can lead it to employ an excessive number of skilled workers (insiders) in the present, causing involuntary unemployment among unskilled workers (outsiders).\n\n**Setting / Institutional Environment.** A firm and a union of insiders engage in a two-period Nash bargaining model. The key dynamic linkage is the stock of insiders. The number of insiders available in the second period (`N̄₂`) depends on the number employed in the first period (`N₁`), because unemployed insiders are more likely to quit the firm than employed insiders. The firm is assumed to be in a sufficiently bad state of the product market in period 1.\n\n### Data / Model Specification\n\n- **Workers and Jobs:** Insiders have firm-specific skills for 'complicated' jobs but can also do 'simple' jobs. Outsiders lack these skills and can only do simple jobs. Both are equally productive in simple jobs.\n- **Surplus Functions:** `V(N₁;s)` is the total surplus (revenue net of outsider wages) generated in period 1 with `N₁` insiders. `B(N̄₂)` is the expected total surplus in period 2, given `N̄₂` available insiders. It is increasing and concave in `N̄₂` (`B' > 0, B'' < 0`) for non-redundant levels of `N̄₂`.\n- **Static Optimum:** The statically optimal level of insider employment in period 1, denoted `N*(s)`, is where the marginal current-period surplus from an insider equals the outsider reservation wage `w_R`. For `N₁ > N*(s)`, `∂V(N₁;s)/∂N₁ < w_R`.\n- **Insider Retention:** The number of insiders available in period 2, `N̄₂`, follows the law of motion:\n\n  \n\\bar{N}_{2}=(1-P_{e})N_{1}+(1-P_{u})(\\bar{N}_{1}-N_{1})+n = h(N_1, n) \\quad \\text{(Eq. (1))}\n \n\nwhere `N̄₁` is the initial stock of insiders, `n` is the number of new trainees, and `P_e` and `P_u` are the probabilities that an employed and an unemployed insider, respectively, leave the firm. The crucial assumption is `P_u > P_e`.\n\n- **Optimization:** The firm-union pair chooses `N₁` to maximize the two-period total surplus. The first-order condition for an interior solution for `N₁` is:\n\n  \n\\frac{\\partial V(N_1;s)}{\\partial N_1} - w_R + \\frac{1}{1+r} B'(\\bar{N}_2) \\frac{\\partial h}{\\partial N_1} = 0 \\quad \\text{(Eq. (2))}\n \n\nwhere `r` is the interest rate.\n\n### Question\n\nBased on the two-period model, consider the firm-union's decision at the statically optimal employment level, `N₁ = N*(s)`. Select all statements that are correct descriptions of the economic trade-offs at this point.", "Answer": ["B", "D"], "qid": "80", "question": "### Background\n\n**Research Question.** This problem analyzes how a firm's need to retain skilled labor for the future can lead it to employ an excessive number of skilled workers (insiders) in the present, causing involuntary unemployment among unskilled workers (outsiders).\n\n**Setting / Institutional Environment.** A firm and a union of insiders engage in a two-period Nash bargaining model. The key dynamic linkage is the stock of insiders. The number of insiders available in the second period (`N̄₂`) depends on the number employed in the first period (`N₁`), because unemployed insiders are more likely to quit the firm than employed insiders. The firm is assumed to be in a sufficiently bad state of the product market in period 1.\n\n### Data / Model Specification\n\n- **Workers and Jobs:** Insiders have firm-specific skills for 'complicated' jobs but can also do 'simple' jobs. Outsiders lack these skills and can only do simple jobs. Both are equally productive in simple jobs.\n- **Surplus Functions:** `V(N₁;s)` is the total surplus (revenue net of outsider wages) generated in period 1 with `N₁` insiders. `B(N̄₂)` is the expected total surplus in period 2, given `N̄₂` available insiders. It is increasing and concave in `N̄₂` (`B' > 0, B'' < 0`) for non-redundant levels of `N̄₂`.\n- **Static Optimum:** The statically optimal level of insider employment in period 1, denoted `N*(s)`, is where the marginal current-period surplus from an insider equals the outsider reservation wage `w_R`. For `N₁ > N*(s)`, `∂V(N₁;s)/∂N₁ < w_R`.\n- **Insider Retention:** The number of insiders available in period 2, `N̄₂`, follows the law of motion:\n\n  \n\\bar{N}_{2}=(1-P_{e})N_{1}+(1-P_{u})(\\bar{N}_{1}-N_{1})+n = h(N_1, n) \\quad \\text{(Eq. (1))}\n \n\nwhere `N̄₁` is the initial stock of insiders, `n` is the number of new trainees, and `P_e` and `P_u` are the probabilities that an employed and an unemployed insider, respectively, leave the firm. The crucial assumption is `P_u > P_e`.\n\n- **Optimization:** The firm-union pair chooses `N₁` to maximize the two-period total surplus. The first-order condition for an interior solution for `N₁` is:\n\n  \n\\frac{\\partial V(N_1;s)}{\\partial N_1} - w_R + \\frac{1}{1+r} B'(\\bar{N}_2) \\frac{\\partial h}{\\partial N_1} = 0 \\quad \\text{(Eq. (2))}\n \n\nwhere `r` is the interest rate.\n\n### Question\n\nBased on the two-period model, consider the firm-union's decision at the statically optimal employment level, `N₁ = N*(s)`. Select all statements that are correct descriptions of the economic trade-offs at this point.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 81, "Question": "### Background\n\n**Research Question.** This problem analyzes the logic of a one-period, static model of a unionized firm to understand why such a framework is insufficient to explain the involuntary unemployment of outsiders.\n\n**Setting / Institutional Environment.** A firm and a union of insiders bargain over the number of insiders to employ, `N`, and their wage, `w_I`. The firm can also hire non-union outsiders at a competitive reservation wage, `w_R`, to perform 'simple' jobs. Insiders have firm-specific skills for 'complicated' jobs but can also perform simple jobs. The outcome of the bargain is efficient, maximizing the total joint surplus of the coalition.\n\n### Data / Model Specification\n\n- **Key Employment Thresholds:**\n    - `N**(s)`: The level of insider employment where, if all insiders are in complicated jobs, their marginal product equals the outsider wage `w_R`.\n    - `N*(s)`: The level of insider employment where, if insiders fill *all* jobs (simple and complicated) and no outsiders are hired, their marginal product equals `w_R`. By construction, `N* > N**`.\n- **Outsider Demand:** The demand for outsiders is positive if `N ≤ N*(s)` and zero if `N > N*(s)`. Involuntary unemployment of outsiders occurs only if the firm operates with `N > N*(s)`.\n- **Marginal Value of Insiders (from Proposition 2):** The marginal value of an insider to the coalition, `V'(N) = ∂V/∂N`, behaves as follows:\n    1.  For `N < N**(s)`: `V'(N) > w_R`\n    2.  For `N**(s) ≤ N ≤ N*(s)`: `V'(N) = w_R`\n    3.  For `N > N*(s)`: `V'(N) < w_R`\n- **Bargaining Objective:** The firm and union choose `N` to maximize their total joint surplus, `S(N) = V(N) - w_R N`.\n\n### Question\n\nIn the one-period static model, the firm and union bargain to choose an insider employment level `N` that maximizes their joint surplus, `S(N)`. Based on the model's assumptions, select all statements that are **incorrect** descriptions of the bargaining outcome and its implications.", "Answer": ["B", "D"], "qid": "81", "question": "### Background\n\n**Research Question.** This problem analyzes the logic of a one-period, static model of a unionized firm to understand why such a framework is insufficient to explain the involuntary unemployment of outsiders.\n\n**Setting / Institutional Environment.** A firm and a union of insiders bargain over the number of insiders to employ, `N`, and their wage, `w_I`. The firm can also hire non-union outsiders at a competitive reservation wage, `w_R`, to perform 'simple' jobs. Insiders have firm-specific skills for 'complicated' jobs but can also perform simple jobs. The outcome of the bargain is efficient, maximizing the total joint surplus of the coalition.\n\n### Data / Model Specification\n\n- **Key Employment Thresholds:**\n    - `N**(s)`: The level of insider employment where, if all insiders are in complicated jobs, their marginal product equals the outsider wage `w_R`.\n    - `N*(s)`: The level of insider employment where, if insiders fill *all* jobs (simple and complicated) and no outsiders are hired, their marginal product equals `w_R`. By construction, `N* > N**`.\n- **Outsider Demand:** The demand for outsiders is positive if `N ≤ N*(s)` and zero if `N > N*(s)`. Involuntary unemployment of outsiders occurs only if the firm operates with `N > N*(s)`.\n- **Marginal Value of Insiders (from Proposition 2):** The marginal value of an insider to the coalition, `V'(N) = ∂V/∂N`, behaves as follows:\n    1.  For `N < N**(s)`: `V'(N) > w_R`\n    2.  For `N**(s) ≤ N ≤ N*(s)`: `V'(N) = w_R`\n    3.  For `N > N*(s)`: `V'(N) < w_R`\n- **Bargaining Objective:** The firm and union choose `N` to maximize their total joint surplus, `S(N) = V(N) - w_R N`.\n\n### Question\n\nIn the one-period static model, the firm and union bargain to choose an insider employment level `N` that maximizes their joint surplus, `S(N)`. Based on the model's assumptions, select all statements that are **incorrect** descriptions of the bargaining outcome and its implications.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 82, "Question": "### Background\n\n**Research Question.** This problem evaluates the use of a structural model to conduct a counterfactual policy simulation, specifically to estimate the economic cost of the Small Business Administration (SBA) set-aside program in timber auctions.\n\n**Setting.** The SBA set-aside program restricts participation in some auctions to small firms (those with fewer than 500 employees). The cost of this program is simulated by comparing actual auction outcomes to a counterfactual world where the program's effects are eliminated.\n\n**Variables and Parameters.**\n- `ηᵢ`: The observable portion of bidder `i`'s private value component (units: $/mbf).\n- `HAULᵢ`: Hauling distance for bidder `i` (units: miles).\n- `SBAᵢ`: An indicator variable equal to 1 if bidder `i` is a small firm, and 0 otherwise.\n- `β₂`: The estimated coefficient on the `SBAᵢ` dummy.\n\n---\n\n### Data / Model Specification\n\nThe observable component of a bidder's value, `ηᵢ`, is specified as:\n  \nηᵢ = β₁ ⋅ HAULᵢ + β₂ ⋅ SBAᵢ\n \n**Table 1: Estimated Value Distribution Parameters**\n| Variable | Coefficient |\n| :--- | :--- |\n| Hauling Miles (`β₁`) | -2.08 |\n| SBA Dummy (`β₂`) | -71.63 |\n| `1/μ` | 39.66 |\n\n**Table 2: Simulated Costs of the SBA Set-Aside Program**\n| Variable | Mean |\n| :--- | :--- |\n| Simulated Winning Bids ($/mbf) | 225.54 |\n| Current Winning Bids ($/mbf) | 200.64 |\n| %Δ Auction Revenue | 14.8% |\n\nThe simulation assumes that small, high-cost firms 'evolve into large, more efficient firms.'\n\n---\n\n### Question\n\nBased on the model, simulation description, and provided data, select all of the following statements that are correct interpretations or valid conclusions.", "Answer": ["A", "C"], "qid": "82", "question": "### Background\n\n**Research Question.** This problem evaluates the use of a structural model to conduct a counterfactual policy simulation, specifically to estimate the economic cost of the Small Business Administration (SBA) set-aside program in timber auctions.\n\n**Setting.** The SBA set-aside program restricts participation in some auctions to small firms (those with fewer than 500 employees). The cost of this program is simulated by comparing actual auction outcomes to a counterfactual world where the program's effects are eliminated.\n\n**Variables and Parameters.**\n- `ηᵢ`: The observable portion of bidder `i`'s private value component (units: $/mbf).\n- `HAULᵢ`: Hauling distance for bidder `i` (units: miles).\n- `SBAᵢ`: An indicator variable equal to 1 if bidder `i` is a small firm, and 0 otherwise.\n- `β₂`: The estimated coefficient on the `SBAᵢ` dummy.\n\n---\n\n### Data / Model Specification\n\nThe observable component of a bidder's value, `ηᵢ`, is specified as:\n  \nηᵢ = β₁ ⋅ HAULᵢ + β₂ ⋅ SBAᵢ\n \n**Table 1: Estimated Value Distribution Parameters**\n| Variable | Coefficient |\n| :--- | :--- |\n| Hauling Miles (`β₁`) | -2.08 |\n| SBA Dummy (`β₂`) | -71.63 |\n| `1/μ` | 39.66 |\n\n**Table 2: Simulated Costs of the SBA Set-Aside Program**\n| Variable | Mean |\n| :--- | :--- |\n| Simulated Winning Bids ($/mbf) | 225.54 |\n| Current Winning Bids ($/mbf) | 200.64 |\n| %Δ Auction Revenue | 14.8% |\n\nThe simulation assumes that small, high-cost firms 'evolve into large, more efficient firms.'\n\n---\n\n### Question\n\nBased on the model, simulation description, and provided data, select all of the following statements that are correct interpretations or valid conclusions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 83, "Question": "### Background\n\n**Research Question.** This problem involves interpreting the results of a structural simulation of merger effects and evaluating the welfare standard used to assess merger efficiencies.\n\n**Setting.** Using an estimated structural model of a timber auction market, the price effects of the most anticompetitive two-firm merger are simulated for each of 51 auctions. The results, including the cost savings required to offset the price effect, are summarized.\n\n**Variables and Parameters.**\n- `%Δ Merged Firm's Winning Bid`: The percentage change in the price paid by the merged firm, conditional on it winning.\n- `%Δ Industry Price`: The expected percentage change in the overall auction price (seller revenue).\n- `Offsetting %Δ MC`: The percentage reduction in marginal cost (increase in value) required for the merged firm to leave the expected industry price unchanged.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Mergers Among Two Best Bidders (Summary Statistics)**\n| Variable | Min | Max | Median | Mean |\n| :--- | :--- | :--- | :--- | :--- |\n| Sum of Merging Shares | 8.4 | 51.9 | 35.1 | 34.3 |\n| %Δ Merged Firm's Winning Bid | -0.4 | -5.4 | -2.7 | -2.6 |\n| %Δ Industry Price | -0.03 | -2.6 | -0.9 | -1.0 |\n| %Δ Merged Firm's Profits | 0.19 | 12.05 | 4.2 | 4.8 |\n| Offsetting %Δ MC | 0.6 | 9.7 | 3.8 | 4.1 |\n\nThe paper states that `%Δ Industry Price` is calculated as the change in the merged firm's winning bid multiplied by the merged firm's probability of winning.\n\n---\n\n### Question\n\nBased on the data in Table 1 and the principles of a second-price auction, select all of the following statements that are valid.", "Answer": ["A", "C"], "qid": "83", "question": "### Background\n\n**Research Question.** This problem involves interpreting the results of a structural simulation of merger effects and evaluating the welfare standard used to assess merger efficiencies.\n\n**Setting.** Using an estimated structural model of a timber auction market, the price effects of the most anticompetitive two-firm merger are simulated for each of 51 auctions. The results, including the cost savings required to offset the price effect, are summarized.\n\n**Variables and Parameters.**\n- `%Δ Merged Firm's Winning Bid`: The percentage change in the price paid by the merged firm, conditional on it winning.\n- `%Δ Industry Price`: The expected percentage change in the overall auction price (seller revenue).\n- `Offsetting %Δ MC`: The percentage reduction in marginal cost (increase in value) required for the merged firm to leave the expected industry price unchanged.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Mergers Among Two Best Bidders (Summary Statistics)**\n| Variable | Min | Max | Median | Mean |\n| :--- | :--- | :--- | :--- | :--- |\n| Sum of Merging Shares | 8.4 | 51.9 | 35.1 | 34.3 |\n| %Δ Merged Firm's Winning Bid | -0.4 | -5.4 | -2.7 | -2.6 |\n| %Δ Industry Price | -0.03 | -2.6 | -0.9 | -1.0 |\n| %Δ Merged Firm's Profits | 0.19 | 12.05 | 4.2 | 4.8 |\n| Offsetting %Δ MC | 0.6 | 9.7 | 3.8 | 4.1 |\n\nThe paper states that `%Δ Industry Price` is calculated as the change in the merged firm's winning bid multiplied by the merged firm's probability of winning.\n\n---\n\n### Question\n\nBased on the data in Table 1 and the principles of a second-price auction, select all of the following statements that are valid.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 84, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical properties of the extreme-value distribution that make it a tractable choice for modeling asymmetric auctions and firm mergers.\n\n**Setting.** The model assumes each bidder `i`'s private value component, `Xᵢ`, is drawn independently from an extreme-value distribution. A key modeling step involves characterizing the distribution of the maximum of these values, which is relevant for determining winners and for modeling the value of a merged entity.\n\n---\n\n### Data / Model Specification\n\nThe cumulative distribution function (CDF) for a single bidder `i`'s value `Xᵢ` is given by:\n  \nFᵢ(t) = Pr(Xᵢ ≤ t) = exp[-exp(-μ(t - ηᵢ))] \n \nThe paper states that the maximum of a set of independent extreme-value variates is also an extreme-value variate. This property is known as 'closure under the maximum function'. In this model, the value of a merged firm is the maximum of the values of its constituent members.\n\n---\n\n### Question\n\nSelect all of the following statements that are correct regarding the model's properties and their interpretation.", "Answer": ["C", "D"], "qid": "84", "question": "### Background\n\n**Research Question.** This problem examines the theoretical properties of the extreme-value distribution that make it a tractable choice for modeling asymmetric auctions and firm mergers.\n\n**Setting.** The model assumes each bidder `i`'s private value component, `Xᵢ`, is drawn independently from an extreme-value distribution. A key modeling step involves characterizing the distribution of the maximum of these values, which is relevant for determining winners and for modeling the value of a merged entity.\n\n---\n\n### Data / Model Specification\n\nThe cumulative distribution function (CDF) for a single bidder `i`'s value `Xᵢ` is given by:\n  \nFᵢ(t) = Pr(Xᵢ ≤ t) = exp[-exp(-μ(t - ηᵢ))] \n \nThe paper states that the maximum of a set of independent extreme-value variates is also an extreme-value variate. This property is known as 'closure under the maximum function'. In this model, the value of a merged firm is the maximum of the values of its constituent members.\n\n---\n\n### Question\n\nSelect all of the following statements that are correct regarding the model's properties and their interpretation.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 85, "Question": "### Background\n\n**Research Question.** This problem addresses the core identification strategy for estimating the parameters of a structural auction model in the presence of unobserved heterogeneity across auctions.\n\n**Setting.** The model specifies a bidder's value `Vᵢ` as the sum of an unobserved auction-specific common component `Y` and a bidder-specific private component `Xᵢ`. The institutional setting is a Forest Service oral auction where losing bids are publicly recorded.\n\n---\n\n### Data / Model Specification\n\nThe value structure is:\n  \nV_i = Y + X_i\n \nThe core identification assumption, based on the oral auction rules, is that losing bidders bid truthfully up to their values. This implies that the observed k-th highest bid from a losing bidder corresponds to the k-th highest value:\n  \nB_{(k)} = V_{(k)} = Y + X_{(k)} \\quad \\text{for } k \\ge 2 \\quad \\text{(Eq. 1)}\n \nThe estimation strategy relies on the difference between the second- and third-highest bids, `Δ = B₍₂₎ - B₍₃₎`.\n\n---\n\n### Question\n\nBased on the model and identification strategy, select all of the following statements that are correct.", "Answer": ["C", "D"], "qid": "85", "question": "### Background\n\n**Research Question.** This problem addresses the core identification strategy for estimating the parameters of a structural auction model in the presence of unobserved heterogeneity across auctions.\n\n**Setting.** The model specifies a bidder's value `Vᵢ` as the sum of an unobserved auction-specific common component `Y` and a bidder-specific private component `Xᵢ`. The institutional setting is a Forest Service oral auction where losing bids are publicly recorded.\n\n---\n\n### Data / Model Specification\n\nThe value structure is:\n  \nV_i = Y + X_i\n \nThe core identification assumption, based on the oral auction rules, is that losing bidders bid truthfully up to their values. This implies that the observed k-th highest bid from a losing bidder corresponds to the k-th highest value:\n  \nB_{(k)} = V_{(k)} = Y + X_{(k)} \\quad \\text{for } k \\ge 2 \\quad \\text{(Eq. 1)}\n \nThe estimation strategy relies on the difference between the second- and third-highest bids, `Δ = B₍₂₎ - B₍₃₎`.\n\n---\n\n### Question\n\nBased on the model and identification strategy, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 86, "Question": "### Background\n\n**Research Question.** This problem examines the foundational asymptotic theory that underpins the statistical tests for comparing Absolute Concentration Curves (ACCs) and the practical use of the bootstrap for inference.\n\n**Setting / Institutional Environment.** The analysis is theoretical, establishing the large-sample properties of the difference between empirical and true ACCs. This result is crucial for justifying the use of test statistics for hypothesis testing when their limiting distributions are not known in closed form.\n\n### Data / Model Specification\n\nLet `A(t)` and `B(t)` be the true, population ACCs for assets Y and Z with respect to a portfolio X. Let `A_n(t)` and `B_n(t)` be their empirical counterparts estimated from a sample of size `n`.\n\nThe entire inference strategy of the paper relies on the weak convergence of the normalized difference process `D_n(t)`:\n\n  \nD_{n}(t) = \\sqrt{n}((A_{n}(t)-B_{n}(t))-(A(t)-B(t))) \\Rightarrow \\Gamma(t) \\quad \\text{(Eq. 1)}\n \n\nwhere `\\Rightarrow` denotes weak convergence and `\\Gamma(t)` is a mean-zero Gaussian process. This result holds under certain regularity conditions, including an i.i.d. assumption on the data.\n\nTo perform inference in practice, an observable bootstrap process `D_n^*(t)` is constructed by drawing a bootstrap sample (size `n`, with replacement) and computing its ACCs `A_n^*(t)` and `B_n^*(t)`:\n\n  \nD_{n}^{*}(t) = \\sqrt{n}((A_{n}^{*}(t)-B_{n}^{*}(t))-(A_{n}(t)-B_{n}(t))) \\quad \\text{(Eq. 2)}\n \n\nIt can be shown that `D_n^*(t)` converges weakly to the same limiting process `\\Gamma(t)`.\n\n### Question\n\nBased on the provided asymptotic theory, select all statements that are **correct** descriptions of the paper's inferential framework and its underlying assumptions.", "Answer": ["A", "C"], "qid": "86", "question": "### Background\n\n**Research Question.** This problem examines the foundational asymptotic theory that underpins the statistical tests for comparing Absolute Concentration Curves (ACCs) and the practical use of the bootstrap for inference.\n\n**Setting / Institutional Environment.** The analysis is theoretical, establishing the large-sample properties of the difference between empirical and true ACCs. This result is crucial for justifying the use of test statistics for hypothesis testing when their limiting distributions are not known in closed form.\n\n### Data / Model Specification\n\nLet `A(t)` and `B(t)` be the true, population ACCs for assets Y and Z with respect to a portfolio X. Let `A_n(t)` and `B_n(t)` be their empirical counterparts estimated from a sample of size `n`.\n\nThe entire inference strategy of the paper relies on the weak convergence of the normalized difference process `D_n(t)`:\n\n  \nD_{n}(t) = \\sqrt{n}((A_{n}(t)-B_{n}(t))-(A(t)-B(t))) \\Rightarrow \\Gamma(t) \\quad \\text{(Eq. 1)}\n \n\nwhere `\\Rightarrow` denotes weak convergence and `\\Gamma(t)` is a mean-zero Gaussian process. This result holds under certain regularity conditions, including an i.i.d. assumption on the data.\n\nTo perform inference in practice, an observable bootstrap process `D_n^*(t)` is constructed by drawing a bootstrap sample (size `n`, with replacement) and computing its ACCs `A_n^*(t)` and `B_n^*(t)`:\n\n  \nD_{n}^{*}(t) = \\sqrt{n}((A_{n}^{*}(t)-B_{n}^{*}(t))-(A_{n}(t)-B_{n}(t))) \\quad \\text{(Eq. 2)}\n \n\nIt can be shown that `D_n^*(t)` converges weakly to the same limiting process `\\Gamma(t)`.\n\n### Question\n\nBased on the provided asymptotic theory, select all statements that are **correct** descriptions of the paper's inferential framework and its underlying assumptions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 87, "Question": "### Background\n\n**Research Question.** This problem explores the architecture of the hypothesis testing framework for comparing Absolute Concentration Curves (ACCs), from the fundamental definitions to the construction of specific, tailored test statistics.\n\n**Setting / Institutional Environment.** An econometrician wishes to formally test various hypotheses about the relationship between two ACCs, `A(t)` and `B(t)`. These hypotheses involve comparing the functions over their entire domain `t` in `[0,1]`.\n\n### Data / Model Specification\n\nThe Absolute Concentration Curve (ACC) for an asset with return `Y` with respect to a portfolio with value `X` (with CDF `F`) is defined as:\n\n  \nA(t) := \\operatorname{E}[Y \\cdot 1_{[X \\leq F^{-1}(t)]}] \\quad \\text{(Eq. 1)}\n \n\nTwo key hypotheses are converted into tests on scalar parameters:\n1.  **Equality (`H_{01}: A = B`):** Tested using the parameter `\\tau_1 := \\sup|A-B| = s(A-B) \\lor s(B-A)`, where `\\lor` is the maximum.\n2.  **Non-intersection (`H_{03}: A \\le B` or `A \\ge B`):** Tested against the alternative of intersection using the parameter `\\tau_3 := s(A-B) \\wedge s(B-A)`, where `\\wedge` is the minimum.\n\n### Question\n\nBased on the provided definitions, select all statements that are **correct** interpretations of the Absolute Concentration Curve (ACC) and the logic of the test parameters.", "Answer": ["A", "B"], "qid": "87", "question": "### Background\n\n**Research Question.** This problem explores the architecture of the hypothesis testing framework for comparing Absolute Concentration Curves (ACCs), from the fundamental definitions to the construction of specific, tailored test statistics.\n\n**Setting / Institutional Environment.** An econometrician wishes to formally test various hypotheses about the relationship between two ACCs, `A(t)` and `B(t)`. These hypotheses involve comparing the functions over their entire domain `t` in `[0,1]`.\n\n### Data / Model Specification\n\nThe Absolute Concentration Curve (ACC) for an asset with return `Y` with respect to a portfolio with value `X` (with CDF `F`) is defined as:\n\n  \nA(t) := \\operatorname{E}[Y \\cdot 1_{[X \\leq F^{-1}(t)]}] \\quad \\text{(Eq. 1)}\n \n\nTwo key hypotheses are converted into tests on scalar parameters:\n1.  **Equality (`H_{01}: A = B`):** Tested using the parameter `\\tau_1 := \\sup|A-B| = s(A-B) \\lor s(B-A)`, where `\\lor` is the maximum.\n2.  **Non-intersection (`H_{03}: A \\le B` or `A \\ge B`):** Tested against the alternative of intersection using the parameter `\\tau_3 := s(A-B) \\wedge s(B-A)`, where `\\wedge` is the minimum.\n\n### Question\n\nBased on the provided definitions, select all statements that are **correct** interpretations of the Absolute Concentration Curve (ACC) and the logic of the test parameters.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 88, "Question": "### Background\n\nThis problem explores the paper's central theoretical contribution: in a dynamic model of unemployment with search frictions, the existence of a “Samuelsonian surplus” creates a fundamental conflict of interest between the current generation and future generations, rendering the allocation that maximizes net output Pareto inefficient.\n\n### Data / Model Specification\n\nConsider a discrete-time, overlapping-generations search model with the following features:\n- There is a population of `L` agents who face a constant per-period probability of death, `d`.\n- Unemployed workers expend search effort `e` at a utility cost `c(e)` to find a productive partner.\n- `X` denotes the number of productive matches (jobs). Each match produces one unit of output.\n- The welfare of a newborn agent (representing all future generations) is their expected lifetime utility, `V_U`, which is proportional to the economy's steady-state net output, `Y(X)`.\n- The aggregate welfare of the current generation, `V`, is the sum of the total value of lifetime net output and a social surplus term, `S(X)`.\n\nThe relevant welfare functions are given by:\n  \nV_U(X) = \\frac{Y(X)}{Ld} \\quad \\text{(Eq. (1))}\n \n  \nV(X) = \\frac{Y(X)}{d} + S(X) \\quad \\text{(Eq. (2))}\n \nwhere:\n- Net output `Y(X)` is gross output `X` minus total search costs. It is assumed to be a concave function of `X`.\n- The social surplus `S(X)` arises from intergenerational transfers created by durable jobs. It is assumed to be strictly increasing in the level of employment `X` (i.e., `S'(X) > 0`).\n\nLet `X_F^*` be the employment level that maximizes the welfare of future generations (i.e., maximizes `Y(X)`), and `X_C^*` be the level that maximizes the welfare of the current generation (i.e., maximizes `V(X)`).\n\n### Question\n\nBased on the model, select all of the following statements that are correct.", "Answer": ["A", "D"], "qid": "88", "question": "### Background\n\nThis problem explores the paper's central theoretical contribution: in a dynamic model of unemployment with search frictions, the existence of a “Samuelsonian surplus” creates a fundamental conflict of interest between the current generation and future generations, rendering the allocation that maximizes net output Pareto inefficient.\n\n### Data / Model Specification\n\nConsider a discrete-time, overlapping-generations search model with the following features:\n- There is a population of `L` agents who face a constant per-period probability of death, `d`.\n- Unemployed workers expend search effort `e` at a utility cost `c(e)` to find a productive partner.\n- `X` denotes the number of productive matches (jobs). Each match produces one unit of output.\n- The welfare of a newborn agent (representing all future generations) is their expected lifetime utility, `V_U`, which is proportional to the economy's steady-state net output, `Y(X)`.\n- The aggregate welfare of the current generation, `V`, is the sum of the total value of lifetime net output and a social surplus term, `S(X)`.\n\nThe relevant welfare functions are given by:\n  \nV_U(X) = \\frac{Y(X)}{Ld} \\quad \\text{(Eq. (1))}\n \n  \nV(X) = \\frac{Y(X)}{d} + S(X) \\quad \\text{(Eq. (2))}\n \nwhere:\n- Net output `Y(X)` is gross output `X` minus total search costs. It is assumed to be a concave function of `X`.\n- The social surplus `S(X)` arises from intergenerational transfers created by durable jobs. It is assumed to be strictly increasing in the level of employment `X` (i.e., `S'(X) > 0`).\n\nLet `X_F^*` be the employment level that maximizes the welfare of future generations (i.e., maximizes `Y(X)`), and `X_C^*` be the level that maximizes the welfare of the current generation (i.e., maximizes `V(X)`).\n\n### Question\n\nBased on the model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem explores the paper's ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem explores the paper's ...", "error": null}}
{"ID": 89, "Question": "### Background\n\nThis problem deconstructs the core mechanism of the Samuelsonian surplus using the paper's simplest framework: a competitive economy with a minimum wage. It examines the laissez-faire equilibrium, the welfare effects of the minimum wage, and the necessary conditions for its existence.\n\n### Data / Model Specification\n\nConsider a discrete-time, overlapping-generations economy with the following features:\n- A population of `L` workers, each endowed with `\\alpha` units of capital.\n- A constant per-period probability of death, `d`.\n- Production is governed by a fixed-proportions function: `X = \\min(K_E, L_E)`, where `X` is output and `K_E, L_E` are employed capital and labor. Cost minimization implies `K_E = L_E`.\n- The economy has surplus labor, enforced by the assumption `\\alpha < 1`, which implies the aggregate capital stock `K = \\alpha L` is less than the labor force `L`.\n- The price of output is normalized to 1, and in a competitive market, this equals the cost of production: `1 = r + w`, where `r` is the return to capital and `w` is the wage.\n- `V_E` and `V_U` are the expected lifetime incomes for employed and unemployed workers, respectively. In this model, the solved expression for `V_U` is `V_U = (\\alpha r + \\alpha w) / d`.\n- Jobs are durable. A surviving worker keeps their job with probability `1-b`. The job survival rate is `h = (1-d)(1-b)`.\n\n### Question\n\nIn the context of the minimum wage model, select all of the following statements that are correct.", "Answer": ["C", "D"], "qid": "89", "question": "### Background\n\nThis problem deconstructs the core mechanism of the Samuelsonian surplus using the paper's simplest framework: a competitive economy with a minimum wage. It examines the laissez-faire equilibrium, the welfare effects of the minimum wage, and the necessary conditions for its existence.\n\n### Data / Model Specification\n\nConsider a discrete-time, overlapping-generations economy with the following features:\n- A population of `L` workers, each endowed with `\\alpha` units of capital.\n- A constant per-period probability of death, `d`.\n- Production is governed by a fixed-proportions function: `X = \\min(K_E, L_E)`, where `X` is output and `K_E, L_E` are employed capital and labor. Cost minimization implies `K_E = L_E`.\n- The economy has surplus labor, enforced by the assumption `\\alpha < 1`, which implies the aggregate capital stock `K = \\alpha L` is less than the labor force `L`.\n- The price of output is normalized to 1, and in a competitive market, this equals the cost of production: `1 = r + w`, where `r` is the return to capital and `w` is the wage.\n- `V_E` and `V_U` are the expected lifetime incomes for employed and unemployed workers, respectively. In this model, the solved expression for `V_U` is `V_U = (\\alpha r + \\alpha w) / d`.\n- Jobs are durable. A surviving worker keeps their job with probability `1-b`. The job survival rate is `h = (1-d)(1-b)`.\n\n### Question\n\nIn the context of the minimum wage model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem deconstructs the core...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem deconstructs the core...", "error": null}}
{"ID": 90, "Question": "### Background\n\nThis problem analyzes a fundamental impossibility result from the paper: no proper subsolution of the No-Envy solution can simultaneously satisfy the desirable properties of Neutrality and Consistency. The proof is a cornerstone of the paper, demonstrating the centrality of the No-Envy solution concept.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of the No-Envy solution `N`, meaning `φ(e) ⊆ N(e)` for all economies `e`. This solution must also satisfy Neutrality (indifference to utility-preserving permutations of bundles) and Consistency (an allocation's restriction to a subgroup must be optimal for that subgroup's subeconomy).\n\n**Theorem 1:** If a subsolution of the no-envy solution (`φ ⊆ N`) satisfies Neutrality and Consistency, then it must coincide with the no-envy solution (`φ = N`).\n\nThe proof relies on **Lemma 1**, which states that for any economy `e` and any envy-free allocation `z ∈ N(e)`, one can construct an augmented economy `e' = (Q ∪ {i₀}, A ∪ {α₀}, M'; u')` and an extended allocation `z'` such that `z'` is envy-free in `e'` and essentially unique (any other `z'' ∈ N(e')` is an indifferent permutation of `z'`).\n\nThe utility functions in `e'` are constructed as follows, where `(α₀, M₀)` is the bundle for the new agent `i₀` in allocation `z'`:\n\n  \n\\forall i \\in Q, \\quad u'_{i}(\\alpha_{0}, M_{0}) = u_{i}(\\sigma(i), m_{\\sigma(i)}) \\quad \\text{(Eq. (1))}\n \n\n  \n\\forall \\alpha \\in A, \\quad u'_{i_0}(\\alpha_{0}, M_{0}) = u'_{i_0}(\\alpha, m_{\\alpha}) \\quad \\text{(Eq. (2))}\n \n\nBased on the logic of the proof for Theorem 1, which of the following statements are valid steps or direct consequences of the argument?\n", "Answer": ["B", "D"], "qid": "90", "question": "### Background\n\nThis problem analyzes a fundamental impossibility result from the paper: no proper subsolution of the No-Envy solution can simultaneously satisfy the desirable properties of Neutrality and Consistency. The proof is a cornerstone of the paper, demonstrating the centrality of the No-Envy solution concept.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of the No-Envy solution `N`, meaning `φ(e) ⊆ N(e)` for all economies `e`. This solution must also satisfy Neutrality (indifference to utility-preserving permutations of bundles) and Consistency (an allocation's restriction to a subgroup must be optimal for that subgroup's subeconomy).\n\n**Theorem 1:** If a subsolution of the no-envy solution (`φ ⊆ N`) satisfies Neutrality and Consistency, then it must coincide with the no-envy solution (`φ = N`).\n\nThe proof relies on **Lemma 1**, which states that for any economy `e` and any envy-free allocation `z ∈ N(e)`, one can construct an augmented economy `e' = (Q ∪ {i₀}, A ∪ {α₀}, M'; u')` and an extended allocation `z'` such that `z'` is envy-free in `e'` and essentially unique (any other `z'' ∈ N(e')` is an indifferent permutation of `z'`).\n\nThe utility functions in `e'` are constructed as follows, where `(α₀, M₀)` is the bundle for the new agent `i₀` in allocation `z'`:\n\n  \n\\forall i \\in Q, \\quad u'_{i}(\\alpha_{0}, M_{0}) = u_{i}(\\sigma(i), m_{\\sigma(i)}) \\quad \\text{(Eq. (1))}\n \n\n  \n\\forall \\alpha \\in A, \\quad u'_{i_0}(\\alpha_{0}, M_{0}) = u'_{i_0}(\\alpha, m_{\\alpha}) \\quad \\text{(Eq. (2))}\n \n\nBased on the logic of the proof for Theorem 1, which of the following statements are valid steps or direct consequences of the argument?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes a fundamenta...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes a fundamenta...", "error": null}}
{"ID": 91, "Question": "### Background\n\nThis problem analyzes a fundamental impossibility result from the paper: no proper subsolution of the No-Envy solution can simultaneously satisfy the desirable properties of Neutrality and Consistency. The proof is a cornerstone of the paper, demonstrating the centrality of the No-Envy solution concept.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of the No-Envy solution `N`, meaning `φ(e) ⊆ N(e)` for all economies `e`. This solution must also satisfy Neutrality and Consistency.\n\n**Theorem 1:** If a subsolution of the no-envy solution (`φ ⊆ N`) satisfies Neutrality and Consistency, then it must coincide with the no-envy solution (`φ = N`).\n\nThe proof relies on **Lemma 1**, which states that for any economy `e` and any envy-free allocation `z ∈ N(e)`, one can construct an augmented economy `e' = (Q ∪ {i₀}, A ∪ {α₀}, M'; u')` and an extended allocation `z'` such that `z'` is envy-free in `e'` and essentially unique.\n\nThe utility functions in `e'` are constructed as follows, where `(α₀, M₀)` is the bundle for the new agent `i₀` in allocation `z'`:\n\n  \n\\forall i \\in Q, \\quad u'_{i}(\\alpha_{0}, M_{0}) = u_{i}(\\sigma(i), m_{\\sigma(i)}) \\quad \\text{(Eq. (1))}\n \n\n  \n\\forall \\alpha \\in A, \\quad u'_{i_0}(\\alpha_{0}, M_{0}) = u'_{i_0}(\\alpha, m_{\\alpha}) \\quad \\text{(Eq. (2))}\n \n\nIn the proof of Lemma 1, it is shown that for any other envy-free allocation `z''` in `e'`, the money vector must be identical (`m'' = m'`). A key step is proving that if one assumes `m''_{β} > m'_{β}` for some old object `β ∈ A`, a contradiction arises. Which of the following statements are valid steps in deriving this contradiction?\n", "Answer": ["B", "C", "D"], "qid": "91", "question": "### Background\n\nThis problem analyzes a fundamental impossibility result from the paper: no proper subsolution of the No-Envy solution can simultaneously satisfy the desirable properties of Neutrality and Consistency. The proof is a cornerstone of the paper, demonstrating the centrality of the No-Envy solution concept.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of the No-Envy solution `N`, meaning `φ(e) ⊆ N(e)` for all economies `e`. This solution must also satisfy Neutrality and Consistency.\n\n**Theorem 1:** If a subsolution of the no-envy solution (`φ ⊆ N`) satisfies Neutrality and Consistency, then it must coincide with the no-envy solution (`φ = N`).\n\nThe proof relies on **Lemma 1**, which states that for any economy `e` and any envy-free allocation `z ∈ N(e)`, one can construct an augmented economy `e' = (Q ∪ {i₀}, A ∪ {α₀}, M'; u')` and an extended allocation `z'` such that `z'` is envy-free in `e'` and essentially unique.\n\nThe utility functions in `e'` are constructed as follows, where `(α₀, M₀)` is the bundle for the new agent `i₀` in allocation `z'`:\n\n  \n\\forall i \\in Q, \\quad u'_{i}(\\alpha_{0}, M_{0}) = u_{i}(\\sigma(i), m_{\\sigma(i)}) \\quad \\text{(Eq. (1))}\n \n\n  \n\\forall \\alpha \\in A, \\quad u'_{i_0}(\\alpha_{0}, M_{0}) = u'_{i_0}(\\alpha, m_{\\alpha}) \\quad \\text{(Eq. (2))}\n \n\nIn the proof of Lemma 1, it is shown that for any other envy-free allocation `z''` in `e'`, the money vector must be identical (`m'' = m'`). A key step is proving that if one assumes `m''_{β} > m'_{β}` for some old object `β ∈ A`, a contradiction arises. Which of the following statements are valid steps in deriving this contradiction?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes a fundamenta...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes a fundamenta...", "error": null}}
{"ID": 92, "Question": "### Background\n\nThis problem investigates the axiomatic power of combining two individually weak consistency properties—Bilateral Consistency and Converse Consistency—to characterize the No-Envy solution `N`.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of `N` (`φ ⊆ N`) and satisfies Neutrality.\n\n**Axioms:**\n- **Bilateral Consistency:** A 'top-down' property. For any `z ∈ φ(e)`, its restriction to any 2-agent subgroup `Q'` must be in the solution for that subeconomy: `z_{Q'} ∈ φ(t_{Q'}^z(e))`. \n- **Converse Consistency:** A 'bottom-up' property. For any feasible allocation `z`, if its restriction to *every* 2-agent subgroup `Q'` is in the solution for that subeconomy (`z_{Q'} ∈ φ(t_{Q'}^z(e))`), then the original allocation must be in the solution: `z ∈ φ(e)`.\n\n**Theorem 2:** If `φ ⊆ N` satisfies Neutrality, Bilateral Consistency, and Converse Consistency, then `φ = N`.\n\nThe proof proceeds by contradiction. It assumes there exists an economy `e` and an allocation `z` such that `z ∈ N(e)` but `z ∉ φ(e)`. Based on this assumption and the provided axioms, which of the following statements are logically necessary consequences?\n", "Answer": ["B", "C", "D"], "qid": "92", "question": "### Background\n\nThis problem investigates the axiomatic power of combining two individually weak consistency properties—Bilateral Consistency and Converse Consistency—to characterize the No-Envy solution `N`.\n\n### Data / Model Specification\n\nConsider a solution `φ` that is a subsolution of `N` (`φ ⊆ N`) and satisfies Neutrality.\n\n**Axioms:**\n- **Bilateral Consistency:** A 'top-down' property. For any `z ∈ φ(e)`, its restriction to any 2-agent subgroup `Q'` must be in the solution for that subeconomy: `z_{Q'} ∈ φ(t_{Q'}^z(e))`. \n- **Converse Consistency:** A 'bottom-up' property. For any feasible allocation `z`, if its restriction to *every* 2-agent subgroup `Q'` is in the solution for that subeconomy (`z_{Q'} ∈ φ(t_{Q'}^z(e))`), then the original allocation must be in the solution: `z ∈ φ(e)`.\n\n**Theorem 2:** If `φ ⊆ N` satisfies Neutrality, Bilateral Consistency, and Converse Consistency, then `φ = N`.\n\nThe proof proceeds by contradiction. It assumes there exists an economy `e` and an allocation `z` such that `z ∈ N(e)` but `z ∉ φ(e)`. Based on this assumption and the provided axioms, which of the following statements are logically necessary consequences?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the axio...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the axio...", "error": null}}
{"ID": 93, "Question": "### Background\n\nThis problem investigates the relationship between equity (no-envy) and efficiency (Pareto optimality) in the context of allocating indivisible objects and money.\n\n### Data / Model Specification\n\nAn allocation `z = (σ, m)` is **envy-free** if `uᵢ(zᵢ) ≥ uᵢ(zⱼ)` for all agents `i, j`. The set of such allocations is `N(e)`.\n\nAn allocation `z` is **Pareto efficient** if there is no other feasible allocation `z'` that makes at least one agent strictly better off without making any agent worse off. The set of such allocations is `P(e)`.\n\n**Proposition 1:** The no-envy solution is a subsolution of the Pareto solution (`N(e) ⊆ P(e)`).\n\nThe proof proceeds by contradiction. It assumes an allocation `z = (σ, m)` is envy-free (`z ∈ N(e)`) but is Pareto dominated by another feasible allocation `z' = (σ', m')`. Which of the following statements are valid steps in the derivation of the contradiction that proves Proposition 1?\n", "Answer": ["B", "C", "D"], "qid": "93", "question": "### Background\n\nThis problem investigates the relationship between equity (no-envy) and efficiency (Pareto optimality) in the context of allocating indivisible objects and money.\n\n### Data / Model Specification\n\nAn allocation `z = (σ, m)` is **envy-free** if `uᵢ(zᵢ) ≥ uᵢ(zⱼ)` for all agents `i, j`. The set of such allocations is `N(e)`.\n\nAn allocation `z` is **Pareto efficient** if there is no other feasible allocation `z'` that makes at least one agent strictly better off without making any agent worse off. The set of such allocations is `P(e)`.\n\n**Proposition 1:** The no-envy solution is a subsolution of the Pareto solution (`N(e) ⊆ P(e)`).\n\nThe proof proceeds by contradiction. It assumes an allocation `z = (σ, m)` is envy-free (`z ∈ N(e)`) but is Pareto dominated by another feasible allocation `z' = (σ', m')`. Which of the following statements are valid steps in the derivation of the contradiction that proves Proposition 1?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the rela...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the rela...", "error": null}}
{"ID": 94, "Question": "### Background\n\n**Research Question.** This question assesses understanding of the core informational assumption—strict signal affiliation—that underpins the paper's model of jump bidding as a strategic signalling game.\n\n**Setting.** A two-bidder, risk-neutral English auction. The value of the object to each bidder depends on their own private signal and their opponent's private signal. The auction is modeled as a two-stage \"metagame\": in Stage 1, bidders choose an opening bid of 0 or a fixed amount `K > 0`. In Stage 2, an open exit auction proceeds. If one bidder makes an unmatched jump bid, they become the \"aggressive\" bidder and their opponent becomes the \"weak\" bidder. Otherwise, standard symmetric bidding ensues.\n\n### Data / Model Specification\n\nThe valuation for bidder `i` is a function of their own signal `X_i` and their opponent's signal `X_j`:\n\n  \nV_i = v(X_i, X_j)\n \n\nwhere `v` is continuous and weakly increasing in both arguments. The signals `X_i` and `X_j` are drawn from a support `(0, \bar{X})` and are **strictly affiliated**. For univariate signals, this is defined by the strict Monotone Likelihood Ratio Property (MLRP). For any `x_j' > x_j` and `x_i' > x_i`, MLRP requires:\n\n  \n\\frac{g(x_{j}|x_{i})}{g(x_{j}|x_{i}^{\\prime})} > \\frac{g(x_{j}^{\\prime}|x_{i})}{g(x_{j}^{\\prime}|x_{i}^{\\prime})} \\quad \\text{(Eq. 1)}\n \n\nIn a symmetric signalling equilibrium, a bidder jumps if their signal `x` is above a certain cutoff `x*`. The expected gain from jump bidding for a bidder with signal `x` who wins is denoted `\\phi(x) = E[S^{*}(X_{j}) - P^{K}(X_{j}) | X_i=x, X_j < x]`, where `S*(x)` is the standard symmetric bid and `P^K(x)` is the price path after a jump.\n\n### Question\n\nAccording to the paper's model, which of the following statements correctly describe the role and implications of the strict signal affiliation (MLRP) assumption?", "Answer": ["A", "D"], "qid": "94", "question": "### Background\n\n**Research Question.** This question assesses understanding of the core informational assumption—strict signal affiliation—that underpins the paper's model of jump bidding as a strategic signalling game.\n\n**Setting.** A two-bidder, risk-neutral English auction. The value of the object to each bidder depends on their own private signal and their opponent's private signal. The auction is modeled as a two-stage \"metagame\": in Stage 1, bidders choose an opening bid of 0 or a fixed amount `K > 0`. In Stage 2, an open exit auction proceeds. If one bidder makes an unmatched jump bid, they become the \"aggressive\" bidder and their opponent becomes the \"weak\" bidder. Otherwise, standard symmetric bidding ensues.\n\n### Data / Model Specification\n\nThe valuation for bidder `i` is a function of their own signal `X_i` and their opponent's signal `X_j`:\n\n  \nV_i = v(X_i, X_j)\n \n\nwhere `v` is continuous and weakly increasing in both arguments. The signals `X_i` and `X_j` are drawn from a support `(0, \bar{X})` and are **strictly affiliated**. For univariate signals, this is defined by the strict Monotone Likelihood Ratio Property (MLRP). For any `x_j' > x_j` and `x_i' > x_i`, MLRP requires:\n\n  \n\\frac{g(x_{j}|x_{i})}{g(x_{j}|x_{i}^{\\prime})} > \\frac{g(x_{j}^{\\prime}|x_{i})}{g(x_{j}^{\\prime}|x_{i}^{\\prime})} \\quad \\text{(Eq. 1)}\n \n\nIn a symmetric signalling equilibrium, a bidder jumps if their signal `x` is above a certain cutoff `x*`. The expected gain from jump bidding for a bidder with signal `x` who wins is denoted `\\phi(x) = E[S^{*}(X_{j}) - P^{K}(X_{j}) | X_i=x, X_j < x]`, where `S*(x)` is the standard symmetric bid and `P^K(x)` is the price path after a jump.\n\n### Question\n\nAccording to the paper's model, which of the following statements correctly describe the role and implications of the strict signal affiliation (MLRP) assumption?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 95, "Question": "### Background\n\n**Research Question.** This question analyzes the strategic incentives and revenue implications of jump bidding using the paper's core motivating example. It compares outcomes from standard auction formats to a hybrid auction involving a potential jump bid.\n\n**Setting.** A two-bidder (Xenia and Yakov) auction for a single good with a common value component. Bidders receive affiliated private signals. We compare outcomes from standard auction formats to a hybrid auction involving a potential jump bid.\n\n### Data / Model Specification\n\nTwo bidders, Xenia and Yakov, receive private signals `X = A + B` and `Y = B + C`, where `A`, `B`, and `C` are independent U(0, 1) random variables. The true common value of the good is `V = (X + Y) / 2`.\n\nThe benchmark symmetric equilibrium bidding functions for standard auctions are:\n- Second-Price / Open Exit Auction: `B^{*}(x) = x`\n- First-Price Auction: `B^{*1}(x) = 2x/3`\n\nConsider a **hybrid auction game**:\n1.  In Stage 1, bidders simultaneously choose to bid 0 or a jump bid of `K = 2/3`.\n2.  If only one bidder jumps, the auction ends, and they win at price `K`.\n3.  If both or neither jump, the auction proceeds as a standard open exit auction (bidders use `B*(x) = x`).\n\nThe symmetric equilibrium for this game involves a cutoff `x* = 1`, where a bidder jumps to `K` if and only if their signal is 1 or greater. The paper shows that for `x* \\leq 1`, the cutoff is determined by the relation `K = 2x*/3`.\n\nThe expected revenues for the three mechanisms are:\n- `ER_2 = 30/36` (Second-Price)\n- `ER_1 = 28/36` (First-Price)\n- `ER_hybrid = 29/36` (Hybrid Jump-Bidding)\n\n### Question\n\nBased on the provided motivating example and its underlying model, which of the following statements are correct?", "Answer": ["A", "D"], "qid": "95", "question": "### Background\n\n**Research Question.** This question analyzes the strategic incentives and revenue implications of jump bidding using the paper's core motivating example. It compares outcomes from standard auction formats to a hybrid auction involving a potential jump bid.\n\n**Setting.** A two-bidder (Xenia and Yakov) auction for a single good with a common value component. Bidders receive affiliated private signals. We compare outcomes from standard auction formats to a hybrid auction involving a potential jump bid.\n\n### Data / Model Specification\n\nTwo bidders, Xenia and Yakov, receive private signals `X = A + B` and `Y = B + C`, where `A`, `B`, and `C` are independent U(0, 1) random variables. The true common value of the good is `V = (X + Y) / 2`.\n\nThe benchmark symmetric equilibrium bidding functions for standard auctions are:\n- Second-Price / Open Exit Auction: `B^{*}(x) = x`\n- First-Price Auction: `B^{*1}(x) = 2x/3`\n\nConsider a **hybrid auction game**:\n1.  In Stage 1, bidders simultaneously choose to bid 0 or a jump bid of `K = 2/3`.\n2.  If only one bidder jumps, the auction ends, and they win at price `K`.\n3.  If both or neither jump, the auction proceeds as a standard open exit auction (bidders use `B*(x) = x`).\n\nThe symmetric equilibrium for this game involves a cutoff `x* = 1`, where a bidder jumps to `K` if and only if their signal is 1 or greater. The paper shows that for `x* \\leq 1`, the cutoff is determined by the relation `K = 2x*/3`.\n\nThe expected revenues for the three mechanisms are:\n- `ER_2 = 30/36` (Second-Price)\n- `ER_1 = 28/36` (First-Price)\n- `ER_hybrid = 29/36` (Hybrid Jump-Bidding)\n\n### Question\n\nBased on the provided motivating example and its underlying model, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 96, "Question": "### Background\n\n**Research Question.** This problem analyzes the long-run determinants of the skill premium in Chile, using a cointegration framework to test predictions from the Heckscher-Ohlin-Samuelson (HOS) model of trade.\n\n**Setting / Institutional Environment.** The study uses annual time series data for Chile from 1960-1996. After establishing that the key variables are non-stationary, a cointegrating regression is estimated to identify a stable long-run equilibrium relationship.\n\n**Variables & Parameters.**\n- `DCG-DEG`: The skill premium, defined as the difference between the log-wage regression coefficients for college graduates and elementary graduates (dependent variable).\n- `Open`: Openness to trade, measured as the volume of trade (exports + imports) as a percentage of GDP.\n- `Ptex`: A relative wholesale price index of textile products, serving as a proxy for the price of goods intensive in unskilled labor.\n- `Univer`: The proportion of the labor force with a college degree, serving as a proxy for the relative supply of skilled labor.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run cointegrating relationship is:\n\n  \n\\big(\\mathrm{DCG}-\\mathrm{DEG}\\big)_t = 1.908 + 0.0131\\big(\\mathrm{Open}\\big)_t - 0.357\\big(\\mathrm{Ptex}\\big)_t - 0.027\\big(\\mathrm{Univer}\\big)_t \n \n\n**Table 1: Cointegration Regression Results (Dependent Variable: DCG-DEG)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| Intercept | 1.908 | (8.81) |\n| Open | 0.0131 | (3.64) |\n| Ptex | -0.357 | (-2.09) |\n| Univer | -0.027 | (-2.61) |\n\n*Notes: Period 1960-1996 (N=37). The reported ADF test statistic on the residuals (-4.924) confirms that the null hypothesis of no cointegration is rejected at the 1% level.* \n\n---\n\nBased on the model and results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "96", "question": "### Background\n\n**Research Question.** This problem analyzes the long-run determinants of the skill premium in Chile, using a cointegration framework to test predictions from the Heckscher-Ohlin-Samuelson (HOS) model of trade.\n\n**Setting / Institutional Environment.** The study uses annual time series data for Chile from 1960-1996. After establishing that the key variables are non-stationary, a cointegrating regression is estimated to identify a stable long-run equilibrium relationship.\n\n**Variables & Parameters.**\n- `DCG-DEG`: The skill premium, defined as the difference between the log-wage regression coefficients for college graduates and elementary graduates (dependent variable).\n- `Open`: Openness to trade, measured as the volume of trade (exports + imports) as a percentage of GDP.\n- `Ptex`: A relative wholesale price index of textile products, serving as a proxy for the price of goods intensive in unskilled labor.\n- `Univer`: The proportion of the labor force with a college degree, serving as a proxy for the relative supply of skilled labor.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run cointegrating relationship is:\n\n  \n\\big(\\mathrm{DCG}-\\mathrm{DEG}\\big)_t = 1.908 + 0.0131\\big(\\mathrm{Open}\\big)_t - 0.357\\big(\\mathrm{Ptex}\\big)_t - 0.027\\big(\\mathrm{Univer}\\big)_t \n \n\n**Table 1: Cointegration Regression Results (Dependent Variable: DCG-DEG)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| Intercept | 1.908 | (8.81) |\n| Open | 0.0131 | (3.64) |\n| Ptex | -0.357 | (-2.09) |\n| Univer | -0.027 | (-2.61) |\n\n*Notes: Period 1960-1996 (N=37). The reported ADF test statistic on the residuals (-4.924) confirms that the null hypothesis of no cointegration is rejected at the 1% level.* \n\n---\n\nBased on the model and results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 97, "Question": "### Background\n\n**Research Question.** This problem investigates the evolution of the skill premium in Chile by examining how the relative wages between different educational groups changed over a period of major economic reforms.\n\n**Setting / Institutional Environment.** The analysis uses survey data on average labor income for heads of households in Chile, categorized by education level, for selected years between 1960 and 1996.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Average Labor Income Ratios by Education Level of Head of Household**\n\n| | 1960 | 1970 | 1980 | 1990 | 1996 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **University/secondary education** | | | | | |\n| **Total** | **2.2** | **2.3** | **2.4** | **3.6** | **3.3** |\n| **Secondary/primary education** | | | | | |\n| **Total** | **2.3** | **2.2** | **2.4** | **2.0** | **1.7** |\n\n---\n\nBased on the data in Table 1, which of the following statements accurately describe the trends in educational wage premiums in Chile? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "97", "question": "### Background\n\n**Research Question.** This problem investigates the evolution of the skill premium in Chile by examining how the relative wages between different educational groups changed over a period of major economic reforms.\n\n**Setting / Institutional Environment.** The analysis uses survey data on average labor income for heads of households in Chile, categorized by education level, for selected years between 1960 and 1996.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Average Labor Income Ratios by Education Level of Head of Household**\n\n| | 1960 | 1970 | 1980 | 1990 | 1996 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **University/secondary education** | | | | | |\n| **Total** | **2.2** | **2.3** | **2.4** | **3.6** | **3.3** |\n| **Secondary/primary education** | | | | | |\n| **Total** | **2.3** | **2.2** | **2.4** | **2.0** | **1.7** |\n\n---\n\nBased on the data in Table 1, which of the following statements accurately describe the trends in educational wage premiums in Chile? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 98, "Question": "### Background\n\n**Research Question.** This problem explores the core price-setting mechanism of the Heckscher-Ohlin-Samuelson (HOS) model of international trade and the conditions under which its key predictions hold or fail.\n\n**Setting / Institutional Environment.** The setting is a standard n-good, n-factor HOS model assuming constant returns to scale technology and perfect competition. The economy is open to trade, which influences domestic goods prices.\n\n**Variables & Parameters.**\n- `w`: An n-dimensional column vector of factor prices (e.g., wage for skilled labor, wage for unskilled labor).\n- `P`: An n-dimensional column vector of final goods prices.\n- `A`: An n x n matrix of technological coefficients, where each element `a_ij` represents the quantity of factor `i` required to produce one unit of good `j`.\n\n---\n\n### Data / Model Specification\n\nThe model is characterized by a set of zero-profit conditions, which state that in a competitive equilibrium, the cost of producing any good must be at least as high as its market price.\n\n  \nAw \\geq P \\quad \\text{(Eq. (1))}\n \n\nIf a country produces a set of goods, then for those goods, the zero-profit condition holds with equality, leading to the following relationship for factor prices:\n\n  \nw = A^{-1}P \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the HOS model as specified, which of the following are **INVALID** conclusions or assumptions? Select all that apply.", "Answer": ["A", "C"], "qid": "98", "question": "### Background\n\n**Research Question.** This problem explores the core price-setting mechanism of the Heckscher-Ohlin-Samuelson (HOS) model of international trade and the conditions under which its key predictions hold or fail.\n\n**Setting / Institutional Environment.** The setting is a standard n-good, n-factor HOS model assuming constant returns to scale technology and perfect competition. The economy is open to trade, which influences domestic goods prices.\n\n**Variables & Parameters.**\n- `w`: An n-dimensional column vector of factor prices (e.g., wage for skilled labor, wage for unskilled labor).\n- `P`: An n-dimensional column vector of final goods prices.\n- `A`: An n x n matrix of technological coefficients, where each element `a_ij` represents the quantity of factor `i` required to produce one unit of good `j`.\n\n---\n\n### Data / Model Specification\n\nThe model is characterized by a set of zero-profit conditions, which state that in a competitive equilibrium, the cost of producing any good must be at least as high as its market price.\n\n  \nAw \\geq P \\quad \\text{(Eq. (1))}\n \n\nIf a country produces a set of goods, then for those goods, the zero-profit condition holds with equality, leading to the following relationship for factor prices:\n\n  \nw = A^{-1}P \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the HOS model as specified, which of the following are **INVALID** conclusions or assumptions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 99, "Question": "### Background\n\n**Research Question.** This problem investigates the central result of the paper: under what conditions a principal in a dynamic moral hazard model with renegotiation prefers to induce an agent to randomize her action—thereby creating endogenous adverse selection—rather than implementing a high-effort action with certainty.\n\n**Setting / Institutional Environment.** A risk-neutral principal contracts with a risk-averse agent over two periods. The agent's action in period 1 is unobservable and has long-term effects on output. Crucially, the contract can be renegotiated at the start of period 2 after the period 1 output is realized. The agent's period 1 action (`a_1` for shirk, `a_2` for diligent) becomes her private information ('type') in the period 2 renegotiation subgame.\n\n### Data / Model Specification\n\nThe technology linking actions to outputs (`x_1 < x_2`) in each period is:\n\n| Action | Prob(x_1) | Prob(x_2) |\n| :--- | :--- | :--- |\n| `a_1` | 1 | 0 |\n| `a_2` | 1-`γ` | `γ` |\n\nLet `h(v)` be the principal's cost of providing `v` units of utility to the agent; `h` is strictly convex. Let `G_j` be the per-period utility cost of action `a_j`, with `G_1 < G_2`. The optimal one-period contract to induce `a_2` consists of utility payments `v_1^* = G_1` and `v_2^* = G_1 + (G_2 - G_1)/γ`.\n\nThe principal's utility from implementing `a_2` with certainty (`p=1`) via an optimal renegotiation-proof contract is `B(a_2)`. The net productivity gain from action `a_2` is parameterized by `K > 0` such that:\n  \n\\Pi(a_2) - \\Pi(a_1) = \\gamma[h(v_2^*) - h(v_1^*)] + K \\quad \\text{(Eq. (1))}\n \nwhere `Π(a_j)` is the expected revenue from action `a_j`.\n\nTo test if a mixed strategy is optimal, the paper constructs a feasible contract `C_tilde` that implements `a_2` with some probability `p_tilde < 1`:\n`C_tilde = {(v_1^*, v_2^*), M(x_1)=((v_1^*, v_1^*), (v_1^*, v_2^*)), M(x_2)=G_2}`.\nThis means period 1 payments are `(v_1^*, v_2^*)`. If `x_1` occurs, the agent can choose between a safe contract paying `v_1^*` or a risky one paying `(v_1^*, v_2^*)` in period 2. If `x_2` occurs, she gets a safe payment `G_2`.\n\nThe principal's net gain from using this mixed-strategy contract `C_tilde` over the optimal pure-strategy (`p=1`) contract is:\n  \n\\text{Gain} = B(\\tilde{p};\\tilde{C}) - B(a_2) = -2(1-\\tilde{p})\\gamma K + \\tilde{p}\\gamma [h(v_2^*) - h(G_2)] \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the model and the provided contracts, select all statements that are correct.", "Answer": ["B", "C"], "qid": "99", "question": "### Background\n\n**Research Question.** This problem investigates the central result of the paper: under what conditions a principal in a dynamic moral hazard model with renegotiation prefers to induce an agent to randomize her action—thereby creating endogenous adverse selection—rather than implementing a high-effort action with certainty.\n\n**Setting / Institutional Environment.** A risk-neutral principal contracts with a risk-averse agent over two periods. The agent's action in period 1 is unobservable and has long-term effects on output. Crucially, the contract can be renegotiated at the start of period 2 after the period 1 output is realized. The agent's period 1 action (`a_1` for shirk, `a_2` for diligent) becomes her private information ('type') in the period 2 renegotiation subgame.\n\n### Data / Model Specification\n\nThe technology linking actions to outputs (`x_1 < x_2`) in each period is:\n\n| Action | Prob(x_1) | Prob(x_2) |\n| :--- | :--- | :--- |\n| `a_1` | 1 | 0 |\n| `a_2` | 1-`γ` | `γ` |\n\nLet `h(v)` be the principal's cost of providing `v` units of utility to the agent; `h` is strictly convex. Let `G_j` be the per-period utility cost of action `a_j`, with `G_1 < G_2`. The optimal one-period contract to induce `a_2` consists of utility payments `v_1^* = G_1` and `v_2^* = G_1 + (G_2 - G_1)/γ`.\n\nThe principal's utility from implementing `a_2` with certainty (`p=1`) via an optimal renegotiation-proof contract is `B(a_2)`. The net productivity gain from action `a_2` is parameterized by `K > 0` such that:\n  \n\\Pi(a_2) - \\Pi(a_1) = \\gamma[h(v_2^*) - h(v_1^*)] + K \\quad \\text{(Eq. (1))}\n \nwhere `Π(a_j)` is the expected revenue from action `a_j`.\n\nTo test if a mixed strategy is optimal, the paper constructs a feasible contract `C_tilde` that implements `a_2` with some probability `p_tilde < 1`:\n`C_tilde = {(v_1^*, v_2^*), M(x_1)=((v_1^*, v_1^*), (v_1^*, v_2^*)), M(x_2)=G_2}`.\nThis means period 1 payments are `(v_1^*, v_2^*)`. If `x_1` occurs, the agent can choose between a safe contract paying `v_1^*` or a risky one paying `(v_1^*, v_2^*)` in period 2. If `x_2` occurs, she gets a safe payment `G_2`.\n\nThe principal's net gain from using this mixed-strategy contract `C_tilde` over the optimal pure-strategy (`p=1`) contract is:\n  \n\\text{Gain} = B(\\tilde{p};\\tilde{C}) - B(a_2) = -2(1-\\tilde{p})\\gamma K + \\tilde{p}\\gamma [h(v_2^*) - h(G_2)] \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the model and the provided contracts, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 100, "Question": "### Background\n\n**Research Question.** This problem derives the optimal incentive contract and analyzes the conditions for its profitability in a benchmark one-period moral hazard model. This serves as a foundational building block for the paper's main dynamic analysis.\n\n**Setting / Institutional Environment.** A risk-neutral principal offers a one-period contract to a risk-averse agent. The agent chooses between a low-cost action `a_1` (shirking) and a high-cost action `a_2` (diligence). The agent's utility function over income `y` is `V(y)`, which is strictly concave. The principal's cost of providing `v` units of utility is `h(v) = V^{-1}(v)`, which is strictly convex. The per-period utility cost of action `a_j` is `G_j`, with `G_1 < G_2`.\n\n### Data / Model Specification\n\nThe technology linking actions to outputs (`x_1 < x_2`) is:\n\n| Action | Prob(x_1) | Prob(x_2) |\n| :--- | :--- | :--- |\n| `a_1` | 1 | 0 |\n| `a_2` | 1-`γ` | `γ` |\n\nTo implement the high-cost action `a_2`, the principal chooses utility payments `v_1` (for `x_1`) and `v_2` (for `x_2`) to minimize expected cost `(1-γ)h(v_1) + γh(v_2)` subject to two constraints:\n1.  **Individual Rationality (IR):** The agent's expected utility must meet her reservation utility (0).\n      \n    (1-\\gamma)v_1 + \\gamma v_2 - G_2 \\geq 0 \\quad \\text{(Eq. (1))}\n     \n2.  **Incentive Compatibility (IC):** The agent must prefer `a_2` over `a_1`. The utility from shirking is `v_1 - G_1`.\n      \n    (1-\\gamma)v_1 + \\gamma v_2 - G_2 \\geq v_1 - G_1 \\quad \\text{(Eq. (2))}\n     \nThe principal will only choose to implement `a_2` if it is profitable, which is captured by Assumption (A.1):\n  \n\\Pi(a_{2}) - \\left[(1-\\gamma)h(v_{1}^{*}) + \\gamma h(v_{2}^{*})\\right] > \\Pi(a_{1}) - h(G_{1}) \\quad \\text{(Eq. (3))}\n \nwhere `Π(a_j)` is expected revenue and `(v_1^*, v_2^*)` is the optimal contract.\n\n### Question\n\nConsider the optimal one-period contract `(v_1^*, v_2^*)` that implements action `a_2`. Select all statements that are correct.", "Answer": ["B", "D"], "qid": "100", "question": "### Background\n\n**Research Question.** This problem derives the optimal incentive contract and analyzes the conditions for its profitability in a benchmark one-period moral hazard model. This serves as a foundational building block for the paper's main dynamic analysis.\n\n**Setting / Institutional Environment.** A risk-neutral principal offers a one-period contract to a risk-averse agent. The agent chooses between a low-cost action `a_1` (shirking) and a high-cost action `a_2` (diligence). The agent's utility function over income `y` is `V(y)`, which is strictly concave. The principal's cost of providing `v` units of utility is `h(v) = V^{-1}(v)`, which is strictly convex. The per-period utility cost of action `a_j` is `G_j`, with `G_1 < G_2`.\n\n### Data / Model Specification\n\nThe technology linking actions to outputs (`x_1 < x_2`) is:\n\n| Action | Prob(x_1) | Prob(x_2) |\n| :--- | :--- | :--- |\n| `a_1` | 1 | 0 |\n| `a_2` | 1-`γ` | `γ` |\n\nTo implement the high-cost action `a_2`, the principal chooses utility payments `v_1` (for `x_1`) and `v_2` (for `x_2`) to minimize expected cost `(1-γ)h(v_1) + γh(v_2)` subject to two constraints:\n1.  **Individual Rationality (IR):** The agent's expected utility must meet her reservation utility (0).\n      \n    (1-\\gamma)v_1 + \\gamma v_2 - G_2 \\geq 0 \\quad \\text{(Eq. (1))}\n     \n2.  **Incentive Compatibility (IC):** The agent must prefer `a_2` over `a_1`. The utility from shirking is `v_1 - G_1`.\n      \n    (1-\\gamma)v_1 + \\gamma v_2 - G_2 \\geq v_1 - G_1 \\quad \\text{(Eq. (2))}\n     \nThe principal will only choose to implement `a_2` if it is profitable, which is captured by Assumption (A.1):\n  \n\\Pi(a_{2}) - \\left[(1-\\gamma)h(v_{1}^{*}) + \\gamma h(v_{2}^{*})\\right] > \\Pi(a_{1}) - h(G_{1}) \\quad \\text{(Eq. (3))}\n \nwhere `Π(a_j)` is expected revenue and `(v_1^*, v_2^*)` is the optimal contract.\n\n### Question\n\nConsider the optimal one-period contract `(v_1^*, v_2^*)` that implements action `a_2`. Select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 101, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n  \n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n \nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\n**Theorem 3.2 (Analytic Continuation & Convergence)** is critical for the algorithm's validity. Which of the following are direct consequences of this theorem, as described in the paper? (Select all that apply)", "Answer": ["B", "C"], "qid": "101", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n  \n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n \nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\n**Theorem 3.2 (Analytic Continuation & Convergence)** is critical for the algorithm's validity. Which of the following are direct consequences of this theorem, as described in the paper? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 102, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's core empirical findings on the interdependent choices of labor supply and housing demand, based on the estimated elasticities for different household types.\n\n**Setting / Institutional Environment.** A joint labor-housing demand system is estimated for several distinct demographic groups using data from the 1976 Panel Study of Income Dynamics. The outputs are wage and income elasticities of labor supply (hours worked) and housing demand. The analysis focuses on interpreting these elasticities to understand household decision-making and the crucial linkages between work and housing choices.\n\n**Variables & Parameters.**\n- `j_h`, `j_s`: Annual hours worked by the head (`h`) and spouse (`s`).\n- `w_h`, `w_s`: Hourly wage for the head and spouse.\n- `r`: Housing services, measured as number of rooms.\n- `ε_{j_i, w_k}`: Elasticity of labor supply of person `i` with respect to the wage of person `k`.\n- `ε_{r, w_h}`: Cross-price elasticity of housing demand with respect to the head's wage.\n- Unit of observation: Household.\n\n---\n\n### Data / Model Specification\n\nSelected estimated elasticities from the paper are presented in the tables below.\n\n**Table 1: Elasticities of Hours Employed**\n\n| Group | Head's Own-Wage (`ε_{j_h, w_h}`) | Spouse's Own-Wage (`ε_{j_s, w_s}`) | Head's Cross-Wage (`ε_{j_h, w_s}`) | Spouse's Cross-Wage (`ε_{j_s, w_h}`) |\n| :--- | :---: | :---: | :---: | :---: |\n| Unmarried Females with Children | +0.106 | N/A | N/A | N/A |\n| Two-Earner Families with Children | -0.002 | -0.086 | -0.004 | -0.194 |\n\n*Note: In this study, the 'head' is the husband and the 'spouse' is the wife.*\n\n**Table 2: Elasticities of Housing Demand**\n\n| Group | Cross-Price w.r.t. Head's Wage (`ε_{r, w_h}`) |\n| :--- | :---: |\n| Two-Earner Families with Children | +0.153 |\n\n---\n\n### Question\n\nBased on the provided elasticities, which of the following statements are correct interpretations of household behavior?", "Answer": ["B", "D"], "qid": "102", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's core empirical findings on the interdependent choices of labor supply and housing demand, based on the estimated elasticities for different household types.\n\n**Setting / Institutional Environment.** A joint labor-housing demand system is estimated for several distinct demographic groups using data from the 1976 Panel Study of Income Dynamics. The outputs are wage and income elasticities of labor supply (hours worked) and housing demand. The analysis focuses on interpreting these elasticities to understand household decision-making and the crucial linkages between work and housing choices.\n\n**Variables & Parameters.**\n- `j_h`, `j_s`: Annual hours worked by the head (`h`) and spouse (`s`).\n- `w_h`, `w_s`: Hourly wage for the head and spouse.\n- `r`: Housing services, measured as number of rooms.\n- `ε_{j_i, w_k}`: Elasticity of labor supply of person `i` with respect to the wage of person `k`.\n- `ε_{r, w_h}`: Cross-price elasticity of housing demand with respect to the head's wage.\n- Unit of observation: Household.\n\n---\n\n### Data / Model Specification\n\nSelected estimated elasticities from the paper are presented in the tables below.\n\n**Table 1: Elasticities of Hours Employed**\n\n| Group | Head's Own-Wage (`ε_{j_h, w_h}`) | Spouse's Own-Wage (`ε_{j_s, w_s}`) | Head's Cross-Wage (`ε_{j_h, w_s}`) | Spouse's Cross-Wage (`ε_{j_s, w_h}`) |\n| :--- | :---: | :---: | :---: | :---: |\n| Unmarried Females with Children | +0.106 | N/A | N/A | N/A |\n| Two-Earner Families with Children | -0.002 | -0.086 | -0.004 | -0.194 |\n\n*Note: In this study, the 'head' is the husband and the 'spouse' is the wife.*\n\n**Table 2: Elasticities of Housing Demand**\n\n| Group | Cross-Price w.r.t. Head's Wage (`ε_{r, w_h}`) |\n| :--- | :---: |\n| Two-Earner Families with Children | +0.153 |\n\n---\n\n### Question\n\nBased on the provided elasticities, which of the following statements are correct interpretations of household behavior?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 103, "Question": "### Background\n\n**Research Question.** This problem requires the interpretation of empirical results from a regression discontinuity design to test the theoretical condition for the optimality of offering a menu of UI contracts.\n\n**Setting.** Researchers use a discontinuity in UI generosity at age 25 in Switzerland to estimate the marginal costs (sensitivity of insurance payments) and marginal benefits (sensitivity of reservation wage) of more generous UI. They estimate these effects separately for two groups of workers, classified by their caseworker as having low search efficiency (type `h`) or high search efficiency (type `l`). The planner's decision to offer a menu of contracts depends on whether the single-crossing condition is met.\n\n**Variables & Parameters.**\n- `MC_i`: Marginal cost for type `i`, estimated as the treatment effect on log insurance payments.\n- `MB_i`: Marginal benefit for type `i`, estimated as the treatment effect on log reservation wage.\n- Type `h`: Job seekers with low search efficiency.\n- Type `l`: Job seekers with high search efficiency.\n\n---\n\n### Data / Model Specification\n\nThe theoretical condition for a separating menu to be optimal is:\n  \nMC_h - MB_h < 0 < MC_l - MB_l\n \nThis implies the planner wants to offer more insurance to type `h` and less to type `l` than the current pooling contract provides. The following table presents the Regression Discontinuity (RD) estimates for `MB` and `MC` for each type.\n\n**Table 1: RD Estimates of Marginal Benefit and Marginal Cost**\n\n| | Marginal Benefit (Reservation Wage) | Marginal Cost (Insurance Payments) |\n|:---|:---:|:---:|\n| **Panel A: Type-h (Low Efficiency)** | | |\n| Treatment Effect | 0.1102 | 0.0733 |\n| **Panel B: Type-l (High Efficiency)** | | |\n| Treatment Effect | 0.0715 | 0.1809 |\n\n---\n\n### Question\n\nBased on the point estimates in Table 1, which of the following statements are valid conclusions within the paper's framework? (Select all that apply.)", "Answer": ["A", "B", "C"], "qid": "103", "question": "### Background\n\n**Research Question.** This problem requires the interpretation of empirical results from a regression discontinuity design to test the theoretical condition for the optimality of offering a menu of UI contracts.\n\n**Setting.** Researchers use a discontinuity in UI generosity at age 25 in Switzerland to estimate the marginal costs (sensitivity of insurance payments) and marginal benefits (sensitivity of reservation wage) of more generous UI. They estimate these effects separately for two groups of workers, classified by their caseworker as having low search efficiency (type `h`) or high search efficiency (type `l`). The planner's decision to offer a menu of contracts depends on whether the single-crossing condition is met.\n\n**Variables & Parameters.**\n- `MC_i`: Marginal cost for type `i`, estimated as the treatment effect on log insurance payments.\n- `MB_i`: Marginal benefit for type `i`, estimated as the treatment effect on log reservation wage.\n- Type `h`: Job seekers with low search efficiency.\n- Type `l`: Job seekers with high search efficiency.\n\n---\n\n### Data / Model Specification\n\nThe theoretical condition for a separating menu to be optimal is:\n  \nMC_h - MB_h < 0 < MC_l - MB_l\n \nThis implies the planner wants to offer more insurance to type `h` and less to type `l` than the current pooling contract provides. The following table presents the Regression Discontinuity (RD) estimates for `MB` and `MC` for each type.\n\n**Table 1: RD Estimates of Marginal Benefit and Marginal Cost**\n\n| | Marginal Benefit (Reservation Wage) | Marginal Cost (Insurance Payments) |\n|:---|:---:|:---:|\n| **Panel A: Type-h (Low Efficiency)** | | |\n| Treatment Effect | 0.1102 | 0.0733 |\n| **Panel B: Type-l (High Efficiency)** | | |\n| Treatment Effect | 0.0715 | 0.1809 |\n\n---\n\n### Question\n\nBased on the point estimates in Table 1, which of the following statements are valid conclusions within the paper's framework? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 104, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical conditions under which it is optimal for a planner to offer a menu of unemployment insurance (UI) contracts to screen heterogeneous workers, rather than a single pooling contract.\n\n**Setting.** A planner faces two types of job seekers, `h` (high) and `l` (low), who are unobservable. Type `h` has a higher marginal rate of substitution (`MRS_{b,a}`) for benefits `b` versus a lump-sum payment `a` than type `l`. The planner seeks to minimize costs for a given level of social welfare. The planner's cost function associated with the moral hazard from inducing a specific reservation wage is `-Ψ(ω)`.\n\n---\n\n### Data / Model Specification\n\nThe planner's problem with separating contracts `(a_h, b_h)` and `(a_l, b_l)` is subject to incentive compatibility (IC) constraints, ensuring each type selects their intended contract. A key proposition states that a menu of separating contracts is Pareto superior to the best pooling contract if and only if `b_h^* > b_l^*`, where `b_i^*` is the benefit level that minimizes the planner's cost function `-Ψ_i(ω_i(b))` for type `i` under perfect information.\n\nThis condition is equivalent to the following condition evaluated at the optimal pooling benefit `b^p`:\n  \n\\frac{\\partial \\Psi_h(\\omega_h(b^p))}{\\partial b} < 0 < \\frac{\\partial \\Psi_l(\\omega_l(b^p))}{\\partial b} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nAssume the single-crossing property (`MRS_h > MRS_l`) and the condition for separation (`b_h^* > b_l^*`) both hold. Which of the following statements accurately describe the properties of an optimal separating menu of contracts compared to the optimal pooling contract `(a^p, b^p)`? (Select all that apply.)", "Answer": ["A", "B", "D"], "qid": "104", "question": "### Background\n\n**Research Question.** This problem explores the theoretical conditions under which it is optimal for a planner to offer a menu of unemployment insurance (UI) contracts to screen heterogeneous workers, rather than a single pooling contract.\n\n**Setting.** A planner faces two types of job seekers, `h` (high) and `l` (low), who are unobservable. Type `h` has a higher marginal rate of substitution (`MRS_{b,a}`) for benefits `b` versus a lump-sum payment `a` than type `l`. The planner seeks to minimize costs for a given level of social welfare. The planner's cost function associated with the moral hazard from inducing a specific reservation wage is `-Ψ(ω)`.\n\n---\n\n### Data / Model Specification\n\nThe planner's problem with separating contracts `(a_h, b_h)` and `(a_l, b_l)` is subject to incentive compatibility (IC) constraints, ensuring each type selects their intended contract. A key proposition states that a menu of separating contracts is Pareto superior to the best pooling contract if and only if `b_h^* > b_l^*`, where `b_i^*` is the benefit level that minimizes the planner's cost function `-Ψ_i(ω_i(b))` for type `i` under perfect information.\n\nThis condition is equivalent to the following condition evaluated at the optimal pooling benefit `b^p`:\n  \n\\frac{\\partial \\Psi_h(\\omega_h(b^p))}{\\partial b} < 0 < \\frac{\\partial \\Psi_l(\\omega_l(b^p))}{\\partial b} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nAssume the single-crossing property (`MRS_h > MRS_l`) and the condition for separation (`b_h^* > b_l^*`) both hold. Which of the following statements accurately describe the properties of an optimal separating menu of contracts compared to the optimal pooling contract `(a^p, b^p)`? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 105, "Question": "### Background\n\n**Research Question.** This problem explores how real-world complexities might affect the optimality of offering a menu of unemployment insurance (UI) contracts.\n\n**Setting.** The baseline model assumes workers know their type (e.g., their job-finding probability `f`) and that their choice of UI contract does not affect their employment prospects. The paper considers several extensions to this baseline.\n\n---\n\n### Scenario\n\nConsider a scenario where a planner has implemented an optimal separating menu of UI contracts based on the baseline model: Contract H (high insurance `b`, low lump-sum `a`) for low-efficiency workers and Contract L (low `b`, high `a`) for high-efficiency workers. Now, two real-world complications arise simultaneously:\n\n1.  **Signalling:** Employers can observe which UI contract a worker chose. They interpret a choice of Contract H as a signal of low job-finding ability, which negatively impacts their wage offers during bargaining.\n2.  **Recession:** A macroeconomic shock lowers the job offer arrival rate `f` for all workers, increasing the expected duration of unemployment for everyone.\n\n---\n\n### Question\n\nGiven these two new complications, which of the following outcomes are plausible consequences for the self-selection of workers into the pre-existing menu of contracts? (Select all that apply.)", "Answer": ["A", "B", "D"], "qid": "105", "question": "### Background\n\n**Research Question.** This problem explores how real-world complexities might affect the optimality of offering a menu of unemployment insurance (UI) contracts.\n\n**Setting.** The baseline model assumes workers know their type (e.g., their job-finding probability `f`) and that their choice of UI contract does not affect their employment prospects. The paper considers several extensions to this baseline.\n\n---\n\n### Scenario\n\nConsider a scenario where a planner has implemented an optimal separating menu of UI contracts based on the baseline model: Contract H (high insurance `b`, low lump-sum `a`) for low-efficiency workers and Contract L (low `b`, high `a`) for high-efficiency workers. Now, two real-world complications arise simultaneously:\n\n1.  **Signalling:** Employers can observe which UI contract a worker chose. They interpret a choice of Contract H as a signal of low job-finding ability, which negatively impacts their wage offers during bargaining.\n2.  **Recession:** A macroeconomic shock lowers the job offer arrival rate `f` for all workers, increasing the expected duration of unemployment for everyone.\n\n---\n\n### Question\n\nGiven these two new complications, which of the following outcomes are plausible consequences for the self-selection of workers into the pre-existing menu of contracts? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 106, "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal (e.g., a seller) can design a mechanism to extract all economic surplus from privately informed agents (e.g., bidders), a scenario known as Full Surplus Extraction (FSE).\n\n**Setting:** A seller designs an incentive compatible (IC) and individually rational (IR) mechanism to sell a single object. An agent's private information is captured by their 'type' $\\theta_i$, which consists of a valuation for the object $\\hat{v}_i(\\theta_i)$ and a belief about others' types $\\hat{b}_i(\\theta_i)$. The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions & Constraints:**\n- **Full Surplus Extraction (FSE):** A prior $p$ permits FSE from bidder $i$ if an IC and IR mechanism exists where the seller's expected revenue equals the bidder's expected valuation.\n- **Individual Rationality (IR):** The expected utility for any type from participating in the mechanism must be non-negative.\n\n### The Question\n\nUnder a prior that permits Full Surplus Extraction (FSE) from a specific bidder, which of the following conditions must hold for that bidder with probability 1? (Select all that apply)", "Answer": ["B", "C"], "qid": "106", "question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal (e.g., a seller) can design a mechanism to extract all economic surplus from privately informed agents (e.g., bidders), a scenario known as Full Surplus Extraction (FSE).\n\n**Setting:** A seller designs an incentive compatible (IC) and individually rational (IR) mechanism to sell a single object. An agent's private information is captured by their 'type' $\\theta_i$, which consists of a valuation for the object $\\hat{v}_i(\\theta_i)$ and a belief about others' types $\\hat{b}_i(\\theta_i)$. The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions & Constraints:**\n- **Full Surplus Extraction (FSE):** A prior $p$ permits FSE from bidder $i$ if an IC and IR mechanism exists where the seller's expected revenue equals the bidder's expected valuation.\n- **Individual Rationality (IR):** The expected utility for any type from participating in the mechanism must be non-negative.\n\n### The Question\n\nUnder a prior that permits Full Surplus Extraction (FSE) from a specific bidder, which of the following conditions must hold for that bidder with probability 1? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "error": null}}
{"ID": 107, "Question": "### Background\n\nIn longitudinal studies, researchers face a fundamental challenge in separating the effects of biological aging (age effect), generational experiences (cohort effect), and contemporaneous events (period effect) on outcomes like risk attitudes. The core issue is the perfect linear dependency among these three time dimensions, known as the age-period-cohort (APC) identification problem.\n\n---\n\n### Data / Model Specification\n\nThe APC identity is:\n\n  \n\\text{Age}_{it} \\equiv \\text{Period}_t - \\text{Cohort}_i \\quad \\text{(Eq. (1))}\n \n\nwhere `i` denotes an individual and `t` denotes the survey year. This identity means that a model including a full set of dummy variables for age, period, and cohort suffers from perfect multicollinearity and cannot be estimated. To overcome this, the authors substitute the set of period dummies with a proxy variable: annual GDP growth.\n\n---\n\n### Question\n\nSelect all statements that accurately describe the age-period-cohort (APC) identification problem and the paper's proposed solution using a proxy variable.", "Answer": ["B", "C"], "qid": "107", "question": "### Background\n\nIn longitudinal studies, researchers face a fundamental challenge in separating the effects of biological aging (age effect), generational experiences (cohort effect), and contemporaneous events (period effect) on outcomes like risk attitudes. The core issue is the perfect linear dependency among these three time dimensions, known as the age-period-cohort (APC) identification problem.\n\n---\n\n### Data / Model Specification\n\nThe APC identity is:\n\n  \n\\text{Age}_{it} \\equiv \\text{Period}_t - \\text{Cohort}_i \\quad \\text{(Eq. (1))}\n \n\nwhere `i` denotes an individual and `t` denotes the survey year. This identity means that a model including a full set of dummy variables for age, period, and cohort suffers from perfect multicollinearity and cannot be estimated. To overcome this, the authors substitute the set of period dummies with a proxy variable: annual GDP growth.\n\n---\n\n### Question\n\nSelect all statements that accurately describe the age-period-cohort (APC) identification problem and the paper's proposed solution using a proxy variable.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn longitudinal studies, researche...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn longitudinal studies, researche...", "error": null}}
{"ID": 108, "Question": "### Background\n\nTo identify the effect of age on risk attitudes, the paper's main strategy is to use annual GDP growth as a proxy for period effects, allowing for simultaneous estimation of age and cohort effects. As a robustness check, the authors also consider an alternative approach: an individual fixed-effects (FE) model, which leverages the panel nature of the data differently.\n\n---\n\n### Question\n\nWhen comparing the paper's main GDP-proxy model to an alternative individual fixed-effects (FE) model for estimating the effect of age on risk attitudes, which of the following statements are correct?", "Answer": ["B", "C"], "qid": "108", "question": "### Background\n\nTo identify the effect of age on risk attitudes, the paper's main strategy is to use annual GDP growth as a proxy for period effects, allowing for simultaneous estimation of age and cohort effects. As a robustness check, the authors also consider an alternative approach: an individual fixed-effects (FE) model, which leverages the panel nature of the data differently.\n\n---\n\n### Question\n\nWhen comparing the paper's main GDP-proxy model to an alternative individual fixed-effects (FE) model for estimating the effect of age on risk attitudes, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo identify the effect of age on r...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo identify the effect of age on r...", "error": null}}
{"ID": 109, "Question": "### Background\n\n**Research Question.** How can the net change in an industry's employment be decomposed into its constituent gross flows?\n\n**Setting.** In a vintage capital model, the total number of production units (or jobs), `N(t)`, evolves through the continuous creation of new units and the destruction of existing units.\n\n### Data / Model Specification\n\nThe net change in the total number of units, `N(t)`, is given by the flow equation:\n  \n\\dot{N}(t) = f(0,t) - \\{f(\\bar{a}(t),t)[1-\\dot{\\bar{a}}(t)]+\\delta N(t)\\} \n \nwhere `f(0,t)` is the creation rate, `ā(t)` is the age of the oldest unit, `ȧ(t)` is its time derivative, `δ` is an exogenous failure rate, and `N(t)` is the total number of units.\n\n### Question\n\nThe total gross job destruction rate is given by the term `D(t) = f(ā(t),t)[1-ȧ(t)] + δN(t)`. Which of the following are the correct economic interpretations of the distinct components of this total destruction rate? (Select all that apply)", "Answer": ["B", "D"], "qid": "109", "question": "### Background\n\n**Research Question.** How can the net change in an industry's employment be decomposed into its constituent gross flows?\n\n**Setting.** In a vintage capital model, the total number of production units (or jobs), `N(t)`, evolves through the continuous creation of new units and the destruction of existing units.\n\n### Data / Model Specification\n\nThe net change in the total number of units, `N(t)`, is given by the flow equation:\n  \n\\dot{N}(t) = f(0,t) - \\{f(\\bar{a}(t),t)[1-\\dot{\\bar{a}}(t)]+\\delta N(t)\\} \n \nwhere `f(0,t)` is the creation rate, `ā(t)` is the age of the oldest unit, `ȧ(t)` is its time derivative, `δ` is an exogenous failure rate, and `N(t)` is the total number of units.\n\n### Question\n\nThe total gross job destruction rate is given by the term `D(t) = f(ā(t),t)[1-ȧ(t)] + δN(t)`. Which of the following are the correct economic interpretations of the distinct components of this total destruction rate? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** How can the...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** How can the...", "error": null}}
{"ID": 110, "Question": "### Background\n\nAn econometrician observes data from a large sample of auctions governed by the Affiliated-Signal (AS) model. For each auction, they observe the number of potential bidders `N`, the number of entrants `n`, and can recover the distribution of entrant values. They may also observe an auction-level instrument `Z` that shifts entry costs.\n\n**Key Assumptions:**\n- **Excludability (Assumption 4):** The instrument `Z` and potential competition `N` affect entry decisions but do not directly affect the underlying value-signal distribution `F(v,s)`.\n- **Value Recovery (Assumption 5):** The auction mechanism is such that the distribution of values for entrants can be nonparametrically identified from their bids.\n- **Stochastic Ordering:** Higher signals are associated with stochastically higher values (`s' > s` implies `F(v|s') ≤ F(v|s)`).\n\n### Data / Model Specification\n\nFrom the data, for each market condition `(N,z)`, the econometrician can identify two objects:\n1.  The entry threshold `ŝ_N(z)`:\n      \n    \\hat{s}_{N}(z) = 1 - \\frac{E[n|N,z]}{N} \\quad \\text{(Eq. (1))}\n     \n2.  The CDF of values among entrants, `F*(v; ŝ)`, which is related to the unobserved structural CDF `F(v|s)` by:\n      \n    (1-\\hat{s})F^{*}(v;\\hat{s}) = \\int_{\\hat{s}}^{1}F(v|t) dt \\quad \\text{(Eq. (2))}\n     \nFor the partial identification case, where the set of identified thresholds `S` is discrete, we define `t-(s)` as the nearest observed threshold below `s`. The backward-difference approximation for `F(v|s)` is:\n  \n\\check{F}^{+}(v|\\hat{s}) \\equiv \\frac{(1-t^{-}(\\hat{s}))F^{*}(v;t^{-}(\\hat{s}))-(1-\\hat{s})F^{*}(v;\\hat{s})}{\\hat{s}-t^{-}(\\hat{s})} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nBased on the provided model and identification strategy, select all statements that are correct.", "Answer": ["A", "C"], "qid": "110", "question": "### Background\n\nAn econometrician observes data from a large sample of auctions governed by the Affiliated-Signal (AS) model. For each auction, they observe the number of potential bidders `N`, the number of entrants `n`, and can recover the distribution of entrant values. They may also observe an auction-level instrument `Z` that shifts entry costs.\n\n**Key Assumptions:**\n- **Excludability (Assumption 4):** The instrument `Z` and potential competition `N` affect entry decisions but do not directly affect the underlying value-signal distribution `F(v,s)`.\n- **Value Recovery (Assumption 5):** The auction mechanism is such that the distribution of values for entrants can be nonparametrically identified from their bids.\n- **Stochastic Ordering:** Higher signals are associated with stochastically higher values (`s' > s` implies `F(v|s') ≤ F(v|s)`).\n\n### Data / Model Specification\n\nFrom the data, for each market condition `(N,z)`, the econometrician can identify two objects:\n1.  The entry threshold `ŝ_N(z)`:\n      \n    \\hat{s}_{N}(z) = 1 - \\frac{E[n|N,z]}{N} \\quad \\text{(Eq. (1))}\n     \n2.  The CDF of values among entrants, `F*(v; ŝ)`, which is related to the unobserved structural CDF `F(v|s)` by:\n      \n    (1-\\hat{s})F^{*}(v;\\hat{s}) = \\int_{\\hat{s}}^{1}F(v|t) dt \\quad \\text{(Eq. (2))}\n     \nFor the partial identification case, where the set of identified thresholds `S` is discrete, we define `t-(s)` as the nearest observed threshold below `s`. The backward-difference approximation for `F(v|s)` is:\n  \n\\check{F}^{+}(v|\\hat{s}) \\equiv \\frac{(1-t^{-}(\\hat{s}))F^{*}(v;t^{-}(\\hat{s}))-(1-\\hat{s})F^{*}(v;\\hat{s})}{\\hat{s}-t^{-}(\\hat{s})} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nBased on the provided model and identification strategy, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn econometrician observes data fr...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn econometrician observes data fr...", "error": null}}
{"ID": 111, "Question": "### Background\n\nThis problem addresses the challenge of identifying entry and bidding behavior in the Affiliated-Signal (AS) model when there is unobserved auction-level heterogeneity `U` (e.g., quality of the good for sale) that is known to bidders but not the econometrician. The model primitives (`F(v|s,u)`, `c(z;u)`) and equilibrium outcomes (`s*(z;u)`, `G*_b(b|...;u)`) are conditional on the realization `u` of `U`.\n\n### Data / Model Specification\n\nFor each of the `N` potential bidders in an auction, a **realized bid** `W_i` is defined:\n  \nW_i = \n\\begin{cases} \nB_i & \\text{if bidder } i \\text{ enters and bids } B_i > 0 \\\\ \n0 & \\text{if bidder } i \\text{ does not enter} \n\\end{cases}\n \nThe CDF of these realized bids, `G*_w(b|N,z;u)`, is a mixture of a point mass at zero (representing non-entry) and the continuous distribution of positive bids from entrants:\n  \nG_{w}^{*}(b|N,z;u) = s_{N}^{*}(z;u) + \\bigl[1-s_{N}^{*}(z;u)\\bigr]G_{b}^{*}(b|N,z;u) \\quad \\text{(Eq. (1))}\n \nwhere `s*_N(z;u)` is the conditional non-entry probability (i.e., the threshold) and `G*_b(b|N,z;u)` is the CDF of bids from those who entered. Assume `G*_w(b|N,z;u)` can be identified from the data and that all submitted bids `B_i` are strictly positive.\n\n### Question\n\nAssuming the conditional CDF of realized bids, `G*_w(b|N,z;u)`, is identified, select all correct statements regarding the identification of other model components.", "Answer": ["B", "C"], "qid": "111", "question": "### Background\n\nThis problem addresses the challenge of identifying entry and bidding behavior in the Affiliated-Signal (AS) model when there is unobserved auction-level heterogeneity `U` (e.g., quality of the good for sale) that is known to bidders but not the econometrician. The model primitives (`F(v|s,u)`, `c(z;u)`) and equilibrium outcomes (`s*(z;u)`, `G*_b(b|...;u)`) are conditional on the realization `u` of `U`.\n\n### Data / Model Specification\n\nFor each of the `N` potential bidders in an auction, a **realized bid** `W_i` is defined:\n  \nW_i = \n\\begin{cases} \nB_i & \\text{if bidder } i \\text{ enters and bids } B_i > 0 \\\\ \n0 & \\text{if bidder } i \\text{ does not enter} \n\\end{cases}\n \nThe CDF of these realized bids, `G*_w(b|N,z;u)`, is a mixture of a point mass at zero (representing non-entry) and the continuous distribution of positive bids from entrants:\n  \nG_{w}^{*}(b|N,z;u) = s_{N}^{*}(z;u) + \\bigl[1-s_{N}^{*}(z;u)\\bigr]G_{b}^{*}(b|N,z;u) \\quad \\text{(Eq. (1))}\n \nwhere `s*_N(z;u)` is the conditional non-entry probability (i.e., the threshold) and `G*_b(b|N,z;u)` is the CDF of bids from those who entered. Assume `G*_w(b|N,z;u)` can be identified from the data and that all submitted bids `B_i` are strictly positive.\n\n### Question\n\nAssuming the conditional CDF of realized bids, `G*_w(b|N,z;u)`, is identified, select all correct statements regarding the identification of other model components.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem addresses the challen...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem addresses the challen...", "error": null}}
{"ID": 112, "Question": "### Background\n\nThe setting is a two-stage Affiliated-Signal (AS) auction model. In Stage 1, each of `N` symmetric potential bidders observes a private signal `S_i` and decides whether to pay a fixed cost `c` to enter. In Stage 2, entrants learn their values `v_i` and bid. The analysis focuses on a symmetric equilibrium where entry is governed by a signal threshold `s*`.\n\n### Data / Model Specification\n\nThe expected profit from entering for a bidder with signal `s_i`, when rivals use threshold `s`, is given by `Π(s_i; s, N)`. The paper establishes that this function is increasing in the bidder's own signal `s_i` and decreasing in the number of potential bidders `N`.\n\nAn interior equilibrium (`s* ∈ (0,1)`) is characterized by a breakeven condition for the marginal entrant. Proposition 1 in the paper establishes the following comparative statics for the equilibrium threshold `s*_N(c)`:\n- For any `N ≥ 1`, `s*_N(c)` is continuous and weakly increasing in `c`.\n- For any `c ≥ 0`, if `N' > N`, then `s*_N'(c) ≥ s*_N(c)`.\n\n### Question\n\nAccording to the equilibrium analysis of the AS model, which of the following statements about the equilibrium entry threshold `s*` are **incorrect**?", "Answer": ["A", "D"], "qid": "112", "question": "### Background\n\nThe setting is a two-stage Affiliated-Signal (AS) auction model. In Stage 1, each of `N` symmetric potential bidders observes a private signal `S_i` and decides whether to pay a fixed cost `c` to enter. In Stage 2, entrants learn their values `v_i` and bid. The analysis focuses on a symmetric equilibrium where entry is governed by a signal threshold `s*`.\n\n### Data / Model Specification\n\nThe expected profit from entering for a bidder with signal `s_i`, when rivals use threshold `s`, is given by `Π(s_i; s, N)`. The paper establishes that this function is increasing in the bidder's own signal `s_i` and decreasing in the number of potential bidders `N`.\n\nAn interior equilibrium (`s* ∈ (0,1)`) is characterized by a breakeven condition for the marginal entrant. Proposition 1 in the paper establishes the following comparative statics for the equilibrium threshold `s*_N(c)`:\n- For any `N ≥ 1`, `s*_N(c)` is continuous and weakly increasing in `c`.\n- For any `c ≥ 0`, if `N' > N`, then `s*_N'(c) ≥ s*_N(c)`.\n\n### Question\n\nAccording to the equilibrium analysis of the AS model, which of the following statements about the equilibrium entry threshold `s*` are **incorrect**?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe setting is a two-stage Affilia...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe setting is a two-stage Affilia...", "error": null}}
{"ID": 113, "Question": "### Background\n\n**Research Question.** This problem analyzes the international welfare effects of a large country's domestic competition policy, demonstrating how it can function as a beggar-thy-neighbor policy.\n\n**Setting / Institutional Environment.** A two-country, infinite-horizon model is considered. The home country implements a policy to suppress competition (`dμ > 0`) in its non-tradable sector. This action affects the world price of the tradable good, leading to a redistribution of wealth between the two countries, which have an initial trade imbalance (`s ≠ 0`).\n\n### Data / Model Specification\n\nThe effects of the home country's policy (`dμ > 0`) on the long-run price of tradables and on national welfare are characterized by the following relationships, assuming a perfectly competitive initial equilibrium (`μ=0`) and that the long-run law of demand holds:\n\n1.  **Price Effect:** The policy raises the long-run current-value price of the tradable good:\n      \n    \\frac{\\hat{\\pi}_{\\infty}}{d\\mu} > 0 \\quad \\text{(Eq. (1))}\n     \n    where `\\hat{\\pi}_{\\infty}` is the percentage change in the long-run price.\n\n2.  **Welfare Effect:** The change in the home country's real wealth (`dU/γ`) is linked to the price change and its initial trade balance (`s`):\n      \n    \\frac{1}{\\gamma}\\frac{d U}{d\\mu} = -\\frac{1}{\\gamma^{*}}\\frac{d U^{*}}{d\\mu} = K \\cdot s \\cdot \\frac{\\hat{\\pi}_{\\infty}}{d\\mu} \\quad \\text{(Eq. (2))}\n     \n    where `K` is a positive constant derived from the model's parameters, `s` is the home country's initial trade surplus, and `U` and `U*` are the welfare levels of the home and foreign countries, respectively.\n\n### Question\n\nConsider a scenario where the home country has an initial trade surplus (`s > 0`) and implements a policy to suppress competition (`dμ > 0`). Based on the model specified above, which of the following statements are correct characterizations of the policy's effects?", "Answer": ["A", "B", "C", "D"], "qid": "113", "question": "### Background\n\n**Research Question.** This problem analyzes the international welfare effects of a large country's domestic competition policy, demonstrating how it can function as a beggar-thy-neighbor policy.\n\n**Setting / Institutional Environment.** A two-country, infinite-horizon model is considered. The home country implements a policy to suppress competition (`dμ > 0`) in its non-tradable sector. This action affects the world price of the tradable good, leading to a redistribution of wealth between the two countries, which have an initial trade imbalance (`s ≠ 0`).\n\n### Data / Model Specification\n\nThe effects of the home country's policy (`dμ > 0`) on the long-run price of tradables and on national welfare are characterized by the following relationships, assuming a perfectly competitive initial equilibrium (`μ=0`) and that the long-run law of demand holds:\n\n1.  **Price Effect:** The policy raises the long-run current-value price of the tradable good:\n      \n    \\frac{\\hat{\\pi}_{\\infty}}{d\\mu} > 0 \\quad \\text{(Eq. (1))}\n     \n    where `\\hat{\\pi}_{\\infty}` is the percentage change in the long-run price.\n\n2.  **Welfare Effect:** The change in the home country's real wealth (`dU/γ`) is linked to the price change and its initial trade balance (`s`):\n      \n    \\frac{1}{\\gamma}\\frac{d U}{d\\mu} = -\\frac{1}{\\gamma^{*}}\\frac{d U^{*}}{d\\mu} = K \\cdot s \\cdot \\frac{\\hat{\\pi}_{\\infty}}{d\\mu} \\quad \\text{(Eq. (2))}\n     \n    where `K` is a positive constant derived from the model's parameters, `s` is the home country's initial trade surplus, and `U` and `U*` are the welfare levels of the home and foreign countries, respectively.\n\n### Question\n\nConsider a scenario where the home country has an initial trade surplus (`s > 0`) and implements a policy to suppress competition (`dμ > 0`). Based on the model specified above, which of the following statements are correct characterizations of the policy's effects?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 114, "Question": "### Background\n\n**Research Question.** This problem analyzes the strategic interaction between a trade-surplus country (S) and a trade-deficit country (D) in setting their domestic competition policies (`μ_S` and `μ_D`).\n\n**Setting / Institutional Environment.** A two-country, infinite-horizon model is considered. Each country `i` chooses its competition level `μ_i ≥ 0` to maximize its own welfare `U_i`. The initial trade balance for country S is `s_S > 0` and for country D is `s_D < 0`.\n\n### Data / Model Specification\n\nThe welfare effect of a country's own policy to suppress competition (`dμ_i > 0`) is given by:\n  \n\\frac{1}{\\gamma_i}\\frac{d U_i}{d\\mu_i} = K \\cdot s_i \\cdot \\frac{\\hat{\\pi}_{\\infty}}{d\\mu_i} \\quad \\text{(Eq. (1))}\n \nwhere `K` is a positive constant and `\\hat{\\pi}_{\\infty}/d\\mu_i > 0`. This means the policy raises the long-run price of the tradable good, regardless of which country implements it. The welfare impact on the implementing country `i` depends on its own initial trade balance `s_i`.\n\n### Question\n\nGiven the strategic incentives described by the model, which of the following statements correctly describe the predicted behavior and outcomes in a non-cooperative policy game between the surplus country (S) and the deficit country (D)?", "Answer": ["A", "B", "C", "D"], "qid": "114", "question": "### Background\n\n**Research Question.** This problem analyzes the strategic interaction between a trade-surplus country (S) and a trade-deficit country (D) in setting their domestic competition policies (`μ_S` and `μ_D`).\n\n**Setting / Institutional Environment.** A two-country, infinite-horizon model is considered. Each country `i` chooses its competition level `μ_i ≥ 0` to maximize its own welfare `U_i`. The initial trade balance for country S is `s_S > 0` and for country D is `s_D < 0`.\n\n### Data / Model Specification\n\nThe welfare effect of a country's own policy to suppress competition (`dμ_i > 0`) is given by:\n  \n\\frac{1}{\\gamma_i}\\frac{d U_i}{d\\mu_i} = K \\cdot s_i \\cdot \\frac{\\hat{\\pi}_{\\infty}}{d\\mu_i} \\quad \\text{(Eq. (1))}\n \nwhere `K` is a positive constant and `\\hat{\\pi}_{\\infty}/d\\mu_i > 0`. This means the policy raises the long-run price of the tradable good, regardless of which country implements it. The welfare impact on the implementing country `i` depends on its own initial trade balance `s_i`.\n\n### Question\n\nGiven the strategic incentives described by the model, which of the following statements correctly describe the predicted behavior and outcomes in a non-cooperative policy game between the surplus country (S) and the deficit country (D)?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 115, "Question": "### Background\n\n**Research Question.** This problem analyzes the dynamic impact of a domestic competition policy on a small country's trade balance, highlighting the difference between short-run and long-run effects.\n\n**Setting / Institutional Environment.** A small open economy experiences a permanent, unanticipated increase in the degree of imperfect competition (`dμ > 0`) in its non-tradable sector. The output of the tradable good is fixed at time `t=0` but is flexible from `t=1` onwards. Consumption and input demands are flexible at all times.\n\n### Data / Model Specification\n\nThe impact of an increase in the competition parameter `μ` on the trade balance `s_t` is given by Theorem 1:\n  \n\\frac{d s_{0}}{d\\mu} > 0 \\quad \\text{(Eq. (1))}\n \n  \n\\frac{d s_{t}}{d\\mu} = -\\frac{1-\\rho}{\\rho}\\frac{d s_{0}}{d\\mu} < 0 \\quad \\text{for } t=1,2,\\ldots \\quad \\text{(Eq. (2))}\n \nThe country's intertemporal budget constraint requires that the present value of all changes in the trade surplus sum to zero.\n\n### Question\n\nBased on the model's assumptions and results, which of the following statements are valid explanations or consequences of the \"short-run trade surplus creation effect\"?", "Answer": ["A", "B", "C", "D"], "qid": "115", "question": "### Background\n\n**Research Question.** This problem analyzes the dynamic impact of a domestic competition policy on a small country's trade balance, highlighting the difference between short-run and long-run effects.\n\n**Setting / Institutional Environment.** A small open economy experiences a permanent, unanticipated increase in the degree of imperfect competition (`dμ > 0`) in its non-tradable sector. The output of the tradable good is fixed at time `t=0` but is flexible from `t=1` onwards. Consumption and input demands are flexible at all times.\n\n### Data / Model Specification\n\nThe impact of an increase in the competition parameter `μ` on the trade balance `s_t` is given by Theorem 1:\n  \n\\frac{d s_{0}}{d\\mu} > 0 \\quad \\text{(Eq. (1))}\n \n  \n\\frac{d s_{t}}{d\\mu} = -\\frac{1-\\rho}{\\rho}\\frac{d s_{0}}{d\\mu} < 0 \\quad \\text{for } t=1,2,\\ldots \\quad \\text{(Eq. (2))}\n \nThe country's intertemporal budget constraint requires that the present value of all changes in the trade surplus sum to zero.\n\n### Question\n\nBased on the model's assumptions and results, which of the following statements are valid explanations or consequences of the \"short-run trade surplus creation effect\"?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 116, "Question": "### Background\n\n**Research Question.** This problem analyzes a critical assumption in the model: the role of leisure in the consumer's utility function.\n\n**Setting / Institutional Environment.** In a small country model, a policy to suppress competition (`dμ > 0`) is implemented. The consumer's utility function is `v(c_t, ℓ_t) = u(c_t) + v(ℓ_t)`, where `c_t` is consumption and `ℓ_t` is leisure.\n\n### Data / Model Specification\n\nThe magnitude of the short-run trade surplus effect is given by:\n  \n\\frac{d s_{0}}{d\\mu} = \\frac{a_{\\scriptscriptstyle{Y C}}c(\\bar{\\ell}-\\ell)\\eta\\rho}{\\text{Denominator}} > 0 \\quad \\text{(Eq. (1))}\n \nwhere the denominator is positive and `η` is a parameter related to the utility from leisure, defined as `η ≡ -v'/[(ℓ̄-ℓ)v'']`. If utility does not depend on leisure, then `v(ℓ_t)` is constant, making its derivatives `v'` and `v''` equal to zero, which in turn makes `η=0`.\n\n### Question\n\nAccording to the paper's analysis, which of the following statements are CORRECT regarding the role and implications of including leisure in the utility function?", "Answer": ["A", "B", "C", "D"], "qid": "116", "question": "### Background\n\n**Research Question.** This problem analyzes a critical assumption in the model: the role of leisure in the consumer's utility function.\n\n**Setting / Institutional Environment.** In a small country model, a policy to suppress competition (`dμ > 0`) is implemented. The consumer's utility function is `v(c_t, ℓ_t) = u(c_t) + v(ℓ_t)`, where `c_t` is consumption and `ℓ_t` is leisure.\n\n### Data / Model Specification\n\nThe magnitude of the short-run trade surplus effect is given by:\n  \n\\frac{d s_{0}}{d\\mu} = \\frac{a_{\\scriptscriptstyle{Y C}}c(\\bar{\\ell}-\\ell)\\eta\\rho}{\\text{Denominator}} > 0 \\quad \\text{(Eq. (1))}\n \nwhere the denominator is positive and `η` is a parameter related to the utility from leisure, defined as `η ≡ -v'/[(ℓ̄-ℓ)v'']`. If utility does not depend on leisure, then `v(ℓ_t)` is constant, making its derivatives `v'` and `v''` equal to zero, which in turn makes `η=0`.\n\n### Question\n\nAccording to the paper's analysis, which of the following statements are CORRECT regarding the role and implications of including leisure in the utility function?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 117, "Question": "### Background\n\nThis problem investigates the conditions for establishing the `\\(\\sqrt{n}\\)`-asymptotic normality of a semiparametric estimator `\\(\\widehat{\\theta}\\)`. The analysis relies on a Taylor-like expansion of the population moment function `\\(M(\\theta, h)\\)` around the true parameters `\\((\\theta_o, h_o)\\)`. This expansion involves both a standard derivative with respect to `\\(\\theta\\)` and a functional \"pathwise\" derivative with respect to `\\(h\\)`.\n\n### Data / Model Specification\n\nLet `\\(\\widehat{\\theta}\\)` be a consistent estimator for the true parameter `\\(\\theta_o\\)`. The estimator is derived from a population moment condition `\\(M(\\theta_o, h_o) = 0\\)`, where `\\(h_o\\)` is an infinite-dimensional nuisance function estimated nonparametrically by `\\(\\widehat{h}\\)`. The asymptotic distribution of `\\(\\widehat{\\theta}\\)` depends on the properties of `\\(M(\\theta, h)\\)` near the true values.\n\nThe ordinary derivative of `\\(M(\\theta, h)\\)` with respect to `\\(\\theta\\)` is denoted `\\(\\Gamma_1(\\theta, h)\\)`. The pathwise derivative of `\\(M(\\theta, h)\\)` with respect to `\\(h\\)` in the direction `\\([\\bar{h}-h]\\)` is denoted `\\(\\Gamma_2(\\theta, h)[\\bar{h}-h]\\)`.\n\n**Theorem 2** in the paper establishes that `\\(\\sqrt{n}(\\widehat{\\theta}-\\theta_{o}) \\Longrightarrow \\mathcal{N}[0,\\Omega]\\)` under a set of regularity conditions. Two key conditions are:\n\n1.  A smoothness condition on `\\(M\\)` with respect to `\\(h\\)`: For `\\((\\theta, h)\\)` in a shrinking neighborhood of `\\((\\theta_o, h_o)\\)`, there exists a constant `\\(c \\ge 0\\)` such that:\n      \n    \\|M(\\theta,h)-M(\\theta,h_{o})-\\Gamma_{2}(\\theta,h_{o})[h-h_{o}]\\| \\le c\\|h-h_{o}\\|_{\\mathcal{H}}^{2} \n     \n    (Eq. (1))\n2.  A minimum convergence rate for the nonparametric estimator:\n      \n    \\|\\widehat{h}-h_{o}\\|_{\\mathcal{H}}=o_{p}(n^{-1/4})\n     \n    (Eq. (2))\n\n### Question\n\nBased on the provided model specification and the logic of Theorem 2, which of the following statements accurately describe the roles of the derivatives and the implications of the convergence rate conditions? Select all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "117", "question": "### Background\n\nThis problem investigates the conditions for establishing the `\\(\\sqrt{n}\\)`-asymptotic normality of a semiparametric estimator `\\(\\widehat{\\theta}\\)`. The analysis relies on a Taylor-like expansion of the population moment function `\\(M(\\theta, h)\\)` around the true parameters `\\((\\theta_o, h_o)\\)`. This expansion involves both a standard derivative with respect to `\\(\\theta\\)` and a functional \"pathwise\" derivative with respect to `\\(h\\)`.\n\n### Data / Model Specification\n\nLet `\\(\\widehat{\\theta}\\)` be a consistent estimator for the true parameter `\\(\\theta_o\\)`. The estimator is derived from a population moment condition `\\(M(\\theta_o, h_o) = 0\\)`, where `\\(h_o\\)` is an infinite-dimensional nuisance function estimated nonparametrically by `\\(\\widehat{h}\\)`. The asymptotic distribution of `\\(\\widehat{\\theta}\\)` depends on the properties of `\\(M(\\theta, h)\\)` near the true values.\n\nThe ordinary derivative of `\\(M(\\theta, h)\\)` with respect to `\\(\\theta\\)` is denoted `\\(\\Gamma_1(\\theta, h)\\)`. The pathwise derivative of `\\(M(\\theta, h)\\)` with respect to `\\(h\\)` in the direction `\\([\\bar{h}-h]\\)` is denoted `\\(\\Gamma_2(\\theta, h)[\\bar{h}-h]\\)`.\n\n**Theorem 2** in the paper establishes that `\\(\\sqrt{n}(\\widehat{\\theta}-\\theta_{o}) \\Longrightarrow \\mathcal{N}[0,\\Omega]\\)` under a set of regularity conditions. Two key conditions are:\n\n1.  A smoothness condition on `\\(M\\)` with respect to `\\(h\\)`: For `\\((\\theta, h)\\)` in a shrinking neighborhood of `\\((\\theta_o, h_o)\\)`, there exists a constant `\\(c \\ge 0\\)` such that:\n      \n    \\|M(\\theta,h)-M(\\theta,h_{o})-\\Gamma_{2}(\\theta,h_{o})[h-h_{o}]\\| \\le c\\|h-h_{o}\\|_{\\mathcal{H}}^{2} \n     \n    (Eq. (1))\n2.  A minimum convergence rate for the nonparametric estimator:\n      \n    \\|\\widehat{h}-h_{o}\\|_{\\mathcal{H}}=o_{p}(n^{-1/4})\n     \n    (Eq. (2))\n\n### Question\n\nBased on the provided model specification and the logic of Theorem 2, which of the following statements accurately describe the roles of the derivatives and the implications of the convergence rate conditions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem investigates the cond...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem investigates the cond...", "error": null}}
{"ID": 118, "Question": "### Background\n\nThis problem concerns the conditions required to ensure the consistency of a semiparametric Z-estimator, `\\(\\widehat{\\theta}\\)`, for a true parameter `\\(\\theta_o\\)`. The framework is that of a general semiparametric model where the parameter of interest `\\(\\theta\\)` is estimated by minimizing a sample criterion function `\\(M_n(\\theta, \\widehat{h})\\)` that depends on a preliminary nonparametric estimate `\\(\\widehat{h}\\)`.\n\n### Data / Model Specification\n\n**Theorem 1** in the paper provides sufficient conditions for the consistency of `\\(\\widehat{\\theta}\\)` (i.e., `\\(\\widehat{\\theta} - \\theta_o = o_p(1)\\)`). Key conditions are:\n\n1.  **Approximate Minimization:** `\\(\\|M_{n}(\\widehat{\\theta},\\widehat{h})\\| \\le \\inf_{\\theta\\in\\Theta}\\|M_{n}(\\theta,\\widehat{h})\\| + o_{p}(1)\\)`\n2.  **Identification:** For all `\\(\\delta>0\\)`, there exists `\\(\\epsilon(\\delta)>0\\)` such that `\\(\\inf_{\\|\\theta-\\theta_{o}\\|>\\delta}\\|M(\\theta,h_{o})\\| \\ge \\epsilon(\\delta)>0\\)`\n3.  **Continuity:** `\\(M(\\theta,h)\\)` is continuous in `\\(h\\)` at `\\(h_o\\)` uniformly in `\\(\\theta\\)`.\n4.  **Consistent Nonparametric Estimator:** `\\(\\|\\widehat{h}-h_{o}\\|_{\\mathcal{H}}=o_{p}(1)\\)`\n5.  **Uniform Convergence:** `\\(\\sup_{\\theta\\in\\Theta, \\|h-h_{o}\\|_{\\mathcal H}\\leq\\delta_{n}} \\frac{\\|M_{n}(\\theta,h)-M(\\theta,h)\\|}{1+\\|M_{n}(\\theta,h)\\|+\\|M(\\theta,h)\\|}=o_{p}(1)\\)` for `\\(\\delta_n=o(1)\\)`\n\n### Question\n\nConsider a scenario where a researcher attempts to estimate a parameter `\\(\\theta_o\\)` using this framework, but one or more conditions of Theorem 1 are violated. Which of the following scenarios correctly identify a violation of a specific condition and its consequence? Select all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "118", "question": "### Background\n\nThis problem concerns the conditions required to ensure the consistency of a semiparametric Z-estimator, `\\(\\widehat{\\theta}\\)`, for a true parameter `\\(\\theta_o\\)`. The framework is that of a general semiparametric model where the parameter of interest `\\(\\theta\\)` is estimated by minimizing a sample criterion function `\\(M_n(\\theta, \\widehat{h})\\)` that depends on a preliminary nonparametric estimate `\\(\\widehat{h}\\)`.\n\n### Data / Model Specification\n\n**Theorem 1** in the paper provides sufficient conditions for the consistency of `\\(\\widehat{\\theta}\\)` (i.e., `\\(\\widehat{\\theta} - \\theta_o = o_p(1)\\)`). Key conditions are:\n\n1.  **Approximate Minimization:** `\\(\\|M_{n}(\\widehat{\\theta},\\widehat{h})\\| \\le \\inf_{\\theta\\in\\Theta}\\|M_{n}(\\theta,\\widehat{h})\\| + o_{p}(1)\\)`\n2.  **Identification:** For all `\\(\\delta>0\\)`, there exists `\\(\\epsilon(\\delta)>0\\)` such that `\\(\\inf_{\\|\\theta-\\theta_{o}\\|>\\delta}\\|M(\\theta,h_{o})\\| \\ge \\epsilon(\\delta)>0\\)`\n3.  **Continuity:** `\\(M(\\theta,h)\\)` is continuous in `\\(h\\)` at `\\(h_o\\)` uniformly in `\\(\\theta\\)`.\n4.  **Consistent Nonparametric Estimator:** `\\(\\|\\widehat{h}-h_{o}\\|_{\\mathcal{H}}=o_{p}(1)\\)`\n5.  **Uniform Convergence:** `\\(\\sup_{\\theta\\in\\Theta, \\|h-h_{o}\\|_{\\mathcal H}\\leq\\delta_{n}} \\frac{\\|M_{n}(\\theta,h)-M(\\theta,h)\\|}{1+\\|M_{n}(\\theta,h)\\|+\\|M(\\theta,h)\\|}=o_{p}(1)\\)` for `\\(\\delta_n=o(1)\\)`\n\n### Question\n\nConsider a scenario where a researcher attempts to estimate a parameter `\\(\\theta_o\\)` using this framework, but one or more conditions of Theorem 1 are violated. Which of the following scenarios correctly identify a violation of a specific condition and its consequence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem concerns the conditio...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem concerns the conditio...", "error": null}}
{"ID": 119, "Question": "### Background\n\n**Research Question.** This problem concerns the derivation and interpretation of a bivariate dynamic model of cash and futures prices under the assumption of finite arbitrage elasticity. It explores the model's microfoundations, its limiting behavior, and the structural determinants of price discovery.\n\n**Setting / Institutional Environment.** The model features a cash market with `N_c` participants, a futures market with `N_f` participants, and a group of arbitrageurs operating in both. Non-arbitrageur demand has a constant elasticity `A`, while arbitrageurs supply arbitrage services with a finite elasticity `H`.\n\n### Data / Model Specification\n\nMarket clearing prices (`C_k`, `F'_k`) are functions of the mean reservation prices of cash (`r^c_k`) and futures (`r^f_k`) market participants.\n\nMean reservation prices evolve from the previous period's clearing prices (`C_{k-1}`, `F'_{k-1}`) plus common and idiosyncratic shocks.\n\nThis process yields the simultaneous price dynamics model:\n  \n\\binom{C_{k}}{F'_{k}}=\\begin{bmatrix} 1-a & a \\\\ b & 1-b \\end{bmatrix} \\binom{C_{k-1}}{F'_{k-1}} + \\binom{u_{k}^{c}}{u_{k}^{f}} \\quad \\text{(Eq. (1))}\n \nwhere the parameters `a` and `b` capture the cross-market influence:\n  \na=\\frac{H/(N_{c} A)}{1+H/(N_{c} A)+H/(N_{f} A)}, \\quad b=\\frac{H/(N_{f} A)}{1+H/(N_{c} A)+H/(N_{f} A)} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the provided model, select all statements that are **correct** regarding the price discovery mechanism.", "Answer": ["C", "D"], "qid": "119", "question": "### Background\n\n**Research Question.** This problem concerns the derivation and interpretation of a bivariate dynamic model of cash and futures prices under the assumption of finite arbitrage elasticity. It explores the model's microfoundations, its limiting behavior, and the structural determinants of price discovery.\n\n**Setting / Institutional Environment.** The model features a cash market with `N_c` participants, a futures market with `N_f` participants, and a group of arbitrageurs operating in both. Non-arbitrageur demand has a constant elasticity `A`, while arbitrageurs supply arbitrage services with a finite elasticity `H`.\n\n### Data / Model Specification\n\nMarket clearing prices (`C_k`, `F'_k`) are functions of the mean reservation prices of cash (`r^c_k`) and futures (`r^f_k`) market participants.\n\nMean reservation prices evolve from the previous period's clearing prices (`C_{k-1}`, `F'_{k-1}`) plus common and idiosyncratic shocks.\n\nThis process yields the simultaneous price dynamics model:\n  \n\\binom{C_{k}}{F'_{k}}=\\begin{bmatrix} 1-a & a \\\\ b & 1-b \\end{bmatrix} \\binom{C_{k-1}}{F'_{k-1}} + \\binom{u_{k}^{c}}{u_{k}^{f}} \\quad \\text{(Eq. (1))}\n \nwhere the parameters `a` and `b` capture the cross-market influence:\n  \na=\\frac{H/(N_{c} A)}{1+H/(N_{c} A)+H/(N_{f} A)}, \\quad b=\\frac{H/(N_{f} A)}{1+H/(N_{c} A)+H/(N_{f} A)} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the provided model, select all statements that are **correct** regarding the price discovery mechanism.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 120, "Question": "### Background\n\n**Research Question.** This problem concerns the derivation and interpretation of a bivariate dynamic model of cash and futures prices under the assumption of finite arbitrage elasticity. It explores the model's microfoundations, its limiting behavior, and the structural determinants of price discovery.\n\n**Setting / Institutional Environment.** The model features a cash market with `N_c` participants, a futures market with `N_f` participants, and a group of arbitrageurs operating in both. Non-arbitrageur demand has a constant elasticity `A`, while arbitrageurs supply arbitrage services with a finite elasticity `H`.\n\n### Data / Model Specification\n\nMarket clearing prices (`C_k`, `F'_k`) are functions of the mean reservation prices of cash (`r^c_k`) and futures (`r^f_k`) market participants.\n\nMean reservation prices evolve from the previous period's clearing prices (`C_{k-1}`, `F'_{k-1}`) plus common and idiosyncratic shocks.\n\nThis process yields the simultaneous price dynamics model:\n  \n\\binom{C_{k}}{F'_{k}}=\\begin{bmatrix} 1-a & a \\\\ b & 1-b \\end{bmatrix} \\binom{C_{k-1}}{F'_{k-1}} + \\binom{u_{k}^{c}}{u_{k}^{f}} \\quad \\text{(Eq. (1))}\n \nwhere the parameters `a` and `b` capture the cross-market influence:\n  \na=\\frac{H/(N_{c} A)}{1+H/(N_{c} A)+H/(N_{f} A)}, \\quad b=\\frac{H/(N_{f} A)}{1+H/(N_{c} A)+H/(N_{f} A)} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nConsider the behavior of the model in the limiting case where the elasticity of supply of arbitrage services (`H`) approaches infinity. Select all statements that **correctly** describe the market in this scenario.", "Answer": ["C", "D"], "qid": "120", "question": "### Background\n\n**Research Question.** This problem concerns the derivation and interpretation of a bivariate dynamic model of cash and futures prices under the assumption of finite arbitrage elasticity. It explores the model's microfoundations, its limiting behavior, and the structural determinants of price discovery.\n\n**Setting / Institutional Environment.** The model features a cash market with `N_c` participants, a futures market with `N_f` participants, and a group of arbitrageurs operating in both. Non-arbitrageur demand has a constant elasticity `A`, while arbitrageurs supply arbitrage services with a finite elasticity `H`.\n\n### Data / Model Specification\n\nMarket clearing prices (`C_k`, `F'_k`) are functions of the mean reservation prices of cash (`r^c_k`) and futures (`r^f_k`) market participants.\n\nMean reservation prices evolve from the previous period's clearing prices (`C_{k-1}`, `F'_{k-1}`) plus common and idiosyncratic shocks.\n\nThis process yields the simultaneous price dynamics model:\n  \n\\binom{C_{k}}{F'_{k}}=\\begin{bmatrix} 1-a & a \\\\ b & 1-b \\end{bmatrix} \\binom{C_{k-1}}{F'_{k-1}} + \\binom{u_{k}^{c}}{u_{k}^{f}} \\quad \\text{(Eq. (1))}\n \nwhere the parameters `a` and `b` capture the cross-market influence:\n  \na=\\frac{H/(N_{c} A)}{1+H/(N_{c} A)+H/(N_{f} A)}, \\quad b=\\frac{H/(N_{f} A)}{1+H/(N_{c} A)+H/(N_{f} A)} \\quad \\text{(Eq. (2))}\n \n\n### Question\n\nConsider the behavior of the model in the limiting case where the elasticity of supply of arbitrage services (`H`) approaches infinity. Select all statements that **correctly** describe the market in this scenario.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 121, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical foundation of higher-order asymptotic corrections for the t-statistic in a linear model with AR(1) errors estimated via Feasible Generalized Least Squares (FGLS). The goal is to understand how these corrections are derived and why they improve upon standard inference.\n\n**Setting / Institutional Environment.** In FGLS, the t-statistic's distribution deviates from the standard t-distribution in finite samples because the AR(1) parameter `\\rho` is estimated. Higher-order asymptotics provide a way to approximate this deviation and correct for it, using either an Edgeworth expansion to adjust the critical value or a Cornish-Fisher expansion to adjust the statistic itself.\n\n**Variables & Parameters.**\n- `t`: The FGLS t-statistic for the hypothesis `e'\\beta - e_0 = 0`.\n- `\\tau`: The asymptotic scale, `1/\\sqrt{T}`.\n- `I_{T-n}(x)` and `i_{T-n}(x)`: The CDF and PDF of a t-distribution with `T-n` degrees of freedom.\n- `p_1, p_2`: Coefficients in the expansion that depend on the properties of the estimators for `\\rho` and `\\sigma^2`.\n- `\\lambda_0, \\lambda, \\lambda_*`: Limiting second moments related to the stochastic expansion of the estimators.\n\n---\n\n### Data / Model Specification\n\nConsider the linear model `y = X\\beta + \\sigma u` where the errors `u` follow a stationary AR(1) process `u_t = \\rho u_{t-1} + \\varepsilon_t`. The FGLS t-statistic is used for inference.\n\n**Lemma 1 (Edgeworth Expansion):** The distribution of the t-statistic admits the expansion:\n\n  \n\\mathrm{Pr}\\left\\{t\\leqslant x\\right\\}=I_{T-n}(x)-\\frac{\\tau^{2}}{2}(p_{1}+p_{2}x^{2})x i_{T-n}(x)+\\mathrm{O}(\\tau^{3}) \\quad \\quad \\text{(Eq. (1))}\n \n\n**Theorem 1 (Cornish-Fisher Correction):** An adjusted statistic, `\\hat{t}`, is approximately distributed as `t_{T-n}`:\n\n  \n\\hat{t}=t-\\frac{\\tau^{2}}{2}(p_{1}+p_{2}t^{2})t \\quad \\quad \\text{(Eq. (2))}\n \n\n**Proposition 1 (Parameter Simplification):** For any asymptotically efficient estimator of `\\rho`, the underlying moment parameters simplify to:\n\n  \n\\lambda_{0}=2, \\quad \\lambda=0, \\quad \\lambda_{*}=1-\\rho^{2} \\quad \\quad \\text{(Eq. (3))}\n \n\nThe coefficient `p_2` in the expansions is given by:\n\n  \np_{2}=(\\lambda_{*}\\kappa^{2}-2\\lambda\\kappa+\\lambda_{0}-2)/4 \\quad \\quad \\text{(Eq. (4))}\n \n\nwhere `\\kappa` is a scalar that depends on the regressors `X` and `\\rho`.\n\n---\n\n### Question\n\nBased on the provided model and theoretical results, select all of the following statements that are mathematically correct and properly interpret the size-correction framework.", "Answer": ["B", "D"], "qid": "121", "question": "### Background\n\n**Research Question.** This problem examines the theoretical foundation of higher-order asymptotic corrections for the t-statistic in a linear model with AR(1) errors estimated via Feasible Generalized Least Squares (FGLS). The goal is to understand how these corrections are derived and why they improve upon standard inference.\n\n**Setting / Institutional Environment.** In FGLS, the t-statistic's distribution deviates from the standard t-distribution in finite samples because the AR(1) parameter `\\rho` is estimated. Higher-order asymptotics provide a way to approximate this deviation and correct for it, using either an Edgeworth expansion to adjust the critical value or a Cornish-Fisher expansion to adjust the statistic itself.\n\n**Variables & Parameters.**\n- `t`: The FGLS t-statistic for the hypothesis `e'\\beta - e_0 = 0`.\n- `\\tau`: The asymptotic scale, `1/\\sqrt{T}`.\n- `I_{T-n}(x)` and `i_{T-n}(x)`: The CDF and PDF of a t-distribution with `T-n` degrees of freedom.\n- `p_1, p_2`: Coefficients in the expansion that depend on the properties of the estimators for `\\rho` and `\\sigma^2`.\n- `\\lambda_0, \\lambda, \\lambda_*`: Limiting second moments related to the stochastic expansion of the estimators.\n\n---\n\n### Data / Model Specification\n\nConsider the linear model `y = X\\beta + \\sigma u` where the errors `u` follow a stationary AR(1) process `u_t = \\rho u_{t-1} + \\varepsilon_t`. The FGLS t-statistic is used for inference.\n\n**Lemma 1 (Edgeworth Expansion):** The distribution of the t-statistic admits the expansion:\n\n  \n\\mathrm{Pr}\\left\\{t\\leqslant x\\right\\}=I_{T-n}(x)-\\frac{\\tau^{2}}{2}(p_{1}+p_{2}x^{2})x i_{T-n}(x)+\\mathrm{O}(\\tau^{3}) \\quad \\quad \\text{(Eq. (1))}\n \n\n**Theorem 1 (Cornish-Fisher Correction):** An adjusted statistic, `\\hat{t}`, is approximately distributed as `t_{T-n}`:\n\n  \n\\hat{t}=t-\\frac{\\tau^{2}}{2}(p_{1}+p_{2}t^{2})t \\quad \\quad \\text{(Eq. (2))}\n \n\n**Proposition 1 (Parameter Simplification):** For any asymptotically efficient estimator of `\\rho`, the underlying moment parameters simplify to:\n\n  \n\\lambda_{0}=2, \\quad \\lambda=0, \\quad \\lambda_{*}=1-\\rho^{2} \\quad \\quad \\text{(Eq. (3))}\n \n\nThe coefficient `p_2` in the expansions is given by:\n\n  \np_{2}=(\\lambda_{*}\\kappa^{2}-2\\lambda\\kappa+\\lambda_{0}-2)/4 \\quad \\quad \\text{(Eq. (4))}\n \n\nwhere `\\kappa` is a scalar that depends on the regressors `X` and `\\rho`.\n\n---\n\n### Question\n\nBased on the provided model and theoretical results, select all of the following statements that are mathematically correct and properly interpret the size-correction framework.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 122, "Question": "## Background\n\nAn observer is assessing players' strategies in a game. Their assessment is captured by a relative probability space $(\\rho, \\Omega)$ over the joint pure strategy space. The paper introduces two key concepts: **weak independence** and **strong independence** of the players' strategy choices (represented by random variables $\\mathbf{s}_1, ..., \\mathbf{s}_n$).\n\n**Strong independence** is characterized by the existence of an approximating sequence of positive product probabilities. A more direct characterization is given by the **Main Characterization Theorem (Theorem 2.13)**, which links strong independence to properties of hypothetical replications of the game.\n\n---\n\n## Data / Model Specification\n\n**Main Characterization Theorem (Theorem 2.13):** The random variables representing players' choices, $\\mathbf{s}_1, ..., \\mathbf{s}_n$, are **strongly independent** if and only if for every integer $T \\ge 1$, there exists a collection of random vectors $\\{\\mathbf{s}^t\\}_{t=1}^T$ (where $\\mathbf{s}^t = (\\mathbf{s}_1^t, ..., \\mathbf{s}_n^t)$) on some common relative probability space such that:\n\n(i) The distribution of each $\\mathbf{s}^t$ is the same as that of the original vector $\\mathbf{s}$.\n(ii) The collection $\\{\\mathbf{s}^t\\}_{t=1}^T$ is **coordinate-wise exchangeable**.\n(iii) The random variables $\\mathbf{s}^1, \\mathbf{s}^2, ..., \\mathbf{s}^T$ are **weakly independent**.\n\n**Coordinate-wise exchangeability** means that for any given player, permuting their sequence of strategy choices across the $T$ hypothetical games does not change the relative probability of the overall outcome sequence.\n\n---\n\n## Question\n\nBased on the Main Characterization Theorem and its interpretation in the paper, which of the following statements are valid conclusions or correct interpretations of its components?\n\nSelect all that apply.", "Answer": ["A", "C", "D"], "qid": "122", "question": "## Background\n\nAn observer is assessing players' strategies in a game. Their assessment is captured by a relative probability space $(\\rho, \\Omega)$ over the joint pure strategy space. The paper introduces two key concepts: **weak independence** and **strong independence** of the players' strategy choices (represented by random variables $\\mathbf{s}_1, ..., \\mathbf{s}_n$).\n\n**Strong independence** is characterized by the existence of an approximating sequence of positive product probabilities. A more direct characterization is given by the **Main Characterization Theorem (Theorem 2.13)**, which links strong independence to properties of hypothetical replications of the game.\n\n---\n\n## Data / Model Specification\n\n**Main Characterization Theorem (Theorem 2.13):** The random variables representing players' choices, $\\mathbf{s}_1, ..., \\mathbf{s}_n$, are **strongly independent** if and only if for every integer $T \\ge 1$, there exists a collection of random vectors $\\{\\mathbf{s}^t\\}_{t=1}^T$ (where $\\mathbf{s}^t = (\\mathbf{s}_1^t, ..., \\mathbf{s}_n^t)$) on some common relative probability space such that:\n\n(i) The distribution of each $\\mathbf{s}^t$ is the same as that of the original vector $\\mathbf{s}$.\n(ii) The collection $\\{\\mathbf{s}^t\\}_{t=1}^T$ is **coordinate-wise exchangeable**.\n(iii) The random variables $\\mathbf{s}^1, \\mathbf{s}^2, ..., \\mathbf{s}^T$ are **weakly independent**.\n\n**Coordinate-wise exchangeability** means that for any given player, permuting their sequence of strategy choices across the $T$ hypothetical games does not change the relative probability of the overall outcome sequence.\n\n---\n\n## Question\n\nBased on the Main Characterization Theorem and its interpretation in the paper, which of the following statements are valid conclusions or correct interpretations of its components?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\nAn observer is assessing players' s...", "model_answer": "This is a detailed response to the question about ## Background\n\nAn observer is assessing players' s...", "error": null}}
{"ID": 123, "Question": "## Background\n\nAn observer's assessment of players' pure strategies is represented by a relative probability space $(\\rho, \\Omega)$. This assessment over pure strategies can induce an assessment $(\\mu, \\pi)$—a set of beliefs and behavioral strategies—on the game tree. A key question is what properties of the observer's initial assessment $\\rho$ guarantee that the induced assessment $(\\mu, \\pi)$ is **consistent** in the sense of Kreps and Wilson.\n\n---\n\n## Data / Model Specification\n\n**Lemma 3.1 states:**\n(i) If the players' strategy choices $\\mathbf{s}_i$ are **weakly independent** with respect to $\\rho$, then $\\rho$ induces an assessment $(\\mu, \\pi)$ on the tree.\n(ii) If the $\\mathbf{s}_i$ are **strongly independent** with respect to $\\rho$, then $\\rho$ induces a **consistent** assessment $(\\mu, \\pi)$ on the tree.\n\n**Key Definitions:**\n- **Weak Independence:** The relative probability of one player's move is unaffected by the moves of other players.\n- **Strong Independence:** The relative probability structure can be approximated by a sequence of strictly positive, independent (product) probability distributions.\n- **Consistent Assessment:** An assessment $(\\mu, \\pi)$ is consistent if it is the limit of assessments derived from a sequence of strictly positive behavioral strategies.\n- **Kuhn's Theorem:** In a game of perfect recall, any distribution on terminal nodes from independent mixed strategies (a product probability on pure strategies) can also be achieved by a profile of behavioral strategies.\n\n---\n\n## Question\n\nWhich of the following statements accurately describe the relationship between the observer's assessment $\\rho$ and the induced assessment $(\\mu, \\pi)$ on the game tree?\n\nSelect all that apply.", "Answer": ["A", "B"], "qid": "123", "question": "## Background\n\nAn observer's assessment of players' pure strategies is represented by a relative probability space $(\\rho, \\Omega)$. This assessment over pure strategies can induce an assessment $(\\mu, \\pi)$—a set of beliefs and behavioral strategies—on the game tree. A key question is what properties of the observer's initial assessment $\\rho$ guarantee that the induced assessment $(\\mu, \\pi)$ is **consistent** in the sense of Kreps and Wilson.\n\n---\n\n## Data / Model Specification\n\n**Lemma 3.1 states:**\n(i) If the players' strategy choices $\\mathbf{s}_i$ are **weakly independent** with respect to $\\rho$, then $\\rho$ induces an assessment $(\\mu, \\pi)$ on the tree.\n(ii) If the $\\mathbf{s}_i$ are **strongly independent** with respect to $\\rho$, then $\\rho$ induces a **consistent** assessment $(\\mu, \\pi)$ on the tree.\n\n**Key Definitions:**\n- **Weak Independence:** The relative probability of one player's move is unaffected by the moves of other players.\n- **Strong Independence:** The relative probability structure can be approximated by a sequence of strictly positive, independent (product) probability distributions.\n- **Consistent Assessment:** An assessment $(\\mu, \\pi)$ is consistent if it is the limit of assessments derived from a sequence of strictly positive behavioral strategies.\n- **Kuhn's Theorem:** In a game of perfect recall, any distribution on terminal nodes from independent mixed strategies (a product probability on pure strategies) can also be achieved by a profile of behavioral strategies.\n\n---\n\n## Question\n\nWhich of the following statements accurately describe the relationship between the observer's assessment $\\rho$ and the induced assessment $(\\mu, \\pi)$ on the game tree?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\nAn observer's assessment of players...", "model_answer": "This is a detailed response to the question about ## Background\n\nAn observer's assessment of players...", "error": null}}
{"ID": 124, "Question": "## Background\n\n**Strong independence** of random variables on a relative probability space is a central concept in the paper. It is a stronger notion than weak independence. One way to characterize it is through an algebraic property of the relative probability function $\\rho$ itself, without reference to approximating sequences.\n\n---\n\n## Data / Model Specification\n\n**Definition of a Cycle (Definition 2.9):** A sequence of pairs of outcomes from the joint range $W$, say $(u, u'), (v, v'), ..., (w, w')$, is called a **cycle** if for each coordinate, the vector of initial values (e.g., $(u_1, v_1, ..., w_1)$ for the first coordinate) is a permutation of the vector of final values (e.g., $(u'_1, v'_1, ..., w'_1)$).\n\n**Cycle Condition Theorem (Theorem 2.10):** A set of random variables are **strongly independent** if and only if for any cycle $(u, u'), (v, v'), ..., (w, w')$, the following condition holds:\n\n  \n\\rho([u],[u']) \\rho([v],[v']) \\cdots \\rho([w],[w']) = 1 \\quad \\text{(Eq. 1)}\n \n\nwhenever the left-hand side is well-defined.\n\nConsider a scenario with two random variables, $\\mathbf{x}$ with range $X = \\{x_1, x_2\\}$ and $\\mathbf{y}$ with range $Y = \\{y_1, y_2\\}$. The joint range is $W = X \\times Y$.\n\n---\n\n## Question\n\nWhich of the following sequences of pairs of outcomes from $W$ constitute a valid **cycle** according to Definition 2.9?\n\nSelect all that apply.", "Answer": ["A", "B", "C"], "qid": "124", "question": "## Background\n\n**Strong independence** of random variables on a relative probability space is a central concept in the paper. It is a stronger notion than weak independence. One way to characterize it is through an algebraic property of the relative probability function $\\rho$ itself, without reference to approximating sequences.\n\n---\n\n## Data / Model Specification\n\n**Definition of a Cycle (Definition 2.9):** A sequence of pairs of outcomes from the joint range $W$, say $(u, u'), (v, v'), ..., (w, w')$, is called a **cycle** if for each coordinate, the vector of initial values (e.g., $(u_1, v_1, ..., w_1)$ for the first coordinate) is a permutation of the vector of final values (e.g., $(u'_1, v'_1, ..., w'_1)$).\n\n**Cycle Condition Theorem (Theorem 2.10):** A set of random variables are **strongly independent** if and only if for any cycle $(u, u'), (v, v'), ..., (w, w')$, the following condition holds:\n\n  \n\\rho([u],[u']) \\rho([v],[v']) \\cdots \\rho([w],[w']) = 1 \\quad \\text{(Eq. 1)}\n \n\nwhenever the left-hand side is well-defined.\n\nConsider a scenario with two random variables, $\\mathbf{x}$ with range $X = \\{x_1, x_2\\}$ and $\\mathbf{y}$ with range $Y = \\{y_1, y_2\\}$. The joint range is $W = X \\times Y$.\n\n---\n\n## Question\n\nWhich of the following sequences of pairs of outcomes from $W$ constitute a valid **cycle** according to Definition 2.9?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\n**Strong independence** of random v...", "model_answer": "This is a detailed response to the question about ## Background\n\n**Strong independence** of random v...", "error": null}}
{"ID": 125, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical relationship between public infrastructure (piped water), private household behavior (health spending), and child health outcomes. It explores why the benefits of public investment may not reach the poor.\n\n**Setting.** The analysis is grounded in a behavioral model where parents choose private health spending `s` to maximize utility, which depends on their own consumption and their child's health. Child health `h` is produced by private spending `s`, public infrastructure `w` (piped water), and other characteristics `x`.\n\n---\n\n### Data / Model Specification\n\nParents choose `s` to maximize `u(y-s, w, x) + h(s, w, x)`, where `y` is income. The health production function `h(s, w, x)` is strictly increasing in `s` and `w`, and weakly concave in `s` (`h_{ss} \\leq 0`). The parental utility function `u(y-s, w, x)` is strictly increasing and concave in consumption (`y-s`). The equilibrium level of child health is `H(w, y, x) \\equiv h[s(w, y, x), w, x]`, where `s(w, y, x)` is the optimal spending level.\n\nThe total effect of piped water on health is given by `H_w = h_s s_w + h_w`, where `h_w` is the direct effect and `h_s s_w` is the indirect behavioral effect.\n\n---\n\n### Question\n\nBased on the behavioral model presented, select all statements that are mathematically or conceptually correct.", "Answer": ["B", "C"], "qid": "125", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical relationship between public infrastructure (piped water), private household behavior (health spending), and child health outcomes. It explores why the benefits of public investment may not reach the poor.\n\n**Setting.** The analysis is grounded in a behavioral model where parents choose private health spending `s` to maximize utility, which depends on their own consumption and their child's health. Child health `h` is produced by private spending `s`, public infrastructure `w` (piped water), and other characteristics `x`.\n\n---\n\n### Data / Model Specification\n\nParents choose `s` to maximize `u(y-s, w, x) + h(s, w, x)`, where `y` is income. The health production function `h(s, w, x)` is strictly increasing in `s` and `w`, and weakly concave in `s` (`h_{ss} \\leq 0`). The parental utility function `u(y-s, w, x)` is strictly increasing and concave in consumption (`y-s`). The equilibrium level of child health is `H(w, y, x) \\equiv h[s(w, y, x), w, x]`, where `s(w, y, x)` is the optimal spending level.\n\nThe total effect of piped water on health is given by `H_w = h_s s_w + h_w`, where `h_w` is the direct effect and `h_s s_w` is the indirect behavioral effect.\n\n---\n\n### Question\n\nBased on the behavioral model presented, select all statements that are mathematically or conceptually correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 126, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical and empirical relationship between public infrastructure (piped water), private household behavior (health spending), and child health outcomes. It explores why the benefits of public investment may not reach the poor.\n\n**Setting.** The analysis is grounded in a behavioral model where parents choose private health spending `s` to maximize utility, which depends on their own consumption and their child's health. Child health `h` is produced by private spending `s`, public infrastructure `w` (piped water), and other characteristics `x`.\n\n---\n\n### Data / Model Specification\n\nParents choose `s` to maximize `u(y-s, w, x) + h(s, w, x)`, where `y` is income. The health production function `h(s, w, x)` is strictly increasing in `s` and `w`. The equilibrium level of child health is `H(w, y, x) \\equiv h[s(w, y, x), w, x]`, where `s(w, y, x)` is the optimal spending level.\n\nThe total effect of piped water on health is `H_w = \\partial H / \\partial w`, and the income gradient of this effect is `H_{wy} = \\partial^2 H / \\partial w \\partial y`. A special case of the model shows that `H_{wy}` has the same sign as `h_{sw} = \\partial^2 h / \\partial s \\partial w`.\n\nThe key empirical results on treatment effect heterogeneity are summarized below.\n\n**Table 1: Impact of Piped Water on Diarrhea Prevalence, by Income Quintile**\n\n| Income Quintile | Impact of Piped Water (ATT) | Std. Error |\n| :--- | :--- | :--- |\n| 1 (Poorest) | +0.0032* | (0.001) |\n| 2 | +0.0007 | (0.001) |\n| 3 | -0.0039* | (0.001) |\n| 4 | -0.0036* | (0.001) |\n| 5 (Richest) | -0.0068* | (0.001) |\n\n*Note: A negative impact means a reduction in diarrhea. * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nLet child health `H` be defined as the negative of diarrhea prevalence (so a health improvement is a positive change in `H`). Given the theoretical model and the empirical results in **Table 1**, select all conclusions that are correctly inferred.", "Answer": ["C", "D"], "qid": "126", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical and empirical relationship between public infrastructure (piped water), private household behavior (health spending), and child health outcomes. It explores why the benefits of public investment may not reach the poor.\n\n**Setting.** The analysis is grounded in a behavioral model where parents choose private health spending `s` to maximize utility, which depends on their own consumption and their child's health. Child health `h` is produced by private spending `s`, public infrastructure `w` (piped water), and other characteristics `x`.\n\n---\n\n### Data / Model Specification\n\nParents choose `s` to maximize `u(y-s, w, x) + h(s, w, x)`, where `y` is income. The health production function `h(s, w, x)` is strictly increasing in `s` and `w`. The equilibrium level of child health is `H(w, y, x) \\equiv h[s(w, y, x), w, x]`, where `s(w, y, x)` is the optimal spending level.\n\nThe total effect of piped water on health is `H_w = \\partial H / \\partial w`, and the income gradient of this effect is `H_{wy} = \\partial^2 H / \\partial w \\partial y`. A special case of the model shows that `H_{wy}` has the same sign as `h_{sw} = \\partial^2 h / \\partial s \\partial w`.\n\nThe key empirical results on treatment effect heterogeneity are summarized below.\n\n**Table 1: Impact of Piped Water on Diarrhea Prevalence, by Income Quintile**\n\n| Income Quintile | Impact of Piped Water (ATT) | Std. Error |\n| :--- | :--- | :--- |\n| 1 (Poorest) | +0.0032* | (0.001) |\n| 2 | +0.0007 | (0.001) |\n| 3 | -0.0039* | (0.001) |\n| 4 | -0.0036* | (0.001) |\n| 5 (Richest) | -0.0068* | (0.001) |\n\n*Note: A negative impact means a reduction in diarrhea. * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nLet child health `H` be defined as the negative of diarrhea prevalence (so a health improvement is a positive change in `H`). Given the theoretical model and the empirical results in **Table 1**, select all conclusions that are correctly inferred.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 127, "Question": "### Background\n\nThe paper explores the claim that marihuana's impact on work is not a fixed physiological constant but an economic choice. The analysis is grounded in a controlled experiment where subjects earned piece-rate wages for weaving belts with full flexibility over work hours. A key finding was that subjects tended to reduce work during periods of intoxication but compensated by increasing efficiency at other times, leaving total output largely unaffected. The authors argue this behavior is driven by economic incentives.\n\n### Question\n\nThe paper's central argument is that the effect of marihuana on work performance is \"endogenously determined.\" Based on the provided background, select all statements that correctly characterize this economic perspective.", "Answer": ["B", "D"], "qid": "127", "question": "### Background\n\nThe paper explores the claim that marihuana's impact on work is not a fixed physiological constant but an economic choice. The analysis is grounded in a controlled experiment where subjects earned piece-rate wages for weaving belts with full flexibility over work hours. A key finding was that subjects tended to reduce work during periods of intoxication but compensated by increasing efficiency at other times, leaving total output largely unaffected. The authors argue this behavior is driven by economic incentives.\n\n### Question\n\nThe paper's central argument is that the effect of marihuana on work performance is \"endogenously determined.\" Based on the provided background, select all statements that correctly characterize this economic perspective.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper explores the claim that ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper explores the claim that ...", "error": null}}
{"ID": 128, "Question": "### Background\n\nThe paper's theoretical framework posits that individuals choose their level of effort and marihuana consumption based on the economic environment. A key concept is the \"opportunity cost of intoxication\"—the income lost due to reduced productivity while under the influence of the drug. In the experiment, a high piece-rate wage and flexible schedule created a high opportunity cost, incentivizing workers to maintain production. The paper proposes a simple model where income `Y` depends on an effective wage `w_e`, labor `L`, and effort `E`, which is negatively affected by marihuana consumption `M`:\n  \nY = w_e \\times L \\times E(M) \\quad \\text{Eq. (1)}\n \n\n### Question\n\nUsing this framework, consider two labor markets where marihuana is legalized: **Market A** (salaried graphic designers with complex, hard-to-monitor creative tasks) and **Market B** (gig-economy warehouse workers paid a piece rate for simple, repetitive tasks). Select all of the following predictions that are consistent with the paper's theoretical framework.", "Answer": ["C", "D"], "qid": "128", "question": "### Background\n\nThe paper's theoretical framework posits that individuals choose their level of effort and marihuana consumption based on the economic environment. A key concept is the \"opportunity cost of intoxication\"—the income lost due to reduced productivity while under the influence of the drug. In the experiment, a high piece-rate wage and flexible schedule created a high opportunity cost, incentivizing workers to maintain production. The paper proposes a simple model where income `Y` depends on an effective wage `w_e`, labor `L`, and effort `E`, which is negatively affected by marihuana consumption `M`:\n  \nY = w_e \\times L \\times E(M) \\quad \\text{Eq. (1)}\n \n\n### Question\n\nUsing this framework, consider two labor markets where marihuana is legalized: **Market A** (salaried graphic designers with complex, hard-to-monitor creative tasks) and **Market B** (gig-economy warehouse workers paid a piece rate for simple, repetitive tasks). Select all of the following predictions that are consistent with the paper's theoretical framework.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical framework ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical framework ...", "error": null}}
{"ID": 129, "Question": "### Background\n\n**Research Question.** This problem seeks to identify the causal effect of relocating outside the New Orleans Metropolitan Statistical Area (MSA) on the academic growth of Orleans evacuees, addressing the endogeneity of location choice.\n\n**Setting / Institutional Environment.** The analysis focuses on Orleans evacuees and their test score growth from a pre-hurricane baseline to 2008 or 2009. A key challenge is that families choose their post-hurricane location, a choice likely correlated with unobserved factors that also affect student achievement (e.g., parental motivation). To address this, an instrumental variable (IV) strategy is employed.\n\n**Variables & Parameters.**\n- Dependent Variable: Growth in standardized math or ELA score from baseline to 2008/2009.\n- Endogenous Regressor: `Orleans evacuee currently outside N.O. MSA`, an indicator variable for relocating outside the New Orleans metro area.\n- Instrument: `FEMA damage assessment to tract`, a measure of the severity of hurricane and flood damage at the student's initial school location, coded on a 0-6 scale.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses a Two-Stage Least Squares (2SLS) approach. The first and second stage results are presented below.\n\n**Table 1: IV Estimates of Relocating Outside the New Orleans MSA**\n\n| | **First Stage** (Dep Var: Outside N.O. MSA) (1) | **Second Stage (IV)** (Dep Var: Math growth) (2) | **OLS** (Dep Var: Math growth) (3) |\n|:---|:---:|:---:|:---:|\n| **Panel A: Key Regressors** | | | |\n| FEMA damage assessment to tract | 0.019 (0.006)** | | |\n| Orleans evacuee currently outside N.O. MSA | | 0.473 (0.094)** | 0.206 (0.032)** |\n| **Panel B: Sample and Model Info** | | | |\n| Sample | Orleans evacuees only | Full Sample | Full Sample |\n| F-statistic on instrument (approx) | (0.019/0.006)² ≈ 10 | | |\n| Observations | 3,259 | 125,202 | 125,202 |\n\n*Notes: Corresponds to Tables 7 and 7 Panel 2 in the source paper. ** significant at 1%. The first stage F-statistic is approximated from the t-statistic. The OLS column shows the coefficient on the location variable from a regression without instrumenting.* \n\n---\n\nBased on the provided information, which of the following statements are valid interpretations or critiques of the instrumental variable strategy?\n", "Answer": ["A", "B", "C"], "qid": "129", "question": "### Background\n\n**Research Question.** This problem seeks to identify the causal effect of relocating outside the New Orleans Metropolitan Statistical Area (MSA) on the academic growth of Orleans evacuees, addressing the endogeneity of location choice.\n\n**Setting / Institutional Environment.** The analysis focuses on Orleans evacuees and their test score growth from a pre-hurricane baseline to 2008 or 2009. A key challenge is that families choose their post-hurricane location, a choice likely correlated with unobserved factors that also affect student achievement (e.g., parental motivation). To address this, an instrumental variable (IV) strategy is employed.\n\n**Variables & Parameters.**\n- Dependent Variable: Growth in standardized math or ELA score from baseline to 2008/2009.\n- Endogenous Regressor: `Orleans evacuee currently outside N.O. MSA`, an indicator variable for relocating outside the New Orleans metro area.\n- Instrument: `FEMA damage assessment to tract`, a measure of the severity of hurricane and flood damage at the student's initial school location, coded on a 0-6 scale.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses a Two-Stage Least Squares (2SLS) approach. The first and second stage results are presented below.\n\n**Table 1: IV Estimates of Relocating Outside the New Orleans MSA**\n\n| | **First Stage** (Dep Var: Outside N.O. MSA) (1) | **Second Stage (IV)** (Dep Var: Math growth) (2) | **OLS** (Dep Var: Math growth) (3) |\n|:---|:---:|:---:|:---:|\n| **Panel A: Key Regressors** | | | |\n| FEMA damage assessment to tract | 0.019 (0.006)** | | |\n| Orleans evacuee currently outside N.O. MSA | | 0.473 (0.094)** | 0.206 (0.032)** |\n| **Panel B: Sample and Model Info** | | | |\n| Sample | Orleans evacuees only | Full Sample | Full Sample |\n| F-statistic on instrument (approx) | (0.019/0.006)² ≈ 10 | | |\n| Observations | 3,259 | 125,202 | 125,202 |\n\n*Notes: Corresponds to Tables 7 and 7 Panel 2 in the source paper. ** significant at 1%. The first stage F-statistic is approximated from the t-statistic. The OLS column shows the coefficient on the location variable from a regression without instrumenting.* \n\n---\n\nBased on the provided information, which of the following statements are valid interpretations or critiques of the instrumental variable strategy?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 130, "Question": "### Background\n\n**Research Question.** This problem investigates heterogeneous treatment effects of hurricane displacement, specifically examining whether the impact on academic growth varies by students' initial (pre-hurricane) academic performance.\n\n**Setting / Institutional Environment.** The analysis uses a fixed-cohort, value-added model. The sample is stratified by students' baseline (pre-hurricane) test score quintile. The model is estimated separately for students in the bottom two quintiles (1 & 2) and the top two quintiles (3 & 4) of the statewide performance distribution.\n\n---\n\n### Data / Model Specification\n\nThe analysis estimates a value-added model on subsamples of the data. Results are presented below.\n\n**Table 1: Effects on Test Score Growth by Initial Performance Quintile**\n\n| | Math growth baseline to 2009 (quintiles 1 and 2) (2) | Math growth baseline to 2009 (quintiles 3 and 4) (4) | ELA growth baseline to 2009 (quintiles 1 and 2) (2) | ELA growth baseline to 2009 (quintiles 3 and 4) (4) |\n|:---|:---:|:---:|:---:|:---:|\n| New Orleans evacuee | 0.207 (0.035)** | 0.118 (0.050)* | 0.241 (0.036)** | 0.062 (0.061) |\n\n*Notes: Corresponds to Table 6 in the source paper. ** significant at 1%, * significant at 5%. The dependent variable is growth in standardized score from baseline to 2009.* \n\n---\n\nBased on the results in Table 1, which of the following conclusions about the heterogeneous effects of displacement for New Orleans evacuees are supported by the data?\n", "Answer": ["B", "C", "D"], "qid": "130", "question": "### Background\n\n**Research Question.** This problem investigates heterogeneous treatment effects of hurricane displacement, specifically examining whether the impact on academic growth varies by students' initial (pre-hurricane) academic performance.\n\n**Setting / Institutional Environment.** The analysis uses a fixed-cohort, value-added model. The sample is stratified by students' baseline (pre-hurricane) test score quintile. The model is estimated separately for students in the bottom two quintiles (1 & 2) and the top two quintiles (3 & 4) of the statewide performance distribution.\n\n---\n\n### Data / Model Specification\n\nThe analysis estimates a value-added model on subsamples of the data. Results are presented below.\n\n**Table 1: Effects on Test Score Growth by Initial Performance Quintile**\n\n| | Math growth baseline to 2009 (quintiles 1 and 2) (2) | Math growth baseline to 2009 (quintiles 3 and 4) (4) | ELA growth baseline to 2009 (quintiles 1 and 2) (2) | ELA growth baseline to 2009 (quintiles 3 and 4) (4) |\n|:---|:---:|:---:|:---:|:---:|\n| New Orleans evacuee | 0.207 (0.035)** | 0.118 (0.050)* | 0.241 (0.036)** | 0.062 (0.061) |\n\n*Notes: Corresponds to Table 6 in the source paper. ** significant at 1%, * significant at 5%. The dependent variable is growth in standardized score from baseline to 2009.* \n\n---\n\nBased on the results in Table 1, which of the following conclusions about the heterogeneous effects of displacement for New Orleans evacuees are supported by the data?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 131, "Question": "### Background\n\n**Research Question.** This problem investigates the causal effect of hurricane-induced school evacuation on students' subsequent college enrollment decisions, contrasting this with the paper's findings on test scores.\n\n**Setting / Institutional Environment.** The analysis employs a difference-in-differences (DiD) framework comparing college-going outcomes for multiple cohorts of students. Cohorts are grouped into those scheduled to graduate before the 2005 hurricanes (pre-period) and those scheduled to graduate after (post-period). The treatment is defined at the school level: whether a student's high school was evacuated.\n\n**Key Findings from the Paper:**\n1.  Orleans evacuees experienced significant long-run *gains* in standardized test scores (approx. +0.18 standard deviations by 2009).\n2.  The DiD estimate for Orleans evacuees' enrollment in *any* college is a statistically significant **-0.042** (a 4.2 percentage point drop).\n3.  The text notes that in a pre-Katrina cohort, 66% of Orleans students attending a two-year college went to the local Delgado Community College.\n\n---\n\nWhich of the following are plausible mechanisms or interpretations that could reconcile the paradoxical findings of *increased* test scores but *decreased* college enrollment for Orleans evacuees?\n", "Answer": ["A", "B", "D"], "qid": "131", "question": "### Background\n\n**Research Question.** This problem investigates the causal effect of hurricane-induced school evacuation on students' subsequent college enrollment decisions, contrasting this with the paper's findings on test scores.\n\n**Setting / Institutional Environment.** The analysis employs a difference-in-differences (DiD) framework comparing college-going outcomes for multiple cohorts of students. Cohorts are grouped into those scheduled to graduate before the 2005 hurricanes (pre-period) and those scheduled to graduate after (post-period). The treatment is defined at the school level: whether a student's high school was evacuated.\n\n**Key Findings from the Paper:**\n1.  Orleans evacuees experienced significant long-run *gains* in standardized test scores (approx. +0.18 standard deviations by 2009).\n2.  The DiD estimate for Orleans evacuees' enrollment in *any* college is a statistically significant **-0.042** (a 4.2 percentage point drop).\n3.  The text notes that in a pre-Katrina cohort, 66% of Orleans students attending a two-year college went to the local Delgado Community College.\n\n---\n\nWhich of the following are plausible mechanisms or interpretations that could reconcile the paradoxical findings of *increased* test scores but *decreased* college enrollment for Orleans evacuees?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 132, "Question": "### Background\n\nThe paper's main identification strategy relies on assumptions about the tail behavior of the model's primitives. An alternative strategy, discussed in Section 5, relies on continuous variation in the covariates and their derivatives.\n\n### Data / Model Specification\n\n*   **Strategy 1 (Tail-based):** Uses data for two covariate values (`x₀`, `x₁`) and analyzes the behavior of `F(t|x)` as `t→0` or `t→∞`. It requires regular variation of primitives like `λ(t)` or `g(s)` at the boundaries.\n\n*   **Strategy 2 (Derivative-based):** Uses data for a continuous range of `x` and analyzes the ratio of partial derivatives `(∂F/∂x) / (∂F/∂t)`. It requires `φ(x)` to be continuously differentiable.\n\n### Question\n\nWhich of the following are correct statements contrasting the assumptions and data requirements of these two strategies? (Select all that apply)", "Answer": ["A", "D"], "qid": "132", "question": "### Background\n\nThe paper's main identification strategy relies on assumptions about the tail behavior of the model's primitives. An alternative strategy, discussed in Section 5, relies on continuous variation in the covariates and their derivatives.\n\n### Data / Model Specification\n\n*   **Strategy 1 (Tail-based):** Uses data for two covariate values (`x₀`, `x₁`) and analyzes the behavior of `F(t|x)` as `t→0` or `t→∞`. It requires regular variation of primitives like `λ(t)` or `g(s)` at the boundaries.\n\n*   **Strategy 2 (Derivative-based):** Uses data for a continuous range of `x` and analyzes the ratio of partial derivatives `(∂F/∂x) / (∂F/∂t)`. It requires `φ(x)` to be continuously differentiable.\n\n### Question\n\nWhich of the following are correct statements contrasting the assumptions and data requirements of these two strategies? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's main identification st...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's main identification st...", "error": null}}
{"ID": 133, "Question": "### Background\n\nA buyer is procuring a good with a single quality attribute `Q` and uses a linear scoring rule `S(p, Q) = wQ - p`. Suppliers have a cost function `c(Q, θ) = θQ²/2`, where `θ` is a private cost parameter drawn from a distribution `F(θ)` on `[θ_min, θ_max]`.\n\n### Data / Model Specification\n\nThe paper establishes that a multi-attribute scoring auction is strategically equivalent to a standard single-object Independent Private Value (IPV) auction. The key concept is the **pseudotype**, `k(θ)`, which is the maximum level of apparent social surplus a supplier can generate. It is defined as:\n\n  \nk(θ) = max_Q {wQ - c(Q, θ)}\n \n\nThis pseudotype `k(θ)` plays the same strategic role as a private value in a standard auction. The supplier's expected payoff is `π = x * k(θ) - s`, where `x` is the win probability and `s` is the expected score payment.\n\n### Question\n\nBased on the model, which of the following statements are correct?\n\nSelect all that apply.", "Answer": ["A", "B", "D"], "qid": "133", "question": "### Background\n\nA buyer is procuring a good with a single quality attribute `Q` and uses a linear scoring rule `S(p, Q) = wQ - p`. Suppliers have a cost function `c(Q, θ) = θQ²/2`, where `θ` is a private cost parameter drawn from a distribution `F(θ)` on `[θ_min, θ_max]`.\n\n### Data / Model Specification\n\nThe paper establishes that a multi-attribute scoring auction is strategically equivalent to a standard single-object Independent Private Value (IPV) auction. The key concept is the **pseudotype**, `k(θ)`, which is the maximum level of apparent social surplus a supplier can generate. It is defined as:\n\n  \nk(θ) = max_Q {wQ - c(Q, θ)}\n \n\nThis pseudotype `k(θ)` plays the same strategic role as a private value in a standard auction. The supplier's expected payoff is `π = x * k(θ) - s`, where `x` is the win probability and `s` is the expected score payment.\n\n### Question\n\nBased on the model, which of the following statements are correct?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA buyer is procuring a good with a...", "model_answer": "This is a detailed response to the question about ### Background\n\nA buyer is procuring a good with a...", "error": null}}
{"ID": 134, "Question": "### Background\n\nA supplier competes in a first-price menu auction, offering a menu of contracts to a buyer with a privately known taste parameter `t`. The supplier's problem is modeled using optimal control, where `U(t)` (utility offered to buyer `t`) is the state variable and `Q(t)` (quality offered to buyer `t`) is the control variable.\n\n### Data / Model Specification\n\nThe first-order condition (FOC) from the Hamiltonian with respect to the control variable `Q` is:\n\n  \n(v_Q(Q,t) - c_Q(Q,θ)) * Pr(U,t)h(t) + λ(t,θ)v_{Qt}(Q,t) = 0 \n \n\nWhere:\n- `v(Q,t)` and `c(Q,θ)` are the buyer's valuation and supplier's cost.\n- `Pr(U,t)` is the probability of winning.\n- `h(t)` is the density of buyer types.\n- `λ(t,θ)` is the co-state variable (Lagrange multiplier) on the buyer's incentive compatibility (IC) constraint.\n- Production is efficient when `v_Q = c_Q`.\n- Assume `Pr(U,t)h(t) > 0` and the sorting condition `v_{Qt} > 0` holds.\n\n### Question\n\nAccording to the model of the first-price menu auction, which of the following statements are valid conclusions?\n\nSelect all that apply.", "Answer": ["A", "C", "D"], "qid": "134", "question": "### Background\n\nA supplier competes in a first-price menu auction, offering a menu of contracts to a buyer with a privately known taste parameter `t`. The supplier's problem is modeled using optimal control, where `U(t)` (utility offered to buyer `t`) is the state variable and `Q(t)` (quality offered to buyer `t`) is the control variable.\n\n### Data / Model Specification\n\nThe first-order condition (FOC) from the Hamiltonian with respect to the control variable `Q` is:\n\n  \n(v_Q(Q,t) - c_Q(Q,θ)) * Pr(U,t)h(t) + λ(t,θ)v_{Qt}(Q,t) = 0 \n \n\nWhere:\n- `v(Q,t)` and `c(Q,θ)` are the buyer's valuation and supplier's cost.\n- `Pr(U,t)` is the probability of winning.\n- `h(t)` is the density of buyer types.\n- `λ(t,θ)` is the co-state variable (Lagrange multiplier) on the buyer's incentive compatibility (IC) constraint.\n- Production is efficient when `v_Q = c_Q`.\n- Assume `Pr(U,t)h(t) > 0` and the sorting condition `v_{Qt} > 0` holds.\n\n### Question\n\nAccording to the model of the first-price menu auction, which of the following statements are valid conclusions?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA supplier competes in a first-pri...", "model_answer": "This is a detailed response to the question about ### Background\n\nA supplier competes in a first-pri...", "error": null}}
{"ID": 135, "Question": "### Background\n\nA buyer is choosing between two procurement mechanisms: a rigid price-only auction with a fixed minimum quality standard `Q_min`, and a flexible scoring auction.\n\n### Data / Model Specification\n\nBy the Revenue Equivalence Theorem, the buyer's expected utility in a symmetric procurement auction is the expected total surplus generated by the second-best supplier.\n\n- **Surplus in Price-Only Auction:** For a supplier of type `θ`, the surplus is fixed at `S_PO(θ) = v(Q_min) - c(Q_min, θ)`.\n- **Surplus in Truthful Scoring Auction:** For a supplier of type `θ`, the surplus is `S_SA(θ) = max_Q {v(Q) - c(Q, θ)}`.\n\n### Question\n\nWhich of the following statements accurately describe the relationship between these two auction formats?\n\nSelect all that apply.", "Answer": ["A", "B", "D"], "qid": "135", "question": "### Background\n\nA buyer is choosing between two procurement mechanisms: a rigid price-only auction with a fixed minimum quality standard `Q_min`, and a flexible scoring auction.\n\n### Data / Model Specification\n\nBy the Revenue Equivalence Theorem, the buyer's expected utility in a symmetric procurement auction is the expected total surplus generated by the second-best supplier.\n\n- **Surplus in Price-Only Auction:** For a supplier of type `θ`, the surplus is fixed at `S_PO(θ) = v(Q_min) - c(Q_min, θ)`.\n- **Surplus in Truthful Scoring Auction:** For a supplier of type `θ`, the surplus is `S_SA(θ) = max_Q {v(Q) - c(Q, θ)}`.\n\n### Question\n\nWhich of the following statements accurately describe the relationship between these two auction formats?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA buyer is choosing between two pr...", "model_answer": "This is a detailed response to the question about ### Background\n\nA buyer is choosing between two pr...", "error": null}}
{"ID": 136, "Question": "### Background\n\nA social planner designs a 'regulatory charter' to govern a long-term investment project undertaken by a profit-maximizing firm. The project spans two periods. Decision-making is delegated to two different short-lived, 'myopic' regulators: R1 in period 1 and R2 in period 2. Each regulator seeks to maximize the welfare of consumers only during their own tenure. The firm is long-lived and maximizes the discounted sum of its profits.\n\nThe core friction arises because key state variables—a technology parameter `θ` that affects benefits in both periods, and an idiosyncratic first-period shock `ε`—are observable to the regulators and the firm, but are not verifiable by a third party (e.g., a court). Therefore, payments in the charter can only be conditioned on observable actions: whether investment occurs and whether it is later adopted.\n\n### Data / Model Specification\n\n**Period 1: Investment Decision**\nAn investment of fixed cost `I` can be made. First-period consumer benefits from investment are `b₁(θ, ε) = αθ + δε`, where `α, δ > 0`. The status quo benefit without investment is `b₀`. It is assumed that `b₁(θ, ε) < I` for all `(θ, ε)`, meaning first-period benefits alone never justify the investment cost.\n\n1.  **Regulator R1's Rule:** R1, caring only about period 1 consumers, authorizes investment if and only if their net welfare is non-negative compared to the status quo. Let `P₁¹` and `P₁⁰` be the payments from period 1 consumers to the firm with and without investment, respectively. R1's rule is:\n      \nb₁(\\theta, \\varepsilon) - P₁¹ \\ge b₀ - P₁⁰ \\quad \\text{(Eq. (1))}\n     \n\n2.  **The Firm's Rule:** The firm's decision depends on expected discounted profits. A higher `θ` signals a higher probability of large second-period consumer benefits, `b₂`, drawn from a distribution `F(b₂|θ)` where `∂F/∂θ < 0`. If the project is adopted in period 2, the firm receives a private, non-transferable benefit `π₂(θ)`, where `π₂'(θ) > 0`. The firm's decision to invest is independent of `ε`.\n\n3.  **The Optimal Charter:** The paper's Proposition 1 characterizes the optimal (second-best) regulatory charter. A key property of the payments set by the planner is the 'no expropriation' condition, which ensures the firm breaks even if the project is undertaken but ultimately abandoned:\n      \nP_{1}^{1} - I + \\beta P_{2}^{10} = 0 \\quad \\text{(Eq. (2))}\n     \n    `P₂¹⁰` is the payment upon abandonment and `β` is the discount factor.\n\n### Question\n\nBased on the model and the properties of the optimal regulatory charter, select all statements that are TRUE.", "Answer": ["A", "B"], "qid": "136", "question": "### Background\n\nA social planner designs a 'regulatory charter' to govern a long-term investment project undertaken by a profit-maximizing firm. The project spans two periods. Decision-making is delegated to two different short-lived, 'myopic' regulators: R1 in period 1 and R2 in period 2. Each regulator seeks to maximize the welfare of consumers only during their own tenure. The firm is long-lived and maximizes the discounted sum of its profits.\n\nThe core friction arises because key state variables—a technology parameter `θ` that affects benefits in both periods, and an idiosyncratic first-period shock `ε`—are observable to the regulators and the firm, but are not verifiable by a third party (e.g., a court). Therefore, payments in the charter can only be conditioned on observable actions: whether investment occurs and whether it is later adopted.\n\n### Data / Model Specification\n\n**Period 1: Investment Decision**\nAn investment of fixed cost `I` can be made. First-period consumer benefits from investment are `b₁(θ, ε) = αθ + δε`, where `α, δ > 0`. The status quo benefit without investment is `b₀`. It is assumed that `b₁(θ, ε) < I` for all `(θ, ε)`, meaning first-period benefits alone never justify the investment cost.\n\n1.  **Regulator R1's Rule:** R1, caring only about period 1 consumers, authorizes investment if and only if their net welfare is non-negative compared to the status quo. Let `P₁¹` and `P₁⁰` be the payments from period 1 consumers to the firm with and without investment, respectively. R1's rule is:\n      \nb₁(\\theta, \\varepsilon) - P₁¹ \\ge b₀ - P₁⁰ \\quad \\text{(Eq. (1))}\n     \n\n2.  **The Firm's Rule:** The firm's decision depends on expected discounted profits. A higher `θ` signals a higher probability of large second-period consumer benefits, `b₂`, drawn from a distribution `F(b₂|θ)` where `∂F/∂θ < 0`. If the project is adopted in period 2, the firm receives a private, non-transferable benefit `π₂(θ)`, where `π₂'(θ) > 0`. The firm's decision to invest is independent of `ε`.\n\n3.  **The Optimal Charter:** The paper's Proposition 1 characterizes the optimal (second-best) regulatory charter. A key property of the payments set by the planner is the 'no expropriation' condition, which ensures the firm breaks even if the project is undertaken but ultimately abandoned:\n      \nP_{1}^{1} - I + \\beta P_{2}^{10} = 0 \\quad \\text{(Eq. (2))}\n     \n    `P₂¹⁰` is the payment upon abandonment and `β` is the discount factor.\n\n### Question\n\nBased on the model and the properties of the optimal regulatory charter, select all statements that are TRUE.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA social planner designs a 'regula...", "model_answer": "This is a detailed response to the question about ### Background\n\nA social planner designs a 'regula...", "error": null}}
{"ID": 137, "Question": "### Background\n\nThis problem analyzes the second-period 'adoption' decision in a two-period regulatory model. In period 1, an investment was made. In period 2, a new, myopic regulator (R2) and the long-lived firm jointly decide whether to adopt the completed project. R2's sole objective is to maximize the welfare of period 2 consumers. The firm's objective is to maximize its own profit. Key information, particularly the technology parameter `θ` that determined the investment's quality, is not verifiable by third parties, so payments cannot be contingent on it.\n\n### Data / Model Specification\n\n1.  **R2's Adoption Rule:** R2 observes the realized benefit to consumers from adoption, `b₂`. If the project is abandoned, consumers receive a status quo benefit `b₀`. Let `P₂¹¹` and `P₂¹⁰` be the payments consumers must make to the firm if the project is adopted or abandoned, respectively. R2 approves adoption if and only if:\n      \nb_{2} - P_{2}^{11} \\ge b_{0} - P_{2}^{10} \\quad \\text{(Eq. (1))}\n     \n\n2.  **Socially Efficient Rule:** The total social surplus from adoption is the sum of consumer benefits (`b₂`) and the firm's private, non-transferable benefit (`π₂(θ)`). The surplus from abandonment is `b₀`. The first-best (socially efficient) rule is to adopt if and only if:\n      \nb_{2} + \\pi_{2}(\\theta) \\ge b_{0} \\quad \\text{(Eq. (2))}\n     \n    The firm's private benefit `π₂(θ)` is strictly increasing in `θ` (`π₂'(θ) > 0`).\n\n3.  **Optimal Charter Payment Rule:** The paper's Proposition 1 shows that the optimal regulatory charter, designed by a social planner to maximize total expected consumer welfare, sets period 2 payments that satisfy the following relationship:\n      \nP_{2}^{10} = P_{2}^{11} + \\pi_{2}(\\hat{\\theta}^{F}) \\quad \\text{(Eq. (3))}\n     \n    Here, `hat(θ) F` is the minimum value of the technology parameter `θ` for which the firm is willing to invest in period 1.\n\n### Question\n\nUnder the optimal regulatory charter described, which of the following statements accurately characterize the second-period adoption decision and its efficiency properties?", "Answer": ["B", "C"], "qid": "137", "question": "### Background\n\nThis problem analyzes the second-period 'adoption' decision in a two-period regulatory model. In period 1, an investment was made. In period 2, a new, myopic regulator (R2) and the long-lived firm jointly decide whether to adopt the completed project. R2's sole objective is to maximize the welfare of period 2 consumers. The firm's objective is to maximize its own profit. Key information, particularly the technology parameter `θ` that determined the investment's quality, is not verifiable by third parties, so payments cannot be contingent on it.\n\n### Data / Model Specification\n\n1.  **R2's Adoption Rule:** R2 observes the realized benefit to consumers from adoption, `b₂`. If the project is abandoned, consumers receive a status quo benefit `b₀`. Let `P₂¹¹` and `P₂¹⁰` be the payments consumers must make to the firm if the project is adopted or abandoned, respectively. R2 approves adoption if and only if:\n      \nb_{2} - P_{2}^{11} \\ge b_{0} - P_{2}^{10} \\quad \\text{(Eq. (1))}\n     \n\n2.  **Socially Efficient Rule:** The total social surplus from adoption is the sum of consumer benefits (`b₂`) and the firm's private, non-transferable benefit (`π₂(θ)`). The surplus from abandonment is `b₀`. The first-best (socially efficient) rule is to adopt if and only if:\n      \nb_{2} + \\pi_{2}(\\theta) \\ge b_{0} \\quad \\text{(Eq. (2))}\n     \n    The firm's private benefit `π₂(θ)` is strictly increasing in `θ` (`π₂'(θ) > 0`).\n\n3.  **Optimal Charter Payment Rule:** The paper's Proposition 1 shows that the optimal regulatory charter, designed by a social planner to maximize total expected consumer welfare, sets period 2 payments that satisfy the following relationship:\n      \nP_{2}^{10} = P_{2}^{11} + \\pi_{2}(\\hat{\\theta}^{F}) \\quad \\text{(Eq. (3))}\n     \n    Here, `hat(θ) F` is the minimum value of the technology parameter `θ` for which the firm is willing to invest in period 1.\n\n### Question\n\nUnder the optimal regulatory charter described, which of the following statements accurately characterize the second-period adoption decision and its efficiency properties?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the second-p...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the second-p...", "error": null}}
{"ID": 138, "Question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the technical conditions required for the tuning parameter `κₙ`.\n\n---\n\n### The Question\n\nFor the paper's test to be both asymptotically valid (i.e., have correct size) and non-conservative (i.e., have good power), the tuning parameter `κₙ` must satisfy two competing conditions relative to the convergence rate `rₙ`:\n\n(i) `κₙ/rₙ → 0`\n(ii) `κₙ → ∞`\n\nWhich of the following statements correctly describe the roles of these conditions? (Select all that apply)", "Answer": ["B", "C"], "qid": "138", "question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the technical conditions required for the tuning parameter `κₙ`.\n\n---\n\n### The Question\n\nFor the paper's test to be both asymptotically valid (i.e., have correct size) and non-conservative (i.e., have good power), the tuning parameter `κₙ` must satisfy two competing conditions relative to the convergence rate `rₙ`:\n\n(i) `κₙ/rₙ → 0`\n(ii) `κₙ → ∞`\n\nWhich of the following statements correctly describe the roles of these conditions? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 139, "Question": "### Background\n\n**Research Question.** How can individual preferences over random allocation procedures be modeled to incorporate both self-interest and a concern for procedural fairness, and how can such a model be empirically tested?\n\n**Setting.** The setting is a choice over lotteries (`p`) in an `n`-person context where one indivisible good is to be allocated. A lottery `p` is a vector of probabilities in the `n-1` dimensional simplex `P`. An individual's preferences are governed by both a self-interest motive and an intrinsic conception of fairness.\n\n### Data / Model Specification\n\nTwo competing models of preference are considered:\n\n**Model 1 (Karni-Safra Fairness).** An individual's preference relation `≻` is represented by a utility function `V` that depends on a self-interest component `κ·p` and a fairness component `σ(p)`. A specific functional form is the additively separable case:\n  \nV(p) = h(κ·p) + σ(p) \nEq. (1)\n \nwhere `h` is a strictly increasing function and `σ(p)` is a strictly quasi-concave function representing the fairness of the allocation procedure `p`.\n\n**Model 2 (Neoclassical Self-Interest).** The agent's utility depends only on their own probability of winning, `p_A`. The utility function is:\n  \nU(p) = p_A \nEq. (2)\n \nIn the experiment, subjects choose a lottery from a set represented by a line segment, `p(λ) = λp¹ + (1-λ)p²` for `λ ∈ [0,1]`, where `p¹` and `p²` are endpoint lotteries.\n\n### Question\n\nBased on the theoretical models and the experimental design described, select ALL statements that are logically correct.", "Answer": ["A", "D"], "qid": "139", "question": "### Background\n\n**Research Question.** How can individual preferences over random allocation procedures be modeled to incorporate both self-interest and a concern for procedural fairness, and how can such a model be empirically tested?\n\n**Setting.** The setting is a choice over lotteries (`p`) in an `n`-person context where one indivisible good is to be allocated. A lottery `p` is a vector of probabilities in the `n-1` dimensional simplex `P`. An individual's preferences are governed by both a self-interest motive and an intrinsic conception of fairness.\n\n### Data / Model Specification\n\nTwo competing models of preference are considered:\n\n**Model 1 (Karni-Safra Fairness).** An individual's preference relation `≻` is represented by a utility function `V` that depends on a self-interest component `κ·p` and a fairness component `σ(p)`. A specific functional form is the additively separable case:\n  \nV(p) = h(κ·p) + σ(p) \nEq. (1)\n \nwhere `h` is a strictly increasing function and `σ(p)` is a strictly quasi-concave function representing the fairness of the allocation procedure `p`.\n\n**Model 2 (Neoclassical Self-Interest).** The agent's utility depends only on their own probability of winning, `p_A`. The utility function is:\n  \nU(p) = p_A \nEq. (2)\n \nIn the experiment, subjects choose a lottery from a set represented by a line segment, `p(λ) = λp¹ + (1-λ)p²` for `λ ∈ [0,1]`, where `p¹` and `p²` are endpoint lotteries.\n\n### Question\n\nBased on the theoretical models and the experimental design described, select ALL statements that are logically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** How can ind...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** How can ind...", "error": null}}
{"ID": 140, "Question": "### Background\n\n**Research Question.** This problem evaluates the finite-sample performance of the proposed bootstrap cointegration test relative to alternative methods, using Monte Carlo simulations.\n\n**Setting.** The simulation DGP is a 4-dimensional VAR(2) process where the true cointegration rank is $r_0=1$. The performance of four tests is compared at a nominal 5% significance level: the standard asymptotic Johansen test ($Q_{r,T}$), Swensen's bootstrap test ($Q_{r,T}^{*SW}$), and the paper's proposed bootstrap test ($Q_{r,T}^*$). Performance is assessed based on empirical rejection frequencies (ERFs) under the null (size) and the alternative (power).\n\n### Data / Model Specification\n\nThe following table summarizes key results from the paper's Table I, showing Empirical Rejection Frequencies (ERFs) for tests conducted at a 5% nominal significance level. The true data generating process has one cointegrating vector ($r_0=1$).\n\n**Table 1: Empirical Rejection Frequencies (ERF) of 5% Nominal Level Tests**\n\n| T | Test | Parameter $\\delta$ | ERF for $H(1)$ (Size) | ERF for $H(0)$ (Power) |\n|:---:|:---|:---:|:---:|:---:|\n| 50 | Asymptotic ($Q_{1,T}$) | 0.0 | 45.0% | 99.9% |\n| 50 | Swensen ($Q_{1,T}^{*SW}$) | 0.0 | 19.0% | 99.0% |\n| 50 | Proposed ($Q_{1,T}^{*}$) | 0.0 | 5.1% | 97.5% |\n| 200 | Asymptotic ($Q_{1,T}$) | 0.0 | 13.0% | 100% |\n| 200 | Swensen ($Q_{1,T}^{*SW}$) | 0.0 | 8.0% | 100% |\n| 200 | Proposed ($Q_{1,T}^{*}$) | 0.0 | 5.2% | 100% |\n\n---\n\nBased on the data in Table 1 for the $T=50$ sample size, select all of the following statements that are correct interpretations or valid conclusions.", "Answer": ["B", "C", "D"], "qid": "140", "question": "### Background\n\n**Research Question.** This problem evaluates the finite-sample performance of the proposed bootstrap cointegration test relative to alternative methods, using Monte Carlo simulations.\n\n**Setting.** The simulation DGP is a 4-dimensional VAR(2) process where the true cointegration rank is $r_0=1$. The performance of four tests is compared at a nominal 5% significance level: the standard asymptotic Johansen test ($Q_{r,T}$), Swensen's bootstrap test ($Q_{r,T}^{*SW}$), and the paper's proposed bootstrap test ($Q_{r,T}^*$). Performance is assessed based on empirical rejection frequencies (ERFs) under the null (size) and the alternative (power).\n\n### Data / Model Specification\n\nThe following table summarizes key results from the paper's Table I, showing Empirical Rejection Frequencies (ERFs) for tests conducted at a 5% nominal significance level. The true data generating process has one cointegrating vector ($r_0=1$).\n\n**Table 1: Empirical Rejection Frequencies (ERF) of 5% Nominal Level Tests**\n\n| T | Test | Parameter $\\delta$ | ERF for $H(1)$ (Size) | ERF for $H(0)$ (Power) |\n|:---:|:---|:---:|:---:|:---:|\n| 50 | Asymptotic ($Q_{1,T}$) | 0.0 | 45.0% | 99.9% |\n| 50 | Swensen ($Q_{1,T}^{*SW}$) | 0.0 | 19.0% | 99.0% |\n| 50 | Proposed ($Q_{1,T}^{*}$) | 0.0 | 5.1% | 97.5% |\n| 200 | Asymptotic ($Q_{1,T}$) | 0.0 | 13.0% | 100% |\n| 200 | Swensen ($Q_{1,T}^{*SW}$) | 0.0 | 8.0% | 100% |\n| 200 | Proposed ($Q_{1,T}^{*}$) | 0.0 | 5.2% | 100% |\n\n---\n\nBased on the data in Table 1 for the $T=50$ sample size, select all of the following statements that are correct interpretations or valid conclusions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 141, "Question": "### Background\n\n**Research Question.** This problem investigates the proper construction of a bootstrap data generating process (DGP) for testing cointegration rank, focusing on how different estimation strategies for the model parameters can affect the validity of the bootstrap.\n\n**Setting.** We compare two alternative bootstrap recursions for generating simulated data under the null hypothesis of cointegration rank $r$, denoted $H(r)$. The key issue is whether the parameters governing short-run dynamics should be estimated under the null hypothesis $H(r)$ or under the alternative $H(p)$. A bootstrap DGP is valid only if it satisfies the \"I(1,r) conditions,\" meaning its characteristic polynomial has exactly $p-r$ unit roots, with all other roots lying outside the unit circle.\n\n### Data / Model Specification\n\nThe paper proposes the following bootstrap recursion, where all parameters are estimated under the null hypothesis $H(r)$:\n  \n\\Delta X_{r,t}^{*}=\\hat{\\alpha}^{(r)}\\hat{\\beta}^{(r)\\prime}X_{r,t-1}^{*}+\\sum_{i=1}^{k-1}\\widehat{\\Gamma}_{i}^{(r)}\\Delta X_{r,t-i}^{*}+\\hat{\\alpha}^{(r)}\\hat{\\rho}^{(r)\\prime}D_{t}+\\hat{\\phi}^{(r)}d_{t}+\\varepsilon_{r,t}^{*} \\quad \\text{(Eq. (1))}\n \nThis is contrasted with Swensen's method, which mixes restricted long-run estimates with unrestricted short-run estimates:\n  \n\\Delta X_{r,t}^{*}=\\hat{\\alpha}^{(r)}\\hat{\\beta}^{(r)\\prime}X_{r,t-1}^{*}+\\sum_{i=1}^{k-1}\\widehat{\\Gamma}_{i}^{(p)}\\Delta X_{r,t-i}^{*}+\\hat{\\alpha}^{(r)}\\hat{\\rho}^{(r)\\prime}D_{t}+\\hat{\\phi}^{(p)}d_{t}+\\varepsilon_{p,t}^{*} \\quad \\text{(Eq. (2))}\n \nwhere `$\\varepsilon_{p,t}^*$` are resampled from the unrestricted model's residuals.\n\n---\n\nConsider the critical case where the null hypothesis is false (the true rank $r_0$ is greater than the tested rank $r$). According to the paper's theoretical analysis, which of the following statements correctly describe the properties and risks of these two bootstrap methods?", "Answer": ["A", "B", "C"], "qid": "141", "question": "### Background\n\n**Research Question.** This problem investigates the proper construction of a bootstrap data generating process (DGP) for testing cointegration rank, focusing on how different estimation strategies for the model parameters can affect the validity of the bootstrap.\n\n**Setting.** We compare two alternative bootstrap recursions for generating simulated data under the null hypothesis of cointegration rank $r$, denoted $H(r)$. The key issue is whether the parameters governing short-run dynamics should be estimated under the null hypothesis $H(r)$ or under the alternative $H(p)$. A bootstrap DGP is valid only if it satisfies the \"I(1,r) conditions,\" meaning its characteristic polynomial has exactly $p-r$ unit roots, with all other roots lying outside the unit circle.\n\n### Data / Model Specification\n\nThe paper proposes the following bootstrap recursion, where all parameters are estimated under the null hypothesis $H(r)$:\n  \n\\Delta X_{r,t}^{*}=\\hat{\\alpha}^{(r)}\\hat{\\beta}^{(r)\\prime}X_{r,t-1}^{*}+\\sum_{i=1}^{k-1}\\widehat{\\Gamma}_{i}^{(r)}\\Delta X_{r,t-i}^{*}+\\hat{\\alpha}^{(r)}\\hat{\\rho}^{(r)\\prime}D_{t}+\\hat{\\phi}^{(r)}d_{t}+\\varepsilon_{r,t}^{*} \\quad \\text{(Eq. (1))}\n \nThis is contrasted with Swensen's method, which mixes restricted long-run estimates with unrestricted short-run estimates:\n  \n\\Delta X_{r,t}^{*}=\\hat{\\alpha}^{(r)}\\hat{\\beta}^{(r)\\prime}X_{r,t-1}^{*}+\\sum_{i=1}^{k-1}\\widehat{\\Gamma}_{i}^{(p)}\\Delta X_{r,t-i}^{*}+\\hat{\\alpha}^{(r)}\\hat{\\rho}^{(r)\\prime}D_{t}+\\hat{\\phi}^{(p)}d_{t}+\\varepsilon_{p,t}^{*} \\quad \\text{(Eq. (2))}\n \nwhere `$\\varepsilon_{p,t}^*$` are resampled from the unrestricted model's residuals.\n\n---\n\nConsider the critical case where the null hypothesis is false (the true rank $r_0$ is greater than the tested rank $r$). According to the paper's theoretical analysis, which of the following statements correctly describe the properties and risks of these two bootstrap methods?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 142, "Question": "### Background\n\n**Research Question.** This problem outlines the theoretical argument for the asymptotic validity of the bootstrap procedure proposed in Algorithm 1, particularly its robustness when testing a null hypothesis $H(r)$ that may be false (i.e., the true rank $r_0 > r$).\n\n**Setting.** The proof of validity rests on a logical chain of three key results: Lemma 1, Proposition 1, and Proposition 2. This chain establishes that even when estimating under a misspecified rank, the resulting bootstrap DGP is well-behaved and yields a test statistic with the correct limiting distribution.\n\n### Data / Model Specification\n\nThe asymptotic validity of the proposed bootstrap procedure is established through the following logical sequence:\n1.  **Lemma 1:** For any tested rank $r \\le r_0$, the Quasi-Maximum Likelihood Estimator (QMLE) $\\hat{\\theta}^{(r)}$ converges in probability to a set of pseudo-true parameters $\\theta_0^{(r)}$, and these pseudo-true parameters satisfy the I(1,r) conditions.\n2.  **Proposition 1:** As a consequence of Lemma 1, the bootstrap sample $\\{X_{r,t}^*\\}$ generated by Algorithm 1 is asymptotically I(1) with cointegration rank $r$.\n3.  **Proposition 2:** Because the bootstrap sample correctly mimics an I(1,r) process, the bootstrap statistic $Q_{r,T}^*$ converges in distribution to the same limit as the original Johansen statistic under the null $H(r)$.\n\n---\n\nBased on this logical chain, which of the following statements are **VALID** inferences or conclusions?", "Answer": ["A", "C", "D"], "qid": "142", "question": "### Background\n\n**Research Question.** This problem outlines the theoretical argument for the asymptotic validity of the bootstrap procedure proposed in Algorithm 1, particularly its robustness when testing a null hypothesis $H(r)$ that may be false (i.e., the true rank $r_0 > r$).\n\n**Setting.** The proof of validity rests on a logical chain of three key results: Lemma 1, Proposition 1, and Proposition 2. This chain establishes that even when estimating under a misspecified rank, the resulting bootstrap DGP is well-behaved and yields a test statistic with the correct limiting distribution.\n\n### Data / Model Specification\n\nThe asymptotic validity of the proposed bootstrap procedure is established through the following logical sequence:\n1.  **Lemma 1:** For any tested rank $r \\le r_0$, the Quasi-Maximum Likelihood Estimator (QMLE) $\\hat{\\theta}^{(r)}$ converges in probability to a set of pseudo-true parameters $\\theta_0^{(r)}$, and these pseudo-true parameters satisfy the I(1,r) conditions.\n2.  **Proposition 1:** As a consequence of Lemma 1, the bootstrap sample $\\{X_{r,t}^*\\}$ generated by Algorithm 1 is asymptotically I(1) with cointegration rank $r$.\n3.  **Proposition 2:** Because the bootstrap sample correctly mimics an I(1,r) process, the bootstrap statistic $Q_{r,T}^*$ converges in distribution to the same limit as the original Johansen statistic under the null $H(r)$.\n\n---\n\nBased on this logical chain, which of the following statements are **VALID** inferences or conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 143, "Question": "### Background\n\n**Research Question.** This problem examines the paper's main result: that under general conditions, markets are \"informative,\" meaning uninformed traders can eventually learn the true structure of the economy from prices. The analysis hinges on showing that the conditions for non-identification—where different underlying states of the world produce statistically identical price data—are non-generic.\n\n**Setting.** The analysis focuses on the \"limit economy,\" where uninformed traders' beliefs have converged to a steady state, `β̄`. In this limit, the sequence of market prices becomes an i.i.d. process. The statistical properties of this process determine whether learning is possible. The proof strategy considers simplified distributions `μ_θ` with a common finite support of size `k` to make the argument precise.\n\n### Data / Model Specification\n\nIn the limit economy, the relationship between the underlying state distribution `μ_θ` and the observed price distribution `υ_θ(β̄)` is given by the pushforward measure:\n  \n\\upsilon_\\theta(\\bar{\\beta}) = \\mu_\\theta \\circ f^{-1}(\\bar{\\beta}, \\cdot)\n\\quad \\quad \\text{(Eq. (1))}\n \nwhere `f` is a selection from the equilibrium price correspondence. A market is informative if and only if these induced price distributions are distinct for each `θ ∈ Θ`.\n\nA necessary condition for two parameters `θ` and `θ'` to be indistinguishable is that the support of their induced price distributions can be made identical. The paper's Lemma 4 shows this non-identification scenario is a non-generic, knife-edge case if `k`, the number of distinct states in the support of `μ_θ`, is sufficiently large. Specifically, the condition is:\n  \nk > \\frac{(I - I_1)(|\\Theta| - 1)}{L - 1} \\quad \\quad \\text{(Inequality (2))}\n \nwhere `L` is the number of goods, `I` is the total number of trader types, `I_1` is the number of informed types, and `|Θ|` is the number of possible underlying states.\n\n### Question\n\nAccording to the logic of Inequality (2), select all of the following changes that would make learning *more difficult* for uninformed traders, thus requiring a larger number of distinct signals (`k`) to guarantee that the market is informative.", "Answer": ["A", "D"], "qid": "143", "question": "### Background\n\n**Research Question.** This problem examines the paper's main result: that under general conditions, markets are \"informative,\" meaning uninformed traders can eventually learn the true structure of the economy from prices. The analysis hinges on showing that the conditions for non-identification—where different underlying states of the world produce statistically identical price data—are non-generic.\n\n**Setting.** The analysis focuses on the \"limit economy,\" where uninformed traders' beliefs have converged to a steady state, `β̄`. In this limit, the sequence of market prices becomes an i.i.d. process. The statistical properties of this process determine whether learning is possible. The proof strategy considers simplified distributions `μ_θ` with a common finite support of size `k` to make the argument precise.\n\n### Data / Model Specification\n\nIn the limit economy, the relationship between the underlying state distribution `μ_θ` and the observed price distribution `υ_θ(β̄)` is given by the pushforward measure:\n  \n\\upsilon_\\theta(\\bar{\\beta}) = \\mu_\\theta \\circ f^{-1}(\\bar{\\beta}, \\cdot)\n\\quad \\quad \\text{(Eq. (1))}\n \nwhere `f` is a selection from the equilibrium price correspondence. A market is informative if and only if these induced price distributions are distinct for each `θ ∈ Θ`.\n\nA necessary condition for two parameters `θ` and `θ'` to be indistinguishable is that the support of their induced price distributions can be made identical. The paper's Lemma 4 shows this non-identification scenario is a non-generic, knife-edge case if `k`, the number of distinct states in the support of `μ_θ`, is sufficiently large. Specifically, the condition is:\n  \nk > \\frac{(I - I_1)(|\\Theta| - 1)}{L - 1} \\quad \\quad \\text{(Inequality (2))}\n \nwhere `L` is the number of goods, `I` is the total number of trader types, `I_1` is the number of informed types, and `|Θ|` is the number of possible underlying states.\n\n### Question\n\nAccording to the logic of Inequality (2), select all of the following changes that would make learning *more difficult* for uninformed traders, thus requiring a larger number of distinct signals (`k`) to guarantee that the market is informative.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 144, "Question": "### Background\n\n**Research Question.** This problem investigates the dynamic properties of the market process, focusing on how traders' beliefs evolve and how this evolution affects market prices. The analysis requires establishing that the system is stable in the long run, which provides the foundation for analyzing whether learning is ultimately successful.\n\n**Setting.** Uninformed traders begin with priors over a finite set `Θ` of possible data-generating processes. In each period, they observe the history of market prices, contained in the information set (sigma-field) `M̃_{t-1}`, and use it to update their beliefs to a posterior `β_it`. This sequence of beliefs is a stochastic process.\n\n### Data / Model Specification\n\nThe paper establishes three key results regarding the dynamics of the system:\n\n1.  **Definition of Learning:** The stochastic process of market data `{p_t}` is **informative** if for each uninformed trader `i`, `lim_{t→∞} β_it(θ) = 1` almost surely when the true parameter is `θ`.\n\n2.  **Belief Convergence (Theorem 1):** The sequence of posterior beliefs `{β_it}` is a martingale. By the Martingale Convergence Theorem, it converges almost surely to a limit random variable `β_i∞`.\n\n3.  **Price Convergence (Lemma 1):** As a consequence of belief convergence and the upper hemi-continuity (u.h.c.) of the equilibrium price correspondence `F(β, s)`, the observed market prices `p_t` almost surely approach the set of equilibrium prices of the \"limit economy.\" That is, `inf{d(p_t, y_t) : y_t ∈ F(β_∞, s_t)}` converges to 0 almost surely, where `d(·,·)` is a distance metric.\n\n### Question\n\nBased on the paper's dynamic analysis, select all of the following statements that are correct.", "Answer": ["B", "C"], "qid": "144", "question": "### Background\n\n**Research Question.** This problem investigates the dynamic properties of the market process, focusing on how traders' beliefs evolve and how this evolution affects market prices. The analysis requires establishing that the system is stable in the long run, which provides the foundation for analyzing whether learning is ultimately successful.\n\n**Setting.** Uninformed traders begin with priors over a finite set `Θ` of possible data-generating processes. In each period, they observe the history of market prices, contained in the information set (sigma-field) `M̃_{t-1}`, and use it to update their beliefs to a posterior `β_it`. This sequence of beliefs is a stochastic process.\n\n### Data / Model Specification\n\nThe paper establishes three key results regarding the dynamics of the system:\n\n1.  **Definition of Learning:** The stochastic process of market data `{p_t}` is **informative** if for each uninformed trader `i`, `lim_{t→∞} β_it(θ) = 1` almost surely when the true parameter is `θ`.\n\n2.  **Belief Convergence (Theorem 1):** The sequence of posterior beliefs `{β_it}` is a martingale. By the Martingale Convergence Theorem, it converges almost surely to a limit random variable `β_i∞`.\n\n3.  **Price Convergence (Lemma 1):** As a consequence of belief convergence and the upper hemi-continuity (u.h.c.) of the equilibrium price correspondence `F(β, s)`, the observed market prices `p_t` almost surely approach the set of equilibrium prices of the \"limit economy.\" That is, `inf{d(p_t, y_t) : y_t ∈ F(β_∞, s_t)}` converges to 0 almost surely, where `d(·,·)` is a distance metric.\n\n### Question\n\nBased on the paper's dynamic analysis, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 145, "Question": "### Background\n\n**Research Question.** This problem examines the microfoundations of the model, connecting agents' foundational beliefs and assumptions to their behavior and, ultimately, to market-level outcomes. It explores how subjective beliefs translate into objective market prices.\n\n**Setting.** The model features a sequence of temporary equilibria in an exchange economy. In each period, there are informed traders who know the payoff-relevant state `s_t` and uninformed traders who must learn about the true data-generating process, indexed by `θ` from a finite set `Θ`. Uninformed traders use past market data to form posterior beliefs `β_it`.\n\n### Data / Model Specification\n\n-   **Aggregate Excess Demand:** The total excess demand `z(p, {β_it}, s)` is the sum of demands from informed traders (a function of `s`) and uninformed traders (a function of their beliefs `{β_it}`).\n-   **Market Clearing:** The equilibrium price `p` must satisfy the market clearing condition `z(p, {β_it}, s) = 0`.\n-   **Simplified Case:** Consider a simplified case with one good (`L=2`, so price `p` is a scalar) and one group of identical uninformed traders whose beliefs are summarized by a single parameter `β ∈ [0,1]`. Assume the equilibrium is unique, so we can write the market clearing price as a function `p*(β, s)`. The market clearing condition is `z(p*(β, s), β, s) = 0`.\n-   **Implicit Function Theorem:** Applying the implicit function theorem to the market clearing condition yields the comparative static:\n      \n    \\frac{\\partial p^*}{\\partial \\beta} = - \\frac{\\partial z / \\partial \\beta}{\\partial z / \\partial p}\n    \\quad \\quad \\text{(Eq. (1))}\n     \n\n### Question\n\nConsider the simplified case described in the model specification. Using the implicit function theorem on the market clearing condition (Eq. (1)), select all statements that correctly describe the comparative static `∂p*/∂β`.", "Answer": ["B", "C"], "qid": "145", "question": "### Background\n\n**Research Question.** This problem examines the microfoundations of the model, connecting agents' foundational beliefs and assumptions to their behavior and, ultimately, to market-level outcomes. It explores how subjective beliefs translate into objective market prices.\n\n**Setting.** The model features a sequence of temporary equilibria in an exchange economy. In each period, there are informed traders who know the payoff-relevant state `s_t` and uninformed traders who must learn about the true data-generating process, indexed by `θ` from a finite set `Θ`. Uninformed traders use past market data to form posterior beliefs `β_it`.\n\n### Data / Model Specification\n\n-   **Aggregate Excess Demand:** The total excess demand `z(p, {β_it}, s)` is the sum of demands from informed traders (a function of `s`) and uninformed traders (a function of their beliefs `{β_it}`).\n-   **Market Clearing:** The equilibrium price `p` must satisfy the market clearing condition `z(p, {β_it}, s) = 0`.\n-   **Simplified Case:** Consider a simplified case with one good (`L=2`, so price `p` is a scalar) and one group of identical uninformed traders whose beliefs are summarized by a single parameter `β ∈ [0,1]`. Assume the equilibrium is unique, so we can write the market clearing price as a function `p*(β, s)`. The market clearing condition is `z(p*(β, s), β, s) = 0`.\n-   **Implicit Function Theorem:** Applying the implicit function theorem to the market clearing condition yields the comparative static:\n      \n    \\frac{\\partial p^*}{\\partial \\beta} = - \\frac{\\partial z / \\partial \\beta}{\\partial z / \\partial p}\n    \\quad \\quad \\text{(Eq. (1))}\n     \n\n### Question\n\nConsider the simplified case described in the model specification. Using the implicit function theorem on the market clearing condition (Eq. (1)), select all statements that correctly describe the comparative static `∂p*/∂β`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 146, "Question": "### Background\n\n**Research Question.** This problem examines the method for making the optimal Weighted Average Power (WAP) test computationally feasible for large samples.\n\n**Setting.** The WAP test requires evaluating a complex, often high-dimensional integral. The paper proposes a saddlepoint approximation, based on the Laplace method, to simplify this calculation.\n\n**Variables and Parameters.**\n- `T(π)`: The exact WAP test statistic, which is an integral over `Θ₁`.\n- `v`: The maximal invariant statistic.\n- `θ`: A `p × 1` vector of parameters governing the error covariance.\n- `θ*`: The specific value of `θ` that minimizes the quadratic form `v'(CΣ(θ)C')⁻¹v`.\n- `n`: Sample size.\n\n---\n\n### Data / Model Specification\n\nThe exact WAP test statistic is given by:\n  \nT(π)=∫_{Θ₁}π(θ)|CΣ(θ)C'|⁻¹/²[v'(CΣ(θ)C')⁻¹v]⁻⁽ⁿ⁻ᵏ⁾/²dθ \n \nThe validity of the saddlepoint approximation relies on a key assumption:\n\n**Assumption 3.** The quantity `v'(CΣ(θ)C')⁻¹v`, regarded as a function of `θ`, is smooth and has an absolute, nondegenerate minimum at `θ*`, which is an interior point of `Θ₁`.\n\nThe paper notes that certain scenarios can violate this assumption, such as when testing for Moving Average (MA) errors (which can have multiple minima) or when the minimum `θ*` occurs on the boundary of the parameter space `Θ₁`.\n\n---\n\n### The Question\n\nAccording to the paper's framework, which of the following statements correctly describe valid reasons for developing the saddlepoint approximation and the conditions under which it might fail?\n", "Answer": ["A", "B", "D"], "qid": "146", "question": "### Background\n\n**Research Question.** This problem examines the method for making the optimal Weighted Average Power (WAP) test computationally feasible for large samples.\n\n**Setting.** The WAP test requires evaluating a complex, often high-dimensional integral. The paper proposes a saddlepoint approximation, based on the Laplace method, to simplify this calculation.\n\n**Variables and Parameters.**\n- `T(π)`: The exact WAP test statistic, which is an integral over `Θ₁`.\n- `v`: The maximal invariant statistic.\n- `θ`: A `p × 1` vector of parameters governing the error covariance.\n- `θ*`: The specific value of `θ` that minimizes the quadratic form `v'(CΣ(θ)C')⁻¹v`.\n- `n`: Sample size.\n\n---\n\n### Data / Model Specification\n\nThe exact WAP test statistic is given by:\n  \nT(π)=∫_{Θ₁}π(θ)|CΣ(θ)C'|⁻¹/²[v'(CΣ(θ)C')⁻¹v]⁻⁽ⁿ⁻ᵏ⁾/²dθ \n \nThe validity of the saddlepoint approximation relies on a key assumption:\n\n**Assumption 3.** The quantity `v'(CΣ(θ)C')⁻¹v`, regarded as a function of `θ`, is smooth and has an absolute, nondegenerate minimum at `θ*`, which is an interior point of `Θ₁`.\n\nThe paper notes that certain scenarios can violate this assumption, such as when testing for Moving Average (MA) errors (which can have multiple minima) or when the minimum `θ*` occurs on the boundary of the parameter space `Θ₁`.\n\n---\n\n### The Question\n\nAccording to the paper's framework, which of the following statements correctly describe valid reasons for developing the saddlepoint approximation and the conditions under which it might fail?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 147, "Question": "### Background\n\n**Research Question.** This problem investigates the asymptotic approximations for the panel data Weighted Average Power (WAP) test statistic, highlighting how the approximation changes depending on whether the panel is 'long' (large `T`) or 'wide' (large `N`).\n\n**Setting.** The WAP test for panel data involves an integral that is computationally intensive. Theorem 3 provides two distinct saddlepoint approximations based on Laplace's method, tailored to different asymptotic scenarios (`N → ∞` or `T → ∞`).\n\n---\n\n### Data / Model Specification\n\nThe exact WAP test statistic for the fixed effects panel model is:\n  \nT(π)=∫_{Θ₁} (π(θ)|V'(Iₙ⊗Ωᵥ(θ))V|⁻¹/²) / ([g₁(θ)]ᴺ/²[g₂(θ)]^[ᴺ(ᵀ⁻¹)⁻ᵏ]/²) dθ \n \nwhere `N` is the number of individuals and `T` is the number of time periods.\n\n**Theorem 3** provides two approximations for `T(π)`:\n1.  **Large `T` asymptotics (`T → ∞`, `N` fixed):** The approximation is based on finding `θ*` that minimizes `g₂(θ)`.\n2.  **Large `N` asymptotics (`N → ∞`, `T` fixed):** The approximation is based on finding `θ*` that minimizes `g(θ) = g₁(θ)[g₂(θ)]ᵀ⁻¹`.\n\n---\n\n### The Question\n\nBased on the structure of the WAP test statistic and the logic of Laplace's method, which of the following statements are correct interpretations or applications of the panel data approximation framework?\n", "Answer": ["B", "C", "D"], "qid": "147", "question": "### Background\n\n**Research Question.** This problem investigates the asymptotic approximations for the panel data Weighted Average Power (WAP) test statistic, highlighting how the approximation changes depending on whether the panel is 'long' (large `T`) or 'wide' (large `N`).\n\n**Setting.** The WAP test for panel data involves an integral that is computationally intensive. Theorem 3 provides two distinct saddlepoint approximations based on Laplace's method, tailored to different asymptotic scenarios (`N → ∞` or `T → ∞`).\n\n---\n\n### Data / Model Specification\n\nThe exact WAP test statistic for the fixed effects panel model is:\n  \nT(π)=∫_{Θ₁} (π(θ)|V'(Iₙ⊗Ωᵥ(θ))V|⁻¹/²) / ([g₁(θ)]ᴺ/²[g₂(θ)]^[ᴺ(ᵀ⁻¹)⁻ᵏ]/²) dθ \n \nwhere `N` is the number of individuals and `T` is the number of time periods.\n\n**Theorem 3** provides two approximations for `T(π)`:\n1.  **Large `T` asymptotics (`T → ∞`, `N` fixed):** The approximation is based on finding `θ*` that minimizes `g₂(θ)`.\n2.  **Large `N` asymptotics (`N → ∞`, `T` fixed):** The approximation is based on finding `θ*` that minimizes `g(θ) = g₁(θ)[g₂(θ)]ᵀ⁻¹`.\n\n---\n\n### The Question\n\nBased on the structure of the WAP test statistic and the logic of Laplace's method, which of the following statements are correct interpretations or applications of the panel data approximation framework?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 148, "Question": "### Background\n\n**Research Question.** This problem investigates a key theoretical result: the equivalence of the optimal invariant test for serial correlation under both fixed effects (FE) and random effects (RE) panel data models.\n\n**Setting.** The paper demonstrates that by imposing a specific invariance requirement, the WAP test derived for the RE model becomes identical to the one derived for the FE model. This provides a single, robust testing procedure.\n\n**Models.**\n- **FE Model:** `yᵢₜ = αᵢ + Xᵢₜδ + uᵢₜ`\n- **RE Model:** `yᵢₜ = Xᵢₜβ + wᵢ + vᵢₜ`\n\n---\n\n### Data / Model Specification\n\nThe derivation of the optimal WAP test for the FE model imposes invariance to transformations of the form `y → y + (Iₙ ⊗ iₜ)a`, where `a` is an `N × 1` vector. This transformation effectively removes the fixed effects `αᵢ` by focusing on within-individual variation.\n\nThe paper shows that if the same invariance is imposed on the RE model, the resulting maximal invariant statistic and its distribution are identical to the FE case. Consequently, the optimal WAP test for serial correlation is the same in both settings.\n\n---\n\n### The Question\n\nWhich of the following statements are valid conclusions or logical extensions of the paper's finding that the optimal WAP test is identical for FE and RE models under the specified invariance?\n", "Answer": ["A", "B", "D"], "qid": "148", "question": "### Background\n\n**Research Question.** This problem investigates a key theoretical result: the equivalence of the optimal invariant test for serial correlation under both fixed effects (FE) and random effects (RE) panel data models.\n\n**Setting.** The paper demonstrates that by imposing a specific invariance requirement, the WAP test derived for the RE model becomes identical to the one derived for the FE model. This provides a single, robust testing procedure.\n\n**Models.**\n- **FE Model:** `yᵢₜ = αᵢ + Xᵢₜδ + uᵢₜ`\n- **RE Model:** `yᵢₜ = Xᵢₜβ + wᵢ + vᵢₜ`\n\n---\n\n### Data / Model Specification\n\nThe derivation of the optimal WAP test for the FE model imposes invariance to transformations of the form `y → y + (Iₙ ⊗ iₜ)a`, where `a` is an `N × 1` vector. This transformation effectively removes the fixed effects `αᵢ` by focusing on within-individual variation.\n\nThe paper shows that if the same invariance is imposed on the RE model, the resulting maximal invariant statistic and its distribution are identical to the FE case. Consequently, the optimal WAP test for serial correlation is the same in both settings.\n\n---\n\n### The Question\n\nWhich of the following statements are valid conclusions or logical extensions of the paper's finding that the optimal WAP test is identical for FE and RE models under the specified invariance?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 149, "Question": "### Background\n\n**Research Question.** This problem reconstructs the paper's central theoretical contribution: the multi-stage proof of global asymptotic stability for optimal paths in systems with separable, strongly convex-concave Hamiltonians. The proof is a cornerstone of the paper, demonstrating that, under specific conditions, the system converges to a unique steady state regardless of the initial state.\n\n**Setting.** We analyze the canonical equations of motion for an optimal control problem. The proof of global stability relies on three distinct but complementary arguments: (i) the existence of a Liapunov function to constrain the system's dynamics, (ii) the local saddle-path stability of the unique steady state, and (iii) boundary conditions on the utility function to rule out non-interior solutions.\n\n### Data / Model Specification\n\nThe optimal path `(k(t), q(t))` is governed by the canonical equations:\n  \n\\dot{k} = D H^1(q) \\quad \\text{and} \\quad \\dot{q} = \\rho q - D H^2(k) \n \nThe analysis relies on the following assumptions and intermediate results from the paper:\n\n1.  **Strong Convexo-Concavity:** The utility components `u(k)` and `v(k̇)` are strongly concave, which implies `H²(k)` is strongly concave (`D²H²` is negative definite) and `H¹(q)` is strongly convex (`D²H¹` is positive definite).\n2.  **Liapunov Condition (Lemma 2):** Along any optimal path, `k̇(t) ⋅ q̇(t) ≤ 0`.\n3.  **Boundary Condition (Assumption 4):** The utility function `u(k)` satisfies an Inada-like condition, where the marginal utility of a capital good `kⁱ` approaches infinity as its stock `kⁱ` approaches zero.\n4.  **Boundedness of Co-state (Theorem 1a):** As a consequence of the Liapunov property, the co-state vector is bounded, i.e., there exists an `M < ∞` such that `|q(t)| ≤ M` for all `t`.\n\n### Question\n\nBased on the provided model and assumptions, select all statements that correctly describe a component of the proof for global asymptotic stability.", "Answer": ["A", "B"], "qid": "149", "question": "### Background\n\n**Research Question.** This problem reconstructs the paper's central theoretical contribution: the multi-stage proof of global asymptotic stability for optimal paths in systems with separable, strongly convex-concave Hamiltonians. The proof is a cornerstone of the paper, demonstrating that, under specific conditions, the system converges to a unique steady state regardless of the initial state.\n\n**Setting.** We analyze the canonical equations of motion for an optimal control problem. The proof of global stability relies on three distinct but complementary arguments: (i) the existence of a Liapunov function to constrain the system's dynamics, (ii) the local saddle-path stability of the unique steady state, and (iii) boundary conditions on the utility function to rule out non-interior solutions.\n\n### Data / Model Specification\n\nThe optimal path `(k(t), q(t))` is governed by the canonical equations:\n  \n\\dot{k} = D H^1(q) \\quad \\text{and} \\quad \\dot{q} = \\rho q - D H^2(k) \n \nThe analysis relies on the following assumptions and intermediate results from the paper:\n\n1.  **Strong Convexo-Concavity:** The utility components `u(k)` and `v(k̇)` are strongly concave, which implies `H²(k)` is strongly concave (`D²H²` is negative definite) and `H¹(q)` is strongly convex (`D²H¹` is positive definite).\n2.  **Liapunov Condition (Lemma 2):** Along any optimal path, `k̇(t) ⋅ q̇(t) ≤ 0`.\n3.  **Boundary Condition (Assumption 4):** The utility function `u(k)` satisfies an Inada-like condition, where the marginal utility of a capital good `kⁱ` approaches infinity as its stock `kⁱ` approaches zero.\n4.  **Boundedness of Co-state (Theorem 1a):** As a consequence of the Liapunov property, the co-state vector is bounded, i.e., there exists an `M < ∞` such that `|q(t)| ≤ M` for all `t`.\n\n### Question\n\nBased on the provided model and assumptions, select all statements that correctly describe a component of the proof for global asymptotic stability.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 150, "Question": "### Background\n\n**Research Question.** This problem applies the paper's general theory of separable Hamiltonian systems to a canonical neoclassical model of firm investment with adjustment costs. The goal is to demonstrate how the abstract stability results translate into concrete predictions about a firm's long-run capital accumulation.\n\n**Setting.** A competitive firm chooses its path of net investment `k̇(t)` to maximize the present discounted value of its profit stream. The firm faces convex costs for adjusting its capital stock and operates in an environment with constant prices.\n\n### Data / Model Specification\n\nThe firm's optimization problem is to choose the path of its capital stock vector `k(t)` to maximize:\n  \n\\max_{\\dot{k}(t)} \\int_{0}^{\\infty} e^{-\\rho t} [l f(k(t)) - g(\\dot{k}(t)) - p \\cdot (\\dot{k}(t) + \\eta k(t))] dt \n \nwhere `l` is the output price, `f(k)` is a strongly concave production function, `g(k̇)` is a convex adjustment cost function, `p` is the vector of capital prices, and `η` is the diagonal matrix of depreciation rates.\n\nThe long-run steady-state capital stock `k̄` is implicitly defined by the condition that the value of the marginal product of capital equals its user cost:\n  \nl \\cdot D f(\\bar{k}) = (\\rho I + \\eta) p \n \n\n### Question\n\nConsider the firm's long-run steady-state capital stock `k̄`. An economic shock occurs, causing the output price `l` to permanently increase. Based on the model, select all correct statements regarding the consequences of this shock.", "Answer": ["A", "B"], "qid": "150", "question": "### Background\n\n**Research Question.** This problem applies the paper's general theory of separable Hamiltonian systems to a canonical neoclassical model of firm investment with adjustment costs. The goal is to demonstrate how the abstract stability results translate into concrete predictions about a firm's long-run capital accumulation.\n\n**Setting.** A competitive firm chooses its path of net investment `k̇(t)` to maximize the present discounted value of its profit stream. The firm faces convex costs for adjusting its capital stock and operates in an environment with constant prices.\n\n### Data / Model Specification\n\nThe firm's optimization problem is to choose the path of its capital stock vector `k(t)` to maximize:\n  \n\\max_{\\dot{k}(t)} \\int_{0}^{\\infty} e^{-\\rho t} [l f(k(t)) - g(\\dot{k}(t)) - p \\cdot (\\dot{k}(t) + \\eta k(t))] dt \n \nwhere `l` is the output price, `f(k)` is a strongly concave production function, `g(k̇)` is a convex adjustment cost function, `p` is the vector of capital prices, and `η` is the diagonal matrix of depreciation rates.\n\nThe long-run steady-state capital stock `k̄` is implicitly defined by the condition that the value of the marginal product of capital equals its user cost:\n  \nl \\cdot D f(\\bar{k}) = (\\rho I + \\eta) p \n \n\n### Question\n\nConsider the firm's long-run steady-state capital stock `k̄`. An economic shock occurs, causing the output price `l` to permanently increase. Based on the model, select all correct statements regarding the consequences of this shock.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 151, "Question": "### Background\n\n**Research Question.** This problem applies the paper's general theory of separable Hamiltonian systems to a canonical neoclassical model of firm investment with adjustment costs. The goal is to demonstrate how the abstract stability results translate into concrete predictions about a firm's long-run capital accumulation.\n\n**Setting.** A competitive firm with perfect foresight faces a changing policy environment. The firm faces convex costs for adjusting its capital stock.\n\n### Data / Model Specification\n\nThe firm's optimization problem is to choose the path of its capital stock vector `k(t)` to maximize its present discounted value. The firm's initial capital stock `k(0)` is at its long-run steady state, `k̄`.\n\nNow, suppose the government introduces a temporary investment tax credit (ITC) that is publicly known to last for exactly 10 years. This policy temporarily lowers the user cost of capital. After 10 years, the user cost of capital will permanently revert to its original level.\n\n### Question\n\nSelect all statements that correctly describe the optimal dynamic path of the firm's capital stock `k(t)` in response to this temporary policy.", "Answer": ["A", "C"], "qid": "151", "question": "### Background\n\n**Research Question.** This problem applies the paper's general theory of separable Hamiltonian systems to a canonical neoclassical model of firm investment with adjustment costs. The goal is to demonstrate how the abstract stability results translate into concrete predictions about a firm's long-run capital accumulation.\n\n**Setting.** A competitive firm with perfect foresight faces a changing policy environment. The firm faces convex costs for adjusting its capital stock.\n\n### Data / Model Specification\n\nThe firm's optimization problem is to choose the path of its capital stock vector `k(t)` to maximize its present discounted value. The firm's initial capital stock `k(0)` is at its long-run steady state, `k̄`.\n\nNow, suppose the government introduces a temporary investment tax credit (ITC) that is publicly known to last for exactly 10 years. This policy temporarily lowers the user cost of capital. After 10 years, the user cost of capital will permanently revert to its original level.\n\n### Question\n\nSelect all statements that correctly describe the optimal dynamic path of the firm's capital stock `k(t)` in response to this temporary policy.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 152, "Question": "### Background\n\n**Research Question.** This problem explores an extension of the paper's stability analysis to a multi-firm industry where the output price is endogenously determined. This requires transforming the problem into an equivalent autonomous social planner's problem.\n\n**Setting.** An industry consists of `m` firms. The market price `l(t)` is determined by an inverse demand function `l(t) = ψ(Σ fⱼ(kⱼ(t)))`, where `ψ' ≤ 0`.\n\nThe social planner's problem generates a separable Hamiltonian `H(q,k) = H¹(q) + H²(k)`. For the paper's stability theorems to apply, the Hessian of the state-dependent component, `D²H²(k)`, must be negative definite.\n\n### Data / Model Specification\n\nThe Hessian matrix of the planner's `H²(k)` component is given by:\n  \nD²H²(k) = Dψ ⋅ zzᵀ + ψB\n \nwhere:\n1.  `Dψ ≤ 0` is the slope of the inverse demand curve.\n2.  `ψ > 0` is the price level.\n3.  `z` is the `nm × 1` vector of all firms' marginal products.\n4.  `B` is an `nm × nm` block-diagonal matrix where each block is the Hessian `D²fⱼ` of a firm's production function. Each `fⱼ` is assumed to be strongly concave.\n\n### Question\n\nFor the paper's stability theorems to apply, `D²H²(k)` must be negative definite. Given its structure in Eq. (1) and the properties of its components, select all statements that are valid steps in the proof of its negative definiteness.", "Answer": ["B", "C"], "qid": "152", "question": "### Background\n\n**Research Question.** This problem explores an extension of the paper's stability analysis to a multi-firm industry where the output price is endogenously determined. This requires transforming the problem into an equivalent autonomous social planner's problem.\n\n**Setting.** An industry consists of `m` firms. The market price `l(t)` is determined by an inverse demand function `l(t) = ψ(Σ fⱼ(kⱼ(t)))`, where `ψ' ≤ 0`.\n\nThe social planner's problem generates a separable Hamiltonian `H(q,k) = H¹(q) + H²(k)`. For the paper's stability theorems to apply, the Hessian of the state-dependent component, `D²H²(k)`, must be negative definite.\n\n### Data / Model Specification\n\nThe Hessian matrix of the planner's `H²(k)` component is given by:\n  \nD²H²(k) = Dψ ⋅ zzᵀ + ψB\n \nwhere:\n1.  `Dψ ≤ 0` is the slope of the inverse demand curve.\n2.  `ψ > 0` is the price level.\n3.  `z` is the `nm × 1` vector of all firms' marginal products.\n4.  `B` is an `nm × nm` block-diagonal matrix where each block is the Hessian `D²fⱼ` of a firm's production function. Each `fⱼ` is assumed to be strongly concave.\n\n### Question\n\nFor the paper's stability theorems to apply, `D²H²(k)` must be negative definite. Given its structure in Eq. (1) and the properties of its components, select all statements that are valid steps in the proof of its negative definiteness.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 153, "Question": "### Background\n\n**Research Question.** This problem explores the foundational mathematical structure of the dynamic optimization problems analyzed in the paper, focusing on the system's canonical equations of motion and its long-run steady state.\n\n**Setting.** We consider a general class of infinite-horizon optimal control problems where the instantaneous utility function is additively separable: `u(k) + v(k̇)`.\n\n### Data / Model Specification\n\nThe agent's problem generates a separable current-value Hamiltonian `H(q, k) = H¹(q) + H²(k)`. The canonical equations of motion are:\n  \n\\dot{k} = D H^1(q) \\quad \\text{and} \\quad \\dot{q} = \\rho q - D H^2(k)\n \nwhere `ρ` is the discount rate. The steady state `(q̄, k̄)` of the system is defined by the conditions `k̇=0` and `q̇=0`.\n\nAssume that `H²(k)` is strongly concave (`D²H²` is negative definite).\n\n### Question\n\nBased on the provided canonical equations, select all correct statements about the system's steady state and its properties.", "Answer": ["A", "B"], "qid": "153", "question": "### Background\n\n**Research Question.** This problem explores the foundational mathematical structure of the dynamic optimization problems analyzed in the paper, focusing on the system's canonical equations of motion and its long-run steady state.\n\n**Setting.** We consider a general class of infinite-horizon optimal control problems where the instantaneous utility function is additively separable: `u(k) + v(k̇)`.\n\n### Data / Model Specification\n\nThe agent's problem generates a separable current-value Hamiltonian `H(q, k) = H¹(q) + H²(k)`. The canonical equations of motion are:\n  \n\\dot{k} = D H^1(q) \\quad \\text{and} \\quad \\dot{q} = \\rho q - D H^2(k)\n \nwhere `ρ` is the discount rate. The steady state `(q̄, k̄)` of the system is defined by the conditions `k̇=0` and `q̇=0`.\n\nAssume that `H²(k)` is strongly concave (`D²H²` is negative definite).\n\n### Question\n\nBased on the provided canonical equations, select all correct statements about the system's steady state and its properties.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 154, "Question": "### Background\n\nAn agent's output `q_i` can be high (`\\overline{q}`) with probability `μ(e_i)` or low (`\\underline{q}`) with probability `1-μ(e_i)`. The agent receives a base wage `\\underline{w}_i` and a bonus `Δw_i` if output is high. The agent's status, `s_i`, is predetermined.\n\n### Data / Model Specification\n\nThe agent's utility function is:\n\n  \nu(w,s,e) = s w - \\psi(e)\n\\quad \\quad \\text{(Eq. 1)}\n \n\nGiven the wage structure, the agent chooses effort `e_i` to maximize her expected utility:\n\n  \n\\max_{e_i} \\quad E[U_i] = s_i [\\mu(e_i) \\Delta w_i + \\underline{w}_i] - \\psi(e_i)\n\\quad \\quad \\text{(Eq. 2)}\n \n\nThe first-order condition (FOC) for the agent's optimal effort `e_i^*` is:\n\n  \n\\frac{\\psi'(e_i^*)}{\\mu'(e_i^*)} = s_i \\Delta w_i\n\\quad \\quad \\text{(Eq. 3)}\n \n\nAssume the following properties for the functions `μ(e)` and `ψ(e)`:\n*   `μ'(e) > 0`, `μ''(e) < 0` (probability of success increases at a decreasing rate with effort)\n*   `ψ'(e) > 0`, `ψ''(e) > 0` (disutility of effort increases at an increasing rate)\n\n---\n\nBased on the provided model, which of the following statements are valid conclusions about the agent's optimal effort choice `e_i^*`?", "Answer": ["A", "C", "D"], "qid": "154", "question": "### Background\n\nAn agent's output `q_i` can be high (`\\overline{q}`) with probability `μ(e_i)` or low (`\\underline{q}`) with probability `1-μ(e_i)`. The agent receives a base wage `\\underline{w}_i` and a bonus `Δw_i` if output is high. The agent's status, `s_i`, is predetermined.\n\n### Data / Model Specification\n\nThe agent's utility function is:\n\n  \nu(w,s,e) = s w - \\psi(e)\n\\quad \\quad \\text{(Eq. 1)}\n \n\nGiven the wage structure, the agent chooses effort `e_i` to maximize her expected utility:\n\n  \n\\max_{e_i} \\quad E[U_i] = s_i [\\mu(e_i) \\Delta w_i + \\underline{w}_i] - \\psi(e_i)\n\\quad \\quad \\text{(Eq. 2)}\n \n\nThe first-order condition (FOC) for the agent's optimal effort `e_i^*` is:\n\n  \n\\frac{\\psi'(e_i^*)}{\\mu'(e_i^*)} = s_i \\Delta w_i\n\\quad \\quad \\text{(Eq. 3)}\n \n\nAssume the following properties for the functions `μ(e)` and `ψ(e)`:\n*   `μ'(e) > 0`, `μ''(e) < 0` (probability of success increases at a decreasing rate with effort)\n*   `ψ'(e) > 0`, `ψ''(e) > 0` (disutility of effort increases at an increasing rate)\n\n---\n\nBased on the provided model, which of the following statements are valid conclusions about the agent's optimal effort choice `e_i^*`?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn agent's output `q_i` can be hig...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn agent's output `q_i` can be hig...", "error": null}}
{"ID": 155, "Question": "### Background\n\nA risk-neutral principal offers short-term (static) contracts to `n` ex-ante identical, risk-neutral agents. A contract specifies an agent's status `s_i`, a base wage `\\underline{w}_i`, and a performance bonus `Δw_i`. The agent's utility is `u(w,s,e) = sw - ψ(e)`. The principal's profit from an agent is `EΠ_i = μ(e_i)(Δq - Δw_i) - \\underline{w}_i`.\n\n### Data / Model Specification\n\nThe optimal short-term contract is characterized by two key propositions:\n\n**Proposition 1.** An optimal solution has the property that `s_i < s_j` if and only if (`\\underline{w}_i = \\underline{w}_j = 0` and `Δw_i < Δw_j`) or (`\\underline{w}_i < \\underline{w}_j`). This means higher status must be paired with higher monetary rewards.\n\n**Proposition 2 (Symbolic Egalitarianism).** It is optimal to give identical agents identical contracts (same status, same compensation scheme).\n\nThe paper explains the logic for Proposition 2 in a simple case: if one agent `j` has higher status (`s_j`) and thus higher utility (`EU_j`) than another agent `i` (`s_i`, `EU_i`), and agent `i`'s participation constraint is met (`EU_i ≥ \\underline{U}`), then agent `j`'s constraint must be slack (`EU_j > \\underline{U}`). If agent `j`'s base wage `\\underline{w}_j` is positive, the principal can slightly reduce it, increasing profit without violating any constraints.\n\n---\n\nAccording to the logic of the static model, which of the following are valid reasons why 'Symbolic Egalitarianism' (Proposition 2) emerges as the optimal strategy for the principal?", "Answer": ["C", "D"], "qid": "155", "question": "### Background\n\nA risk-neutral principal offers short-term (static) contracts to `n` ex-ante identical, risk-neutral agents. A contract specifies an agent's status `s_i`, a base wage `\\underline{w}_i`, and a performance bonus `Δw_i`. The agent's utility is `u(w,s,e) = sw - ψ(e)`. The principal's profit from an agent is `EΠ_i = μ(e_i)(Δq - Δw_i) - \\underline{w}_i`.\n\n### Data / Model Specification\n\nThe optimal short-term contract is characterized by two key propositions:\n\n**Proposition 1.** An optimal solution has the property that `s_i < s_j` if and only if (`\\underline{w}_i = \\underline{w}_j = 0` and `Δw_i < Δw_j`) or (`\\underline{w}_i < \\underline{w}_j`). This means higher status must be paired with higher monetary rewards.\n\n**Proposition 2 (Symbolic Egalitarianism).** It is optimal to give identical agents identical contracts (same status, same compensation scheme).\n\nThe paper explains the logic for Proposition 2 in a simple case: if one agent `j` has higher status (`s_j`) and thus higher utility (`EU_j`) than another agent `i` (`s_i`, `EU_i`), and agent `i`'s participation constraint is met (`EU_i ≥ \\underline{U}`), then agent `j`'s constraint must be slack (`EU_j > \\underline{U}`). If agent `j`'s base wage `\\underline{w}_j` is positive, the principal can slightly reduce it, increasing profit without violating any constraints.\n\n---\n\nAccording to the logic of the static model, which of the following are valid reasons why 'Symbolic Egalitarianism' (Proposition 2) emerges as the optimal strategy for the principal?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA risk-neutral principal offers sh...", "model_answer": "This is a detailed response to the question about ### Background\n\nA risk-neutral principal offers sh...", "error": null}}
{"ID": 156, "Question": "### Background\n\nA principal designs a long-term contract for a two-period overlapping generations (OLG) model. The principal can commit to future rewards. Agents' preferences feature a complementarity between status and income, `u(w,s,e) = sw - ψ(e)`. The principal's key finding is that in a short-term (static) setting, it is optimal to grant all identical agents the same status ('Symbolic Egalitarianism').\n\n### Data / Model Specification\n\nThe main result of the dynamic OLG model is summarized by:\n\n**Proposition 3 (Incentives through promotion).** In any steady state of a profit-maximizing solution:\n(i) Young agents receive minimal status and no monetary incentives: `s_1 = \\underline{w}_1 = Δw_1 = 0`.\n(ii) Old agents are differentiated by performance: `s_h > s_l`.\n(iii) Monetary rewards for old agents align with status: `\\underline{w}_h ≥ \\underline{w}_l` and `Δw_h ≥ Δw_l`, with at least one inequality strict.\n\n---\n\nWhich of the following statements correctly explain why the optimal contract strategy shifts so dramatically from 'Symbolic Egalitarianism' in the static model to 'Incentives through Promotion' in the dynamic model?", "Answer": ["A", "C", "D"], "qid": "156", "question": "### Background\n\nA principal designs a long-term contract for a two-period overlapping generations (OLG) model. The principal can commit to future rewards. Agents' preferences feature a complementarity between status and income, `u(w,s,e) = sw - ψ(e)`. The principal's key finding is that in a short-term (static) setting, it is optimal to grant all identical agents the same status ('Symbolic Egalitarianism').\n\n### Data / Model Specification\n\nThe main result of the dynamic OLG model is summarized by:\n\n**Proposition 3 (Incentives through promotion).** In any steady state of a profit-maximizing solution:\n(i) Young agents receive minimal status and no monetary incentives: `s_1 = \\underline{w}_1 = Δw_1 = 0`.\n(ii) Old agents are differentiated by performance: `s_h > s_l`.\n(iii) Monetary rewards for old agents align with status: `\\underline{w}_h ≥ \\underline{w}_l` and `Δw_h ≥ Δw_l`, with at least one inequality strict.\n\n---\n\nWhich of the following statements correctly explain why the optimal contract strategy shifts so dramatically from 'Symbolic Egalitarianism' in the static model to 'Incentives through Promotion' in the dynamic model?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA principal designs a long-term co...", "model_answer": "This is a detailed response to the question about ### Background\n\nA principal designs a long-term co...", "error": null}}
{"ID": 157, "Question": "### Background\n\n**Research Question.** This problem critically examines the three distinct theoretical arguments the paper uses to justify its central claim: that wealth in a stochastic exchange economy converges to a Gamma distribution.\n\n**Setting.** The analysis considers a large N-agent, 2-good economy. The goal is to derive the functional form of the steady-state wealth distribution from first principles.\n\n### Data / Model Specification\n\nThe paper presents three arguments:\n1.  **Heuristic Microstate Counting:** Draws an analogy with statistical mechanics. Its core assumption is that all possible ways an individual can allocate their wealth between the two goods are equally likely.\n2.  **Qualitative Argument (Lukacs's Theorem):** Relies on a purely probabilistic theorem. Its core assumption is that for any finite subgroup of agents, their total wealth `S_k` is statistically independent of the way that wealth is partitioned among them (the vector of shares `W_j/S_k`).\n3.  **Formal Probabilistic Proof:** A rigorous derivation based on the asymptotic properties of the model. A key intermediate result is that for large `N`, normalized price deviations are asymptotically normal with a variance given by:\n      \n    \\text{Var}(\\sqrt{N}(\\vartheta_{t}-\\vartheta)) = \\frac{\\sigma^{2}\\mu_{2}}{\\beta^{2}\\mu^{2}} \\quad \\text{(Eq. (1))}\n     \n    where `σ²` is the variance of preferences and `μ₂` is the second moment of the wealth distribution.\n\n### Question\n\nBased on the paper's analysis of its theoretical arguments, select **all** statements that are correct.", "Answer": ["A", "B"], "qid": "157", "question": "### Background\n\n**Research Question.** This problem critically examines the three distinct theoretical arguments the paper uses to justify its central claim: that wealth in a stochastic exchange economy converges to a Gamma distribution.\n\n**Setting.** The analysis considers a large N-agent, 2-good economy. The goal is to derive the functional form of the steady-state wealth distribution from first principles.\n\n### Data / Model Specification\n\nThe paper presents three arguments:\n1.  **Heuristic Microstate Counting:** Draws an analogy with statistical mechanics. Its core assumption is that all possible ways an individual can allocate their wealth between the two goods are equally likely.\n2.  **Qualitative Argument (Lukacs's Theorem):** Relies on a purely probabilistic theorem. Its core assumption is that for any finite subgroup of agents, their total wealth `S_k` is statistically independent of the way that wealth is partitioned among them (the vector of shares `W_j/S_k`).\n3.  **Formal Probabilistic Proof:** A rigorous derivation based on the asymptotic properties of the model. A key intermediate result is that for large `N`, normalized price deviations are asymptotically normal with a variance given by:\n      \n    \\text{Var}(\\sqrt{N}(\\vartheta_{t}-\\vartheta)) = \\frac{\\sigma^{2}\\mu_{2}}{\\beta^{2}\\mu^{2}} \\quad \\text{(Eq. (1))}\n     \n    where `σ²` is the variance of preferences and `μ₂` is the second moment of the wealth distribution.\n\n### Question\n\nBased on the paper's analysis of its theoretical arguments, select **all** statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 158, "Question": "### Background\n\n**Research Question.** This problem reconstructs the core mechanics of the paper's stochastic exchange economy, from individual optimization to the resulting evolution of wealth.\n\n**Setting.** A pure exchange economy consists of `N` individuals trading two goods, A and B. Good A serves as the numéraire. Agents' preferences for holding good A are given by a stochastic parameter `f_it`.\n\n### Data / Model Specification\n\nFrom the agent's utility maximization and market-clearing conditions, the paper derives the law of motion for an individual's wealth:\n  \nw_{i,t+1} = \\left( f_{it} + \\frac{\\vartheta_{t+1}}{\\vartheta_{t}}(1-f_{it}) \\right) w_{it} \\quad \\text{(Eq. (1))}\n \nwhere `w_it` is the wealth of agent `i` at the start of period `t`, `f_it` is their preference for good A in period `t`, and `ϑ_t` is the price of good B in terms of good A.\n\nThe growth factor `w_{i,t+1}/w_{it}` can be interpreted as the gross return on the agent's portfolio, which is allocated with share `f_it` to good A and `1-f_it` to good B.\n\n### Question\n\nSuppose that between period `t` and `t+1`, the relative price of good B falls by 20%, such that the price ratio `ϑ_{t+1}/ϑ_t = 0.8`. Based on the law of motion in **Eq. (1)**, select **all** statements that correctly describe the outcomes for agents with different preferences.", "Answer": ["A", "B"], "qid": "158", "question": "### Background\n\n**Research Question.** This problem reconstructs the core mechanics of the paper's stochastic exchange economy, from individual optimization to the resulting evolution of wealth.\n\n**Setting.** A pure exchange economy consists of `N` individuals trading two goods, A and B. Good A serves as the numéraire. Agents' preferences for holding good A are given by a stochastic parameter `f_it`.\n\n### Data / Model Specification\n\nFrom the agent's utility maximization and market-clearing conditions, the paper derives the law of motion for an individual's wealth:\n  \nw_{i,t+1} = \\left( f_{it} + \\frac{\\vartheta_{t+1}}{\\vartheta_{t}}(1-f_{it}) \\right) w_{it} \\quad \\text{(Eq. (1))}\n \nwhere `w_it` is the wealth of agent `i` at the start of period `t`, `f_it` is their preference for good A in period `t`, and `ϑ_t` is the price of good B in terms of good A.\n\nThe growth factor `w_{i,t+1}/w_{it}` can be interpreted as the gross return on the agent's portfolio, which is allocated with share `f_it` to good A and `1-f_it` to good B.\n\n### Question\n\nSuppose that between period `t` and `t+1`, the relative price of good B falls by 20%, such that the price ratio `ϑ_{t+1}/ϑ_t = 0.8`. Based on the law of motion in **Eq. (1)**, select **all** statements that correctly describe the outcomes for agents with different preferences.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 159, "Question": "### Background\n\n**Research Question.** This problem investigates the conflict between achieving strong efficiency (Ordinal Efficiency) and guaranteeing minimum levels of fairness in strategy-proof object allocation mechanisms for four or more agents.\n\n**Setting / Institutional Environment.** We consider an object allocation problem with `N≥4` agents. The focus is on mechanisms that are Strategy-Proof (SP) and Ordinally Efficient (OE).\n\n### Data / Model Specification\n\nThe analysis is based on a key impossibility result and known properties of the widely used Random Serial Dictatorship (RSD) mechanism.\n\n**Axioms and Definitions:**\n- **Ordinal Efficiency (OE):** An assignment is not stochastically dominated by any other assignment.\n- **Strategy-Proofness (SP):** No agent can benefit by misreporting preferences.\n- **Equal Division Lower Bound (EDLB):** A fairness criterion requiring that for every agent `a`, their probabilistic assignment `P_a` must first-order stochastically dominate the equal division assignment `ED`, where each object is assigned with probability `1/N`.\n- **Dominance:** An assignment `P` dominates an assignment `P'` if `P` is weakly preferred by all agents and strictly preferred by at least one.\n\n**Key Results and Facts:**\n- **Theorem 3:** For `N≥4`, there does not exist a mechanism that is simultaneously Ordinally Efficient (OE), Strategy-Proof (SP), and satisfies the Equal Division Lower Bound (EDLB).\n- **Fact about RSD:** The Random Serial Dictatorship (RSD) mechanism is known to be Strategy-Proof (SP) and to satisfy the Equal Division Lower Bound (EDLB).\n\n### Question\n\nBased on the provided information, which of the following statements are logically correct conclusions for the `N≥4` case? Select all that apply.", "Answer": ["A", "C"], "qid": "159", "question": "### Background\n\n**Research Question.** This problem investigates the conflict between achieving strong efficiency (Ordinal Efficiency) and guaranteeing minimum levels of fairness in strategy-proof object allocation mechanisms for four or more agents.\n\n**Setting / Institutional Environment.** We consider an object allocation problem with `N≥4` agents. The focus is on mechanisms that are Strategy-Proof (SP) and Ordinally Efficient (OE).\n\n### Data / Model Specification\n\nThe analysis is based on a key impossibility result and known properties of the widely used Random Serial Dictatorship (RSD) mechanism.\n\n**Axioms and Definitions:**\n- **Ordinal Efficiency (OE):** An assignment is not stochastically dominated by any other assignment.\n- **Strategy-Proofness (SP):** No agent can benefit by misreporting preferences.\n- **Equal Division Lower Bound (EDLB):** A fairness criterion requiring that for every agent `a`, their probabilistic assignment `P_a` must first-order stochastically dominate the equal division assignment `ED`, where each object is assigned with probability `1/N`.\n- **Dominance:** An assignment `P` dominates an assignment `P'` if `P` is weakly preferred by all agents and strictly preferred by at least one.\n\n**Key Results and Facts:**\n- **Theorem 3:** For `N≥4`, there does not exist a mechanism that is simultaneously Ordinally Efficient (OE), Strategy-Proof (SP), and satisfies the Equal Division Lower Bound (EDLB).\n- **Fact about RSD:** The Random Serial Dictatorship (RSD) mechanism is known to be Strategy-Proof (SP) and to satisfy the Equal Division Lower Bound (EDLB).\n\n### Question\n\nBased on the provided information, which of the following statements are logically correct conclusions for the `N≥4` case? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 160, "Question": "### Background\n\n**Research Question.** This problem explores the unique axiomatic characterization of the Random Serial Dictatorship (RSD) mechanism and its connection to a classic impossibility theorem, highlighting the crucial role of different efficiency standards.\n\n**Setting / Institutional Environment.** We consider an object allocation problem with `N≥3` agents. The analysis involves both ordinal axioms (like SP and ExPE) and cardinal axioms (Symmetry, Ex-ante Efficiency) that depend on agents' utilities.\n\n### Data / Model Specification\n\nThe analysis connects a characterization result with an impossibility theorem.\n\n**Axioms and Definitions:**\n- **Strategy-Proofness (SP):** Truthful reporting is a weakly dominant strategy.\n- **Ex-post Efficiency (ExPE):** The outcome is a lottery over Pareto efficient deterministic assignments.\n- **Symmetry:** A cardinal fairness axiom requiring agents with identical utility functions to receive the same expected utility.\n- **Ex-ante Efficiency:** A cardinal efficiency axiom requiring that no other assignment can make at least one agent better off in expectation without making any other agent worse off.\n\n**Key Results and Facts:**\n- **Proposition 2:** For `N=3`, a mechanism is Strategy-Proof (SP), Ex-post Efficient (ExPE), and Symmetric if and only if it is RSD.\n- **Corollary 4 (Zhou's Impossibility):** For `N≥3`, there does not exist a mechanism that is Strategy-Proof (SP), Ex-ante Efficient, and Symmetric.\n- **Logical/Factual Relationships:**\n    1. Ex-ante Efficiency is a stronger requirement than Ex-post Efficiency (Ex-ante Efficiency ⇒ ExPE).\n    2. RSD is known to be ExPE, but it is **not** Ex-ante Efficient.\n\n### Question\n\nSelect all statements that represent valid steps in a proof by contradiction that derives Zhou's Impossibility (Corollary 4) for `N=3`, using the provided information.", "Answer": ["C", "D"], "qid": "160", "question": "### Background\n\n**Research Question.** This problem explores the unique axiomatic characterization of the Random Serial Dictatorship (RSD) mechanism and its connection to a classic impossibility theorem, highlighting the crucial role of different efficiency standards.\n\n**Setting / Institutional Environment.** We consider an object allocation problem with `N≥3` agents. The analysis involves both ordinal axioms (like SP and ExPE) and cardinal axioms (Symmetry, Ex-ante Efficiency) that depend on agents' utilities.\n\n### Data / Model Specification\n\nThe analysis connects a characterization result with an impossibility theorem.\n\n**Axioms and Definitions:**\n- **Strategy-Proofness (SP):** Truthful reporting is a weakly dominant strategy.\n- **Ex-post Efficiency (ExPE):** The outcome is a lottery over Pareto efficient deterministic assignments.\n- **Symmetry:** A cardinal fairness axiom requiring agents with identical utility functions to receive the same expected utility.\n- **Ex-ante Efficiency:** A cardinal efficiency axiom requiring that no other assignment can make at least one agent better off in expectation without making any other agent worse off.\n\n**Key Results and Facts:**\n- **Proposition 2:** For `N=3`, a mechanism is Strategy-Proof (SP), Ex-post Efficient (ExPE), and Symmetric if and only if it is RSD.\n- **Corollary 4 (Zhou's Impossibility):** For `N≥3`, there does not exist a mechanism that is Strategy-Proof (SP), Ex-ante Efficient, and Symmetric.\n- **Logical/Factual Relationships:**\n    1. Ex-ante Efficiency is a stronger requirement than Ex-post Efficiency (Ex-ante Efficiency ⇒ ExPE).\n    2. RSD is known to be ExPE, but it is **not** Ex-ante Efficient.\n\n### Question\n\nSelect all statements that represent valid steps in a proof by contradiction that derives Zhou's Impossibility (Corollary 4) for `N=3`, using the provided information.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 161, "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental incompatibility of efficiency, incentive compatibility, and fairness in object allocation mechanisms, focusing on how using weaker axioms can lead to stronger impossibility results.\n\n**Setting / Institutional Environment.** We consider mechanisms for allocating indivisible objects to `N≥3` agents. The focus is on the mutual compatibility of a set of desirable axiomatic properties.\n\n### Data / Model Specification\n\nThe analysis centers on two key propositions and the logical relationships between the axioms they employ.\n\n**Axioms and Definitions:**\n- **Ex-post Efficiency (ExPE):** The outcome must be a lottery over Pareto efficient deterministic assignments.\n- **Strategy-Proofness (SP):** Truthful reporting is a weakly dominant strategy for all agents.\n- **Envy-Freeness (EF):** Every agent weakly prefers their own probabilistic assignment to any other agent's.\n- **Upper Shuffle-Proofness (USP):** A weaker incentive property than SP. An agent cannot change the total probability of getting an object strictly preferred to `h` by shuffling their preferences above `h`.\n- **Upper Envy-Freeness (UEF):** A weaker fairness property than EF. Any two agents with identical upper contour sets for a house `h` must receive equal assignment probabilities for `h`.\n\n**Key Propositions and Logical Links:**\n- **Theorem 1:** For `N≥3`, there does not exist a mechanism that is simultaneously ExPE, SP, and EF.\n- **Lemma 1:** For `N≥3`, there does not exist a mechanism that is simultaneously ExPE, USP, and UEF.\n- **Logical Hierarchy:** Strategy-Proofness is a stronger condition than Upper Shuffle-Proofness (SP ⇒ USP), and Envy-Freeness is a stronger condition than Upper Envy-Freeness (EF ⇒ UEF).\n\n### Question\n\nBased on the provided information, which of the following statements correctly describe the logical relationship between Lemma 1 and Theorem 1? Select all that apply.", "Answer": ["C", "D"], "qid": "161", "question": "### Background\n\n**Research Question.** This problem investigates the fundamental incompatibility of efficiency, incentive compatibility, and fairness in object allocation mechanisms, focusing on how using weaker axioms can lead to stronger impossibility results.\n\n**Setting / Institutional Environment.** We consider mechanisms for allocating indivisible objects to `N≥3` agents. The focus is on the mutual compatibility of a set of desirable axiomatic properties.\n\n### Data / Model Specification\n\nThe analysis centers on two key propositions and the logical relationships between the axioms they employ.\n\n**Axioms and Definitions:**\n- **Ex-post Efficiency (ExPE):** The outcome must be a lottery over Pareto efficient deterministic assignments.\n- **Strategy-Proofness (SP):** Truthful reporting is a weakly dominant strategy for all agents.\n- **Envy-Freeness (EF):** Every agent weakly prefers their own probabilistic assignment to any other agent's.\n- **Upper Shuffle-Proofness (USP):** A weaker incentive property than SP. An agent cannot change the total probability of getting an object strictly preferred to `h` by shuffling their preferences above `h`.\n- **Upper Envy-Freeness (UEF):** A weaker fairness property than EF. Any two agents with identical upper contour sets for a house `h` must receive equal assignment probabilities for `h`.\n\n**Key Propositions and Logical Links:**\n- **Theorem 1:** For `N≥3`, there does not exist a mechanism that is simultaneously ExPE, SP, and EF.\n- **Lemma 1:** For `N≥3`, there does not exist a mechanism that is simultaneously ExPE, USP, and UEF.\n- **Logical Hierarchy:** Strategy-Proofness is a stronger condition than Upper Shuffle-Proofness (SP ⇒ USP), and Envy-Freeness is a stronger condition than Upper Envy-Freeness (EF ⇒ UEF).\n\n### Question\n\nBased on the provided information, which of the following statements correctly describe the logical relationship between Lemma 1 and Theorem 1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 162, "Question": "### Background\n\n**Research Question.** This problem addresses the identification of causal effects in a system where an individual's outcome and their social capital are simultaneously determined and influenced by peer group averages of both variables.\n\n**Setting / Institutional Environment.** The analysis uses a simultaneous equations linear-in-means model. This framework is appropriate when social capital is not a fixed characteristic but an active choice, co-determined with the main outcome of interest.\n\n**Variables & Parameters.**\n- `$\\omega_{i}$`, `$SC_i$`: Individual outcome and social capital choice for individual `i`.\n- `$\\mathbf{X}_{i}$`: An `r`-dim vector of individual-level controls.\n- `$\\mathbf{Y}_{g(i)}$`: An `s`-dim vector of group-level contextual controls.\n- `$E(\\omega_{g(i)}|F_{g(i)})$`, `$E(SC_{g(i)}|F_{g(i)})$`: Expected group averages of outcome and social capital (endogenous peer effects).\n- `$\\mathbf{X}_{g(i)}$`: Group average of `$\\mathbf{X}_{i}$`.\n- Unit of observation: Individual `i` in group `g(i)`.\n\n---\n\n### Data / Model Specification\n\nThe system is defined by two equations:\n\n  \n\\omega_{i} = k + \\mathbf{cX}_{i} + \\mathbf{dY}_{g(i)} + J_{1}E(\\omega_{g(i)}|F_{g(i)}) + J_{2}E(SC_{g(i)}|F_{g(i)}) + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nSC_{i} = \\bar{k} + \\bar{\\mathbf{c}}\\mathbf{X}_{i} + \\bar{\\mathbf{d}}\\mathbf{Y}_{g(i)} + \\bar{J}_{1}E(\\omega_{g(i)}|F_{g(i)}) + \\bar{J}_{2}E(SC_{g(i)}|F_{g(i)}) + \\eta_{i} \\quad \\text{(Eq. 2)}\n \n\nAccording to the paper, identification of the structural parameters in Eq. (1) requires that the dimension of the linear space spanned by `$(1, \\mathbf{X}_i, \\mathbf{X}_{g(i)}, \\mathbf{Y}_{g(i)})$` is at least `r+s+3`.\n\n---\n\n### Question\n\nBased on the model of endogenous social capital, which of the following statements are valid conclusions regarding the identification of the parameters in Eq. (1)? Select all that apply.", "Answer": ["A", "D"], "qid": "162", "question": "### Background\n\n**Research Question.** This problem addresses the identification of causal effects in a system where an individual's outcome and their social capital are simultaneously determined and influenced by peer group averages of both variables.\n\n**Setting / Institutional Environment.** The analysis uses a simultaneous equations linear-in-means model. This framework is appropriate when social capital is not a fixed characteristic but an active choice, co-determined with the main outcome of interest.\n\n**Variables & Parameters.**\n- `$\\omega_{i}$`, `$SC_i$`: Individual outcome and social capital choice for individual `i`.\n- `$\\mathbf{X}_{i}$`: An `r`-dim vector of individual-level controls.\n- `$\\mathbf{Y}_{g(i)}$`: An `s`-dim vector of group-level contextual controls.\n- `$E(\\omega_{g(i)}|F_{g(i)})$`, `$E(SC_{g(i)}|F_{g(i)})$`: Expected group averages of outcome and social capital (endogenous peer effects).\n- `$\\mathbf{X}_{g(i)}$`: Group average of `$\\mathbf{X}_{i}$`.\n- Unit of observation: Individual `i` in group `g(i)`.\n\n---\n\n### Data / Model Specification\n\nThe system is defined by two equations:\n\n  \n\\omega_{i} = k + \\mathbf{cX}_{i} + \\mathbf{dY}_{g(i)} + J_{1}E(\\omega_{g(i)}|F_{g(i)}) + J_{2}E(SC_{g(i)}|F_{g(i)}) + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nSC_{i} = \\bar{k} + \\bar{\\mathbf{c}}\\mathbf{X}_{i} + \\bar{\\mathbf{d}}\\mathbf{Y}_{g(i)} + \\bar{J}_{1}E(\\omega_{g(i)}|F_{g(i)}) + \\bar{J}_{2}E(SC_{g(i)}|F_{g(i)}) + \\eta_{i} \\quad \\text{(Eq. 2)}\n \n\nAccording to the paper, identification of the structural parameters in Eq. (1) requires that the dimension of the linear space spanned by `$(1, \\mathbf{X}_i, \\mathbf{X}_{g(i)}, \\mathbf{Y}_{g(i)})$` is at least `r+s+3`.\n\n---\n\n### Question\n\nBased on the model of endogenous social capital, which of the following statements are valid conclusions regarding the identification of the parameters in Eq. (1)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 163, "Question": "### Background\n\n**Research Question.** This problem examines the conditions required to identify the causal effect of a predetermined social capital measure when individual outcomes are also shaped by contextual and endogenous peer effects.\n\n**Setting / Institutional Environment.** The analysis uses a linear-in-means model of social interactions where social capital is treated as a predetermined, group-level characteristic.\n\n**Variables & Parameters.**\n- `$\\omega_{i}$`: Outcome for individual `i`.\n- `$\\mathbf{X}_{i}$`: An `r`-dimension vector of individual-level controls.\n- `$\\mathbf{Y}_{g(i)}$`: An `s`-dimension vector of group-level contextual controls.\n- `$SC_{g(i)}$`: A predetermined measure of social capital in group `g(i)`.\n- `$E(\\omega_{g(i)}|F_{g(i)})$`: The endogenous peer effect term.\n- `$\\mathbf{X}_{g(i)}$`: The group average of `$\\mathbf{X}_{i}$`.\n- Unit of observation: Individual `i` in group `g(i)`.\n\n---\n\n### Data / Model Specification\n\nThe structural model is:\n\n  \n\\omega_{i} = k + \\mathbf{cX}_{i} + \\mathbf{dY}_{g(i)} + J_{1}E(\\omega_{g(i)}|F_{g(i)}) + J_{2}SC_{g(i)} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n \n\nUnder rational expectations, the model has the following reduced form:\n\n  \n\\omega_{i} = \\frac{k}{1-J_{1}} + \\mathbf{cX}_{i} + \\frac{J_{1}\\mathbf{c}}{1-J_{1}}\\mathbf{X}_{g(i)} + \\frac{\\mathbf{d}}{1-J_{1}}\\mathbf{Y}_{g(i)} + \\frac{J_{2}}{1-J_{1}}SC_{g(i)} + \\varepsilon_{i} \\quad \\text{(Eq. 2)}\n \n\nThe paper states that identification of the structural parameters in Eq. (1) fails if `$\\mathbf{X}_{g(i)}$` is perfectly collinear with `$(1, \\mathbf{Y}_{g(i)}, SC_{g(i)})$`.\n\n---\n\n### Question\n\nSuppose the identification condition fails because the only individual-level variable in `$\\mathbf{X}_i$` has a group average `$\\mathbf{X}_{g(i)}$` that is already included as a contextual control in `$\\mathbf{Y}_{g(i)}$`. Which of the following conclusions are INCORRECT based on the provided model? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "163", "question": "### Background\n\n**Research Question.** This problem examines the conditions required to identify the causal effect of a predetermined social capital measure when individual outcomes are also shaped by contextual and endogenous peer effects.\n\n**Setting / Institutional Environment.** The analysis uses a linear-in-means model of social interactions where social capital is treated as a predetermined, group-level characteristic.\n\n**Variables & Parameters.**\n- `$\\omega_{i}$`: Outcome for individual `i`.\n- `$\\mathbf{X}_{i}$`: An `r`-dimension vector of individual-level controls.\n- `$\\mathbf{Y}_{g(i)}$`: An `s`-dimension vector of group-level contextual controls.\n- `$SC_{g(i)}$`: A predetermined measure of social capital in group `g(i)`.\n- `$E(\\omega_{g(i)}|F_{g(i)})$`: The endogenous peer effect term.\n- `$\\mathbf{X}_{g(i)}$`: The group average of `$\\mathbf{X}_{i}$`.\n- Unit of observation: Individual `i` in group `g(i)`.\n\n---\n\n### Data / Model Specification\n\nThe structural model is:\n\n  \n\\omega_{i} = k + \\mathbf{cX}_{i} + \\mathbf{dY}_{g(i)} + J_{1}E(\\omega_{g(i)}|F_{g(i)}) + J_{2}SC_{g(i)} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n \n\nUnder rational expectations, the model has the following reduced form:\n\n  \n\\omega_{i} = \\frac{k}{1-J_{1}} + \\mathbf{cX}_{i} + \\frac{J_{1}\\mathbf{c}}{1-J_{1}}\\mathbf{X}_{g(i)} + \\frac{\\mathbf{d}}{1-J_{1}}\\mathbf{Y}_{g(i)} + \\frac{J_{2}}{1-J_{1}}SC_{g(i)} + \\varepsilon_{i} \\quad \\text{(Eq. 2)}\n \n\nThe paper states that identification of the structural parameters in Eq. (1) fails if `$\\mathbf{X}_{g(i)}$` is perfectly collinear with `$(1, \\mathbf{Y}_{g(i)}, SC_{g(i)})$`.\n\n---\n\n### Question\n\nSuppose the identification condition fails because the only individual-level variable in `$\\mathbf{X}_i$` has a group average `$\\mathbf{X}_{g(i)}$` that is already included as a contextual control in `$\\mathbf{Y}_{g(i)}$`. Which of the following conclusions are INCORRECT based on the provided model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 164, "Question": "### Background\n\n**Research Question.** This problem analyzes the severe identification challenges that arise when attempting to estimate the causal effects of social capital using only aggregate (group-level) data.\n\n**Setting / Institutional Environment.** The analysis considers a system of two simultaneous equations for average group outcome and average group social capital. All data is at the group level `g`.\n\n**Variables & Parameters.**\n- `$\\omega_{g}$`, `$SC_g$`: Average outcome and social capital for group `g`.\n- `$\\mathbf{Y}_{g}$`: A vector of exogenous group-level characteristics.\n- Unit of observation: Group `g`.\n\n---\n\n### Data / Model Specification\n\nThe aggregated system is:\n\n  \n\\omega_{g} = k + \\mathbf{dY}_{g} + J_{1}\\omega_{g} + J_{2}SC_{g} + \\varepsilon_{g} \\quad \\text{(Eq. 1)}\n \n\n  \nSC_{g} = \\bar{k} + \\bar{\\mathbf{d}}\\mathbf{Y}_{g} + \\bar{J}_{1}\\omega_{g} + \\bar{J}_{2}SC_{g} + \\eta_{g} \\quad \\text{(Eq. 2)}\n \n\nThe paper establishes that to identify Eq. (1), one must rely on exclusion restrictions: exogenous variables `$\\mathbf{Y}_g$` that appear in the social capital equation (Eq. 2) but not the outcome equation (Eq. 1).\n\n---\n\n### Question\n\nA researcher studying cross-country growth (`$\\omega_g$`) and trust (`$SC_g$`) uses ethno-linguistic fractionalization (ELF) as an instrument for trust. The logic is that ELF affects trust (`$SC_g$`) but does not directly affect growth (`$\\omega_g$`). A critic argues the instrument is invalid because ELF *also* directly harms growth by fostering political instability. \n\nAssuming the critic is correct, which of the following statements accurately describe the econometric consequences? Select all that apply.\n\n**Assumptions:**\n- The true direct effect of ELF on growth is negative.\n- The effect of ELF on trust is negative.\n- The true effect of trust on growth (`$J_2$`) is positive.", "Answer": ["A", "C", "D"], "qid": "164", "question": "### Background\n\n**Research Question.** This problem analyzes the severe identification challenges that arise when attempting to estimate the causal effects of social capital using only aggregate (group-level) data.\n\n**Setting / Institutional Environment.** The analysis considers a system of two simultaneous equations for average group outcome and average group social capital. All data is at the group level `g`.\n\n**Variables & Parameters.**\n- `$\\omega_{g}$`, `$SC_g$`: Average outcome and social capital for group `g`.\n- `$\\mathbf{Y}_{g}$`: A vector of exogenous group-level characteristics.\n- Unit of observation: Group `g`.\n\n---\n\n### Data / Model Specification\n\nThe aggregated system is:\n\n  \n\\omega_{g} = k + \\mathbf{dY}_{g} + J_{1}\\omega_{g} + J_{2}SC_{g} + \\varepsilon_{g} \\quad \\text{(Eq. 1)}\n \n\n  \nSC_{g} = \\bar{k} + \\bar{\\mathbf{d}}\\mathbf{Y}_{g} + \\bar{J}_{1}\\omega_{g} + \\bar{J}_{2}SC_{g} + \\eta_{g} \\quad \\text{(Eq. 2)}\n \n\nThe paper establishes that to identify Eq. (1), one must rely on exclusion restrictions: exogenous variables `$\\mathbf{Y}_g$` that appear in the social capital equation (Eq. 2) but not the outcome equation (Eq. 1).\n\n---\n\n### Question\n\nA researcher studying cross-country growth (`$\\omega_g$`) and trust (`$SC_g$`) uses ethno-linguistic fractionalization (ELF) as an instrument for trust. The logic is that ELF affects trust (`$SC_g$`) but does not directly affect growth (`$\\omega_g$`). A critic argues the instrument is invalid because ELF *also* directly harms growth by fostering political instability. \n\nAssuming the critic is correct, which of the following statements accurately describe the econometric consequences? Select all that apply.\n\n**Assumptions:**\n- The true direct effect of ELF on growth is negative.\n- The effect of ELF on trust is negative.\n- The true effect of trust on growth (`$J_2$`) is positive.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 165, "Question": "### Background\n\n**Research Question.** This problem requires the interpretation of numerical results from a calibrated general equilibrium model to evaluate the causal effects of tax reform on aggregate efficiency and welfare inequality.\n\n**Setting / Institutional Environment.** The exercise compares a 'Status quo' tax system with capital (`\\tau_K`), labor (`\\tau_n`), and consumption (`\\tau_c`) taxes to four alternative revenue-neutral policies. The reforms gradually eliminate capital and labor taxes, replacing the revenue with a consumption tax. The model is calibrated to the US economy.\n\n**Variables & Parameters.**\n- `\\tau_n, \\tau_K, \\tau_c`: Tax rates on labor income, capital income, and consumption.\n- `\\alpha(p)/\\gamma(p)`: The ratio of the welfare return on human wealth to the welfare return on nonhuman wealth. An increase in this ratio, under certain conditions, signals a reduction in inequality.\n- `\\nu_r`: The utility of the representative agent, a measure of aggregate efficiency.\n- `\\lambda`: The consumption-equivalent welfare gain, representing the percentage increase in status-quo consumption needed to equal the welfare under the new policy.\n\n---\n\n### Data / Model Specification\n\nThe calibrated model produces the following results for different tax policies:\n\n**Table 1: Increasing the Tax Rate on Consumption**\n\n| Policy             | Status quo | Policy 1 | Policy 2 | Policy 3 | Policy 4 |\n|--------------------|------------|----------|----------|----------|----------|\n| `\\tau_n` (Labor Tax)   | 0.23       | 0.35     | 0.21     | 0.15     | 0        |\n| `\\tau_K` (Capital Tax)  | 0.50       | 0        | 0        | 0        | 0        |\n| `\\tau_c` (Cons. Tax)    | 0          | 0        | 0.14     | 0.18     | 0.29     |\n| `\\alpha(p)/\\gamma(p)`      | 3.7        | 2.9      | 3.6      | 3.7      | 4.8      |\n| `\\nu_r; \\lambda`         | 5.6; 0%    | 5.8; 2%  | 6.2; 6%  | 6.4; 7%  | 6.7; 10% |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid interpretations of the model's results?\n", "Answer": ["A", "C", "D"], "qid": "165", "question": "### Background\n\n**Research Question.** This problem requires the interpretation of numerical results from a calibrated general equilibrium model to evaluate the causal effects of tax reform on aggregate efficiency and welfare inequality.\n\n**Setting / Institutional Environment.** The exercise compares a 'Status quo' tax system with capital (`\\tau_K`), labor (`\\tau_n`), and consumption (`\\tau_c`) taxes to four alternative revenue-neutral policies. The reforms gradually eliminate capital and labor taxes, replacing the revenue with a consumption tax. The model is calibrated to the US economy.\n\n**Variables & Parameters.**\n- `\\tau_n, \\tau_K, \\tau_c`: Tax rates on labor income, capital income, and consumption.\n- `\\alpha(p)/\\gamma(p)`: The ratio of the welfare return on human wealth to the welfare return on nonhuman wealth. An increase in this ratio, under certain conditions, signals a reduction in inequality.\n- `\\nu_r`: The utility of the representative agent, a measure of aggregate efficiency.\n- `\\lambda`: The consumption-equivalent welfare gain, representing the percentage increase in status-quo consumption needed to equal the welfare under the new policy.\n\n---\n\n### Data / Model Specification\n\nThe calibrated model produces the following results for different tax policies:\n\n**Table 1: Increasing the Tax Rate on Consumption**\n\n| Policy             | Status quo | Policy 1 | Policy 2 | Policy 3 | Policy 4 |\n|--------------------|------------|----------|----------|----------|----------|\n| `\\tau_n` (Labor Tax)   | 0.23       | 0.35     | 0.21     | 0.15     | 0        |\n| `\\tau_K` (Capital Tax)  | 0.50       | 0        | 0        | 0        | 0        |\n| `\\tau_c` (Cons. Tax)    | 0          | 0        | 0.14     | 0.18     | 0.29     |\n| `\\alpha(p)/\\gamma(p)`      | 3.7        | 2.9      | 3.6      | 3.7      | 4.8      |\n| `\\nu_r; \\lambda`         | 5.6; 0%    | 5.8; 2%  | 6.2; 6%  | 6.4; 7%  | 6.7; 10% |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid interpretations of the model's results?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 166, "Question": "### Background\n\n**Research Question.** This problem explores the conditions under which different tax systems produce identical real economic outcomes and how this equivalence can be used to analyze tax policy.\n\n**Setting / Institutional Environment.** We analyze a general equilibrium model where the government can finance its expenditures using taxes on consumption (`\\tau_c`), labor income (`\\tau_n`), and a virtual lump-sum levy (`L`) on agents' initial wealth (`A_{i0}`).\n\n---\n\n### Data / Model Specification\n\nThe household's intertemporal budget constraint under a policy `f^A = (\\tau_c^A, \\tau_n^A, 0, 0, 0)` is:\n  \n\\sum_{t=0}^{\\infty}d_{t}(1+\\tau_{c}^{A})C_{i t}=\\sum_{t=0}^{\\infty}d_{t}(1-\\tau_{n}^{A})w_{t}E_{i}N_{i t}+(1+r_{0})A_{i0} \\quad \\text{(Eq. (1))}\n \n**Lemma:** Policy `f^A` is equivalent to a virtual policy `f^B = (0, \\tau_n^B, 0, 0, L)` where `1-L = 1/(1+\\tau_c^A)` and `1-\\tau_n^B = (1-\\tau_n^A)/(1+\\tau_c^A)`.\n\n**Definition:** A tax is non-distortionary if it does not alter any marginal decisions, such as the trade-off between labor and leisure or consumption and savings.\n\n---\n\nBased on the provided model and definitions, which of the following statements about tax equivalence and efficiency are correct?\n", "Answer": ["A", "B", "C"], "qid": "166", "question": "### Background\n\n**Research Question.** This problem explores the conditions under which different tax systems produce identical real economic outcomes and how this equivalence can be used to analyze tax policy.\n\n**Setting / Institutional Environment.** We analyze a general equilibrium model where the government can finance its expenditures using taxes on consumption (`\\tau_c`), labor income (`\\tau_n`), and a virtual lump-sum levy (`L`) on agents' initial wealth (`A_{i0}`).\n\n---\n\n### Data / Model Specification\n\nThe household's intertemporal budget constraint under a policy `f^A = (\\tau_c^A, \\tau_n^A, 0, 0, 0)` is:\n  \n\\sum_{t=0}^{\\infty}d_{t}(1+\\tau_{c}^{A})C_{i t}=\\sum_{t=0}^{\\infty}d_{t}(1-\\tau_{n}^{A})w_{t}E_{i}N_{i t}+(1+r_{0})A_{i0} \\quad \\text{(Eq. (1))}\n \n**Lemma:** Policy `f^A` is equivalent to a virtual policy `f^B = (0, \\tau_n^B, 0, 0, L)` where `1-L = 1/(1+\\tau_c^A)` and `1-\\tau_n^B = (1-\\tau_n^A)/(1+\\tau_c^A)`.\n\n**Definition:** A tax is non-distortionary if it does not alter any marginal decisions, such as the trade-off between labor and leisure or consumption and savings.\n\n---\n\nBased on the provided model and definitions, which of the following statements about tax equivalence and efficiency are correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 167, "Question": "### Background\n\n**Research Question.** This problem examines the paper's method for assessing the distributional consequences of a tax reform when households are heterogeneous in two dimensions: initial wealth and labor efficiency.\n\n**Setting / Institutional Environment.** Household welfare is expressed as an affine function of its characteristics. Inequality is measured by comparing the ratio of welfare between any two households.\n\n---\n\n### Data / Model Specification\n\nThe transformed indirect utility of household `i` is:\n  \n\\nu_{i}=\\alpha(p)E_{i}^{\\frac{\\varphi}{\\varphi-1}}+\\gamma(p)A_{i0} \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(p)E_{i}^{\\frac{\\varphi}{\\varphi-1}}` represents human wealth and `\\gamma(p)A_{i0}` represents nonhuman wealth.\n\n**Proposition:** A policy reform reduces inequality for a pair of agents `i` and `j` (where `\\nu_i < \\nu_j`) if the welfare ratio `\\nu_i / \\nu_j` increases. This occurs if two conditions hold:\n1. The relative return to human wealth increases: `\\alpha(p^1)/\\gamma(p^1) \\ge \\alpha(p^2)/\\gamma(p^2)`.\n2. The human-to-financial wealth ratio is decreasing in welfare: `E_{i}^{\\varphi/(\\varphi-1)}/A_{i0} \\ge E_{j}^{\\varphi/(\\varphi-1)}/A_{j0}`.\n\n---\n\nConsider a tax reform that increases the relative return to human wealth (`\\alpha(p)/\\gamma(p)`). According to the proposition and its underlying logic, which of the following scenarios would result in an **INCREASE** in inequality for the specific pair of households described?\n", "Answer": ["A", "B"], "qid": "167", "question": "### Background\n\n**Research Question.** This problem examines the paper's method for assessing the distributional consequences of a tax reform when households are heterogeneous in two dimensions: initial wealth and labor efficiency.\n\n**Setting / Institutional Environment.** Household welfare is expressed as an affine function of its characteristics. Inequality is measured by comparing the ratio of welfare between any two households.\n\n---\n\n### Data / Model Specification\n\nThe transformed indirect utility of household `i` is:\n  \n\\nu_{i}=\\alpha(p)E_{i}^{\\frac{\\varphi}{\\varphi-1}}+\\gamma(p)A_{i0} \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(p)E_{i}^{\\frac{\\varphi}{\\varphi-1}}` represents human wealth and `\\gamma(p)A_{i0}` represents nonhuman wealth.\n\n**Proposition:** A policy reform reduces inequality for a pair of agents `i` and `j` (where `\\nu_i < \\nu_j`) if the welfare ratio `\\nu_i / \\nu_j` increases. This occurs if two conditions hold:\n1. The relative return to human wealth increases: `\\alpha(p^1)/\\gamma(p^1) \\ge \\alpha(p^2)/\\gamma(p^2)`.\n2. The human-to-financial wealth ratio is decreasing in welfare: `E_{i}^{\\varphi/(\\varphi-1)}/A_{i0} \\ge E_{j}^{\\varphi/(\\varphi-1)}/A_{j0}`.\n\n---\n\nConsider a tax reform that increases the relative return to human wealth (`\\alpha(p)/\\gamma(p)`). According to the proposition and its underlying logic, which of the following scenarios would result in an **INCREASE** in inequality for the specific pair of households described?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 168, "Question": "### Background\n\n**Research Question.** This problem investigates the limits of Nash implementation for two agents when the choice rule is required to be Pareto efficient and the domain of preferences is unrestricted, a key negative result in mechanism design.\n\n**Setting / Institutional Environment.** A two-agent mechanism design environment where the domain of preferences is unrestricted, meaning any strict ordering of outcomes is a possible preference profile. This setting is a benchmark for applications in contracting and bargaining, where parties desire efficient outcomes under a wide range of possible future states.\n\n**Variables & Parameters.**\n- `I = {1, 2}`: A set of two agents.\n- `A`: A set of feasible outcomes.\n- `Θ`: The set of all possible preference profiles (unrestricted domain of strong orderings).\n- `f(θ)`: The choice rule, assumed to be Pareto efficient and non-dictatorial.\n- `L_i(a,θ)`: The lower contour set for agent `i` at `a` under `θ`.\n- `SU_i(a,θ)`: The strict upper contour set, defined as `A - L_i(a,θ)`.\n\n---\n\n### Data / Model Specification\n\n**Corollary 2.** A two-agent, Pareto efficient choice rule `f` with an unrestricted domain of strong orderings can be Nash implemented if and only if `f` is dictatorial.\n\n**Proof Sketch.** The proof proceeds by contradiction. It assumes `f` is non-dictatorial and Nash implementable, which implies the existence of `a ∈ f(θ)` and `b ∈ f(φ)` such that `SU_1(a,θ)` and `SU_2(b,φ)` are both non-empty. The first major step of the proof establishes a crucial lemma: these two strict upper contour sets must be disjoint, i.e., `SU_1(a,θ) ∩ SU_2(b,φ) = ∅`.\n\n---\n\n### Question\n\nGiven the disjoint sets lemma (`SU_1(a,θ) ∩ SU_2(b,φ) = ∅`), select all statements that represent a **valid and necessary step** in the final stage of the proof by contradiction that establishes Corollary 2.", "Answer": ["B", "C"], "qid": "168", "question": "### Background\n\n**Research Question.** This problem investigates the limits of Nash implementation for two agents when the choice rule is required to be Pareto efficient and the domain of preferences is unrestricted, a key negative result in mechanism design.\n\n**Setting / Institutional Environment.** A two-agent mechanism design environment where the domain of preferences is unrestricted, meaning any strict ordering of outcomes is a possible preference profile. This setting is a benchmark for applications in contracting and bargaining, where parties desire efficient outcomes under a wide range of possible future states.\n\n**Variables & Parameters.**\n- `I = {1, 2}`: A set of two agents.\n- `A`: A set of feasible outcomes.\n- `Θ`: The set of all possible preference profiles (unrestricted domain of strong orderings).\n- `f(θ)`: The choice rule, assumed to be Pareto efficient and non-dictatorial.\n- `L_i(a,θ)`: The lower contour set for agent `i` at `a` under `θ`.\n- `SU_i(a,θ)`: The strict upper contour set, defined as `A - L_i(a,θ)`.\n\n---\n\n### Data / Model Specification\n\n**Corollary 2.** A two-agent, Pareto efficient choice rule `f` with an unrestricted domain of strong orderings can be Nash implemented if and only if `f` is dictatorial.\n\n**Proof Sketch.** The proof proceeds by contradiction. It assumes `f` is non-dictatorial and Nash implementable, which implies the existence of `a ∈ f(θ)` and `b ∈ f(φ)` such that `SU_1(a,θ)` and `SU_2(b,φ)` are both non-empty. The first major step of the proof establishes a crucial lemma: these two strict upper contour sets must be disjoint, i.e., `SU_1(a,θ) ∩ SU_2(b,φ) = ∅`.\n\n---\n\n### Question\n\nGiven the disjoint sets lemma (`SU_1(a,θ) ∩ SU_2(b,φ) = ∅`), select all statements that represent a **valid and necessary step** in the final stage of the proof by contradiction that establishes Corollary 2.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 169, "Question": "### Background\n\n**Research Question.** This problem explores the full characterization of Nash-implementable choice rules for three or more agents, bridging the gap between the necessary and sufficient conditions identified in Maskin's seminal work.\n\n**Setting / Institutional Environment.** We consider a standard complete-information mechanism design setting with `n ≥ 3` agents. The analysis involves defining a new condition, Condition μ, and showing its relationship to both Nash implementation and Maskin's original conditions.\n\n**Variables & Parameters.**\n- `I = {1, ..., n}`: A finite set of agents, with `n ≥ 3`.\n- `A`: A set of feasible outcomes.\n- `f(θ)`: The choice rule to be implemented.\n- `B`: The range of a mechanism.\n- `C_i(a,θ)`: The set of outcomes agent `i` can unilaterally achieve.\n- `L_i(a,θ)`: The lower contour set for agent `i` at `a` under `θ`.\n- `M_i(C,θ)`: The set of maximal elements in a set `C` for agent `i` under profile `θ`.\n\n---\n\n### Data / Model Specification\n\n**Definition 1 (Monotonicity).** A choice rule `f` is monotonic if for all `θ, θ*` and `a ∈ f(θ)`, `L_i(a,θ) ⊂ L_i(a,θ*)` for all `i` implies `a ∈ f(θ*)`.\n\n**Definition 2 (No Veto Power - NVP).** `f` satisfies NVP if for any `i`, `a ∈ ∩_{j≠i} M_j(A,θ)` implies `a ∈ f(θ)`.\n\n**Condition μ.** A choice rule `f` satisfies Condition μ if there exist sets `B` and `C_i(a,θ)` such that for all `θ*`:\n(i) If `a ∈ ∩_{i∈I} M_i(C_i(a,θ), θ*)`, then `a ∈ f(θ*)`.\n(ii) If `c ∈ M_i(C_i(a,θ), θ*) ∩ [∩_{j≠i} M_j(B, θ*)]` for some `i`, then `c ∈ f(θ*)`.\n(iii) If `d ∈ ∩_{i∈I} M_i(B, θ*)`, then `d ∈ f(θ*)`.\n\n**Theorem 1.** For `n ≥ 3`, `f` is Nash implementable if and only if it satisfies Condition μ.\n\n---\n\n### Question\n\nTo show that Condition μ is strictly weaker than 'Monotonicity + NVP', the paper proves that the latter implies the former. This proof makes the strategic choice to set `B = A` and `C_i(a,θ) = L_i(a,θ)`. Based on this setup, which of the following statements about the relationship between the conditions are correct?", "Answer": ["A", "B"], "qid": "169", "question": "### Background\n\n**Research Question.** This problem explores the full characterization of Nash-implementable choice rules for three or more agents, bridging the gap between the necessary and sufficient conditions identified in Maskin's seminal work.\n\n**Setting / Institutional Environment.** We consider a standard complete-information mechanism design setting with `n ≥ 3` agents. The analysis involves defining a new condition, Condition μ, and showing its relationship to both Nash implementation and Maskin's original conditions.\n\n**Variables & Parameters.**\n- `I = {1, ..., n}`: A finite set of agents, with `n ≥ 3`.\n- `A`: A set of feasible outcomes.\n- `f(θ)`: The choice rule to be implemented.\n- `B`: The range of a mechanism.\n- `C_i(a,θ)`: The set of outcomes agent `i` can unilaterally achieve.\n- `L_i(a,θ)`: The lower contour set for agent `i` at `a` under `θ`.\n- `M_i(C,θ)`: The set of maximal elements in a set `C` for agent `i` under profile `θ`.\n\n---\n\n### Data / Model Specification\n\n**Definition 1 (Monotonicity).** A choice rule `f` is monotonic if for all `θ, θ*` and `a ∈ f(θ)`, `L_i(a,θ) ⊂ L_i(a,θ*)` for all `i` implies `a ∈ f(θ*)`.\n\n**Definition 2 (No Veto Power - NVP).** `f` satisfies NVP if for any `i`, `a ∈ ∩_{j≠i} M_j(A,θ)` implies `a ∈ f(θ)`.\n\n**Condition μ.** A choice rule `f` satisfies Condition μ if there exist sets `B` and `C_i(a,θ)` such that for all `θ*`:\n(i) If `a ∈ ∩_{i∈I} M_i(C_i(a,θ), θ*)`, then `a ∈ f(θ*)`.\n(ii) If `c ∈ M_i(C_i(a,θ), θ*) ∩ [∩_{j≠i} M_j(B, θ*)]` for some `i`, then `c ∈ f(θ*)`.\n(iii) If `d ∈ ∩_{i∈I} M_i(B, θ*)`, then `d ∈ f(θ*)`.\n\n**Theorem 1.** For `n ≥ 3`, `f` is Nash implementable if and only if it satisfies Condition μ.\n\n---\n\n### Question\n\nTo show that Condition μ is strictly weaker than 'Monotonicity + NVP', the paper proves that the latter implies the former. This proof makes the strategic choice to set `B = A` and `C_i(a,θ) = L_i(a,θ)`. Based on this setup, which of the following statements about the relationship between the conditions are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 170, "Question": "### Background\n\n**Research Question.** This problem explores two distinct methods for circumventing the general impossibility of non-dictatorial, efficient Nash implementation for two agents by imposing plausible restrictions on the economic environment.\n\n**Setting / Institutional Environment.** A two-agent mechanism design environment (`n=2`). The analysis focuses on two separate sets of assumptions that restore the possibility of implementation.\n\n**Variables & Parameters.**\n- `range(f)`: The set of all outcomes selected by `f` for at least one profile `θ`.\n- `SL_i(a,θ)`: The strict lower contour set for agent `i` at `a` under `θ`.\n\n---\n\n### Data / Model Specification\n\n**The Challenge.** For `n=2`, Nash implementation requires satisfying Condition μ2, which includes a difficult 'off-diagonal' check (μ2(iv)) to rule out unwanted equilibria.\n\n**Approach 1: The 'Bad Outcome' Method**\n- **Definition (Bad Outcome).** An outcome `z` is a 'bad outcome' if it is strictly worse for both agents than any outcome in `range(f)`.\n- **Result (Corollary 3).** If a bad outcome exists, a choice rule `f` satisfying Monotonicity and Restricted Veto Power (RVP) is Nash implementable.\n\n**Approach 2: The 'Economic Environment' Method**\n- **Assumption E.** Preferences are well-behaved (e.g., `M_i(SL_i(a,θ), θ*) = ∅`).\n- **Definition (Nonempty Lower Intersection - NLI).** For any `a ∈ f(θ)` and `b ∈ f(φ)`, `SL_1(a,θ) ∩ SL_2(b,φ) ≠ ∅`.\n- **Result (Corollary 4).** Under Assumption E, a choice rule `f` satisfying Monotonicity and NLI is Nash implementable.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements accurately describe how these two approaches overcome the challenges of two-agent Nash implementation, particularly the off-diagonal check in Condition μ2(iv)?", "Answer": ["A", "D"], "qid": "170", "question": "### Background\n\n**Research Question.** This problem explores two distinct methods for circumventing the general impossibility of non-dictatorial, efficient Nash implementation for two agents by imposing plausible restrictions on the economic environment.\n\n**Setting / Institutional Environment.** A two-agent mechanism design environment (`n=2`). The analysis focuses on two separate sets of assumptions that restore the possibility of implementation.\n\n**Variables & Parameters.**\n- `range(f)`: The set of all outcomes selected by `f` for at least one profile `θ`.\n- `SL_i(a,θ)`: The strict lower contour set for agent `i` at `a` under `θ`.\n\n---\n\n### Data / Model Specification\n\n**The Challenge.** For `n=2`, Nash implementation requires satisfying Condition μ2, which includes a difficult 'off-diagonal' check (μ2(iv)) to rule out unwanted equilibria.\n\n**Approach 1: The 'Bad Outcome' Method**\n- **Definition (Bad Outcome).** An outcome `z` is a 'bad outcome' if it is strictly worse for both agents than any outcome in `range(f)`.\n- **Result (Corollary 3).** If a bad outcome exists, a choice rule `f` satisfying Monotonicity and Restricted Veto Power (RVP) is Nash implementable.\n\n**Approach 2: The 'Economic Environment' Method**\n- **Assumption E.** Preferences are well-behaved (e.g., `M_i(SL_i(a,θ), θ*) = ∅`).\n- **Definition (Nonempty Lower Intersection - NLI).** For any `a ∈ f(θ)` and `b ∈ f(φ)`, `SL_1(a,θ) ∩ SL_2(b,φ) ≠ ∅`.\n- **Result (Corollary 4).** Under Assumption E, a choice rule `f` satisfying Monotonicity and NLI is Nash implementable.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements accurately describe how these two approaches overcome the challenges of two-agent Nash implementation, particularly the off-diagonal check in Condition μ2(iv)?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 171, "Question": "### Background\n\nA simulation experiment combines previously estimated relationships for manufacturing share, sectoral composition, and end-of-pipe (EOP) intensity to project total industrial water pollution (measured as Biological Oxygen Demand, BOD) across a range of per capita income levels. The goal is to test the Environmental Kuznets Curve (EKC) hypothesis, which posits an inverted-U relationship between pollution and income.\n\n### Data / Model Specification\n\nThe underlying decomposition is: Total Pollution = (Total Output) × (Mfg. Share) × (Compositional Intensity) × (EOP Intensity). Total output is proxied by per capita income.\n\n**Table 1: Industrial Pollution and Economic Development Simulation**\n\n| Income (USD) | Mfg. share | BOD intensity | EOP intensity | Total BOD | Variable share | Variable BOD | Variable EOP |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 500 | 11.0 | 5.4 | 1.00 | 100 | 100 | 100 | 100 |\n| 2500 | 16.9 | 4.6 | 0.25 | 167 | 771 | 428 | 127 |\n| 12,000 | 22.6 | 4.2 | 0.07 | 255 | 4953 | 1859 | 160 |\n| 20,000 | 19.5 | 4.3 | 0.04 | 249 | 7904 | 3559 | 175 |\n\n*Note: `Total BOD` is an index of simulated total pollution. `Variable share`, `Variable BOD`, and `Variable EOP` are counterfactual pollution indices where only the named factor (and scale) is allowed to vary, holding the other two constant at their initial levels.*\n\n---\n\nBased on the simulation results in Table 1, which of the following statements are valid interpretations of the relationship between industrial water pollution and economic development?\n", "Answer": ["A", "B", "C"], "qid": "171", "question": "### Background\n\nA simulation experiment combines previously estimated relationships for manufacturing share, sectoral composition, and end-of-pipe (EOP) intensity to project total industrial water pollution (measured as Biological Oxygen Demand, BOD) across a range of per capita income levels. The goal is to test the Environmental Kuznets Curve (EKC) hypothesis, which posits an inverted-U relationship between pollution and income.\n\n### Data / Model Specification\n\nThe underlying decomposition is: Total Pollution = (Total Output) × (Mfg. Share) × (Compositional Intensity) × (EOP Intensity). Total output is proxied by per capita income.\n\n**Table 1: Industrial Pollution and Economic Development Simulation**\n\n| Income (USD) | Mfg. share | BOD intensity | EOP intensity | Total BOD | Variable share | Variable BOD | Variable EOP |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 500 | 11.0 | 5.4 | 1.00 | 100 | 100 | 100 | 100 |\n| 2500 | 16.9 | 4.6 | 0.25 | 167 | 771 | 428 | 127 |\n| 12,000 | 22.6 | 4.2 | 0.07 | 255 | 4953 | 1859 | 160 |\n| 20,000 | 19.5 | 4.3 | 0.04 | 249 | 7904 | 3559 | 175 |\n\n*Note: `Total BOD` is an index of simulated total pollution. `Variable share`, `Variable BOD`, and `Variable EOP` are counterfactual pollution indices where only the named factor (and scale) is allowed to vary, holding the other two constant at their initial levels.*\n\n---\n\nBased on the simulation results in Table 1, which of the following statements are valid interpretations of the relationship between industrial water pollution and economic development?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA simulation experiment combines p...", "model_answer": "This is a detailed response to the question about ### Background\n\nA simulation experiment combines p...", "error": null}}
{"ID": 172, "Question": "### Background\n\nThe paper uses cross-country data to estimate reduced-form models for the pollution intensity (`P/Q`) and labor intensity (`L/Q`) of industrial production. The goal is to understand the overall relationship between these intensities and economic development, as measured by per capita income (`y`).\n\n### Data / Model Specification\n\nThe estimated models are:\n\n  \n\\ln(P_{jk}/Q_{jk}) = \\dots + \\phi_{y} \\ln y_{j} + \\varepsilon_{jk} \n\\quad \\text{(Eq. (1))}\n \n\n  \n\\ln(L_{jk}/Q_{jk}) = \\dots + \\gamma_{y} \\ln y_{j} + \\nu_{jk} \n\\quad \\text{(Eq. (2))}\n \n\n**Table 1: Reduced-Form Regression Results**\n\n| Dependent variable: | `ln(Pollution/output)` | `ln(Labor/output)` | `ln(Pollution/labor)` |\n| :--- | :--- | :--- | :--- |\n| **Independent variables** | **Coefficient (t-stat)** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| Log income (`y`) | -0.875 (-3.26) | -1.003 (-17.041) | 0.120 (0.449) |\n\n---\n\nBased on the regression results in Table 1, which of the following conclusions are statistically supported?\n", "Answer": ["A", "B", "C"], "qid": "172", "question": "### Background\n\nThe paper uses cross-country data to estimate reduced-form models for the pollution intensity (`P/Q`) and labor intensity (`L/Q`) of industrial production. The goal is to understand the overall relationship between these intensities and economic development, as measured by per capita income (`y`).\n\n### Data / Model Specification\n\nThe estimated models are:\n\n  \n\\ln(P_{jk}/Q_{jk}) = \\dots + \\phi_{y} \\ln y_{j} + \\varepsilon_{jk} \n\\quad \\text{(Eq. (1))}\n \n\n  \n\\ln(L_{jk}/Q_{jk}) = \\dots + \\gamma_{y} \\ln y_{j} + \\nu_{jk} \n\\quad \\text{(Eq. (2))}\n \n\n**Table 1: Reduced-Form Regression Results**\n\n| Dependent variable: | `ln(Pollution/output)` | `ln(Labor/output)` | `ln(Pollution/labor)` |\n| :--- | :--- | :--- | :--- |\n| **Independent variables** | **Coefficient (t-stat)** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| Log income (`y`) | -0.875 (-3.26) | -1.003 (-17.041) | 0.120 (0.449) |\n\n---\n\nBased on the regression results in Table 1, which of the following conclusions are statistically supported?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper uses cross-country data ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper uses cross-country data ...", "error": null}}
{"ID": 173, "Question": "### Background\n\nThe paper investigates the causal channels through which economic development affects industrial pollution intensity (`P/Q`). Two key channels are rising wages (`W_L`) and stricter environmental regulation (`R`). However, since both wages and regulation tend to increase with per capita income, it is econometrically challenging to separate their individual effects.\n\n### Data / Model Specification\n\nConsider the following regression results for pollution intensity, `ln(P/Q)`:\n\n*   **Model A:** When `ln(W_L)` is the only key regressor, its coefficient is -1.211 and highly significant.\n*   **Model B:** When `ln(R)` is the only key regressor, its coefficient is -4.885 and highly significant.\n*   **Model C:** When both `ln(W_L)` and `ln(R)` are included, the coefficient on `ln(W_L)` remains significant, while the coefficient on `ln(R)` becomes statistically insignificant.\n\nThe paper states that `ln(W_L)` and `ln(R)` are highly collinear.\n\n---\n\nWhich of the following statements represent valid econometric interpretations or conclusions based on this information?\n", "Answer": ["A", "D"], "qid": "173", "question": "### Background\n\nThe paper investigates the causal channels through which economic development affects industrial pollution intensity (`P/Q`). Two key channels are rising wages (`W_L`) and stricter environmental regulation (`R`). However, since both wages and regulation tend to increase with per capita income, it is econometrically challenging to separate their individual effects.\n\n### Data / Model Specification\n\nConsider the following regression results for pollution intensity, `ln(P/Q)`:\n\n*   **Model A:** When `ln(W_L)` is the only key regressor, its coefficient is -1.211 and highly significant.\n*   **Model B:** When `ln(R)` is the only key regressor, its coefficient is -4.885 and highly significant.\n*   **Model C:** When both `ln(W_L)` and `ln(R)` are included, the coefficient on `ln(W_L)` remains significant, while the coefficient on `ln(R)` becomes statistically insignificant.\n\nThe paper states that `ln(W_L)` and `ln(R)` are highly collinear.\n\n---\n\nWhich of the following statements represent valid econometric interpretations or conclusions based on this information?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper investigates the causal ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper investigates the causal ...", "error": null}}
{"ID": 174, "Question": "### Background\n\n**Research Question.** This problem examines the strategic structure of a high-stakes, one-shot prisoner's dilemma game, focusing on individual rationality and the potential role of social preferences.\n\n**Setting / Institutional Environment.** In the final stage of the game show *Friend or Foe?*, a two-person team that has accumulated a joint trust fund, *W* > 0, must decide how to divide it. Each player simultaneously and secretly chooses to either cooperate ('Friend') or defect ('Foe').\n\n---\n\n### Data / Model Specification\n\nThe monetary payoff structure for Player *i*, *π_i*, is as follows:\n1.  **Friend-Friend:** If both players choose 'Friend', *π_i* = *W*/2.\n2.  **Friend-Foe:** If Player *i* chooses 'Friend' and their partner chooses 'Foe', *π_i* = 0.\n3.  **Foe-Friend:** If Player *i* chooses 'Foe' and their partner chooses 'Friend', *π_i* = *W*.\n4.  **Foe-Foe:** If both players choose 'Foe', *π_i* = 0.\n\nNow consider an extension where players have Fehr-Schmidt-type inequality aversion. A player's utility is given by:\n\n  \nU_i(\\pi_i, \\pi_j) = \\pi_i - \\alpha \\cdot \\max(\\pi_j - \\pi_i, 0) - \\beta \\cdot \\max(\\pi_i - \\pi_j, 0) \\quad \\text{(Eq. 1)}\n \n\nwhere *π_i* and *π_j* are the monetary payoffs, *α* represents disutility from disadvantageous inequality ('envy'), and *β* represents disutility from advantageous inequality ('guilt'), with 0 ≤ *β* < 1 and *α* ≥ *β*.\n\n---\n\n### Question\n\nBased on the provided game structure and utility model, select all of the following statements that are correct.", "Answer": ["B", "C"], "qid": "174", "question": "### Background\n\n**Research Question.** This problem examines the strategic structure of a high-stakes, one-shot prisoner's dilemma game, focusing on individual rationality and the potential role of social preferences.\n\n**Setting / Institutional Environment.** In the final stage of the game show *Friend or Foe?*, a two-person team that has accumulated a joint trust fund, *W* > 0, must decide how to divide it. Each player simultaneously and secretly chooses to either cooperate ('Friend') or defect ('Foe').\n\n---\n\n### Data / Model Specification\n\nThe monetary payoff structure for Player *i*, *π_i*, is as follows:\n1.  **Friend-Friend:** If both players choose 'Friend', *π_i* = *W*/2.\n2.  **Friend-Foe:** If Player *i* chooses 'Friend' and their partner chooses 'Foe', *π_i* = 0.\n3.  **Foe-Friend:** If Player *i* chooses 'Foe' and their partner chooses 'Friend', *π_i* = *W*.\n4.  **Foe-Foe:** If both players choose 'Foe', *π_i* = 0.\n\nNow consider an extension where players have Fehr-Schmidt-type inequality aversion. A player's utility is given by:\n\n  \nU_i(\\pi_i, \\pi_j) = \\pi_i - \\alpha \\cdot \\max(\\pi_j - \\pi_i, 0) - \\beta \\cdot \\max(\\pi_i - \\pi_j, 0) \\quad \\text{(Eq. 1)}\n \n\nwhere *π_i* and *π_j* are the monetary payoffs, *α* represents disutility from disadvantageous inequality ('envy'), and *β* represents disutility from advantageous inequality ('guilt'), with 0 ≤ *β* < 1 and *α* ≥ *β*.\n\n---\n\n### Question\n\nBased on the provided game structure and utility model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 175, "Question": "### Background\n\n**Research Question.** This problem examines the strategic structure of a high-stakes, one-shot prisoner's dilemma game, focusing on individual rationality and the potential role of social preferences.\n\n**Setting / Institutional Environment.** In the final stage of the game show *Friend or Foe?*, a two-person team that has accumulated a joint trust fund, *W* > 0, must decide how to divide it. Each player simultaneously and secretly chooses to either cooperate ('Friend') or defect ('Foe').\n\n---\n\n### Data / Model Specification\n\nConsider players who have Fehr-Schmidt-type inequality aversion. A player's utility is given by:\n\n  \nU_i(\\pi_i, \\pi_j) = \\pi_i - \\alpha \\cdot \\max(\\pi_j - \\pi_i, 0) - \\beta \\cdot \\max(\\pi_i - \\pi_j, 0) \\quad \\text{(Eq. 1)}\n \n\nwhere *π_i* and *π_j* are the monetary payoffs, *α* represents disutility from disadvantageous inequality ('envy'), and *β* represents disutility from advantageous inequality ('guilt'), with 0 ≤ *β* < 1 and *α* ≥ *β*. Suppose a player believes their partner will cooperate with probability *p*.\n\n---\n\n### Question\n\nSelect all statements that are INCORRECT descriptions of a player's strategic decision-making under uncertainty, according to the utility model in Eq. (1).", "Answer": ["A", "B", "C"], "qid": "175", "question": "### Background\n\n**Research Question.** This problem examines the strategic structure of a high-stakes, one-shot prisoner's dilemma game, focusing on individual rationality and the potential role of social preferences.\n\n**Setting / Institutional Environment.** In the final stage of the game show *Friend or Foe?*, a two-person team that has accumulated a joint trust fund, *W* > 0, must decide how to divide it. Each player simultaneously and secretly chooses to either cooperate ('Friend') or defect ('Foe').\n\n---\n\n### Data / Model Specification\n\nConsider players who have Fehr-Schmidt-type inequality aversion. A player's utility is given by:\n\n  \nU_i(\\pi_i, \\pi_j) = \\pi_i - \\alpha \\cdot \\max(\\pi_j - \\pi_i, 0) - \\beta \\cdot \\max(\\pi_i - \\pi_j, 0) \\quad \\text{(Eq. 1)}\n \n\nwhere *π_i* and *π_j* are the monetary payoffs, *α* represents disutility from disadvantageous inequality ('envy'), and *β* represents disutility from advantageous inequality ('guilt'), with 0 ≤ *β* < 1 and *α* ≥ *β*. Suppose a player believes their partner will cooperate with probability *p*.\n\n---\n\n### Question\n\nSelect all statements that are INCORRECT descriptions of a player's strategic decision-making under uncertainty, according to the utility model in Eq. (1).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 176, "Question": "### Background\n\nA researcher models a T-period game where a seller sets linear prices and a buyer with private information about their demand intercept (`a_L` or `a_H`) chooses a quantity. The analysis focuses on a perfect Bayesian equilibrium (PBE) where the buyer's type is revealed in every period.\n\n### Data / Model Specification\n\nIn this separating PBE, the low-type buyer distorts their demand downwards to prevent the high-type buyer from mimicking them. The low-type's 'effective' demand intercept in a period `t` (with `t` periods remaining) is `a_{Lt}`. The high-type's demand is `q_{Ht} = a_H - p_t`, while the low-type's is `q_{Lt} = a_{Lt} - p_t`. The value of `a_{Lt}` is determined by the recursive formula (for `t ≥ 2`):\n\n  \na_{Lt} = a_H - \\frac{1}{2}\\sqrt{\\delta(a_H - a_{L,t-1})(3a_H - a_{L,t-1})} \\quad \\text{(Eq. (1))}\n \n\nwhere `δ` is the common discount factor and `a_{L1} = a_L`.\n\n### Question\n\nBased on the economic logic of this separating equilibrium and the provided formula, which of the following statements are correct interpretations or consequences of the model?\n\nSelect all that apply.", "Answer": ["A", "C", "D"], "qid": "176", "question": "### Background\n\nA researcher models a T-period game where a seller sets linear prices and a buyer with private information about their demand intercept (`a_L` or `a_H`) chooses a quantity. The analysis focuses on a perfect Bayesian equilibrium (PBE) where the buyer's type is revealed in every period.\n\n### Data / Model Specification\n\nIn this separating PBE, the low-type buyer distorts their demand downwards to prevent the high-type buyer from mimicking them. The low-type's 'effective' demand intercept in a period `t` (with `t` periods remaining) is `a_{Lt}`. The high-type's demand is `q_{Ht} = a_H - p_t`, while the low-type's is `q_{Lt} = a_{Lt} - p_t`. The value of `a_{Lt}` is determined by the recursive formula (for `t ≥ 2`):\n\n  \na_{Lt} = a_H - \\frac{1}{2}\\sqrt{\\delta(a_H - a_{L,t-1})(3a_H - a_{L,t-1})} \\quad \\text{(Eq. (1))}\n \n\nwhere `δ` is the common discount factor and `a_{L1} = a_L`.\n\n### Question\n\nBased on the economic logic of this separating equilibrium and the provided formula, which of the following statements are correct interpretations or consequences of the model?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA researcher models a T-period gam...", "model_answer": "This is a detailed response to the question about ### Background\n\nA researcher models a T-period gam...", "error": null}}
{"ID": 177, "Question": "### Background\n\nA researcher models a dynamic monopoly pricing game with a seller and a buyer of a privately known type (`a_L` or `a_H`). The paper analyzes a separating equilibrium where the low-type buyer signals their type by distorting their demand. This question compares the welfare outcomes of this Asymmetric Information (AI) equilibrium to a Complete Information (CI) benchmark where the buyer's type is common knowledge.\n\n### Data / Model Specification\n\n- **Utility:** A buyer's per-period utility from consuming quantity `q` at price `p` is `(a_i-p)^2 - (a_i-p-q)^2`.\n- **AI Equilibrium:** A seller who is certain the buyer is a low type (`μ=0`) in a period `t` (with `t` periods remaining) sets the price `p_t^{AI} = a_{Lt}/2`. The low type chooses quantity `q_{Lt}^{AI} = a_{Lt}/2`.\n- **CI Benchmark:** A seller who knows the buyer is a low type sets the price `p^{CI} = a_L/2`. The low type chooses quantity `q_L^{CI} = a_L/2`.\n\nIn the AI equilibrium, `a_{Lt} < a_L` for `t>1`.\n\n### Question\n\nIn this model, which of the following are valid reasons why a buyer (either high or low type) might be strictly better off under Asymmetric Information (AI) compared to the Complete Information (CI) benchmark?\n\nSelect all that apply.", "Answer": ["A", "B", "C"], "qid": "177", "question": "### Background\n\nA researcher models a dynamic monopoly pricing game with a seller and a buyer of a privately known type (`a_L` or `a_H`). The paper analyzes a separating equilibrium where the low-type buyer signals their type by distorting their demand. This question compares the welfare outcomes of this Asymmetric Information (AI) equilibrium to a Complete Information (CI) benchmark where the buyer's type is common knowledge.\n\n### Data / Model Specification\n\n- **Utility:** A buyer's per-period utility from consuming quantity `q` at price `p` is `(a_i-p)^2 - (a_i-p-q)^2`.\n- **AI Equilibrium:** A seller who is certain the buyer is a low type (`μ=0`) in a period `t` (with `t` periods remaining) sets the price `p_t^{AI} = a_{Lt}/2`. The low type chooses quantity `q_{Lt}^{AI} = a_{Lt}/2`.\n- **CI Benchmark:** A seller who knows the buyer is a low type sets the price `p^{CI} = a_L/2`. The low type chooses quantity `q_L^{CI} = a_L/2`.\n\nIn the AI equilibrium, `a_{Lt} < a_L` for `t>1`.\n\n### Question\n\nIn this model, which of the following are valid reasons why a buyer (either high or low type) might be strictly better off under Asymmetric Information (AI) compared to the Complete Information (CI) benchmark?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA researcher models a dynamic mono...", "model_answer": "This is a detailed response to the question about ### Background\n\nA researcher models a dynamic mono...", "error": null}}
{"ID": 178, "Question": "### Background\n\n**Research Question.** This problem examines the econometric strategy used to identify the causal effect of trade liberalization on economic and social outcomes. The core of the strategy is a \"shift-share\" or Bartik-style research design.\n\n**Setting / Institutional Environment.** The study leverages Cambodia's 2004 accession to the WTO, which involved large, heterogeneous tariff reductions across industries. The identification strategy exploits the fact that districts had different pre-existing industrial compositions, leading to differential local exposure to these national tariff changes.\n\n### Data / Model Specification\n\nThe district-level exposure to trade liberalization, `Tariff_dt`, is constructed as a weighted average of national industry-level tariffs, where the weights are the district's pre-liberalization (1998) industry employment shares:\n  \nTariff_{dt} = \\sum_{i} Empshare_{id}^{1998} \\times Tariff_{it} \\quad \\text{(Eq. (1))}\n \nwhere `i` is industry, `d` is district, and `t` is time.\n\nFor two-period panel data (e.g., 1998 and 2008 Census), the model is:\n  \ny_{jdt} = \\alpha + \\beta Tariff_{dt} + \\theta X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\lambda \\mu_{t} \\Delta y_{d,pre} + \\epsilon_{jdt} \\quad \\text{(Eq. (2))}\n \nwhere `y_jdt` is an outcome for individual `j`, `X_jdt` are individual controls, `μ_t` are year fixed effects, `γ_d` are district fixed effects, and `Δy_d,pre` is the district-level pre-liberalization trend in the outcome variable.\n\nFor multi-period panel data (e.g., DHS surveys), the model is modified to include district-specific linear time trends (`δ_d t`):\n  \ny_{jdt} = \\alpha' + \\beta' Tariff_{dt} + \\theta' X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\delta_{d}t + \\epsilon_{jdt} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nBased on the provided model specifications, select all statements that accurately describe the identification strategy.", "Answer": ["A", "C"], "qid": "178", "question": "### Background\n\n**Research Question.** This problem examines the econometric strategy used to identify the causal effect of trade liberalization on economic and social outcomes. The core of the strategy is a \"shift-share\" or Bartik-style research design.\n\n**Setting / Institutional Environment.** The study leverages Cambodia's 2004 accession to the WTO, which involved large, heterogeneous tariff reductions across industries. The identification strategy exploits the fact that districts had different pre-existing industrial compositions, leading to differential local exposure to these national tariff changes.\n\n### Data / Model Specification\n\nThe district-level exposure to trade liberalization, `Tariff_dt`, is constructed as a weighted average of national industry-level tariffs, where the weights are the district's pre-liberalization (1998) industry employment shares:\n  \nTariff_{dt} = \\sum_{i} Empshare_{id}^{1998} \\times Tariff_{it} \\quad \\text{(Eq. (1))}\n \nwhere `i` is industry, `d` is district, and `t` is time.\n\nFor two-period panel data (e.g., 1998 and 2008 Census), the model is:\n  \ny_{jdt} = \\alpha + \\beta Tariff_{dt} + \\theta X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\lambda \\mu_{t} \\Delta y_{d,pre} + \\epsilon_{jdt} \\quad \\text{(Eq. (2))}\n \nwhere `y_jdt` is an outcome for individual `j`, `X_jdt` are individual controls, `μ_t` are year fixed effects, `γ_d` are district fixed effects, and `Δy_d,pre` is the district-level pre-liberalization trend in the outcome variable.\n\nFor multi-period panel data (e.g., DHS surveys), the model is modified to include district-specific linear time trends (`δ_d t`):\n  \ny_{jdt} = \\alpha' + \\beta' Tariff_{dt} + \\theta' X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\delta_{d}t + \\epsilon_{jdt} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nBased on the provided model specifications, select all statements that accurately describe the identification strategy.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 179, "Question": "### Background\n\n**Research Question.** This problem examines the econometric strategy used to identify the causal effect of trade liberalization on economic and social outcomes. The core of the strategy is a \"shift-share\" or Bartik-style research design.\n\n**Setting / Institutional Environment.** The study leverages Cambodia's 2004 accession to the WTO, which involved large, heterogeneous tariff reductions across industries. The identification strategy exploits the fact that districts had different pre-existing industrial compositions, leading to differential local exposure to these national tariff changes.\n\n### Data / Model Specification\n\nThe district-level exposure to trade liberalization, `Tariff_dt`, is constructed as a weighted average of national industry-level tariffs, where the weights are the district's pre-liberalization (1998) industry employment shares:\n  \nTariff_{dt} = \\sum_{i} Empshare_{id}^{1998} \\times Tariff_{it} \\quad \\text{(Eq. (1))}\n \nwhere `i` is industry, `d` is district, and `t` is time.\n\nFor two-period panel data (e.g., 1998 and 2008 Census), the model is:\n  \ny_{jdt} = \\alpha + \\beta Tariff_{dt} + \\theta X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\lambda \\mu_{t} \\Delta y_{d,pre} + \\epsilon_{jdt} \\quad \\text{(Eq. (2))}\n \nwhere `y_jdt` is an outcome for individual `j` and `β` is the coefficient of interest.\n\n### Question\n\nConsider a hypothetical scenario where an unobserved factor, `Shock_dt`, representing a new agricultural technology, is introduced. This shock boosts rural productivity post-liberalization and is more prevalent in districts that had less exposure to manufacturing tariff cuts (i.e., districts with higher post-period `Tariff_dt` values). Assume this shock reduces demand for paid (non-agricultural) employment. Furthermore, recall the econometric critique that standard errors in shift-share designs can be problematic. Select all correct conclusions based on this scenario and critique.", "Answer": ["B", "C"], "qid": "179", "question": "### Background\n\n**Research Question.** This problem examines the econometric strategy used to identify the causal effect of trade liberalization on economic and social outcomes. The core of the strategy is a \"shift-share\" or Bartik-style research design.\n\n**Setting / Institutional Environment.** The study leverages Cambodia's 2004 accession to the WTO, which involved large, heterogeneous tariff reductions across industries. The identification strategy exploits the fact that districts had different pre-existing industrial compositions, leading to differential local exposure to these national tariff changes.\n\n### Data / Model Specification\n\nThe district-level exposure to trade liberalization, `Tariff_dt`, is constructed as a weighted average of national industry-level tariffs, where the weights are the district's pre-liberalization (1998) industry employment shares:\n  \nTariff_{dt} = \\sum_{i} Empshare_{id}^{1998} \\times Tariff_{it} \\quad \\text{(Eq. (1))}\n \nwhere `i` is industry, `d` is district, and `t` is time.\n\nFor two-period panel data (e.g., 1998 and 2008 Census), the model is:\n  \ny_{jdt} = \\alpha + \\beta Tariff_{dt} + \\theta X_{jdt} + \\dots + \\mu_{t} + \\gamma_{d} + \\lambda \\mu_{t} \\Delta y_{d,pre} + \\epsilon_{jdt} \\quad \\text{(Eq. (2))}\n \nwhere `y_jdt` is an outcome for individual `j` and `β` is the coefficient of interest.\n\n### Question\n\nConsider a hypothetical scenario where an unobserved factor, `Shock_dt`, representing a new agricultural technology, is introduced. This shock boosts rural productivity post-liberalization and is more prevalent in districts that had less exposure to manufacturing tariff cuts (i.e., districts with higher post-period `Tariff_dt` values). Assume this shock reduces demand for paid (non-agricultural) employment. Furthermore, recall the econometric critique that standard errors in shift-share designs can be problematic. Select all correct conclusions based on this scenario and critique.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 180, "Question": "### Background\n\n**Research Question.** This problem explores the paper's central thesis: that the failure of temporary trade protection to stimulate innovation can be explained by rational, time-inconsistent government incentives that create credibility problems for firms. It synthesizes the firm's behavioral response to policy uncertainty with the government's underlying optimization problem.\n\n**Setting / Institutional Environment.** A domestic firm considers R&D investment under a temporary tariff scheduled to end at time `T`. Separately, a welfare-maximizing government sets trade policy knowing its choice affects the firm's R&D, which in turn affects national welfare.\n\n### Data / Model Specification\n\nA welfare-maximizing government sets a permanent tariff `τ` to maximize the discounted sum of domestic welfare `U(τ)`:\n  \nU(\\tau) \\equiv \\frac{\\underline{w}(\\tau) + h[k(\\tau)] \\overline{w}(\\tau)/r}{r + h[k(\\tau)]} \n \nwhere `\\underline{w}(\\tau)` and `\\overline{w}(\\tau)` are the pre- and post-innovation instantaneous welfare flows, and `k(τ)` is the firm's optimal R&D response to the tariff, with `k'(τ) > 0`. Innovation is welfare-improving, so `\\overline{w}(\\tau) > \\underline{w}(\\tau)`.\n\nThe first-order condition for the government's optimal ex-ante tariff, `τ^o`, is:\n  \n[\\underline{w}'(\\tau) + \\overline{w}'(\\tau)h/r](r+h) + [\\overline{w}(\\tau) - \\underline{w}(\\tau)]h'[k(\\tau)]k'(\\tau) = 0 \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the government's optimization problem, select all statements that correctly describe the rational foundations for a firm's credibility concerns regarding temporary protection.", "Answer": ["B", "C"], "qid": "180", "question": "### Background\n\n**Research Question.** This problem explores the paper's central thesis: that the failure of temporary trade protection to stimulate innovation can be explained by rational, time-inconsistent government incentives that create credibility problems for firms. It synthesizes the firm's behavioral response to policy uncertainty with the government's underlying optimization problem.\n\n**Setting / Institutional Environment.** A domestic firm considers R&D investment under a temporary tariff scheduled to end at time `T`. Separately, a welfare-maximizing government sets trade policy knowing its choice affects the firm's R&D, which in turn affects national welfare.\n\n### Data / Model Specification\n\nA welfare-maximizing government sets a permanent tariff `τ` to maximize the discounted sum of domestic welfare `U(τ)`:\n  \nU(\\tau) \\equiv \\frac{\\underline{w}(\\tau) + h[k(\\tau)] \\overline{w}(\\tau)/r}{r + h[k(\\tau)]} \n \nwhere `\\underline{w}(\\tau)` and `\\overline{w}(\\tau)` are the pre- and post-innovation instantaneous welfare flows, and `k(τ)` is the firm's optimal R&D response to the tariff, with `k'(τ) > 0`. Innovation is welfare-improving, so `\\overline{w}(\\tau) > \\underline{w}(\\tau)`.\n\nThe first-order condition for the government's optimal ex-ante tariff, `τ^o`, is:\n  \n[\\underline{w}'(\\tau) + \\overline{w}'(\\tau)h/r](r+h) + [\\overline{w}(\\tau) - \\underline{w}(\\tau)]h'[k(\\tau)]k'(\\tau) = 0 \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the government's optimization problem, select all statements that correctly describe the rational foundations for a firm's credibility concerns regarding temporary protection.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 181, "Question": "### Background\n\n**Research Question.** This problem investigates how the choice of trade policy instrument—tariffs versus quantitative restrictions (quotas)—differentially affects a domestic firm's incentive to innovate, even when both policies are fully credible.\n\n**Setting / Institutional Environment.** A domestic firm can invest in R&D to improve its technology. The government implements a credible temporary trade policy. We compare the effects of a tariff (`τ`) and a non-restrictive import quota (`Q*`).\n\n### Data / Model Specification\n\n**1. Tariff Protection:**\nA tariff `τ` raises the foreign firm's costs. Its effect on innovation is governed by:\n*   **Assumption A:** `\\overline{π}'(τ) > \\underline{π}'(τ) > 0`. A tariff increases profits in both pre- and post-innovation states, but the increase is larger after innovation.\n\n**2. Quota Protection:**\nA quota `Q*` is 'not sufficiently restrictive' if it binds before innovation but becomes non-binding after innovation. This leads to the following profit relationships:\n  \n\\overline{π}(Q^*) = \\overline{π}^{f} \\quad \\text{and} \\quad \\underline{π}(Q^*) > \\underline{π}^{f} \n \nwhere `f` denotes free trade.\n\n### Question\n\nSelect all statements that correctly describe the mechanisms through which credible temporary tariffs and non-restrictive quotas affect a firm's R&D investment incentives.", "Answer": ["A", "C"], "qid": "181", "question": "### Background\n\n**Research Question.** This problem investigates how the choice of trade policy instrument—tariffs versus quantitative restrictions (quotas)—differentially affects a domestic firm's incentive to innovate, even when both policies are fully credible.\n\n**Setting / Institutional Environment.** A domestic firm can invest in R&D to improve its technology. The government implements a credible temporary trade policy. We compare the effects of a tariff (`τ`) and a non-restrictive import quota (`Q*`).\n\n### Data / Model Specification\n\n**1. Tariff Protection:**\nA tariff `τ` raises the foreign firm's costs. Its effect on innovation is governed by:\n*   **Assumption A:** `\\overline{π}'(τ) > \\underline{π}'(τ) > 0`. A tariff increases profits in both pre- and post-innovation states, but the increase is larger after innovation.\n\n**2. Quota Protection:**\nA quota `Q*` is 'not sufficiently restrictive' if it binds before innovation but becomes non-binding after innovation. This leads to the following profit relationships:\n  \n\\overline{π}(Q^*) = \\overline{π}^{f} \\quad \\text{and} \\quad \\underline{π}(Q^*) > \\underline{π}^{f} \n \nwhere `f` denotes free trade.\n\n### Question\n\nSelect all statements that correctly describe the mechanisms through which credible temporary tariffs and non-restrictive quotas affect a firm's R&D investment incentives.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 182, "Question": "### Background\n\n**Research Question.** This problem concerns the paper's baseline model of a firm's optimal R&D investment strategy under free trade, which serves as a benchmark for all policy analysis.\n\n**Setting / Institutional Environment.** A domestic firm invests in R&D to catch up with a foreign competitor in a stationary, continuous-time, infinite-horizon model under free trade.\n\n### Data / Model Specification\n\nThe success of R&D is a stochastic process with hazard rate `h(k)`, where `k` is R&D investment. The firm maximizes its value function `v(k)`:\n  \nv(k) = \\frac{\\underline{π}^{f} - k + h(k) \\overline{π}^{f} / r}{r + h(k)}\n \nThis maximization yields the first-order condition (FOC) for the optimal investment `k^f`:\n  \nh'(k) [\\overline{π}^{f} - (\\underline{π}^{f} - k)] = r + h(k) \\quad \\text{(Eq. (1))}\n \nwhere `r` is the interest rate, `\\underline{π}^{f}` is pre-innovation profit, and `\\overline{π}^{f}` is post-innovation profit.\n\n### Question\n\nSelect all statements that are **INCORRECT** descriptions of the free-trade model or its properties.", "Answer": ["A", "B"], "qid": "182", "question": "### Background\n\n**Research Question.** This problem concerns the paper's baseline model of a firm's optimal R&D investment strategy under free trade, which serves as a benchmark for all policy analysis.\n\n**Setting / Institutional Environment.** A domestic firm invests in R&D to catch up with a foreign competitor in a stationary, continuous-time, infinite-horizon model under free trade.\n\n### Data / Model Specification\n\nThe success of R&D is a stochastic process with hazard rate `h(k)`, where `k` is R&D investment. The firm maximizes its value function `v(k)`:\n  \nv(k) = \\frac{\\underline{π}^{f} - k + h(k) \\overline{π}^{f} / r}{r + h(k)}\n \nThis maximization yields the first-order condition (FOC) for the optimal investment `k^f`:\n  \nh'(k) [\\overline{π}^{f} - (\\underline{π}^{f} - k)] = r + h(k) \\quad \\text{(Eq. (1))}\n \nwhere `r` is the interest rate, `\\underline{π}^{f}` is pre-innovation profit, and `\\overline{π}^{f}` is post-innovation profit.\n\n### Question\n\nSelect all statements that are **INCORRECT** descriptions of the free-trade model or its properties.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 183, "Question": "### Background\n\n**Research Question.** This problem re-evaluates the classic finite-sample comparison between the Minimum Chi-Square (MCS) and Maximum Likelihood (ML) based estimators for the grouped logit model, highlighting the crucial role of computational procedures.\n\n**Setting / Institutional Environment.** The analysis focuses on four experimental designs originally studied by Berkson. A key distinction is made between two computational procedures: the 'Berkson' procedure, which treated estimators asymmetrically, and the 'full 2n-rule' procedure, which applies a consistent data adjustment rule to all estimators.\n\n**Variables & Parameters.**\n- `MCS`: The Minimum Chi-Square estimator.\n- `MLBC`: A bias-corrected Maximum Likelihood estimator, designed to have the same first-order bias as MCS.\n- `MSE`: Mean Squared Error, the primary metric for estimator performance.\n\n### Data / Model Specification\n\nTable 1 presents the exact MSE for the slope coefficient for the MCS and MLBC estimators under two different computational procedures. The results are for the 'Berkson 1' design, a symmetric three-dose experiment with `n=10` observations per dose.\n\n**Table 1: Exact MSE for the Slope Coefficient (Berkson 1 Design)**\n| Procedure | Estimator | MSE |\n| :--- | :--- | :--- |\n| **Full 2n-rule** | MCS | 0.271 |\n| | MLBC | 0.257 |\n| **Berkson** | MCS | 0.267 |\n| | MLBC | 0.277 |\n\nAmemiya's second-order asymptotic theory predicts that the MSE of MLBC should be less than or equal to the MSE of MCS.\n\n### Question\n\nBased on the data in Table 1 and the provided context, select all of the following statements that are correct.\n", "Answer": ["A", "B", "D"], "qid": "183", "question": "### Background\n\n**Research Question.** This problem re-evaluates the classic finite-sample comparison between the Minimum Chi-Square (MCS) and Maximum Likelihood (ML) based estimators for the grouped logit model, highlighting the crucial role of computational procedures.\n\n**Setting / Institutional Environment.** The analysis focuses on four experimental designs originally studied by Berkson. A key distinction is made between two computational procedures: the 'Berkson' procedure, which treated estimators asymmetrically, and the 'full 2n-rule' procedure, which applies a consistent data adjustment rule to all estimators.\n\n**Variables & Parameters.**\n- `MCS`: The Minimum Chi-Square estimator.\n- `MLBC`: A bias-corrected Maximum Likelihood estimator, designed to have the same first-order bias as MCS.\n- `MSE`: Mean Squared Error, the primary metric for estimator performance.\n\n### Data / Model Specification\n\nTable 1 presents the exact MSE for the slope coefficient for the MCS and MLBC estimators under two different computational procedures. The results are for the 'Berkson 1' design, a symmetric three-dose experiment with `n=10` observations per dose.\n\n**Table 1: Exact MSE for the Slope Coefficient (Berkson 1 Design)**\n| Procedure | Estimator | MSE |\n| :--- | :--- | :--- |\n| **Full 2n-rule** | MCS | 0.271 |\n| | MLBC | 0.257 |\n| **Berkson** | MCS | 0.267 |\n| | MLBC | 0.277 |\n\nAmemiya's second-order asymptotic theory predicts that the MSE of MLBC should be less than or equal to the MSE of MCS.\n\n### Question\n\nBased on the data in Table 1 and the provided context, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 184, "Question": "### Background\n\n**Research Question.** This problem investigates how a key feature of experimental design—the number of distinct groups or 'doses'—influences the relative finite-sample performance of competing estimators for the grouped logit model.\n\n**Setting / Institutional Environment.** The analysis considers a series of experiments based on the 'Berkson 1' symmetric design, but varies the number of equally spaced doses, `T`, from 3 to 33. The number of observations per dose is held fixed at `n=10`. All estimators are calculated using the consistent 'full 2n-rule' procedure.\n\n**Variables & Parameters.**\n- `MCS`: The Minimum Chi-Square estimator.\n- `ML`: The standard Maximum Likelihood estimator.\n- `MLBC`: The bias-corrected Maximum Likelihood estimator.\n- `T`: The number of distinct doses (design points).\n- `MSE`: Mean Squared Error.\n\n### Data / Model Specification\n\nTable 1 shows the MSE for the slope coefficient for the three estimators as the number of doses `T` increases.\n\n**Table 1: MSE of Slope Estimators as Number of Doses `T` Varies (`n=10`)**\n| Number of Doses `T` | MSE(MCS) | MSE(ML) | MSE(MLBC) |\n| :--- | :--- | :--- | :--- |\n| 3 | 0.271 | 0.295 | 0.257 |\n| 5 | 0.184 | 0.207 | 0.171 |\n| 9 | 0.112 | 0.127 | 0.104 |\n| 17 | 0.067 | 0.072 | 0.061 |\n| 33 | 0.040 | 0.038 | 0.037 |\n\n### Question\n\nBased on the data in Table 1, which of the following conclusions about the relative performance of the estimators are **INCORRECT**?\n", "Answer": ["A", "B"], "qid": "184", "question": "### Background\n\n**Research Question.** This problem investigates how a key feature of experimental design—the number of distinct groups or 'doses'—influences the relative finite-sample performance of competing estimators for the grouped logit model.\n\n**Setting / Institutional Environment.** The analysis considers a series of experiments based on the 'Berkson 1' symmetric design, but varies the number of equally spaced doses, `T`, from 3 to 33. The number of observations per dose is held fixed at `n=10`. All estimators are calculated using the consistent 'full 2n-rule' procedure.\n\n**Variables & Parameters.**\n- `MCS`: The Minimum Chi-Square estimator.\n- `ML`: The standard Maximum Likelihood estimator.\n- `MLBC`: The bias-corrected Maximum Likelihood estimator.\n- `T`: The number of distinct doses (design points).\n- `MSE`: Mean Squared Error.\n\n### Data / Model Specification\n\nTable 1 shows the MSE for the slope coefficient for the three estimators as the number of doses `T` increases.\n\n**Table 1: MSE of Slope Estimators as Number of Doses `T` Varies (`n=10`)**\n| Number of Doses `T` | MSE(MCS) | MSE(ML) | MSE(MLBC) |\n| :--- | :--- | :--- | :--- |\n| 3 | 0.271 | 0.295 | 0.257 |\n| 5 | 0.184 | 0.207 | 0.171 |\n| 9 | 0.112 | 0.127 | 0.104 |\n| 17 | 0.067 | 0.072 | 0.061 |\n| 33 | 0.040 | 0.038 | 0.037 |\n\n### Question\n\nBased on the data in Table 1, which of the following conclusions about the relative performance of the estimators are **INCORRECT**?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 185, "Question": "### Background\n\n**Research Question.** This problem examines the methodological challenges in comparing estimators for the grouped logit model, focusing on the role of ad-hoc computational rules for handling problematic data realizations.\n\n**Setting / Institutional Environment.** In finite samples, particularly with small cell sizes `n_t`, observed success frequencies `r_t` can be 0 or 1. This creates computational problems for some estimators. The choice of how to handle these cases, a `q`-rule, can significantly affect the calculated bias and MSE of an estimator.\n\n### Data / Model Specification\n\nThe Minimum Chi-Square (MCS) estimator is based on the empirical log-odds, `log(r_t / (1-r_t))`. The '2n-rule' is a `q`-rule that replaces `r_t` with `r_t^*` before calculating the estimator, where `r_t^*` is defined as:\n\n  \nr_t^* = \n\\begin{cases} \n\\frac{1}{2n_t} & \\text{if } r_t = 0 \\\\\n1 - \\frac{1}{2n_t} & \\text{if } r_t = 1\n\\end{cases}\n\\quad \\text{(Eq. (1))}\n \n\nBerkson was criticized for applying this rule only to MCS, as it \"tends to moderate tail behavior.\" The paper advocates for a 'full 2n-rule' procedure, where Eq. (1) is applied to the data before calculating *any* estimator (MCS, ML, or MLBC).\n\n### Question\n\nSelect all of the following statements that correctly describe the '2n-rule' and its consequences.\n", "Answer": ["A", "B", "C"], "qid": "185", "question": "### Background\n\n**Research Question.** This problem examines the methodological challenges in comparing estimators for the grouped logit model, focusing on the role of ad-hoc computational rules for handling problematic data realizations.\n\n**Setting / Institutional Environment.** In finite samples, particularly with small cell sizes `n_t`, observed success frequencies `r_t` can be 0 or 1. This creates computational problems for some estimators. The choice of how to handle these cases, a `q`-rule, can significantly affect the calculated bias and MSE of an estimator.\n\n### Data / Model Specification\n\nThe Minimum Chi-Square (MCS) estimator is based on the empirical log-odds, `log(r_t / (1-r_t))`. The '2n-rule' is a `q`-rule that replaces `r_t` with `r_t^*` before calculating the estimator, where `r_t^*` is defined as:\n\n  \nr_t^* = \n\\begin{cases} \n\\frac{1}{2n_t} & \\text{if } r_t = 0 \\\\\n1 - \\frac{1}{2n_t} & \\text{if } r_t = 1\n\\end{cases}\n\\quad \\text{(Eq. (1))}\n \n\nBerkson was criticized for applying this rule only to MCS, as it \"tends to moderate tail behavior.\" The paper advocates for a 'full 2n-rule' procedure, where Eq. (1) is applied to the data before calculating *any* estimator (MCS, ML, or MLBC).\n\n### Question\n\nSelect all of the following statements that correctly describe the '2n-rule' and its consequences.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 186, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the paper's generalized cointegration model, which accommodates highly persistent processes with roots 'local to unity', and the statistical properties of the proposed efficient detrending procedure.\n\n**Setting / Institutional Environment.** The framework is a Vector Autoregression (VAR) generalized with a quasi-difference (QD) operator, `I - AL`, to model local-to-unity roots. The analysis relies on rotating the coordinate system to isolate different dynamic behaviors.\n\n**Variables & Parameters.**\n- `y_t`: An `n x 1` vector of time series variables.\n- `A`: An `n x n` matrix defining the QD operator.\n- `C`: An `s x s` matrix of localizing parameters, where `s = n - r`.\n- `β`: An `n x r` matrix of `r` cointegrating vectors.\n- `β_⊥`: An `n x s` matrix, orthogonal to `β`, spanning the space of common trends.\n\n---\n\n### Data / Model Specification\n\nThe generalized VAR model with a quasi-difference operator is defined by:\n  \n-\\Pi y_{t}+\\Pi^{*}(L)(I-A L)y_{t}=\\underline{B}x_{t}+\\varepsilon_{t}\n \nThe matrix `A` is specified as:\n  \nA = \\beta\\beta' + \\beta_\\perp \\exp(T^{-1}C)\\beta_\\perp'\n\n\\quad \\text{(Eq. 1)}\n \nThis structure is designed to separate the stationary and non-stationary dynamics of the system.\n\n---\n\n### Question\n\nBased on the model specification, which of the following statements about the Quasi-Differencing (QD) model and detrending procedure are correct? Select all that apply.", "Answer": ["B", "D"], "qid": "186", "question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the paper's generalized cointegration model, which accommodates highly persistent processes with roots 'local to unity', and the statistical properties of the proposed efficient detrending procedure.\n\n**Setting / Institutional Environment.** The framework is a Vector Autoregression (VAR) generalized with a quasi-difference (QD) operator, `I - AL`, to model local-to-unity roots. The analysis relies on rotating the coordinate system to isolate different dynamic behaviors.\n\n**Variables & Parameters.**\n- `y_t`: An `n x 1` vector of time series variables.\n- `A`: An `n x n` matrix defining the QD operator.\n- `C`: An `s x s` matrix of localizing parameters, where `s = n - r`.\n- `β`: An `n x r` matrix of `r` cointegrating vectors.\n- `β_⊥`: An `n x s` matrix, orthogonal to `β`, spanning the space of common trends.\n\n---\n\n### Data / Model Specification\n\nThe generalized VAR model with a quasi-difference operator is defined by:\n  \n-\\Pi y_{t}+\\Pi^{*}(L)(I-A L)y_{t}=\\underline{B}x_{t}+\\varepsilon_{t}\n \nThe matrix `A` is specified as:\n  \nA = \\beta\\beta' + \\beta_\\perp \\exp(T^{-1}C)\\beta_\\perp'\n\n\\quad \\text{(Eq. 1)}\n \nThis structure is designed to separate the stationary and non-stationary dynamics of the system.\n\n---\n\n### Question\n\nBased on the model specification, which of the following statements about the Quasi-Differencing (QD) model and detrending procedure are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 187, "Question": "### Background\n\nAn analyst is studying the causal effect of income growth on democratic institutions using a country-year panel dataset. The analysis uses a two-stage least squares (2SLS) approach where country-specific oil price shocks are used as an instrument for potentially endogenous GDP growth.\n\n### Data / Model Specification\n\nThe analyst is working with the following regression output tables from the study.\n\n**Table 1: 2SLS Estimates of the Effect of Income on Democracy**\n| | (1) `Δ`Polity2 | (2) `Δ`Exconst | (3) `Δ`Exrec | (4) `Δ`Polcomp |\n|:---|:---:|:---:|:---:|:---:|\n| `Δ`LnGDP | 4.39*** | 1.10** | 1.31*** | 2.59*** |\n| | (3.27) | (2.09) | (3.10) | (3.94) |\n| First-stage F-statistic | 45 | 45 | 45 | 45 |\n*Notes: t-statistics in parentheses. `***` p<0.01, `**` p<0.05. The instrument is the 3-year oil price shock.* \n\n**Table 2: Dynamic Effects of Oil Price Shocks on Democracy (`Δ`Polity2)**\n| | (1) LS | (2) GMM |\n|:---|:---:|:---:|\n| 3-year oil price shock (`β`) | 1.67*** | 1.70*** |\n| | (3.22) | (2.93) |\n| Lagged democracy (Level) (`γ`) | -0.10*** | -0.09*** |\n| | (-8.98) | (-3.12) |\n*Notes: t-statistics in parentheses. `***` p<0.01.* \n\nBased on the provided tables, which of the following statements are valid interpretations or calculations?\n", "Answer": ["B", "C", "D"], "qid": "187", "question": "### Background\n\nAn analyst is studying the causal effect of income growth on democratic institutions using a country-year panel dataset. The analysis uses a two-stage least squares (2SLS) approach where country-specific oil price shocks are used as an instrument for potentially endogenous GDP growth.\n\n### Data / Model Specification\n\nThe analyst is working with the following regression output tables from the study.\n\n**Table 1: 2SLS Estimates of the Effect of Income on Democracy**\n| | (1) `Δ`Polity2 | (2) `Δ`Exconst | (3) `Δ`Exrec | (4) `Δ`Polcomp |\n|:---|:---:|:---:|:---:|:---:|\n| `Δ`LnGDP | 4.39*** | 1.10** | 1.31*** | 2.59*** |\n| | (3.27) | (2.09) | (3.10) | (3.94) |\n| First-stage F-statistic | 45 | 45 | 45 | 45 |\n*Notes: t-statistics in parentheses. `***` p<0.01, `**` p<0.05. The instrument is the 3-year oil price shock.* \n\n**Table 2: Dynamic Effects of Oil Price Shocks on Democracy (`Δ`Polity2)**\n| | (1) LS | (2) GMM |\n|:---|:---:|:---:|\n| 3-year oil price shock (`β`) | 1.67*** | 1.70*** |\n| | (3.22) | (2.93) |\n| Lagged democracy (Level) (`γ`) | -0.10*** | -0.09*** |\n| | (-8.98) | (-3.12) |\n*Notes: t-statistics in parentheses. `***` p<0.01.* \n\nBased on the provided tables, which of the following statements are valid interpretations or calculations?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analyst is studying the causal ...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analyst is studying the causal ...", "error": null}}
{"ID": 188, "Question": "### Background\n\nAn analyst is studying the causal effect of income growth on democratic institutions using a country-year panel dataset. The analysis uses a two-stage least squares (2SLS) approach where country-specific oil price shocks are used as an instrument for potentially endogenous GDP growth.\n\n### Data / Model Specification\n\nThe instrumental variable is constructed as an interaction:\n  \nOilPriceShock_{c,t} = \\Theta_{c} \\times \\Delta \\ln OilPrice_{t} \\quad \\text{(Eq. (1))}\n \nwhere `Θ_c` is a country-specific, time-invariant measure of net oil exports as a share of GDP, and `Δ ln OilPrice_t` is the global, time-varying change in log oil prices.\n\nThe structural equation is:\n  \n\\Delta Democ_{c,t} = \\alpha_{c} + \\beta_{t} + \\delta \\Delta \\ln GDP_{c,t} + \\varepsilon_{c,t} \\quad \\text{(Eq. (2))}\n \nwhere `Δ ln GDP_c,t` is instrumented by `OilPriceShock_c,t`.\n\n### Scenario\n\nConsider a plausible violation of the IV exclusion restriction: in oil-dependent autocracies, a positive oil price shock provides the regime with windfall revenues that, independent of overall GDP growth, can be used to increase state repressive capacity (e.g., funding security services). This increased repression directly hinders democratization.\n\nGiven this scenario, which of the following statements about the IV strategy and its potential bias are correct?\n", "Answer": ["A", "B", "D"], "qid": "188", "question": "### Background\n\nAn analyst is studying the causal effect of income growth on democratic institutions using a country-year panel dataset. The analysis uses a two-stage least squares (2SLS) approach where country-specific oil price shocks are used as an instrument for potentially endogenous GDP growth.\n\n### Data / Model Specification\n\nThe instrumental variable is constructed as an interaction:\n  \nOilPriceShock_{c,t} = \\Theta_{c} \\times \\Delta \\ln OilPrice_{t} \\quad \\text{(Eq. (1))}\n \nwhere `Θ_c` is a country-specific, time-invariant measure of net oil exports as a share of GDP, and `Δ ln OilPrice_t` is the global, time-varying change in log oil prices.\n\nThe structural equation is:\n  \n\\Delta Democ_{c,t} = \\alpha_{c} + \\beta_{t} + \\delta \\Delta \\ln GDP_{c,t} + \\varepsilon_{c,t} \\quad \\text{(Eq. (2))}\n \nwhere `Δ ln GDP_c,t` is instrumented by `OilPriceShock_c,t`.\n\n### Scenario\n\nConsider a plausible violation of the IV exclusion restriction: in oil-dependent autocracies, a positive oil price shock provides the regime with windfall revenues that, independent of overall GDP growth, can be used to increase state repressive capacity (e.g., funding security services). This increased repression directly hinders democratization.\n\nGiven this scenario, which of the following statements about the IV strategy and its potential bias are correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analyst is studying the causal ...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analyst is studying the causal ...", "error": null}}
{"ID": 189, "Question": "### Background\n\nThis problem analyzes the core dynamic properties of a North-South trade model, focusing on how the interplay of innovation, technology transfer, and capital mobility endogenously determines a persistent wage gap in the long-run equilibrium.\n\nThe economy's state is described by two variables: $r = n_N/n_S$, the ratio of goods produced in the North to the South, and $k = K_N/K_S$, the ratio of capital stocks. A long-run equilibrium is a stable point $(r^*, k^*)$ where the laws of motion for these variables are zero.\n\n### Data / Model Specification\n\nThe dynamic system is governed by two differential equations:\n\n1.  **Evolution of Relative Product Variety ($r$):**\n      \n    \\dot{r} = i r - (1+r)f(p) \\quad \\text{(Eq. (1))}\n     \n    where $i>0$ is a constant rate of innovation in the North, and $f(p)$ is the rate of technology transfer to the South. The transfer rate is a positive function of the terms of trade, $p$, with the properties $f'(p)>0$ and $f(1)=0$. A value of $p>1$ indicates that production costs are lower in the South, creating an incentive for technology transfer.\n\n2.  **Evolution of Relative Capital Stock ($k$):**\n      \n    \\dot{k} = g(q) \\quad \\text{(Eq. (2))}\n     \n    where $q = q_N/q_S$ is the ratio of the return to capital in the North to that in the South. The capital flow function $g(q)$ has the properties $g'(q)>0$ and $g(1)=0$, meaning capital flows to the region with the higher return, and flows cease when returns are equalized.\n\nIn this model, the terms of trade, $p$, are defined as the ratio of the minimum unit cost of production in the North to that in the South for any good producible in both regions. Both regions are assumed to have access to the same production technology.\n\n### Question\n\nIn the model's long-run equilibrium, a persistent wage gap ($w_N > w_S$) emerges as a central result. Select all of the following conditions that are necessary premises for the formal proof of this wage gap.", "Answer": ["B", "D"], "qid": "189", "question": "### Background\n\nThis problem analyzes the core dynamic properties of a North-South trade model, focusing on how the interplay of innovation, technology transfer, and capital mobility endogenously determines a persistent wage gap in the long-run equilibrium.\n\nThe economy's state is described by two variables: $r = n_N/n_S$, the ratio of goods produced in the North to the South, and $k = K_N/K_S$, the ratio of capital stocks. A long-run equilibrium is a stable point $(r^*, k^*)$ where the laws of motion for these variables are zero.\n\n### Data / Model Specification\n\nThe dynamic system is governed by two differential equations:\n\n1.  **Evolution of Relative Product Variety ($r$):**\n      \n    \\dot{r} = i r - (1+r)f(p) \\quad \\text{(Eq. (1))}\n     \n    where $i>0$ is a constant rate of innovation in the North, and $f(p)$ is the rate of technology transfer to the South. The transfer rate is a positive function of the terms of trade, $p$, with the properties $f'(p)>0$ and $f(1)=0$. A value of $p>1$ indicates that production costs are lower in the South, creating an incentive for technology transfer.\n\n2.  **Evolution of Relative Capital Stock ($k$):**\n      \n    \\dot{k} = g(q) \\quad \\text{(Eq. (2))}\n     \n    where $q = q_N/q_S$ is the ratio of the return to capital in the North to that in the South. The capital flow function $g(q)$ has the properties $g'(q)>0$ and $g(1)=0$, meaning capital flows to the region with the higher return, and flows cease when returns are equalized.\n\nIn this model, the terms of trade, $p$, are defined as the ratio of the minimum unit cost of production in the North to that in the South for any good producible in both regions. Both regions are assumed to have access to the same production technology.\n\n### Question\n\nIn the model's long-run equilibrium, a persistent wage gap ($w_N > w_S$) emerges as a central result. Select all of the following conditions that are necessary premises for the formal proof of this wage gap.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the core dyn...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the core dyn...", "error": null}}
{"ID": 190, "Question": "### Background\n\nThis problem analyzes the full dynamic consequences of labor force growth in the South, a key policy-relevant experiment in the model. It highlights the tension between short-run effects, which appear beneficial to the North, and long-run effects, which are detrimental to Northern labor due to induced flows of capital and technology.\n\nWe begin in a stable long-run equilibrium and introduce a one-time, permanent increase in the Southern labor force, $L_S$. We then trace the adjustment of the system to the new long-run equilibrium.\n\n### Data / Model Specification\n\nThe model's dynamics are driven by the evolution of relative product variety ($r=n_N/n_S$) and relative capital stocks ($k=K_N/K_S$). The key price variables are the terms of trade ($p$) and the relative return to capital ($q=q_N/q_S$).\n\nKey results from the static model and dynamic analysis:\n*   The immediate impact of an increase in $L_S$ is an improvement in the North's terms of trade ($p$ increases).\n*   The increase in $L_S$ makes capital relatively scarcer in the South, causing the relative return to capital in the North, $q$, to fall.\n*   In the new long-run equilibrium, the North's relative product variety ($r$), relative capital stock ($k$), and terms of trade ($p$) are all lower than their initial equilibrium values.\n\n### Question\n\nFollowing a permanent increase in the Southern labor force ($L_S$), the model predicts a dynamic adjustment path. Select all statements that correctly describe the effects on the Northern economy.", "Answer": ["C", "D"], "qid": "190", "question": "### Background\n\nThis problem analyzes the full dynamic consequences of labor force growth in the South, a key policy-relevant experiment in the model. It highlights the tension between short-run effects, which appear beneficial to the North, and long-run effects, which are detrimental to Northern labor due to induced flows of capital and technology.\n\nWe begin in a stable long-run equilibrium and introduce a one-time, permanent increase in the Southern labor force, $L_S$. We then trace the adjustment of the system to the new long-run equilibrium.\n\n### Data / Model Specification\n\nThe model's dynamics are driven by the evolution of relative product variety ($r=n_N/n_S$) and relative capital stocks ($k=K_N/K_S$). The key price variables are the terms of trade ($p$) and the relative return to capital ($q=q_N/q_S$).\n\nKey results from the static model and dynamic analysis:\n*   The immediate impact of an increase in $L_S$ is an improvement in the North's terms of trade ($p$ increases).\n*   The increase in $L_S$ makes capital relatively scarcer in the South, causing the relative return to capital in the North, $q$, to fall.\n*   In the new long-run equilibrium, the North's relative product variety ($r$), relative capital stock ($k$), and terms of trade ($p$) are all lower than their initial equilibrium values.\n\n### Question\n\nFollowing a permanent increase in the Southern labor force ($L_S$), the model predicts a dynamic adjustment path. Select all statements that correctly describe the effects on the Northern economy.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the full dyn...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the full dyn...", "error": null}}
{"ID": 191, "Question": "### Background\n\nThis problem examines the foundations of the static general equilibrium in the North-South trade model. Understanding how the demand and supply sides interact at a single point in time is crucial for the subsequent dynamic analysis. The model features complete specialization, with the North producing $n_N$ “new” goods and the South producing $n_S$ “old” goods.\n\n### Data / Model Specification\n\n**Demand Side:** All consumers share a CES utility function over all $n = n_N + n_S$ goods:\n  \nU=\\left(\\sum_{i=1}^{n}c_{i}^{\\theta}\\right)^{1/\\theta}, \\quad 0<\\theta<1 \\quad \\text{(Eq. (1))}\n \nThis implies a relative demand for representative Northern ($c_N$) and Southern ($c_S$) goods that depends on the terms of trade, $p$ (the price of a Northern good relative to a Southern good).\n\n**Supply Side:** Perfect competition ensures price equals unit cost. Factor markets clear, ensuring full employment of the fixed local endowments of labor ($L_j$) and capital ($K_j$):\n  \na_{L N} n_{N} c_{N}=L_{N} \\quad \\text{and} \\quad a_{K N} n_{N} c_{N}=K_{N} \\quad \\text{(Eq. (2))}\n \n  \na_{L S} n_{S} c_{S}=L_{S} \\quad \\text{and} \\quad a_{K S} n_{S} c_{S}=K_{S} \\quad \\text{(Eq. (3))}\n \nwhere $a_{ij}$ are the unit input requirements of factor $i$ in region $j$, which are functions of the local factor price ratio $w_j/q_j$.\n\n### Question\n\nBased on the static model's specification, select all statements that correctly describe the structural relationships governing the economy at a point in time.", "Answer": ["B", "C"], "qid": "191", "question": "### Background\n\nThis problem examines the foundations of the static general equilibrium in the North-South trade model. Understanding how the demand and supply sides interact at a single point in time is crucial for the subsequent dynamic analysis. The model features complete specialization, with the North producing $n_N$ “new” goods and the South producing $n_S$ “old” goods.\n\n### Data / Model Specification\n\n**Demand Side:** All consumers share a CES utility function over all $n = n_N + n_S$ goods:\n  \nU=\\left(\\sum_{i=1}^{n}c_{i}^{\\theta}\\right)^{1/\\theta}, \\quad 0<\\theta<1 \\quad \\text{(Eq. (1))}\n \nThis implies a relative demand for representative Northern ($c_N$) and Southern ($c_S$) goods that depends on the terms of trade, $p$ (the price of a Northern good relative to a Southern good).\n\n**Supply Side:** Perfect competition ensures price equals unit cost. Factor markets clear, ensuring full employment of the fixed local endowments of labor ($L_j$) and capital ($K_j$):\n  \na_{L N} n_{N} c_{N}=L_{N} \\quad \\text{and} \\quad a_{K N} n_{N} c_{N}=K_{N} \\quad \\text{(Eq. (2))}\n \n  \na_{L S} n_{S} c_{S}=L_{S} \\quad \\text{and} \\quad a_{K S} n_{S} c_{S}=K_{S} \\quad \\text{(Eq. (3))}\n \nwhere $a_{ij}$ are the unit input requirements of factor $i$ in region $j$, which are functions of the local factor price ratio $w_j/q_j$.\n\n### Question\n\nBased on the static model's specification, select all statements that correctly describe the structural relationships governing the economy at a point in time.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem examines the foundati...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem examines the foundati...", "error": null}}
{"ID": 192, "Question": "### Background\n\nAn analysis is conducted for \"Large\" firms (200-500 kW) during the summer to evaluate a hypothetical policy change. The policy moves these firms from a flat electricity rate to a time-of-use (TOU) demand rate with an effective energy price ratio of 2.50. This change is designed to be revenue-neutral for the utility.\n\n### Data / Model Specification\n\nBased on an estimated elasticity of substitution, the study predicts consumption responses and calculates welfare changes for large firms under the hypothetical TOU demand rate.\n\n**Table 1: Predicted Response for Large Firms (200-500 kW), Summer**\n\n| Scenario              | Effective Energy Price Ratio | Predicted Peak Period Consumption Share |\n| :-------------------- | :--------------------------: | :-------------------------------------: |\n| Control (Flat Rate)   |            1.0000            |                 0.2443                  |\n| Hypothetical TOU Rate |            2.5000            |                 0.2257                  |\n\n**Additional Information:**\n- For large firms, the welfare-constant price index for this policy change is 0.989, implying a **1.1% welfare gain**.\n- This welfare gain is equivalent to a monetary saving of **$79.10 per month** for the average firm in this category.\n\n---\n\nUsing the provided data, select all of the following statements that are correct.\n", "Answer": ["A", "B", "C"], "qid": "192", "question": "### Background\n\nAn analysis is conducted for \"Large\" firms (200-500 kW) during the summer to evaluate a hypothetical policy change. The policy moves these firms from a flat electricity rate to a time-of-use (TOU) demand rate with an effective energy price ratio of 2.50. This change is designed to be revenue-neutral for the utility.\n\n### Data / Model Specification\n\nBased on an estimated elasticity of substitution, the study predicts consumption responses and calculates welfare changes for large firms under the hypothetical TOU demand rate.\n\n**Table 1: Predicted Response for Large Firms (200-500 kW), Summer**\n\n| Scenario              | Effective Energy Price Ratio | Predicted Peak Period Consumption Share |\n| :-------------------- | :--------------------------: | :-------------------------------------: |\n| Control (Flat Rate)   |            1.0000            |                 0.2443                  |\n| Hypothetical TOU Rate |            2.5000            |                 0.2257                  |\n\n**Additional Information:**\n- For large firms, the welfare-constant price index for this policy change is 0.989, implying a **1.1% welfare gain**.\n- This welfare gain is equivalent to a monetary saving of **$79.10 per month** for the average firm in this category.\n\n---\n\nUsing the provided data, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis is conducted for \"Larg...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis is conducted for \"Larg...", "error": null}}
{"ID": 193, "Question": "### Background\n\nThe analysis uses estimates from a Constant Elasticity of Substitution (CES) model to investigate how firms' response to time-of-use (TOU) electricity pricing varies by firm size, rate type, and season.\n\n### Data / Model Specification\n\nTable 1 presents key estimates of the Elasticity of Substitution (ES, or `σ`) for different subsamples. A positive ES indicates that peak and off-peak electricity are substitutes, while a negative ES would suggest they are complements. The t-ratio tests the null hypothesis that the ES is zero.\n\n**Table 1: Selected Estimates of the Elasticity of Substitution (`σ`)**\n\n| Rate Type | kW Demand  | Season | ES (`σ̂`) | t-Ratio |\n| :-------- | :--------- | :----: | :-------------------: | :-----: |\n| Demand    | 200-500 kW | S      |        0.1131         |  3.431  |\n| Energy    | Overall    | S      |       -0.03872        | -2.046  |\n\n---\n\nBased on the data in Table 1 and economic principles, which of the following conclusions are supported?\n", "Answer": ["A", "C", "D"], "qid": "193", "question": "### Background\n\nThe analysis uses estimates from a Constant Elasticity of Substitution (CES) model to investigate how firms' response to time-of-use (TOU) electricity pricing varies by firm size, rate type, and season.\n\n### Data / Model Specification\n\nTable 1 presents key estimates of the Elasticity of Substitution (ES, or `σ`) for different subsamples. A positive ES indicates that peak and off-peak electricity are substitutes, while a negative ES would suggest they are complements. The t-ratio tests the null hypothesis that the ES is zero.\n\n**Table 1: Selected Estimates of the Elasticity of Substitution (`σ`)**\n\n| Rate Type | kW Demand  | Season | ES (`σ̂`) | t-Ratio |\n| :-------- | :--------- | :----: | :-------------------: | :-----: |\n| Demand    | 200-500 kW | S      |        0.1131         |  3.431  |\n| Energy    | Overall    | S      |       -0.03872        | -2.046  |\n\n---\n\nBased on the data in Table 1 and economic principles, which of the following conclusions are supported?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe analysis uses estimates from a...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe analysis uses estimates from a...", "error": null}}
{"ID": 194, "Question": "### Background\n\nThe relationship between electricity consumption and price is estimated using a Constant Elasticity of Substitution (CES) functional form:\n\n  \n\\log\\left(\\frac{Q_{\\text{peak}}}{Q_{\\text{offpeak}}}\\right) = \\alpha - \\sigma \\log\\left(\\frac{P_{\\text{peak}}}{P_{\\text{offpeak}}}\\right) + X'\\beta + \\epsilon \\quad \\text{(Eq. 1)}\n \n\nwhere `σ` is the elasticity of substitution. The paper notes a potential design flaw: \"a self-selection aspect that allowed for qualified exemptions... introduced an element of bias since customers with high peak usage generally tended not to participate.\"\n\nAssume that firms with high peak usage are those for whom electricity is a more critical, less flexible input in their production process.\n\n---\n\nBased on this information, which of the following statements are valid implications of the self-selection issue described?\n", "Answer": ["B", "C", "D"], "qid": "194", "question": "### Background\n\nThe relationship between electricity consumption and price is estimated using a Constant Elasticity of Substitution (CES) functional form:\n\n  \n\\log\\left(\\frac{Q_{\\text{peak}}}{Q_{\\text{offpeak}}}\\right) = \\alpha - \\sigma \\log\\left(\\frac{P_{\\text{peak}}}{P_{\\text{offpeak}}}\\right) + X'\\beta + \\epsilon \\quad \\text{(Eq. 1)}\n \n\nwhere `σ` is the elasticity of substitution. The paper notes a potential design flaw: \"a self-selection aspect that allowed for qualified exemptions... introduced an element of bias since customers with high peak usage generally tended not to participate.\"\n\nAssume that firms with high peak usage are those for whom electricity is a more critical, less flexible input in their production process.\n\n---\n\nBased on this information, which of the following statements are valid implications of the self-selection issue described?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe relationship between electrici...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe relationship between electrici...", "error": null}}
{"ID": 195, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical framework for testing whether economic forecasts are rational, focusing on the precise hierarchy of concepts from the strongest condition (full rationality) to weaker, more easily testable conditions (partial rationality and unbiasedness).\n\n**Variables & Parameters.**\n- `A_{t+f}`: The realized value of an economic variable in period `t+f`.\n- `P_{t}^{f}`: The prediction (forecast) of `A_{t+f}` made in period `t` for `f` periods ahead.\n- `I_{t}`: The set of all relevant information available to a forecaster at time `t`.\n- `S_{t}`: A subset of the available information, `S_{t} \\subseteq I_{t}`, that is actually used to form the forecast.\n\n---\n\n### Data / Model Specification\n\nThe paper defines a hierarchy of rationality concepts:\n1.  A forecast `P_{t}^{f}` is **fully rational** if it is the conditional expectation of the outcome given all available information:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|I_{t}] \\quad \\text{(Eq. (1))}\n     \n    \n2.  A forecast `P_{t}^{f}` is **partially rational** if it is the conditional expectation of the outcome given the subset of information actually used:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|S_{t}] \\quad \\text{(Eq. (2))}\n     \n    \n3.  A forecast `P_{t}^{f}` is **unbiased** if it is the conditional expectation of the outcome given the forecast itself:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|P_{t}^{f}] \\quad \\text{(Eq. (3))}\n     \n\n---\n\nBased on the definitions provided, select all statements that correctly describe the logical relationships between the different concepts of forecast rationality.", "Answer": ["C", "D"], "qid": "195", "question": "### Background\n\n**Research Question.** This problem explores the theoretical framework for testing whether economic forecasts are rational, focusing on the precise hierarchy of concepts from the strongest condition (full rationality) to weaker, more easily testable conditions (partial rationality and unbiasedness).\n\n**Variables & Parameters.**\n- `A_{t+f}`: The realized value of an economic variable in period `t+f`.\n- `P_{t}^{f}`: The prediction (forecast) of `A_{t+f}` made in period `t` for `f` periods ahead.\n- `I_{t}`: The set of all relevant information available to a forecaster at time `t`.\n- `S_{t}`: A subset of the available information, `S_{t} \\subseteq I_{t}`, that is actually used to form the forecast.\n\n---\n\n### Data / Model Specification\n\nThe paper defines a hierarchy of rationality concepts:\n1.  A forecast `P_{t}^{f}` is **fully rational** if it is the conditional expectation of the outcome given all available information:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|I_{t}] \\quad \\text{(Eq. (1))}\n     \n    \n2.  A forecast `P_{t}^{f}` is **partially rational** if it is the conditional expectation of the outcome given the subset of information actually used:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|S_{t}] \\quad \\text{(Eq. (2))}\n     \n    \n3.  A forecast `P_{t}^{f}` is **unbiased** if it is the conditional expectation of the outcome given the forecast itself:\n    \n      \n    P_{t}^{f} = E[A_{t+f}|P_{t}^{f}] \\quad \\text{(Eq. (3))}\n     \n\n---\n\nBased on the definitions provided, select all statements that correctly describe the logical relationships between the different concepts of forecast rationality.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 196, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical framework and econometric challenges of testing whether economic forecasts are rational.\n\n**Variables & Parameters.**\n- `A_{t+f}`: The realized value of an economic variable.\n- `P_{t}^{f}`: The prediction of `A_{t+f}` made at time `t`.\n- `I_{t}`: The set of all relevant information available at time `t`.\n\n---\n\n### Data / Model Specification\n\nRationality concepts are tested using two main regression frameworks:\n- **Test for Full Rationality:** Regressing the forecast error on the information set `I_t`.\n  \n    \n  (A_{t+f}-P_{t}^{f}) = R_{f}(I_{t}) + u_{t+f} \\quad \\text{(Eq. (1))}\n   \n  \n- **Test for Unbiasedness:** Regressing the actual outcome on the forecast.\n  \n    \n  A_{t+f} = \\alpha + \\beta P_{t}^{f} + u_{t+f} \\quad \\text{(Eq. (2))}\n   \n\n---\n\nSelect all statements that correctly describe the testable implications or econometric challenges related to testing for rationality.", "Answer": ["C", "D"], "qid": "196", "question": "### Background\n\n**Research Question.** This problem explores the theoretical framework and econometric challenges of testing whether economic forecasts are rational.\n\n**Variables & Parameters.**\n- `A_{t+f}`: The realized value of an economic variable.\n- `P_{t}^{f}`: The prediction of `A_{t+f}` made at time `t`.\n- `I_{t}`: The set of all relevant information available at time `t`.\n\n---\n\n### Data / Model Specification\n\nRationality concepts are tested using two main regression frameworks:\n- **Test for Full Rationality:** Regressing the forecast error on the information set `I_t`.\n  \n    \n  (A_{t+f}-P_{t}^{f}) = R_{f}(I_{t}) + u_{t+f} \\quad \\text{(Eq. (1))}\n   \n  \n- **Test for Unbiasedness:** Regressing the actual outcome on the forecast.\n  \n    \n  A_{t+f} = \\alpha + \\beta P_{t}^{f} + u_{t+f} \\quad \\text{(Eq. (2))}\n   \n\n---\n\nSelect all statements that correctly describe the testable implications or econometric challenges related to testing for rationality.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 197, "Question": "### Background\n\n**Research Question.** This problem addresses the specific econometric challenges of testing for forecast rationality when forecasts are made for multiple periods ahead (`f > 1`), which induces serial correlation in the forecast errors even if forecasts are rational.\n\n**Setting / Institutional Environment.** In a time-series context, an `f`-period ahead forecast is made at time `t` before the outcomes from `t+1` to `t+f-1` are known. This overlapping information structure means that unexpected shocks can affect multiple forecast errors, violating the standard OLS assumption of uncorrelated errors and invalidating standard inference.\n\n---\n\n### Data / Model Specification\n\nThe general linear model for testing rationality is:\n\n  \ny_{t+f} = x_{t}'\\beta + u_{t+f} \\quad \\text{(Eq. (1))}\n \n\nUnder the rational expectations hypothesis, the forecast error `u_{t+f}` must be uncorrelated with any information available at time `t`. This implies `Cov(u_{t+f}, u_{t+f-s}) = 0` for `s \\ge f`, but allows for correlation for `s < f`. This error structure is consistent with a moving average process of order `f-1`, or `MA(f-1)`.\n\n---\n\nAccording to the paper's econometric framework for testing rationality with multi-period forecasts, which of the following statements are correct?", "Answer": ["A", "D"], "qid": "197", "question": "### Background\n\n**Research Question.** This problem addresses the specific econometric challenges of testing for forecast rationality when forecasts are made for multiple periods ahead (`f > 1`), which induces serial correlation in the forecast errors even if forecasts are rational.\n\n**Setting / Institutional Environment.** In a time-series context, an `f`-period ahead forecast is made at time `t` before the outcomes from `t+1` to `t+f-1` are known. This overlapping information structure means that unexpected shocks can affect multiple forecast errors, violating the standard OLS assumption of uncorrelated errors and invalidating standard inference.\n\n---\n\n### Data / Model Specification\n\nThe general linear model for testing rationality is:\n\n  \ny_{t+f} = x_{t}'\\beta + u_{t+f} \\quad \\text{(Eq. (1))}\n \n\nUnder the rational expectations hypothesis, the forecast error `u_{t+f}` must be uncorrelated with any information available at time `t`. This implies `Cov(u_{t+f}, u_{t+f-s}) = 0` for `s \\ge f`, but allows for correlation for `s < f`. This error structure is consistent with a moving average process of order `f-1`, or `MA(f-1)`.\n\n---\n\nAccording to the paper's econometric framework for testing rationality with multi-period forecasts, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 198, "Question": "### Background\n\n**Research Question.** This problem investigates the emergence of market failures within a mature, largely private financial system and the consequent role of government intervention.\n\n**Setting.** The context is the British capital market from the late 19th to mid-20th century. This market is described as having a \"statist start\" (Bank of England founded to fund government debt) but subsequently following an \"orthodox path\" of private institutional development, integrating local markets into a national one centered in London.\n\n**Key Concepts.**\n\n*   **Orthodox Path:** A historical mode of financial development characterized by the spontaneous evolution of institutions to contain lender exuberance and integrate markets, driven by private actors.\n*   **Market Gaps:** The text identifies two persistent failures in the British market:\n    1.  **The Macmillan Gap:** A market failure in equity finance for medium-sized enterprises. Firms needing less than £100,000 could raise capital locally, while those needing more than £1,500,000 could issue shares in London. The gap existed for capital sums between these thresholds.\n    2.  **The Temporal Gap:** A lack of financing for medium-term debt (5 to 10 years), which was needed for equipment finance. The market was well-developed for very short-term trade finance (90-180 days) and long-term borrowing (10-20 years), but not for the intermediate maturities.\n*   **Coase Theorem (in this context):** The proposition that if transaction costs are low, the specific form of institutions does not matter for achieving efficient outcomes, as private arrangements will emerge to fill any needs.\n*   **Corrective Government Institutions:** Specialized agencies created to fill identified market gaps, including:\n    *   **Industrial and Commercial Finance Corporation (ICFC):** Established after 1945 \"to fill the Macmillan gap.\"\n    *   **Export Credit Guarantees Department (ECGD):** Established in the 1930s \"to assist in financing the export of capital equipment.\"\n\n---\n\n### Data / Model Specification\n\nThis is a conceptual problem based on historical analysis. The key data points are the financial thresholds defining the Macmillan Gap.\n\n---\n\n### The Question\n\nThe text describes the British capital market as having evolved along an 'orthodox path' yet still containing significant failures. Based on the analysis provided, which of the following statements accurately describe these market gaps? (Select all that apply)", "Answer": ["A", "C", "D"], "qid": "198", "question": "### Background\n\n**Research Question.** This problem investigates the emergence of market failures within a mature, largely private financial system and the consequent role of government intervention.\n\n**Setting.** The context is the British capital market from the late 19th to mid-20th century. This market is described as having a \"statist start\" (Bank of England founded to fund government debt) but subsequently following an \"orthodox path\" of private institutional development, integrating local markets into a national one centered in London.\n\n**Key Concepts.**\n\n*   **Orthodox Path:** A historical mode of financial development characterized by the spontaneous evolution of institutions to contain lender exuberance and integrate markets, driven by private actors.\n*   **Market Gaps:** The text identifies two persistent failures in the British market:\n    1.  **The Macmillan Gap:** A market failure in equity finance for medium-sized enterprises. Firms needing less than £100,000 could raise capital locally, while those needing more than £1,500,000 could issue shares in London. The gap existed for capital sums between these thresholds.\n    2.  **The Temporal Gap:** A lack of financing for medium-term debt (5 to 10 years), which was needed for equipment finance. The market was well-developed for very short-term trade finance (90-180 days) and long-term borrowing (10-20 years), but not for the intermediate maturities.\n*   **Coase Theorem (in this context):** The proposition that if transaction costs are low, the specific form of institutions does not matter for achieving efficient outcomes, as private arrangements will emerge to fill any needs.\n*   **Corrective Government Institutions:** Specialized agencies created to fill identified market gaps, including:\n    *   **Industrial and Commercial Finance Corporation (ICFC):** Established after 1945 \"to fill the Macmillan gap.\"\n    *   **Export Credit Guarantees Department (ECGD):** Established in the 1930s \"to assist in financing the export of capital equipment.\"\n\n---\n\n### Data / Model Specification\n\nThis is a conceptual problem based on historical analysis. The key data points are the financial thresholds defining the Macmillan Gap.\n\n---\n\n### The Question\n\nThe text describes the British capital market as having evolved along an 'orthodox path' yet still containing significant failures. Based on the analysis provided, which of the following statements accurately describe these market gaps? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 199, "Question": "### Background\n\n**Research Question.** This problem examines the dynamic properties of the author's proposed alternative wage-price model and the robustness of its stability.\n\n**Setting and Sample.** The author estimates a re-specified model for the UK economy (1921-61) using both Ordinary Least Squares (OLS) and Two-Stage Least Squares (2SLS) as a robustness check.\n\n---\n\n### Data / Model Specification\n\nThe non-zero roots of the characteristic equation from each estimation are:\n- **OLS Roots:** `λ₁ = -0.88`, `λ₂ = 0.80`, `λ₃ = 0.50`\n- **2SLS Roots:** `λ₁ = -0.88`, `λ₂ = 0.76`, `λ₃ = 0.61`\n\n---\n\n### Question\n\nGiven the non-zero characteristic roots from both the OLS and 2SLS estimations, select all of the following conclusions that are valid.", "Answer": ["A", "C", "D"], "qid": "199", "question": "### Background\n\n**Research Question.** This problem examines the dynamic properties of the author's proposed alternative wage-price model and the robustness of its stability.\n\n**Setting and Sample.** The author estimates a re-specified model for the UK economy (1921-61) using both Ordinary Least Squares (OLS) and Two-Stage Least Squares (2SLS) as a robustness check.\n\n---\n\n### Data / Model Specification\n\nThe non-zero roots of the characteristic equation from each estimation are:\n- **OLS Roots:** `λ₁ = -0.88`, `λ₂ = 0.80`, `λ₃ = 0.50`\n- **2SLS Roots:** `λ₁ = -0.88`, `λ₂ = 0.76`, `λ₃ = 0.61`\n\n---\n\n### Question\n\nGiven the non-zero characteristic roots from both the OLS and 2SLS estimations, select all of the following conclusions that are valid.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 200, "Question": "### Background\n\n**Research Question.** This problem investigates the welfare implications of consumers' decisions to acquire pre-search information about product characteristics in a market with price-sensitive demand. It seeks to determine if the privately optimal level of information acquisition is also socially optimal.\n\n**Setting.** The market consists of two firms and a continuum of consumers. An individual consumer's demand for a firm's product, once chosen, is downward-sloping. The model analyzes how market outcomes change as the fraction `k` of consumers who are \"informed\" about their product matches ex-ante increases.\n\n### Data / Model Specification\n\nA consumer's utility from purchasing from firm `i` at price `p_i` is `u_{li}(p_{i}) = y + v(p_{i}) + ε_{li}`, where `v(p_i)` is the conditional surplus. `v(p)` is strictly decreasing and strictly convex (`v''(p) > 0`), implying a downward-sloping individual demand `q(p) = -v'(p) > 0`.\n\nConditional total surplus per consumer, `S`, is the sum of conditional consumer surplus and firm profit:\n\n  \nS(p) = v(p) + p \\cdot q(p) = v(p) - p \\cdot v'(p) \n \n\nThe paper establishes (Proposition 8) that the equilibrium price `p_k^*` in a market with a fraction `k` of informed consumers is an increasing function of `k`.\n\n### Question\n\nBased on the model with price-sensitive demand, select all of the following statements that are correct.", "Answer": ["C", "D"], "qid": "200", "question": "### Background\n\n**Research Question.** This problem investigates the welfare implications of consumers' decisions to acquire pre-search information about product characteristics in a market with price-sensitive demand. It seeks to determine if the privately optimal level of information acquisition is also socially optimal.\n\n**Setting.** The market consists of two firms and a continuum of consumers. An individual consumer's demand for a firm's product, once chosen, is downward-sloping. The model analyzes how market outcomes change as the fraction `k` of consumers who are \"informed\" about their product matches ex-ante increases.\n\n### Data / Model Specification\n\nA consumer's utility from purchasing from firm `i` at price `p_i` is `u_{li}(p_{i}) = y + v(p_{i}) + ε_{li}`, where `v(p_i)` is the conditional surplus. `v(p)` is strictly decreasing and strictly convex (`v''(p) > 0`), implying a downward-sloping individual demand `q(p) = -v'(p) > 0`.\n\nConditional total surplus per consumer, `S`, is the sum of conditional consumer surplus and firm profit:\n\n  \nS(p) = v(p) + p \\cdot q(p) = v(p) - p \\cdot v'(p) \n \n\nThe paper establishes (Proposition 8) that the equilibrium price `p_k^*` in a market with a fraction `k` of informed consumers is an increasing function of `k`.\n\n### Question\n\nBased on the model with price-sensitive demand, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 201, "Question": "### Background\n\n**Research Question.** This problem investigates the determination of equilibrium price in a market populated by a mix of consumers: some who are fully informed about their product matches ex-ante, and others who are uninformed and must engage in costly search.\n\n**Setting.** A duopoly market faces a unit mass of consumers. A fraction `k` are \"fully informed\": they know their match values with both firms before searching. The remaining `1-k` are \"uninformed.\" All consumers incur a search cost `c` to sample a firm and have a budget constraint `y`.\n\n### Data / Model Specification\n\nInformed consumers know their best match and go to that firm first. They will not switch to the other firm for an unexpected price increase `δ` as long as `δ < c`. Their demand is therefore locally inelastic. Uninformed consumers' demand, `D_u`, is more price-sensitive.\n\nFor small price deviations around a symmetric equilibrium `p^*`, the total demand for firm 1 is:\n\n  \nD_{1}(p_{1},p^{*})=(1-k)D_{u}(p_{1},p^{*})+\\frac{k}{2} \\quad \\text{(Eq. 1)}\n \n\nThe equilibrium price in a market with only uninformed consumers (`k=0`) is denoted `p_u^*`.\n\n### Question\n\nSelect all correct statements regarding the determination of the equilibrium price `p_k^*` in this mixed market.", "Answer": ["A", "C"], "qid": "201", "question": "### Background\n\n**Research Question.** This problem investigates the determination of equilibrium price in a market populated by a mix of consumers: some who are fully informed about their product matches ex-ante, and others who are uninformed and must engage in costly search.\n\n**Setting.** A duopoly market faces a unit mass of consumers. A fraction `k` are \"fully informed\": they know their match values with both firms before searching. The remaining `1-k` are \"uninformed.\" All consumers incur a search cost `c` to sample a firm and have a budget constraint `y`.\n\n### Data / Model Specification\n\nInformed consumers know their best match and go to that firm first. They will not switch to the other firm for an unexpected price increase `δ` as long as `δ < c`. Their demand is therefore locally inelastic. Uninformed consumers' demand, `D_u`, is more price-sensitive.\n\nFor small price deviations around a symmetric equilibrium `p^*`, the total demand for firm 1 is:\n\n  \nD_{1}(p_{1},p^{*})=(1-k)D_{u}(p_{1},p^{*})+\\frac{k}{2} \\quad \\text{(Eq. 1)}\n \n\nThe equilibrium price in a market with only uninformed consumers (`k=0`) is denoted `p_u^*`.\n\n### Question\n\nSelect all correct statements regarding the determination of the equilibrium price `p_k^*` in this mixed market.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 202, "Question": "### Background\n\n**Research Question.** This problem examines the determination of the symmetric equilibrium price in a market where all consumers are uninformed about their product matches and must incur a positive cost to search.\n\n**Setting.** The market has two firms. All consumers are \"uninformed,\" meaning they do not know their match values `ε_i` with either firm before sampling. To discover a firm's price and their match value, a consumer must incur a search cost `c > 0`.\n\n### Data / Model Specification\n\nAn uninformed consumer who samples one firm will only search the second firm if the expected gain exceeds the search cost `c`. This decision is governed by a critical match value threshold, `x̂`, defined by:\n\n  \n\\int_{\\hat{x}}^{b}(\\epsilon-{\\hat{x}})f(\\epsilon)d\\epsilon=c \\quad \\text{(Eq. 1)}\n \n\nA consumer who observes a match `ε_1` at the first firm will stop searching if `ε_1 > x̂`.\n\n### Question\n\nSelect all statements that are **incorrect** descriptions of the uninformed consumer search model presented in the paper.", "Answer": ["A", "B"], "qid": "202", "question": "### Background\n\n**Research Question.** This problem examines the determination of the symmetric equilibrium price in a market where all consumers are uninformed about their product matches and must incur a positive cost to search.\n\n**Setting.** The market has two firms. All consumers are \"uninformed,\" meaning they do not know their match values `ε_i` with either firm before sampling. To discover a firm's price and their match value, a consumer must incur a search cost `c > 0`.\n\n### Data / Model Specification\n\nAn uninformed consumer who samples one firm will only search the second firm if the expected gain exceeds the search cost `c`. This decision is governed by a critical match value threshold, `x̂`, defined by:\n\n  \n\\int_{\\hat{x}}^{b}(\\epsilon-{\\hat{x}})f(\\epsilon)d\\epsilon=c \\quad \\text{(Eq. 1)}\n \n\nA consumer who observes a match `ε_1` at the first firm will stop searching if `ε_1 > x̂`.\n\n### Question\n\nSelect all statements that are **incorrect** descriptions of the uninformed consumer search model presented in the paper.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 203, "Question": "### Background\n\nA social planner designs a redistributive tax system where agents can engage in tax avoidance. The planner's goal is to maximize utilitarian welfare. Agents privately observe their income `w` and can report any amount `y ∈ [0, w]`, converting the concealed portion `w-y` into `g(w-y)` units of secret consumption.\n\n### Data / Model Specification\n\nThe key assumption about the tax avoidance technology is that it features economies of scale, which is modeled as a superadditive cost function:\n\n**Assumption 1.** The function `g` is superadditive. For all `w, w' ≥ 0`,\n  \ng(w+w') \\geq g(w) + g(w')\n \nThe paper demonstrates that under this assumption, any tax scheme that induces avoidance is dominated by an avoidance-free scheme. The optimal tax scheme makes agents report their full income (`r*(w) = w`) and receive a net transfer `ν*(w)`:\n  \n\\nu^*(w) = g(w) + C \\quad \\text{where } C = \\int_{0}^{+\\infty} (t - g(t)) dF(t)\n \nThis scheme makes every agent indifferent between reporting their entire income and reporting the minimum possible income.\n\n### Question\n\nBased on the model and its core assumption, which of the following statements are valid implications or interpretations of the optimal tax system?\n", "Answer": ["B", "C", "D"], "qid": "203", "question": "### Background\n\nA social planner designs a redistributive tax system where agents can engage in tax avoidance. The planner's goal is to maximize utilitarian welfare. Agents privately observe their income `w` and can report any amount `y ∈ [0, w]`, converting the concealed portion `w-y` into `g(w-y)` units of secret consumption.\n\n### Data / Model Specification\n\nThe key assumption about the tax avoidance technology is that it features economies of scale, which is modeled as a superadditive cost function:\n\n**Assumption 1.** The function `g` is superadditive. For all `w, w' ≥ 0`,\n  \ng(w+w') \\geq g(w) + g(w')\n \nThe paper demonstrates that under this assumption, any tax scheme that induces avoidance is dominated by an avoidance-free scheme. The optimal tax scheme makes agents report their full income (`r*(w) = w`) and receive a net transfer `ν*(w)`:\n  \n\\nu^*(w) = g(w) + C \\quad \\text{where } C = \\int_{0}^{+\\infty} (t - g(t)) dF(t)\n \nThis scheme makes every agent indifferent between reporting their entire income and reporting the minimum possible income.\n\n### Question\n\nBased on the model and its core assumption, which of the following statements are valid implications or interpretations of the optimal tax system?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA social planner designs a redistr...", "model_answer": "This is a detailed response to the question about ### Background\n\nA social planner designs a redistr...", "error": null}}
{"ID": 204, "Question": "### Background\n\nAn economy features risk-averse agents (with CARA utility `u(x) = -e^{-αx}`) who can add fair lotteries to their initial income `w₀` before a social planner implements an ex-post optimal tax scheme. The planner is constrained by a tax avoidance technology `g(w)`.\n\n### Data / Model Specification\n\nThe ex-post optimal net transfer at date 1 is given by:\n  \n\\nu^{**}(w) = g(w) + \\int_{0}^{+\\infty} (t-g(t)) dF_1(t) \\quad \\text{(Eq. 1)}\n \nwhere `w` is the realized date-1 income and `F₁` is its distribution. An agent's decision to take risk depends on the shape of their effective utility over pre-tax income, `u(g(w))`. Risk-taking occurs if this function is not concave. The value of the agent's problem is given by the concavification of `u∘g`, denoted `overline(u∘g)(w₀)`.\n\nConsider a specific avoidance technology `g(w)` that is piecewise linear with a convex kink at income level `c`:\n  \ng(w) = (1-\\lambda)w + \\mathbf{1}_{\\{w \\geq c\\}} \\Delta\\lambda (w-c)\n \nwhere `λ > Δλ > 0`. This function `g(w)` is convex, which can make the composite function `u(g(w))` non-concave over some interval `(w_low, w_high)` where `w_low < c < w_high`.\n\n### Question\n\nGiven this specific piecewise linear avoidance technology `g(w)`, which of the following statements accurately describe the equilibrium risk-taking behavior and its consequences?\n", "Answer": ["B", "C"], "qid": "204", "question": "### Background\n\nAn economy features risk-averse agents (with CARA utility `u(x) = -e^{-αx}`) who can add fair lotteries to their initial income `w₀` before a social planner implements an ex-post optimal tax scheme. The planner is constrained by a tax avoidance technology `g(w)`.\n\n### Data / Model Specification\n\nThe ex-post optimal net transfer at date 1 is given by:\n  \n\\nu^{**}(w) = g(w) + \\int_{0}^{+\\infty} (t-g(t)) dF_1(t) \\quad \\text{(Eq. 1)}\n \nwhere `w` is the realized date-1 income and `F₁` is its distribution. An agent's decision to take risk depends on the shape of their effective utility over pre-tax income, `u(g(w))`. Risk-taking occurs if this function is not concave. The value of the agent's problem is given by the concavification of `u∘g`, denoted `overline(u∘g)(w₀)`.\n\nConsider a specific avoidance technology `g(w)` that is piecewise linear with a convex kink at income level `c`:\n  \ng(w) = (1-\\lambda)w + \\mathbf{1}_{\\{w \\geq c\\}} \\Delta\\lambda (w-c)\n \nwhere `λ > Δλ > 0`. This function `g(w)` is convex, which can make the composite function `u(g(w))` non-concave over some interval `(w_low, w_high)` where `w_low < c < w_high`.\n\n### Question\n\nGiven this specific piecewise linear avoidance technology `g(w)`, which of the following statements accurately describe the equilibrium risk-taking behavior and its consequences?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn economy features risk-averse ag...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn economy features risk-averse ag...", "error": null}}
{"ID": 205, "Question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period sales are given by:\n  \nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n \nThe resulting Nash equilibrium price for Firm A is:\n  \np_{2}^{A} = c + \\frac{1}{\\mu+\\nu} \\left[ t + \\frac{1}{3}(2\\sigma^{A}-1)((1-\\mu-\\nu)t+\\mu s) \\right] \\quad \\text{(Eq. 2)}\n \nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nGiven the equilibrium price formula in Eq. (2), which of the following statements correctly describe the market outcomes in extreme scenarios? (Select all that apply)", "Answer": ["B", "C"], "qid": "205", "question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period sales are given by:\n  \nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n \nThe resulting Nash equilibrium price for Firm A is:\n  \np_{2}^{A} = c + \\frac{1}{\\mu+\\nu} \\left[ t + \\frac{1}{3}(2\\sigma^{A}-1)((1-\\mu-\\nu)t+\\mu s) \\right] \\quad \\text{(Eq. 2)}\n \nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nGiven the equilibrium price formula in Eq. (2), which of the following statements correctly describe the market outcomes in extreme scenarios? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis question analyzes the second-...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis question analyzes the second-...", "error": null}}
{"ID": 206, "Question": "### Background\n\n**Research Question.** This problem investigates the paper's central finding: that a stable, Pareto-efficient nonsunspot equilibrium can become unstable for intermediate levels of market participation constraints, potentially triggering a bifurcation to an inefficient sunspot equilibrium. The analysis hinges on decomposing the stability problem of the high-dimensional sunspot economy into smaller, more tractable sub-problems.\n\n**Setting.** A sunspot economy `\\mathcal{E}(\\pmb{\\lambda}, \\bar{\\omega})` with `S=2` equiprobable states and `\\ell` goods per state. We analyze the stability of a nonsunspot equilibrium price vector `\\pmb{p}^* = (\\bar{p}, \\bar{p})`.\n\n### Data / Model Specification\n\nThe stability of the equilibrium is determined by the eigenvalues of the `2\\ell \\times 2\\ell` matrix `\\mathbf{J}L`, where `\\mathbf{J}` is the Jacobian of aggregate excess demand and `L` is a block-diagonal matrix of endogenous adjustment speeds. For `S=2`, these matrices have the block structure:\n\n  \n\\mathbf{J} = \\begin{bmatrix} A(\\pmb{\\lambda}) & B(\\pmb{\\lambda}) \\\\ B(\\pmb{\\lambda}) & A(\\pmb{\\lambda}) \\end{bmatrix}, \\quad L = \\begin{bmatrix} \\Lambda & \\mathbf{0} \\\\ \\mathbf{0} & \\Lambda \\end{bmatrix}\n \n\nwhere `A(\\pmb{\\lambda})` and `B(\\pmb{\\lambda})` are `\\ell \\times \\ell` matrices dependent on the vector of consumer constraint levels `\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)`, and `\\Lambda` is the `\\ell \\times \\ell` diagonal matrix of adjustment speeds from the certainty economy.\n\nA key result from the paper's appendix (Lemma 13) states that the `2\\ell` eigenvalues of the stability matrix `\\mathbf{J}L` are the union of the `\\ell` eigenvalues of `(A(\\pmb{\\lambda}) + B(\\pmb{\\lambda}))\\Lambda` and the `\\ell` eigenvalues of `(A(\\pmb{\\lambda}) - B(\\pmb{\\lambda}))\\Lambda`.\n\nThe block matrices are defined by the following relationships:\n1.  `A(\\pmb{\\lambda}) + B(\\pmb{\\lambda}) = \\bar{J}`, where `\\bar{J}` is the Jacobian of aggregate excess demand in the corresponding certainty economy.\n2.  `A(\\pmb{\\lambda}) - B(\\pmb{\\lambda}) = \\sum_{i=1}^m \\left( (1-\\lambda_i)(A_i - B_i) + \\lambda_i(A_i + B_i) \\right)`, where `A_i` and `B_i` are the building blocks of the unconstrained demand Jacobian for consumer `i`.\n\nThe **S-property** is a strong stability condition requiring that for each consumer `i`, the individual unconstrained demand Jacobian `\\mathbf{J}_i = \\begin{bmatrix} A_i & B_i \\\\ B_i & A_i \\end{bmatrix}` is negative semidefinite. This implies that `A_i+B_i` is negative semidefinite and it is given that `A_i-B_i` is negative definite.\n\n### Question\n\nBased on the provided model specifications, select all statements that are correct descriptions of the stability of the nonsunspot equilibrium under different market constraint levels.", "Answer": ["A", "D"], "qid": "206", "question": "### Background\n\n**Research Question.** This problem investigates the paper's central finding: that a stable, Pareto-efficient nonsunspot equilibrium can become unstable for intermediate levels of market participation constraints, potentially triggering a bifurcation to an inefficient sunspot equilibrium. The analysis hinges on decomposing the stability problem of the high-dimensional sunspot economy into smaller, more tractable sub-problems.\n\n**Setting.** A sunspot economy `\\mathcal{E}(\\pmb{\\lambda}, \\bar{\\omega})` with `S=2` equiprobable states and `\\ell` goods per state. We analyze the stability of a nonsunspot equilibrium price vector `\\pmb{p}^* = (\\bar{p}, \\bar{p})`.\n\n### Data / Model Specification\n\nThe stability of the equilibrium is determined by the eigenvalues of the `2\\ell \\times 2\\ell` matrix `\\mathbf{J}L`, where `\\mathbf{J}` is the Jacobian of aggregate excess demand and `L` is a block-diagonal matrix of endogenous adjustment speeds. For `S=2`, these matrices have the block structure:\n\n  \n\\mathbf{J} = \\begin{bmatrix} A(\\pmb{\\lambda}) & B(\\pmb{\\lambda}) \\\\ B(\\pmb{\\lambda}) & A(\\pmb{\\lambda}) \\end{bmatrix}, \\quad L = \\begin{bmatrix} \\Lambda & \\mathbf{0} \\\\ \\mathbf{0} & \\Lambda \\end{bmatrix}\n \n\nwhere `A(\\pmb{\\lambda})` and `B(\\pmb{\\lambda})` are `\\ell \\times \\ell` matrices dependent on the vector of consumer constraint levels `\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)`, and `\\Lambda` is the `\\ell \\times \\ell` diagonal matrix of adjustment speeds from the certainty economy.\n\nA key result from the paper's appendix (Lemma 13) states that the `2\\ell` eigenvalues of the stability matrix `\\mathbf{J}L` are the union of the `\\ell` eigenvalues of `(A(\\pmb{\\lambda}) + B(\\pmb{\\lambda}))\\Lambda` and the `\\ell` eigenvalues of `(A(\\pmb{\\lambda}) - B(\\pmb{\\lambda}))\\Lambda`.\n\nThe block matrices are defined by the following relationships:\n1.  `A(\\pmb{\\lambda}) + B(\\pmb{\\lambda}) = \\bar{J}`, where `\\bar{J}` is the Jacobian of aggregate excess demand in the corresponding certainty economy.\n2.  `A(\\pmb{\\lambda}) - B(\\pmb{\\lambda}) = \\sum_{i=1}^m \\left( (1-\\lambda_i)(A_i - B_i) + \\lambda_i(A_i + B_i) \\right)`, where `A_i` and `B_i` are the building blocks of the unconstrained demand Jacobian for consumer `i`.\n\nThe **S-property** is a strong stability condition requiring that for each consumer `i`, the individual unconstrained demand Jacobian `\\mathbf{J}_i = \\begin{bmatrix} A_i & B_i \\\\ B_i & A_i \\end{bmatrix}` is negative semidefinite. This implies that `A_i+B_i` is negative semidefinite and it is given that `A_i-B_i` is negative definite.\n\n### Question\n\nBased on the provided model specifications, select all statements that are correct descriptions of the stability of the nonsunspot equilibrium under different market constraint levels.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 207, "Question": "### Background\n\n**Research Question.** This problem explores the core mechanism of the sunspot model: the distinction between consumers who can transfer wealth across states of nature (unconstrained) and those who cannot (constrained). Understanding this friction is key to explaining how sunspot equilibria can exist.\n\n**Setting.** A sunspot economy with `S` states of nature, `\\ell` goods, and `m` types of consumers. For each consumer type `i`, a fraction `\\lambda_i \\in [0,1]` is constrained, and `1-\\lambda_i` is unconstrained.\n\n### Data / Model Specification\n\nA consumer's preferences over state-contingent consumption bundles `\\pmb{x}_i = (x_i(1), ..., x_i(S))` are represented by an expected utility function:\n\n  \nv_i(\\pmb{x}_i) = \\sum_{s=1}^{S} \\pi(s) u_i(x_i(s)) \\quad \\text{(Eq. 1)}\n \n\nwhere `u_i` is a smooth, strictly increasing, and strictly concave utility function.\n\nAn **unconstrained** consumer faces a single, pooled budget constraint:\n\n  \n\\sum_{s=1}^{S} p(s) \\cdot x_i(s) \\le \\left(\\sum_{s=1}^{S} p(s)\\right) \\cdot \\bar{\\omega}_i \\quad \\text{(Eq. 2)}\n \n\nA **constrained** consumer faces `S` separate budget constraints:\n\n  \np(s) \\cdot x_i(s) \\le p(s) \\cdot \\bar{\\omega}_i \\quad \\text{for each } s = 1, ..., S \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the utility maximization problems for constrained and unconstrained consumers, select all statements that are correct.", "Answer": ["C", "D"], "qid": "207", "question": "### Background\n\n**Research Question.** This problem explores the core mechanism of the sunspot model: the distinction between consumers who can transfer wealth across states of nature (unconstrained) and those who cannot (constrained). Understanding this friction is key to explaining how sunspot equilibria can exist.\n\n**Setting.** A sunspot economy with `S` states of nature, `\\ell` goods, and `m` types of consumers. For each consumer type `i`, a fraction `\\lambda_i \\in [0,1]` is constrained, and `1-\\lambda_i` is unconstrained.\n\n### Data / Model Specification\n\nA consumer's preferences over state-contingent consumption bundles `\\pmb{x}_i = (x_i(1), ..., x_i(S))` are represented by an expected utility function:\n\n  \nv_i(\\pmb{x}_i) = \\sum_{s=1}^{S} \\pi(s) u_i(x_i(s)) \\quad \\text{(Eq. 1)}\n \n\nwhere `u_i` is a smooth, strictly increasing, and strictly concave utility function.\n\nAn **unconstrained** consumer faces a single, pooled budget constraint:\n\n  \n\\sum_{s=1}^{S} p(s) \\cdot x_i(s) \\le \\left(\\sum_{s=1}^{S} p(s)\\right) \\cdot \\bar{\\omega}_i \\quad \\text{(Eq. 2)}\n \n\nA **constrained** consumer faces `S` separate budget constraints:\n\n  \np(s) \\cdot x_i(s) \\le p(s) \\cdot \\bar{\\omega}_i \\quad \\text{for each } s = 1, ..., S \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the utility maximization problems for constrained and unconstrained consumers, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 208, "Question": "### Background\n\n**Research Question.** This problem explores the foundations of the paper's analysis: the price adjustment dynamics and stability conditions in a standard general equilibrium model without uncertainty (the 'certainty economy').\n\n**Setting.** A certainty economy with `\\ell` goods and `m` consumers. The economy is described by consumer endowments and preferences, which give rise to an aggregate excess demand function.\n\n### Data / Model Specification\n\nThe evolution of the price vector `\\bar{p}(t)` is governed by the differential equation:\n\n  \n\\dot{\\bar{p}}(t) \\boxdot \\bar{r} = \\bar{p}(t) \\boxdot \\bar{z}(\\bar{p}(t), \\bar{\\omega}) \\quad \\text{(Eq. 1)}\n \n\nwhere `\\bar{r}` is the vector of total resources, `\\bar{z}` is the aggregate excess demand function, and `\\boxdot` is the coordinate-wise product. This dynamic implies that for each good `k`:\n\n  \n\\dot{p}_k(t) = \\left( \\frac{p_k(t)}{r_k} \\right) z_k(\\bar{p}(t), \\bar{\\omega}) \\quad \\text{(Eq. 2)}\n \n\nAggregate excess demand satisfies Walras' Law: `\\bar{p} \\cdot \\bar{z}(\\bar{p}, \\bar{\\omega}) = 0`.\n\nA (hyperbolic) equilibrium `(\\bar{p}, \\bar{\\omega})` is defined as **stable** if it is a locally asymptotically stable fixed point of this dynamical system. At a **no-trade equilibrium**, the Jacobian of aggregate excess demand, `\\bar{J}(\\bar{p}, \\bar{\\omega})`, is known to be symmetric and negative semidefinite.\n\n### Question\n\nRegarding the price dynamics and stability in the certainty economy as described, select all statements that are correct.", "Answer": ["C", "D"], "qid": "208", "question": "### Background\n\n**Research Question.** This problem explores the foundations of the paper's analysis: the price adjustment dynamics and stability conditions in a standard general equilibrium model without uncertainty (the 'certainty economy').\n\n**Setting.** A certainty economy with `\\ell` goods and `m` consumers. The economy is described by consumer endowments and preferences, which give rise to an aggregate excess demand function.\n\n### Data / Model Specification\n\nThe evolution of the price vector `\\bar{p}(t)` is governed by the differential equation:\n\n  \n\\dot{\\bar{p}}(t) \\boxdot \\bar{r} = \\bar{p}(t) \\boxdot \\bar{z}(\\bar{p}(t), \\bar{\\omega}) \\quad \\text{(Eq. 1)}\n \n\nwhere `\\bar{r}` is the vector of total resources, `\\bar{z}` is the aggregate excess demand function, and `\\boxdot` is the coordinate-wise product. This dynamic implies that for each good `k`:\n\n  \n\\dot{p}_k(t) = \\left( \\frac{p_k(t)}{r_k} \\right) z_k(\\bar{p}(t), \\bar{\\omega}) \\quad \\text{(Eq. 2)}\n \n\nAggregate excess demand satisfies Walras' Law: `\\bar{p} \\cdot \\bar{z}(\\bar{p}, \\bar{\\omega}) = 0`.\n\nA (hyperbolic) equilibrium `(\\bar{p}, \\bar{\\omega})` is defined as **stable** if it is a locally asymptotically stable fixed point of this dynamical system. At a **no-trade equilibrium**, the Jacobian of aggregate excess demand, `\\bar{J}(\\bar{p}, \\bar{\\omega})`, is known to be symmetric and negative semidefinite.\n\n### Question\n\nRegarding the price dynamics and stability in the certainty economy as described, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 209, "Question": "### Background\n\nA study compared prices for anti-malarial drugs in Uganda obtained via three methods: Standardized Patient (SP) purchases, real customer surveys, and vendor inventory listings. The core empirical model is a fixed-effects regression:\n\n  \nPrice_{ist} = \\alpha_{0} + \\alpha_{1} SP_{ist} + \\alpha_{2} RealCustomer_{ist} + \\gamma_{s} + \\text{Brand}_i + \\lambda_t + \\epsilon_{ist} \n \n\nwhere `Eq. (1)` has `Price` as the dependent variable, `SP` and `RealCustomer` are indicators for the data collection method (with vendor inventory listing as the omitted category), $\\gamma_s$ is an outlet fixed effect, and $\\text{Brand}_i$ is a drug brand fixed effect.\n\n### Data / Model Specification\n\n**Table 1: Regression-Adjusted Prices of Antimalarial Drugs by Data Collection Approach**\n\n| | All Drugs (Level) (1) | All Drugs (Log) (2) | AL Only (Level) (3) | AL Only (Log) (4) |\n|:---|:---:|:---:|:---:|:---:|\n| **SPPurchase** | 0.285** | 0.172*** | 0.335* | 0.171*** |\n| | (0.123) | (0.026) | (0.176) | (0.028) |\n| **RealCustomerSurvey** | 0.180* | 0.094*** | 0.378*** | 0.141*** |\n| | (0.095) | (0.030) | (0.133) | (0.033) |\n| **Observations** | 3462 | 3460 | 1828 | 1826 |\n| **Dep Var Mean (Inventory)** | 3.065 | 0.794 | 2.984 | 0.975 |\n\n*Notes: The dependent variable is price in 2013 USD. The omitted category is the retail inventory listing. All specifications include outlet and drug brand fixed effects. Robust standard errors clustered at the outlet level are in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nBased on the results in Table 1, which of the following statements are valid interpretations or conclusions?", "Answer": ["A", "B", "D"], "qid": "209", "question": "### Background\n\nA study compared prices for anti-malarial drugs in Uganda obtained via three methods: Standardized Patient (SP) purchases, real customer surveys, and vendor inventory listings. The core empirical model is a fixed-effects regression:\n\n  \nPrice_{ist} = \\alpha_{0} + \\alpha_{1} SP_{ist} + \\alpha_{2} RealCustomer_{ist} + \\gamma_{s} + \\text{Brand}_i + \\lambda_t + \\epsilon_{ist} \n \n\nwhere `Eq. (1)` has `Price` as the dependent variable, `SP` and `RealCustomer` are indicators for the data collection method (with vendor inventory listing as the omitted category), $\\gamma_s$ is an outlet fixed effect, and $\\text{Brand}_i$ is a drug brand fixed effect.\n\n### Data / Model Specification\n\n**Table 1: Regression-Adjusted Prices of Antimalarial Drugs by Data Collection Approach**\n\n| | All Drugs (Level) (1) | All Drugs (Log) (2) | AL Only (Level) (3) | AL Only (Log) (4) |\n|:---|:---:|:---:|:---:|:---:|\n| **SPPurchase** | 0.285** | 0.172*** | 0.335* | 0.171*** |\n| | (0.123) | (0.026) | (0.176) | (0.028) |\n| **RealCustomerSurvey** | 0.180* | 0.094*** | 0.378*** | 0.141*** |\n| | (0.095) | (0.030) | (0.133) | (0.033) |\n| **Observations** | 3462 | 3460 | 1828 | 1826 |\n| **Dep Var Mean (Inventory)** | 3.065 | 0.794 | 2.984 | 0.975 |\n\n*Notes: The dependent variable is price in 2013 USD. The omitted category is the retail inventory listing. All specifications include outlet and drug brand fixed effects. Robust standard errors clustered at the outlet level are in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nBased on the results in Table 1, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study compared prices for anti-m...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study compared prices for anti-m...", "error": null}}
{"ID": 210, "Question": "### Background\n\nA study investigated why prices reported in vendor inventory listings differ from prices paid in Standardized Patient (SP) transactions. The analysis modeled the probability of a price discrepancy at the outlet level using a Linear Probability Model (LPM).\n\n### Data / Model Specification\n\nThe dependent variable, `Ever_Price_Discrepancy`, is an indicator equal to 1 if any SP purchase price at an outlet was outside the range of prices reported in that vendor's inventory listing. Results for the sample of Artemether-lumefantrine (AL) drugs are below.\n\n**Table 1: Correlates of Ever Price Disagreement (AL Only Sample)**\n\n| | No Parish FE (3) | With Parish FE (4) |\n|:---|:---:|:---:|\n| **# Drugs listed** | -0.049*** | -0.046*** |\n| | (0.009) | (0.009) |\n| **# Pharmacies in market** | -0.113*** | -0.094** |\n| | (0.038) | (0.045) |\n| **% Customers buying on credit** | 0.028*** | 0.018* |\n| | (0.009) | (0.010) |\n| **Observations** | 369 | 369 |\n| **R²** | 0.211 | 0.291 |\n| **Mean Dep Var** | 0.691 | 0.691 |\n\n*Notes: Robust standard errors in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nBased on the results in Table 1, which of the following statements represent valid interpretations or plausible economic mechanisms consistent with the findings?", "Answer": ["A", "B", "C", "D"], "qid": "210", "question": "### Background\n\nA study investigated why prices reported in vendor inventory listings differ from prices paid in Standardized Patient (SP) transactions. The analysis modeled the probability of a price discrepancy at the outlet level using a Linear Probability Model (LPM).\n\n### Data / Model Specification\n\nThe dependent variable, `Ever_Price_Discrepancy`, is an indicator equal to 1 if any SP purchase price at an outlet was outside the range of prices reported in that vendor's inventory listing. Results for the sample of Artemether-lumefantrine (AL) drugs are below.\n\n**Table 1: Correlates of Ever Price Disagreement (AL Only Sample)**\n\n| | No Parish FE (3) | With Parish FE (4) |\n|:---|:---:|:---:|\n| **# Drugs listed** | -0.049*** | -0.046*** |\n| | (0.009) | (0.009) |\n| **# Pharmacies in market** | -0.113*** | -0.094** |\n| | (0.038) | (0.045) |\n| **% Customers buying on credit** | 0.028*** | 0.018* |\n| | (0.009) | (0.010) |\n| **Observations** | 369 | 369 |\n| **R²** | 0.211 | 0.291 |\n| **Mean Dep Var** | 0.691 | 0.691 |\n\n*Notes: Robust standard errors in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nBased on the results in Table 1, which of the following statements represent valid interpretations or plausible economic mechanisms consistent with the findings?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study investigated why prices re...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study investigated why prices re...", "error": null}}
{"ID": 211, "Question": "### Background\n\nA study investigated why the set of drugs purchased by Standardized Patients (SPs) often differs from the set of drugs reported by vendors in inventory listings. A key feature of the market is 'drug diversion,' where drugs intended for free distribution are illicitly sold.\n\n### Data / Model Specification\n\nThe analysis uses a Linear Probability Model (LPM) where the dependent variable, `Drug in Vendor & SP Data`, is 1 if the brand purchased by the SP was also listed in that outlet's vendor survey.\n\n**Table 1: LPMs of Selection Into Vendor Listing Using SP Data**\n\n| | All Drugs (1) | AL Only (3) |\n|:---|:---:|:---:|\n| **Diverted drug** | -0.095 | -0.157* |\n| | (0.084) | (0.093) |\n| **Drug picked from open** | 0.092** | 0.108** |\n| | (0.045) | (0.048) |\n| **Observations** | 835 | 724 |\n| **Mean Dep. Var.** | 0.59 | 0.60 |\n\n*Notes: Robust standard errors in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nWhich of the following statements are valid conclusions or interpretations that can be drawn from the analysis presented in Table 1?", "Answer": ["A", "B", "C", "D"], "qid": "211", "question": "### Background\n\nA study investigated why the set of drugs purchased by Standardized Patients (SPs) often differs from the set of drugs reported by vendors in inventory listings. A key feature of the market is 'drug diversion,' where drugs intended for free distribution are illicitly sold.\n\n### Data / Model Specification\n\nThe analysis uses a Linear Probability Model (LPM) where the dependent variable, `Drug in Vendor & SP Data`, is 1 if the brand purchased by the SP was also listed in that outlet's vendor survey.\n\n**Table 1: LPMs of Selection Into Vendor Listing Using SP Data**\n\n| | All Drugs (1) | AL Only (3) |\n|:---|:---:|:---:|\n| **Diverted drug** | -0.095 | -0.157* |\n| | (0.084) | (0.093) |\n| **Drug picked from open** | 0.092** | 0.108** |\n| | (0.045) | (0.048) |\n| **Observations** | 835 | 724 |\n| **Mean Dep. Var.** | 0.59 | 0.60 |\n\n*Notes: Robust standard errors in parentheses. ***p<0.01, **p<0.05, *p<0.1.*\n\n---\n\nWhich of the following statements are valid conclusions or interpretations that can be drawn from the analysis presented in Table 1?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study investigated why the set o...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study investigated why the set o...", "error": null}}
{"ID": 212, "Question": "### Background\n\nA study compared prices for anti-malarial drugs in Uganda obtained via three methods: Standardized Patient (SP) purchases, real customer surveys, and vendor inventory listings. The core empirical model is a fixed-effects regression that controls for time-invariant outlet and drug brand characteristics.\n\n### Data / Model Specification\n\nThe model is specified as:\n  \nPrice_{ist} = \\alpha_{0} + \\alpha_{1} SP_{ist} + \\alpha_{2} RealCustomer_{ist} + \\gamma_{s} + \\delta'X + \\epsilon_{ist} \n \nwhere `Eq. (1)` has `Price` as the dependent variable, `SP` and `RealCustomer` are indicators for the data collection method, and $\\gamma_s$ is an outlet-specific fixed effect.\n\n---\n\nRegarding the identification strategy and potential econometric issues in this study, which of the following statements are **INCORRECT**?", "Answer": ["A", "B"], "qid": "212", "question": "### Background\n\nA study compared prices for anti-malarial drugs in Uganda obtained via three methods: Standardized Patient (SP) purchases, real customer surveys, and vendor inventory listings. The core empirical model is a fixed-effects regression that controls for time-invariant outlet and drug brand characteristics.\n\n### Data / Model Specification\n\nThe model is specified as:\n  \nPrice_{ist} = \\alpha_{0} + \\alpha_{1} SP_{ist} + \\alpha_{2} RealCustomer_{ist} + \\gamma_{s} + \\delta'X + \\epsilon_{ist} \n \nwhere `Eq. (1)` has `Price` as the dependent variable, `SP` and `RealCustomer` are indicators for the data collection method, and $\\gamma_s$ is an outlet-specific fixed effect.\n\n---\n\nRegarding the identification strategy and potential econometric issues in this study, which of the following statements are **INCORRECT**?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study compared prices for anti-m...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study compared prices for anti-m...", "error": null}}
{"ID": 213, "Question": "### Background\n\nAn economic model analyzes a Free Trade Area (FTA) with two member countries (1 and 2) and two firms: an inside firm (Firm I) and an outside firm (Firm O). The firms produce imperfectly substitutable goods and compete in prices. The introduction of Rules of Origin (ROOs) is compared to a baseline without ROOs. ROOs create two effects for Firm O: a harmful **anticircumvention effect** (it can no longer use the lowest tariff country for all its exports) and a beneficial **price-discrimination effect** (it can set different prices in countries 1 and 2).\n\n### Data / Model Specification\n\nThe following table presents numerical simulation results showing the change in profits for Firm I (`Δπᴵ`) and Firm O (`Δπᴼ`) when ROOs are introduced. The baseline parameters are `A=5` (total market size), `B=0.5` (total substitutability), `Δa=1` (market size difference), and `t₂=0.7` (low tariff).\n\n**Table 1: Profit Changes from Introducing ROOs (`Δπᴵ`, `Δπᴼ`)**\n\n| `Δt` | `Δb = 0` | | `Δb = 0.1` | | `Δb = -0.1` | |\n|---|---:|---:|---:|---:|---:|---:|\n| | **Firm I** | **Firm O** | **Firm I** | **Firm O** | **Firm I** | **Firm O** |\n| 0.0 | 0.000 | 0.125 | 0.043 | 0.169 | -0.032 | 0.088 |\n| 0.5 | 0.094 | -0.472 | 0.157 | -0.445 | 0.043 | -0.492 |\n| 1.0 | 0.189 | -0.946 | 0.272 | -0.937 | 0.119 | -0.948 |\n| 1.5 | 0.285 | -1.297 | 0.388 | -1.307 | 0.195 | -1.280 |\n| 2.0 | 0.382 | -1.525 | 0.506 | -1.555 | 0.272 | -1.489 |\n| 2.5 | 0.480 | -1.629 | 0.626 | -1.680 | 0.349 | -1.574 |\n\n*   `Δb`: Difference in substitutability, `b₁ - b₂`.\n*   `Δt`: Difference in tariffs, `t₁ - t₂`.\n\n---\n\nBased on the data in Table 1, select all of the following scenarios that demonstrate an **anticompetitive outcome**, where the introduction of ROOs leads to an increase in profits for **both** Firm I and Firm O.", "Answer": ["A", "D"], "qid": "213", "question": "### Background\n\nAn economic model analyzes a Free Trade Area (FTA) with two member countries (1 and 2) and two firms: an inside firm (Firm I) and an outside firm (Firm O). The firms produce imperfectly substitutable goods and compete in prices. The introduction of Rules of Origin (ROOs) is compared to a baseline without ROOs. ROOs create two effects for Firm O: a harmful **anticircumvention effect** (it can no longer use the lowest tariff country for all its exports) and a beneficial **price-discrimination effect** (it can set different prices in countries 1 and 2).\n\n### Data / Model Specification\n\nThe following table presents numerical simulation results showing the change in profits for Firm I (`Δπᴵ`) and Firm O (`Δπᴼ`) when ROOs are introduced. The baseline parameters are `A=5` (total market size), `B=0.5` (total substitutability), `Δa=1` (market size difference), and `t₂=0.7` (low tariff).\n\n**Table 1: Profit Changes from Introducing ROOs (`Δπᴵ`, `Δπᴼ`)**\n\n| `Δt` | `Δb = 0` | | `Δb = 0.1` | | `Δb = -0.1` | |\n|---|---:|---:|---:|---:|---:|---:|\n| | **Firm I** | **Firm O** | **Firm I** | **Firm O** | **Firm I** | **Firm O** |\n| 0.0 | 0.000 | 0.125 | 0.043 | 0.169 | -0.032 | 0.088 |\n| 0.5 | 0.094 | -0.472 | 0.157 | -0.445 | 0.043 | -0.492 |\n| 1.0 | 0.189 | -0.946 | 0.272 | -0.937 | 0.119 | -0.948 |\n| 1.5 | 0.285 | -1.297 | 0.388 | -1.307 | 0.195 | -1.280 |\n| 2.0 | 0.382 | -1.525 | 0.506 | -1.555 | 0.272 | -1.489 |\n| 2.5 | 0.480 | -1.629 | 0.626 | -1.680 | 0.349 | -1.574 |\n\n*   `Δb`: Difference in substitutability, `b₁ - b₂`.\n*   `Δt`: Difference in tariffs, `t₁ - t₂`.\n\n---\n\nBased on the data in Table 1, select all of the following scenarios that demonstrate an **anticompetitive outcome**, where the introduction of ROOs leads to an increase in profits for **both** Firm I and Firm O.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn economic model analyzes a Free ...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn economic model analyzes a Free ...", "error": null}}
{"ID": 214, "Question": "### Background\n\nAn economic model analyzes a Free Trade Area (FTA) with two member countries (1 and 2) and two firms: an inside firm (Firm I) and an outside firm (Firm O). The firms produce imperfectly substitutable goods and compete in prices. The introduction of Rules of Origin (ROOs) is compared to a baseline without ROOs. ROOs create two effects for Firm O: a harmful **anticomvention effect** and a beneficial **price-discrimination effect**.\n\n### Data / Model Specification\n\nThe following table presents numerical simulation results showing the change in profits for Firm I (`Δπᴵ`) and Firm O (`Δπᴼ`) when ROOs are introduced. The baseline parameters are `A=5` (total market size), `B=0.5` (total substitutability), `Δa=1` (market size difference), and `t₂=0.7` (low tariff).\n\n**Table 1: Profit Changes from Introducing ROOs (`Δπᴵ`, `Δπᴼ`)**\n\n| `Δt` | `Δb = 0` | | `Δb = 0.1` | | `Δb = -0.1` | |\n|---|---:|---:|---:|---:|---:|---:|\n| | **Firm I** | **Firm O** | **Firm I** | **Firm O** | **Firm I** | **Firm O** |\n| 0.0 | 0.000 | 0.125 | 0.043 | 0.169 | -0.032 | 0.088 |\n| 0.5 | 0.094 | -0.472 | 0.157 | -0.445 | 0.043 | -0.492 |\n| 1.0 | 0.189 | -0.946 | 0.272 | -0.937 | 0.119 | -0.948 |\n| 1.5 | 0.285 | -1.297 | 0.388 | -1.307 | 0.195 | -1.280 |\n| 2.0 | 0.382 | -1.525 | 0.506 | -1.555 | 0.272 | -1.489 |\n| 2.5 | 0.480 | -1.629 | 0.626 | -1.680 | 0.349 | -1.574 |\n\n*   `Δb`: Difference in substitutability, `b₁ - b₂`.\n*   `Δt`: Difference in tariffs, `t₁ - t₂`.\n\n---\n\nAn FTA authority is considering implementing ROOs and wants to set the tariff differential `Δt` to maximize the total producer surplus (`Δπᴵ + Δπᴼ`). Based on the data in Table 1, which of the following statements are correct conclusions?\n", "Answer": ["A", "C", "D"], "qid": "214", "question": "### Background\n\nAn economic model analyzes a Free Trade Area (FTA) with two member countries (1 and 2) and two firms: an inside firm (Firm I) and an outside firm (Firm O). The firms produce imperfectly substitutable goods and compete in prices. The introduction of Rules of Origin (ROOs) is compared to a baseline without ROOs. ROOs create two effects for Firm O: a harmful **anticomvention effect** and a beneficial **price-discrimination effect**.\n\n### Data / Model Specification\n\nThe following table presents numerical simulation results showing the change in profits for Firm I (`Δπᴵ`) and Firm O (`Δπᴼ`) when ROOs are introduced. The baseline parameters are `A=5` (total market size), `B=0.5` (total substitutability), `Δa=1` (market size difference), and `t₂=0.7` (low tariff).\n\n**Table 1: Profit Changes from Introducing ROOs (`Δπᴵ`, `Δπᴼ`)**\n\n| `Δt` | `Δb = 0` | | `Δb = 0.1` | | `Δb = -0.1` | |\n|---|---:|---:|---:|---:|---:|---:|\n| | **Firm I** | **Firm O** | **Firm I** | **Firm O** | **Firm I** | **Firm O** |\n| 0.0 | 0.000 | 0.125 | 0.043 | 0.169 | -0.032 | 0.088 |\n| 0.5 | 0.094 | -0.472 | 0.157 | -0.445 | 0.043 | -0.492 |\n| 1.0 | 0.189 | -0.946 | 0.272 | -0.937 | 0.119 | -0.948 |\n| 1.5 | 0.285 | -1.297 | 0.388 | -1.307 | 0.195 | -1.280 |\n| 2.0 | 0.382 | -1.525 | 0.506 | -1.555 | 0.272 | -1.489 |\n| 2.5 | 0.480 | -1.629 | 0.626 | -1.680 | 0.349 | -1.574 |\n\n*   `Δb`: Difference in substitutability, `b₁ - b₂`.\n*   `Δt`: Difference in tariffs, `t₁ - t₂`.\n\n---\n\nAn FTA authority is considering implementing ROOs and wants to set the tariff differential `Δt` to maximize the total producer surplus (`Δπᴵ + Δπᴼ`). Based on the data in Table 1, which of the following statements are correct conclusions?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn economic model analyzes a Free ...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn economic model analyzes a Free ...", "error": null}}
{"ID": 215, "Question": "### Background\n\nIn a model of an FTA, the introduction of Rules of Origin (ROOs) can paradoxically harm the inside firm (Firm I) by causing its equilibrium price to fall. This occurs when a negative **price-discrimination effect** outweighs the positive **anticircumvention effect**. The price-discrimination effect's sign depends on the difference in product substitutability between the two FTA member countries, `Δb = b₁ - b₂`.\n\n### Data / Model Specification\n\nThe equilibrium price for Firm I with ROOs is given by:\n  \n\\hat{p}^{I} = \\frac{2A + a_1b_1 + a_2b_2 + t_1b_1 + t_2b_2}{\\Gamma}\n \n<p align=\"center\">Eq. (1)</p>\n\nwhere `A = a₁ + a₂`, `t₁` and `t₂` are tariffs, `a₁` and `a₂` are market sizes, `b₁` and `b₂` are substitutability parameters, and `Γ = 8 - b₁² - b₂² > 0` is a stability condition.\n\n---\n\nBased on the model, which of the following statements correctly describe the strategic effects of ROOs on Firm I's price (`p̂ᴵ`)?\n", "Answer": ["A", "B"], "qid": "215", "question": "### Background\n\nIn a model of an FTA, the introduction of Rules of Origin (ROOs) can paradoxically harm the inside firm (Firm I) by causing its equilibrium price to fall. This occurs when a negative **price-discrimination effect** outweighs the positive **anticircumvention effect**. The price-discrimination effect's sign depends on the difference in product substitutability between the two FTA member countries, `Δb = b₁ - b₂`.\n\n### Data / Model Specification\n\nThe equilibrium price for Firm I with ROOs is given by:\n  \n\\hat{p}^{I} = \\frac{2A + a_1b_1 + a_2b_2 + t_1b_1 + t_2b_2}{\\Gamma}\n \n<p align=\"center\">Eq. (1)</p>\n\nwhere `A = a₁ + a₂`, `t₁` and `t₂` are tariffs, `a₁` and `a₂` are market sizes, `b₁` and `b₂` are substitutability parameters, and `Γ = 8 - b₁² - b₂² > 0` is a stability condition.\n\n---\n\nBased on the model, which of the following statements correctly describe the strategic effects of ROOs on Firm I's price (`p̂ᴵ`)?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn a model of an FTA, the introduc...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn a model of an FTA, the introduc...", "error": null}}
{"ID": 216, "Question": "### Background\n\nThe net welfare effect of Rules of Origin (ROOs) on an FTA's member countries depends on a trade-off between changes in producer surplus (`Δπᴵ`), consumer surplus (`ΔS`), and tariff revenue (`ΔR`). The paper establishes that for ROOs to be welfare-enhancing (`ΔW > 0`), a specific combination of market conditions and tariff policies is required.\n\n### Data / Model Specification\n\nFor the case where product substitutability is the same in both member countries (`Δb = 0`), the model yields the following general results:\n*   `Δπᴵ ≥ 0`: The inside firm's profit does not decrease.\n*   `ΔS ≤ 0`: Total consumer surplus does not increase.\n*   `ΔR` is concave (inverse U-shaped) with respect to the tariff differential `Δt`, and is more likely to be positive when the initial tariff `t₂` is low.\n\n---\n\nAccording to the model's analysis, which of the following conditions would make it **LESS LIKELY** for ROOs to improve the total welfare of the FTA (`ΔW > 0`)?\n", "Answer": ["A", "B", "C", "D"], "qid": "216", "question": "### Background\n\nThe net welfare effect of Rules of Origin (ROOs) on an FTA's member countries depends on a trade-off between changes in producer surplus (`Δπᴵ`), consumer surplus (`ΔS`), and tariff revenue (`ΔR`). The paper establishes that for ROOs to be welfare-enhancing (`ΔW > 0`), a specific combination of market conditions and tariff policies is required.\n\n### Data / Model Specification\n\nFor the case where product substitutability is the same in both member countries (`Δb = 0`), the model yields the following general results:\n*   `Δπᴵ ≥ 0`: The inside firm's profit does not decrease.\n*   `ΔS ≤ 0`: Total consumer surplus does not increase.\n*   `ΔR` is concave (inverse U-shaped) with respect to the tariff differential `Δt`, and is more likely to be positive when the initial tariff `t₂` is low.\n\n---\n\nAccording to the model's analysis, which of the following conditions would make it **LESS LIKELY** for ROOs to improve the total welfare of the FTA (`ΔW > 0`)?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe net welfare effect of Rules of...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe net welfare effect of Rules of...", "error": null}}
{"ID": 217, "Question": "### Background\n\n**Research Question.** This problem concerns how to empirically distinguish between the “child endowment” and “birth plannedness” hypotheses as explanations for why children born in auspicious years achieve better outcomes.\n\n**Setting / Institutional Environment.** The analysis is set in Vietnam, where a crucial feature of the horoscope is that its designation as auspicious is specific to the gender of the child. This feature is the key to the identification strategy.\n\nTwo primary hypotheses are considered:\n- **Child Endowment Channel:** Children born with a favorable horoscope (i.e., their gender matches the year's auspicious designation, making them *ex-post lucky*) receive greater parental investment, making the horoscope self-fulfilling.\n- **Birth Plannedness Channel:** Parents are more likely to plan for a child in a year they believe to be auspicious (*ex-ante planned*). These planned children benefit from more favorable environments, leading to better outcomes, irrespective of their ultimate gender.\n\nConsider a hypothetical family with three siblings:\n- **Bao (boy)** and **Giang (girl)** are twins, born in a year that is auspicious for boys only.\n- **Nga (girl)** is their younger sister, born in a year that is auspicious for neither boys nor girls.\n\n### Question\n\nBased on the logic of the two competing hypotheses, which of the following predictions about the relative educational outcomes (`S`) of the three siblings are correct? Select all that apply.", "Answer": ["C", "D"], "qid": "217", "question": "### Background\n\n**Research Question.** This problem concerns how to empirically distinguish between the “child endowment” and “birth plannedness” hypotheses as explanations for why children born in auspicious years achieve better outcomes.\n\n**Setting / Institutional Environment.** The analysis is set in Vietnam, where a crucial feature of the horoscope is that its designation as auspicious is specific to the gender of the child. This feature is the key to the identification strategy.\n\nTwo primary hypotheses are considered:\n- **Child Endowment Channel:** Children born with a favorable horoscope (i.e., their gender matches the year's auspicious designation, making them *ex-post lucky*) receive greater parental investment, making the horoscope self-fulfilling.\n- **Birth Plannedness Channel:** Parents are more likely to plan for a child in a year they believe to be auspicious (*ex-ante planned*). These planned children benefit from more favorable environments, leading to better outcomes, irrespective of their ultimate gender.\n\nConsider a hypothetical family with three siblings:\n- **Bao (boy)** and **Giang (girl)** are twins, born in a year that is auspicious for boys only.\n- **Nga (girl)** is their younger sister, born in a year that is auspicious for neither boys nor girls.\n\n### Question\n\nBased on the logic of the two competing hypotheses, which of the following predictions about the relative educational outcomes (`S`) of the three siblings are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 218, "Question": "### Background\n\n**Research Question.** This problem concerns the formal empirical strategy used to distinguish between the “child endowment” and “birth plannedness” hypotheses.\n\n**Setting / Institutional Environment.** The paper uses a family fixed-effects model to compare siblings. The key explanatory variables are:\n- `G_t`: An indicator for an *ex-ante planned* birth (year `t` is auspicious for *either* gender).\n- `g_ijt`: An indicator for *ex-post luck* (child `i`'s gender matches the year's specific omen).\n\nThe conceptual regression model is:\n  \n\\text{Education}_{ijt} = \\beta_1 G_t + \\beta_2 g_{ijt} + \\text{Controls} + \\eta_j + \\epsilon_{ijt}\n \nwhere `\\eta_j` is a family fixed effect.\n\n### Question\n\nRegarding the paper's identification strategy and findings, select all statements that are correct.", "Answer": ["A", "D"], "qid": "218", "question": "### Background\n\n**Research Question.** This problem concerns the formal empirical strategy used to distinguish between the “child endowment” and “birth plannedness” hypotheses.\n\n**Setting / Institutional Environment.** The paper uses a family fixed-effects model to compare siblings. The key explanatory variables are:\n- `G_t`: An indicator for an *ex-ante planned* birth (year `t` is auspicious for *either* gender).\n- `g_ijt`: An indicator for *ex-post luck* (child `i`'s gender matches the year's specific omen).\n\nThe conceptual regression model is:\n  \n\\text{Education}_{ijt} = \\beta_1 G_t + \\beta_2 g_{ijt} + \\text{Controls} + \\eta_j + \\epsilon_{ijt}\n \nwhere `\\eta_j` is a family fixed effect.\n\n### Question\n\nRegarding the paper's identification strategy and findings, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 219, "Question": "### Background\n\n**Research Question.** This problem examines the core econometric challenge in estimating the causal effect of being born in an auspicious year: unobserved parental heterogeneity.\n\n**Setting / Institutional Environment.** The analysis compares two approaches. The first is a cross-sectional OLS model. The second is a within-family (sibling comparison) fixed-effects (FE) model, designed to control for “cohort composition” bias, where parents who plan births may also be inherently better at investing in their children.\n\nThe baseline cross-sectional OLS model is:\n  \ny_{ijt} = c_{OLS} G_{t} + X_{ijt}b + e_{ijt} \\quad \\text{(Eq. 1)}\n \nThe family fixed-effects (FE) specification is:\n  \ny_{ijt} = c_{FE} G_{t} + X_{ijt}b + \\eta_{j} + u_{ijt} \\quad \\text{(Eq. 2)}\n \nwhere `y_ijt` is the child's education, `G_t` is an indicator for an auspicious birth year, and `\\eta_j` captures all time-invariant factors for family `j`.\n\n### Question\n\nRegarding the estimation of the effect of being born in an auspicious year (`G_t`) on education, select all statements that are **incorrect**.", "Answer": ["C", "D"], "qid": "219", "question": "### Background\n\n**Research Question.** This problem examines the core econometric challenge in estimating the causal effect of being born in an auspicious year: unobserved parental heterogeneity.\n\n**Setting / Institutional Environment.** The analysis compares two approaches. The first is a cross-sectional OLS model. The second is a within-family (sibling comparison) fixed-effects (FE) model, designed to control for “cohort composition” bias, where parents who plan births may also be inherently better at investing in their children.\n\nThe baseline cross-sectional OLS model is:\n  \ny_{ijt} = c_{OLS} G_{t} + X_{ijt}b + e_{ijt} \\quad \\text{(Eq. 1)}\n \nThe family fixed-effects (FE) specification is:\n  \ny_{ijt} = c_{FE} G_{t} + X_{ijt}b + \\eta_{j} + u_{ijt} \\quad \\text{(Eq. 2)}\n \nwhere `y_ijt` is the child's education, `G_t` is an indicator for an auspicious birth year, and `\\eta_j` captures all time-invariant factors for family `j`.\n\n### Question\n\nRegarding the estimation of the effect of being born in an auspicious year (`G_t`) on education, select all statements that are **incorrect**.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 220, "Question": "### Background\n\n**Research Question.** This problem examines the causal effect of reported on-the-job sex discrimination on a woman's probability of subsequently changing employers, a key measure of labor market attachment.\n\n**Setting / Institutional Environment.** The analysis uses a logit model on longitudinal data from the National Longitudinal Survey of Young Women (NLSYW). A central challenge is that women who report discrimination may be systematically different from those who do not (heterogeneity bias). The paper's preferred identification strategy attempts to solve this by focusing on a subsample of women who initially reported no discrimination, and then comparing the future outcomes of those who later \"switch\" to reporting discrimination with those who continue to report no discrimination.\n\n### Data / Model Specification\n\nThe underlying model is a logit of the form `P(Change of Employer = 1) = F(α + D_it*δ + X_it'*β)`, where `F(.)` is the logistic CDF.\n\n**Table 1: Logit Estimates for the Effect of Discrimination on Employer Change**\n\n| Specification | (2) First Report (Restricted Sample) | (3) Second Report (\"Switcher\" Estimate) |\n| :--- | :---: | :---: |\n| `D_it` Coefficient (Std. Err.) | 0.28 (0.16) | 0.52 (0.21) |\n| Partial Derivative | [0.07] | [0.13] |\n\n*Note: Column (2) uses the first discrimination report for the same subsample of women used in Column (3). Column (3) defines `D_it` as switching to report discrimination at the second report, for women who reported no discrimination at the first report.* \n\n---\n\nBased on the provided information, which of the following statements are valid interpretations or conclusions?", "Answer": ["B", "C", "D"], "qid": "220", "question": "### Background\n\n**Research Question.** This problem examines the causal effect of reported on-the-job sex discrimination on a woman's probability of subsequently changing employers, a key measure of labor market attachment.\n\n**Setting / Institutional Environment.** The analysis uses a logit model on longitudinal data from the National Longitudinal Survey of Young Women (NLSYW). A central challenge is that women who report discrimination may be systematically different from those who do not (heterogeneity bias). The paper's preferred identification strategy attempts to solve this by focusing on a subsample of women who initially reported no discrimination, and then comparing the future outcomes of those who later \"switch\" to reporting discrimination with those who continue to report no discrimination.\n\n### Data / Model Specification\n\nThe underlying model is a logit of the form `P(Change of Employer = 1) = F(α + D_it*δ + X_it'*β)`, where `F(.)` is the logistic CDF.\n\n**Table 1: Logit Estimates for the Effect of Discrimination on Employer Change**\n\n| Specification | (2) First Report (Restricted Sample) | (3) Second Report (\"Switcher\" Estimate) |\n| :--- | :---: | :---: |\n| `D_it` Coefficient (Std. Err.) | 0.28 (0.16) | 0.52 (0.21) |\n| Partial Derivative | [0.07] | [0.13] |\n\n*Note: Column (2) uses the first discrimination report for the same subsample of women used in Column (3). Column (3) defines `D_it` as switching to report discrimination at the second report, for women who reported no discrimination at the first report.* \n\n---\n\nBased on the provided information, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 221, "Question": "### Background\n\n**Research Question.** This problem explores whether women who report experiencing sex discrimination are systematically different from those who do not, and how this potential selection complicates the causal identification of discrimination's effects.\n\n**Setting / Institutional Environment.** The analysis uses descriptive statistics from the National Longitudinal Survey of Young Women. Two comparisons are central: (1) a cross-sectional comparison of women who do and do not report discrimination at their first survey, and (2) a longitudinal comparison of women who initially report no discrimination, tracking those who continue to report no discrimination versus those who \"switch\" to reporting discrimination at a later survey.\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Discrimination Report Status (Standard Errors in Parentheses)**\n\n| Variable | (1) Discrim. at 1st Report | (2) No Discrim. at 1st Report | (3) No Discrim. (1st) -> No Discrim. (2nd) | (4) No Discrim. (1st) -> Discrim. (2nd) |\n| :--- | :---: | :---: | :---: | :---: |\n| Log real wage | 1.99 (0.05) | 1.85 (0.01) | 1.85 (0.01) | 1.89 (0.04) |\n| Schooling (years) | 14.13 (0.20) | 12.93 (0.06) | 12.96 (0.06) | 13.23 (0.23) |\n| Number of children | 0.63 (0.07) | 0.99 (0.03) | 1.04 (0.03) | 0.76 (0.10) |\n\n---\n\nBased on the data in Table 1 and the study's premise, which of the following statements accurately describe the selection issues and the identification strategy?\n", "Answer": ["A", "B", "D"], "qid": "221", "question": "### Background\n\n**Research Question.** This problem explores whether women who report experiencing sex discrimination are systematically different from those who do not, and how this potential selection complicates the causal identification of discrimination's effects.\n\n**Setting / Institutional Environment.** The analysis uses descriptive statistics from the National Longitudinal Survey of Young Women. Two comparisons are central: (1) a cross-sectional comparison of women who do and do not report discrimination at their first survey, and (2) a longitudinal comparison of women who initially report no discrimination, tracking those who continue to report no discrimination versus those who \"switch\" to reporting discrimination at a later survey.\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Discrimination Report Status (Standard Errors in Parentheses)**\n\n| Variable | (1) Discrim. at 1st Report | (2) No Discrim. at 1st Report | (3) No Discrim. (1st) -> No Discrim. (2nd) | (4) No Discrim. (1st) -> Discrim. (2nd) |\n| :--- | :---: | :---: | :---: | :---: |\n| Log real wage | 1.99 (0.05) | 1.85 (0.01) | 1.85 (0.01) | 1.89 (0.04) |\n| Schooling (years) | 14.13 (0.20) | 12.93 (0.06) | 12.96 (0.06) | 13.23 (0.23) |\n| Number of children | 0.63 (0.07) | 0.99 (0.03) | 1.04 (0.03) | 0.76 (0.10) |\n\n---\n\nBased on the data in Table 1 and the study's premise, which of the following statements accurately describe the selection issues and the identification strategy?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 222, "Question": "### Background\n\n**Research Question.** This problem tests a key prediction of the \"feedback hypothesis\": whether experiencing on-the-job sex discrimination leads to lower subsequent wage growth for women who remain employed.\n\n**Setting / Institutional Environment.** The analysis regresses future wage growth on an indicator for reporting discrimination. A primary econometric challenge is non-random sample selection, as wage growth is only observed for women who continue to work. A two-equation selection model is used to address this.\n\n### Data / Model Specification\n\nThe study uses a selection model:\n\n  \n\\text{Wage Change Eq: } \\Delta w_{it'} = \\alpha + D_{it}\\delta + \\epsilon_{it'} \\quad \\text{(Eq. (1))}\n \n\n  \n\\text{Selection Eq: } P^{*}_{it'} > \\gamma + Z_{it}\\pi + \\eta_{it'} \\quad \\text{(observed if employed)} \\quad \\text{(Eq. (2))}\n \nIt is assumed that `(ε, η)` follow a bivariate normal distribution with correlation `ρ`. The paper hypothesizes that among women who report discrimination, \"only women with particularly high wage growth may work in subsequent years.\"\n\n---\n\nGiven this framework, which of the following statements about the selection model and its identification are correct?\n", "Answer": ["A", "C", "D"], "qid": "222", "question": "### Background\n\n**Research Question.** This problem tests a key prediction of the \"feedback hypothesis\": whether experiencing on-the-job sex discrimination leads to lower subsequent wage growth for women who remain employed.\n\n**Setting / Institutional Environment.** The analysis regresses future wage growth on an indicator for reporting discrimination. A primary econometric challenge is non-random sample selection, as wage growth is only observed for women who continue to work. A two-equation selection model is used to address this.\n\n### Data / Model Specification\n\nThe study uses a selection model:\n\n  \n\\text{Wage Change Eq: } \\Delta w_{it'} = \\alpha + D_{it}\\delta + \\epsilon_{it'} \\quad \\text{(Eq. (1))}\n \n\n  \n\\text{Selection Eq: } P^{*}_{it'} > \\gamma + Z_{it}\\pi + \\eta_{it'} \\quad \\text{(observed if employed)} \\quad \\text{(Eq. (2))}\n \nIt is assumed that `(ε, η)` follow a bivariate normal distribution with correlation `ρ`. The paper hypothesizes that among women who report discrimination, \"only women with particularly high wage growth may work in subsequent years.\"\n\n---\n\nGiven this framework, which of the following statements about the selection model and its identification are correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 223, "Question": "### Background\n\n**Research Question.** This problem tests the \"feedback hypothesis,\" which posits that labor market discrimination causes women to reduce their labor market attachment and increase specialization in household production, against the standard human capital model.\n\n**Setting / Institutional Environment.** The analysis finds that women who switch their self-reported discrimination status (from \"no\" to \"yes\") are subsequently more likely to have children and change employers. This supports the feedback hypothesis, suggesting that negative labor market experiences push women towards household production or job mobility.\n\n### The Question\n\nConsider the economic implications of the finding that reported discrimination leads women to have more children and change employers. Which of the following statements represent valid economic reasoning based on this context?\n", "Answer": ["A", "B", "D"], "qid": "223", "question": "### Background\n\n**Research Question.** This problem tests the \"feedback hypothesis,\" which posits that labor market discrimination causes women to reduce their labor market attachment and increase specialization in household production, against the standard human capital model.\n\n**Setting / Institutional Environment.** The analysis finds that women who switch their self-reported discrimination status (from \"no\" to \"yes\") are subsequently more likely to have children and change employers. This supports the feedback hypothesis, suggesting that negative labor market experiences push women towards household production or job mobility.\n\n### The Question\n\nConsider the economic implications of the finding that reported discrimination leads women to have more children and change employers. Which of the following statements represent valid economic reasoning based on this context?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 224, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of the term structure of interest rates, focusing on the two primary definitions of the long-term interest rate: the Hicksian rate (for zero-coupon bonds) and the Lutzian rate (for coupon-bearing bonds selling at par). The goal is to derive the formulas for extracting implied forward rates from each type of long rate and to understand the practical consequences of their differences.\n\n**Setting / Institutional Environment.** We assume a bond market with certain expectations about future one-period interest rates (`r_j`). The price of any bond is the present value of its cash flows, discounted by this sequence of forward rates.\n\n### Data / Model Specification\n\nThe `n`-period Hicksian long rate, `R'_n`, is the yield on an `n`-period zero-coupon bond, defined by:\n  \n(1+R'_{n})^{n} = (1+r_{1})(1+r_{2})\\ldots(1+r_{n}) \\quad \\text{(Eq. (1))}\n \nThe `n`-period Lutzian long rate, `R''_n`, is the coupon rate (and yield) of an `n`-period bond that sells at par. It is defined recursively in terms of the forward rates `r_j`.\n\n### Question\n\nBased on the definitions provided, select all of the following statements that are mathematically correct.", "Answer": ["B", "D"], "qid": "224", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of the term structure of interest rates, focusing on the two primary definitions of the long-term interest rate: the Hicksian rate (for zero-coupon bonds) and the Lutzian rate (for coupon-bearing bonds selling at par). The goal is to derive the formulas for extracting implied forward rates from each type of long rate and to understand the practical consequences of their differences.\n\n**Setting / Institutional Environment.** We assume a bond market with certain expectations about future one-period interest rates (`r_j`). The price of any bond is the present value of its cash flows, discounted by this sequence of forward rates.\n\n### Data / Model Specification\n\nThe `n`-period Hicksian long rate, `R'_n`, is the yield on an `n`-period zero-coupon bond, defined by:\n  \n(1+R'_{n})^{n} = (1+r_{1})(1+r_{2})\\ldots(1+r_{n}) \\quad \\text{(Eq. (1))}\n \nThe `n`-period Lutzian long rate, `R''_n`, is the coupon rate (and yield) of an `n`-period bond that sells at par. It is defined recursively in terms of the forward rates `r_j`.\n\n### Question\n\nBased on the definitions provided, select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 225, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of the term structure of interest rates, focusing on the two primary definitions of the long-term interest rate: the Hicksian rate (for zero-coupon bonds) and the Lutzian rate (for coupon-bearing bonds selling at par). The goal is to derive the formulas for extracting implied forward rates from each type of long rate and to understand the practical consequences of their differences.\n\n**Setting / Institutional Environment.** We assume a bond market with certain expectations about future one-period interest rates (`r_j`). The price of any bond is the present value of its cash flows, discounted by this sequence of forward rates.\n\n### Data / Model Specification\n\nThe true one-period forward rates are `r_1 = 2%` and `r_2 = 4%`.\n\nThe `n`-period Hicksian long rate, `R'_n`, is defined by:\n  \n(1+R'_n)^n = (1+r_1)...(1+r_n) \\quad \\text{(Eq. (1))}\n \nThe price `P_n` of an `n`-period bond with coupon `C` and face value `F` is the present value of its cash flows, discounted by the true forward rates.\n\n### Question\n\nA treasury analyst issues a new 2-period bond with a coupon rate `C` set equal to the 2-period Hicksian long rate, `R'_2`, calculated from the true forward rates. Select all of the following statements that are correct conclusions.", "Answer": ["A", "B"], "qid": "225", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of the term structure of interest rates, focusing on the two primary definitions of the long-term interest rate: the Hicksian rate (for zero-coupon bonds) and the Lutzian rate (for coupon-bearing bonds selling at par). The goal is to derive the formulas for extracting implied forward rates from each type of long rate and to understand the practical consequences of their differences.\n\n**Setting / Institutional Environment.** We assume a bond market with certain expectations about future one-period interest rates (`r_j`). The price of any bond is the present value of its cash flows, discounted by this sequence of forward rates.\n\n### Data / Model Specification\n\nThe true one-period forward rates are `r_1 = 2%` and `r_2 = 4%`.\n\nThe `n`-period Hicksian long rate, `R'_n`, is defined by:\n  \n(1+R'_n)^n = (1+r_1)...(1+r_n) \\quad \\text{(Eq. (1))}\n \nThe price `P_n` of an `n`-period bond with coupon `C` and face value `F` is the present value of its cash flows, discounted by the true forward rates.\n\n### Question\n\nA treasury analyst issues a new 2-period bond with a coupon rate `C` set equal to the 2-period Hicksian long rate, `R'_2`, calculated from the true forward rates. Select all of the following statements that are correct conclusions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 226, "Question": "### Background\nThis problem analyzes a two-bidder (1 and 2), two-good (α and β) sequential sealed-bid auction under complete information. A key concept is the 'critical value' for bidder `i` for the first good, which acts as a reservation price. For the parameters in this study, it is defined as:\n\n  \nc^i = \\frac{V^i(1) - V^i(2) + I^j}{2} \\quad \\text{Eq. (1)}\n \nwhere `V^i(1)` and `V^i(2)` are bidder `i`'s valuations for the first and second goods sold, respectively, and `I^j` is the budget of the opposing bidder `j`.\n\nThe theory of Nash equilibrium predicts that the set of equilibrium prices for the first good is the interval `W = [min(c^1, c^2), max(c^1, c^2)]`. The refinement to (trembling-hand) perfect equilibrium predicts a unique price: the upper bound of `W` under second-price rules.\n\n### Data / Model Specification\nThe parameters for three experiments, all conducted under second-price rules, are given in **Table 1**. Good α is sold first in all three experiments.\n\n**Table 1**: Experimental Design Parameters\n\n| Experiment | Bidder | Valuations | Income | \n| :--- | :--- | :--- | :--- |\n| **Baseline (1)** | 1 | V¹(α)=300, V¹(β)=200 | I¹=400 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n| **Valuation Change (2)** | 1 | V¹(α)=240, V¹(β)=200 | I¹=400 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n| **Income Change (3)** | 1 | V¹(α)=300, V¹(β)=200 | I¹=450 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n\n### Question\nBased on the provided model and data, select all of the following statements that are theoretically correct predictions of the perfect equilibrium concept.", "Answer": ["A", "B", "D"], "qid": "226", "question": "### Background\nThis problem analyzes a two-bidder (1 and 2), two-good (α and β) sequential sealed-bid auction under complete information. A key concept is the 'critical value' for bidder `i` for the first good, which acts as a reservation price. For the parameters in this study, it is defined as:\n\n  \nc^i = \\frac{V^i(1) - V^i(2) + I^j}{2} \\quad \\text{Eq. (1)}\n \nwhere `V^i(1)` and `V^i(2)` are bidder `i`'s valuations for the first and second goods sold, respectively, and `I^j` is the budget of the opposing bidder `j`.\n\nThe theory of Nash equilibrium predicts that the set of equilibrium prices for the first good is the interval `W = [min(c^1, c^2), max(c^1, c^2)]`. The refinement to (trembling-hand) perfect equilibrium predicts a unique price: the upper bound of `W` under second-price rules.\n\n### Data / Model Specification\nThe parameters for three experiments, all conducted under second-price rules, are given in **Table 1**. Good α is sold first in all three experiments.\n\n**Table 1**: Experimental Design Parameters\n\n| Experiment | Bidder | Valuations | Income | \n| :--- | :--- | :--- | :--- |\n| **Baseline (1)** | 1 | V¹(α)=300, V¹(β)=200 | I¹=400 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n| **Valuation Change (2)** | 1 | V¹(α)=240, V¹(β)=200 | I¹=400 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n| **Income Change (3)** | 1 | V¹(α)=300, V¹(β)=200 | I¹=450 |\n| | 2 | V²(α)=300, V²(β)=200 | I²=400 |\n\n### Question\nBased on the provided model and data, select all of the following statements that are theoretically correct predictions of the perfect equilibrium concept.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\nThis problem analyzes a two-bidder ...", "model_answer": "This is a detailed response to the question about ### Background\nThis problem analyzes a two-bidder ...", "error": null}}
{"ID": 227, "Question": "### Background\nIn a sequential auction with two bidders (1, 2) and two goods (α, β), the concept of a 'critical value' (`c^i`) is introduced for each bidder `i` in the first-stage auction. The perfect equilibrium concept predicts that under second-price rules, the equilibrium price for the first good will be the maximum of the two bidders' critical values. A key implication is that the order in which goods are sold can affect prices.\n\n### Data / Model Specification\nThe critical value for bidder `i` when good 1 is sold first is given by:\n\n  \nc^i = \\frac{V^i(1) - V^i(2) + I^j}{2} \\quad \\text{Eq. (1)}\n \nwhere `j` is the other bidder. After the first good is sold, the second-stage auction occurs. The standard equilibrium outcome is that the bidder with the higher reservation price (minimum of their valuation and remaining budget) obtains the good at a price equal to the lower reservation price.\n\nConsider the following parameters:\n*   **Bidder 1:** `V^1(α)=240`, `V^1(β)=200`, `I^1=400`\n*   **Bidder 2:** `V^2(α)=300`, `V^2(β)=200`, `I^2=400`\n\n### Question\nConsider two scenarios: one where good α is sold first, and another where good β is sold first. Based on the model, select all of the following outcomes that are predicted to occur.", "Answer": ["A", "B", "C"], "qid": "227", "question": "### Background\nIn a sequential auction with two bidders (1, 2) and two goods (α, β), the concept of a 'critical value' (`c^i`) is introduced for each bidder `i` in the first-stage auction. The perfect equilibrium concept predicts that under second-price rules, the equilibrium price for the first good will be the maximum of the two bidders' critical values. A key implication is that the order in which goods are sold can affect prices.\n\n### Data / Model Specification\nThe critical value for bidder `i` when good 1 is sold first is given by:\n\n  \nc^i = \\frac{V^i(1) - V^i(2) + I^j}{2} \\quad \\text{Eq. (1)}\n \nwhere `j` is the other bidder. After the first good is sold, the second-stage auction occurs. The standard equilibrium outcome is that the bidder with the higher reservation price (minimum of their valuation and remaining budget) obtains the good at a price equal to the lower reservation price.\n\nConsider the following parameters:\n*   **Bidder 1:** `V^1(α)=240`, `V^1(β)=200`, `I^1=400`\n*   **Bidder 2:** `V^2(α)=300`, `V^2(β)=200`, `I^2=400`\n\n### Question\nConsider two scenarios: one where good α is sold first, and another where good β is sold first. Based on the model, select all of the following outcomes that are predicted to occur.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\nIn a sequential auction with two bi...", "model_answer": "This is a detailed response to the question about ### Background\nIn a sequential auction with two bi...", "error": null}}
{"ID": 228, "Question": "## Background\n\n**Research Question.** This problem investigates the causal effect of an additional year of compulsory schooling on earnings, focusing on whether this effect differs by gender.\n\n**Setting / Institutional Environment.** The analysis uses a regression discontinuity design based on the 1947 British law that raised the minimum school leaving age from 14 to 15. This law change serves as an instrument for years of schooling. The study uses the General Household Survey (GHS) dataset.\n\n**Variables & Parameters.**\n- `Schooling`: Age left school (years).\n- `Log Weekly Earnings`: Log of weekly earnings.\n- `LAW`: The instrumental variable, indicating the individual was subject to the higher school leaving age.\n- `α₁`: The first-stage coefficient, measuring the effect of `LAW` on `Schooling`.\n- `γ₁`: The reduced-form coefficient, measuring the effect of `LAW` on `Log Weekly Earnings`.\n- `β₁`: The structural parameter of interest, the return to schooling.\n- Unit of observation: Individual `i`, analyzed in gender-specific subsamples.\n\n---\n\n## Data / Model Specification\n\nThe analysis uses a 2SLS framework. The first-stage and reduced-form equations are:\n  \nSchooling_i = \\alpha_0 + \\alpha_1 LAW_i + f(YOB_i) + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n  \n\\text{Log Weekly Earnings}_i = \\gamma_0 + \\gamma_1 LAW_i + g(YOB_i) + e_i \\quad \\text{(Eq. (2))}\n \nThe 2SLS estimator for the return to schooling is the ratio of the reduced-form to the first-stage coefficient: `β̂₁ = γ̂₁ / α̂₁`.\n\n**Table 1: First-Stage and Reduced-Form Effects of 1947 Law, by Gender (GHS Data)**\n\n| Panel | Dependent Variable | Men | Women |\n| :--- | :--- | :--- | :--- |\n| A | Schooling | 0.472 | 0.548 |\n| | | (0.026)** | (0.044)** |\n| B | Log Weekly Earnings | 0.028 | 0.014 |\n| | | (0.013)* | (0.018) |\n\n*Notes: Synthesized from the paper's Table 3. Estimates are from the GHS sample with no age controls. All specifications include a quartic function of year-of-birth. Robust standard errors clustered by year-of-birth in parentheses. * significant at 5%; ** significant at 1%.*\n\n---\n\nBased on the provided data and model, select all of the following statements that are correct interpretations or valid calculations.", "Answer": ["B", "C", "D"], "qid": "228", "question": "## Background\n\n**Research Question.** This problem investigates the causal effect of an additional year of compulsory schooling on earnings, focusing on whether this effect differs by gender.\n\n**Setting / Institutional Environment.** The analysis uses a regression discontinuity design based on the 1947 British law that raised the minimum school leaving age from 14 to 15. This law change serves as an instrument for years of schooling. The study uses the General Household Survey (GHS) dataset.\n\n**Variables & Parameters.**\n- `Schooling`: Age left school (years).\n- `Log Weekly Earnings`: Log of weekly earnings.\n- `LAW`: The instrumental variable, indicating the individual was subject to the higher school leaving age.\n- `α₁`: The first-stage coefficient, measuring the effect of `LAW` on `Schooling`.\n- `γ₁`: The reduced-form coefficient, measuring the effect of `LAW` on `Log Weekly Earnings`.\n- `β₁`: The structural parameter of interest, the return to schooling.\n- Unit of observation: Individual `i`, analyzed in gender-specific subsamples.\n\n---\n\n## Data / Model Specification\n\nThe analysis uses a 2SLS framework. The first-stage and reduced-form equations are:\n  \nSchooling_i = \\alpha_0 + \\alpha_1 LAW_i + f(YOB_i) + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n  \n\\text{Log Weekly Earnings}_i = \\gamma_0 + \\gamma_1 LAW_i + g(YOB_i) + e_i \\quad \\text{(Eq. (2))}\n \nThe 2SLS estimator for the return to schooling is the ratio of the reduced-form to the first-stage coefficient: `β̂₁ = γ̂₁ / α̂₁`.\n\n**Table 1: First-Stage and Reduced-Form Effects of 1947 Law, by Gender (GHS Data)**\n\n| Panel | Dependent Variable | Men | Women |\n| :--- | :--- | :--- | :--- |\n| A | Schooling | 0.472 | 0.548 |\n| | | (0.026)** | (0.044)** |\n| B | Log Weekly Earnings | 0.028 | 0.014 |\n| | | (0.013)* | (0.018) |\n\n*Notes: Synthesized from the paper's Table 3. Estimates are from the GHS sample with no age controls. All specifications include a quartic function of year-of-birth. Robust standard errors clustered by year-of-birth in parentheses. * significant at 5%; ** significant at 1%.*\n\n---\n\nBased on the provided data and model, select all of the following statements that are correct interpretations or valid calculations.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "model_answer": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "error": null}}
{"ID": 229, "Question": "## Background\n\n**Research Question.** This problem investigates the replicability of a previously published high estimate of the returns to compulsory schooling.\n\n**Setting / Institutional Environment.** The analysis uses a 2SLS strategy to estimate the returns to schooling, instrumenting years of schooling with the 1947 British compulsory schooling law change. The study compares its findings to a prior influential paper by Oreopoulos (2006) that used the same GHS dataset.\n\n---\n\n## Data / Model Specification\n\nThe analysis uses a standard 2SLS model. The key results from a reconciliation exercise are presented in Table 1 below.\n\n**Table 1: 2SLS Effects of 1947 Law on Log Weekly Earnings (Pooled Sample)**\n\n| | **1st Stage: Schooling** | **2SLS: Weekly Earnings** |\n| :--- | :--- | :--- |\n| | `α̂₁` (SE) | `β̂₁` (SE) |\n| **Row 1: Preferred Spec.** | 0.506 (0.031)** | 0.021 (0.024) |\n| N=85,766 | | |\n| **Row 4: Oreopoulos Sample** | 0.405 (0.064)** | 0.030 (0.063) |\n| N=55,088 | | |\n| **Row 5: Oreopoulos Spec.** | 0.408 (0.063)** | 0.072 (0.041) |\n| N=55,088 | | |\n\n*Notes: Simplified from original Table 2. Row 1 includes a female dummy. Rows 4 and 5 do not. Row 5 uses Oreopoulos's specification but on a corrected sample. Robust standard errors clustered by year-of-birth in parentheses. ** significant at 1%.*\n\n---\n\nBased on the reconciliation exercise shown in Table 1, select all of the following statements that represent valid econometric conclusions.", "Answer": ["A", "B", "D"], "qid": "229", "question": "## Background\n\n**Research Question.** This problem investigates the replicability of a previously published high estimate of the returns to compulsory schooling.\n\n**Setting / Institutional Environment.** The analysis uses a 2SLS strategy to estimate the returns to schooling, instrumenting years of schooling with the 1947 British compulsory schooling law change. The study compares its findings to a prior influential paper by Oreopoulos (2006) that used the same GHS dataset.\n\n---\n\n## Data / Model Specification\n\nThe analysis uses a standard 2SLS model. The key results from a reconciliation exercise are presented in Table 1 below.\n\n**Table 1: 2SLS Effects of 1947 Law on Log Weekly Earnings (Pooled Sample)**\n\n| | **1st Stage: Schooling** | **2SLS: Weekly Earnings** |\n| :--- | :--- | :--- |\n| | `α̂₁` (SE) | `β̂₁` (SE) |\n| **Row 1: Preferred Spec.** | 0.506 (0.031)** | 0.021 (0.024) |\n| N=85,766 | | |\n| **Row 4: Oreopoulos Sample** | 0.405 (0.064)** | 0.030 (0.063) |\n| N=55,088 | | |\n| **Row 5: Oreopoulos Spec.** | 0.408 (0.063)** | 0.072 (0.041) |\n| N=55,088 | | |\n\n*Notes: Simplified from original Table 2. Row 1 includes a female dummy. Rows 4 and 5 do not. Row 5 uses Oreopoulos's specification but on a corrected sample. Robust standard errors clustered by year-of-birth in parentheses. ** significant at 1%.*\n\n---\n\nBased on the reconciliation exercise shown in Table 1, select all of the following statements that represent valid econometric conclusions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "model_answer": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "error": null}}
{"ID": 230, "Question": "## Background\n\n**Research Question.** This problem examines the application of the Two-Sample Two-Stage Least Squares (TS2SLS) method and the theoretical interpretation of the IV estimator as a Local Average Treatment Effect (LATE).\n\n**Setting / Institutional Environment.** The analysis leverages the 1947 British compulsory schooling law as an instrument for education. The study uses two primary datasets: the General Household Survey (GHS), which has schooling data, and the New Earnings Survey Panel Dataset (NESPD), which has superior earnings data but no schooling data. This necessitates a TS2SLS approach.\n\n**LATE Framework.** The population can be categorized into Compliers (behavior changed by the law), Always-Takers (unaffected, always stay in school), and Never-Takers (unaffected, always leave early). The IV estimator identifies the average treatment effect only for the Complier sub-group.\n\n---\n\n## The Questions\n\nBased on the paper's discussion of its methodology and interpretation, select all of the following statements that are valid.", "Answer": ["A", "B", "C"], "qid": "230", "question": "## Background\n\n**Research Question.** This problem examines the application of the Two-Sample Two-Stage Least Squares (TS2SLS) method and the theoretical interpretation of the IV estimator as a Local Average Treatment Effect (LATE).\n\n**Setting / Institutional Environment.** The analysis leverages the 1947 British compulsory schooling law as an instrument for education. The study uses two primary datasets: the General Household Survey (GHS), which has schooling data, and the New Earnings Survey Panel Dataset (NESPD), which has superior earnings data but no schooling data. This necessitates a TS2SLS approach.\n\n**LATE Framework.** The population can be categorized into Compliers (behavior changed by the law), Always-Takers (unaffected, always stay in school), and Never-Takers (unaffected, always leave early). The IV estimator identifies the average treatment effect only for the Complier sub-group.\n\n---\n\n## The Questions\n\nBased on the paper's discussion of its methodology and interpretation, select all of the following statements that are valid.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "model_answer": "This is a detailed response to the question about ## Background\n\n**Research Question.** This problem...", "error": null}}
{"ID": 231, "Question": "### Background\n\nThe paper's central existence proof for a general equilibrium in a dynamic economy avoids the strong assumptions of prior work. The strategy is to first prove existence for a simplified case (Proposition 2), then use a novel perturbation method to extend the result to the general case (Proposition 3).\n\n### Data / Model Specification\n\nThe proof of **Proposition 3** constructs a sequence of `κ`-perturbed economies. For each `κ=4, 5, ...`, endowments `ω^α` are temporarily redistributed to `ω^α(κ)` as follows:\n\n  \n\\omega^{\\alpha}(\\kappa) = (1-\\kappa^{-1})\\omega^{\\alpha} + \\kappa^{-1}(2^{-\\alpha}p^{*} - \\varepsilon^{\\alpha}) \\quad \\text{for } \\alpha \\neq \\kappa\n \n\n  \n\\omega^{\\kappa}(\\kappa) = (1-\\kappa^{-1})\\omega^{\\kappa} + \\kappa^{-1}(2^{-\\kappa}p^{*} - \\varepsilon^{\\kappa}) + \\kappa^{-1}\\omega \\quad \\text{for } \\alpha = \\kappa\n \n\nwhere `p^*` is a strictly positive vector of goods and `Σ_α ε^α = p^*`. A competitive equilibrium is found in each `κ`-perturbed economy, and the limit of this sequence of equilibria is shown to be a transfer equilibrium for the original economy.\n\n### Question\n\nSelect all statements that correctly describe the properties and purpose of constructing the `κ`-perturbed economies.\n", "Answer": ["A", "B"], "qid": "231", "question": "### Background\n\nThe paper's central existence proof for a general equilibrium in a dynamic economy avoids the strong assumptions of prior work. The strategy is to first prove existence for a simplified case (Proposition 2), then use a novel perturbation method to extend the result to the general case (Proposition 3).\n\n### Data / Model Specification\n\nThe proof of **Proposition 3** constructs a sequence of `κ`-perturbed economies. For each `κ=4, 5, ...`, endowments `ω^α` are temporarily redistributed to `ω^α(κ)` as follows:\n\n  \n\\omega^{\\alpha}(\\kappa) = (1-\\kappa^{-1})\\omega^{\\alpha} + \\kappa^{-1}(2^{-\\alpha}p^{*} - \\varepsilon^{\\alpha}) \\quad \\text{for } \\alpha \\neq \\kappa\n \n\n  \n\\omega^{\\kappa}(\\kappa) = (1-\\kappa^{-1})\\omega^{\\kappa} + \\kappa^{-1}(2^{-\\kappa}p^{*} - \\varepsilon^{\\kappa}) + \\kappa^{-1}\\omega \\quad \\text{for } \\alpha = \\kappa\n \n\nwhere `p^*` is a strictly positive vector of goods and `Σ_α ε^α = p^*`. A competitive equilibrium is found in each `κ`-perturbed economy, and the limit of this sequence of equilibria is shown to be a transfer equilibrium for the original economy.\n\n### Question\n\nSelect all statements that correctly describe the properties and purpose of constructing the `κ`-perturbed economies.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's central existence proo...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's central existence proo...", "error": null}}
{"ID": 232, "Question": "### Background\n\nThe paper's proof strategy first establishes the existence of a **transfer equilibrium** `(p, (x^α))` where agents' budgets need not balance with their initial endowments. Proposition 3 specifically guarantees a **minimal transfer equilibrium** satisfying:\n\n  \n p \\cdot x^{\\alpha} \\ge p \\cdot \\omega^{\\alpha} \\quad \\text{for each } \\alpha \\in A \\quad \\text{(Eq. (1))}\n \n\nProposition 4 then provides conditions under which this is also a full **competitive equilibrium**, where `p·x^α = p·ω^α` for all agents.\n\n### Question\n\nAccording to the logic in Proposition 4, select all conditions that are sufficient to ensure a minimal transfer equilibrium satisfying Eq. (1) is also a full competitive equilibrium.\n", "Answer": ["A", "B"], "qid": "232", "question": "### Background\n\nThe paper's proof strategy first establishes the existence of a **transfer equilibrium** `(p, (x^α))` where agents' budgets need not balance with their initial endowments. Proposition 3 specifically guarantees a **minimal transfer equilibrium** satisfying:\n\n  \n p \\cdot x^{\\alpha} \\ge p \\cdot \\omega^{\\alpha} \\quad \\text{for each } \\alpha \\in A \\quad \\text{(Eq. (1))}\n \n\nProposition 4 then provides conditions under which this is also a full **competitive equilibrium**, where `p·x^α = p·ω^α` for all agents.\n\n### Question\n\nAccording to the logic in Proposition 4, select all conditions that are sufficient to ensure a minimal transfer equilibrium satisfying Eq. (1) is also a full competitive equilibrium.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's proof strategy first e...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's proof strategy first e...", "error": null}}
{"ID": 233, "Question": "### Background\n\nTo demonstrate the necessity of its non-standard assumption that consumption sets for infinitely lived agents are `R_+^∞`, the paper presents a counter-example (Example 1) of an economy that fails to have an equilibrium. This economy satisfies all standard assumptions, but one agent has a more restrictive (but still closed and convex) consumption set.\n\n### Data / Model Specification\n\n**Economy A (Example 1)**\n- **Agent 0:** Preferences `u^0(x) = x_1`. Consumption Set `X^0 = {x ∈ R_+^∞ : x_γ + 2^{γ-3}·(x_2 - 1) ≥ 0 for γ=3, 4, ...}`.\n- **Agent 1:** Standard preferences and consumption set `X^1 = R_+^∞`.\n- **Endowments:** `ω^0 = ω^1 = (2, 2, 2, ...)`.\n\nThe proof shows no equilibrium exists by assuming one does and deriving a contradiction.\n\n### Question\n\nIn the analysis of Economy A, which of the following statements are correct steps in the proof by contradiction?\n", "Answer": ["A", "D"], "qid": "233", "question": "### Background\n\nTo demonstrate the necessity of its non-standard assumption that consumption sets for infinitely lived agents are `R_+^∞`, the paper presents a counter-example (Example 1) of an economy that fails to have an equilibrium. This economy satisfies all standard assumptions, but one agent has a more restrictive (but still closed and convex) consumption set.\n\n### Data / Model Specification\n\n**Economy A (Example 1)**\n- **Agent 0:** Preferences `u^0(x) = x_1`. Consumption Set `X^0 = {x ∈ R_+^∞ : x_γ + 2^{γ-3}·(x_2 - 1) ≥ 0 for γ=3, 4, ...}`.\n- **Agent 1:** Standard preferences and consumption set `X^1 = R_+^∞`.\n- **Endowments:** `ω^0 = ω^1 = (2, 2, 2, ...)`.\n\nThe proof shows no equilibrium exists by assuming one does and deriving a contradiction.\n\n### Question\n\nIn the analysis of Economy A, which of the following statements are correct steps in the proof by contradiction?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo demonstrate the necessity of it...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo demonstrate the necessity of it...", "error": null}}
{"ID": 234, "Question": "### Background\n\nTo demonstrate the necessity of its assumption that only a finite number of agents desire any particular good (Assumption 7), the paper presents a counter-example (Example 2) where this is violated. An infinite number of agents desire a common good (good 0), and no competitive equilibrium exists.\n\n### Data / Model Specification\n\n**Economy B (Example 2)**\n- **Agents `α ∈ {1, 2, ...}`:** Preferences `u^α(x) = x_0 + x_α`.\n- **Endowments:** Agent `α` is endowed with one unit of good `α-1` and one unit of good `α`.\n\nThe proof shows no competitive equilibrium exists by assuming one does and deriving a contradiction.\n\n### Question\n\nIn the analysis of Economy B, which of the following statements are correct steps in the proof by contradiction?\n", "Answer": ["B", "C"], "qid": "234", "question": "### Background\n\nTo demonstrate the necessity of its assumption that only a finite number of agents desire any particular good (Assumption 7), the paper presents a counter-example (Example 2) where this is violated. An infinite number of agents desire a common good (good 0), and no competitive equilibrium exists.\n\n### Data / Model Specification\n\n**Economy B (Example 2)**\n- **Agents `α ∈ {1, 2, ...}`:** Preferences `u^α(x) = x_0 + x_α`.\n- **Endowments:** Agent `α` is endowed with one unit of good `α-1` and one unit of good `α`.\n\nThe proof shows no competitive equilibrium exists by assuming one does and deriving a contradiction.\n\n### Question\n\nIn the analysis of Economy B, which of the following statements are correct steps in the proof by contradiction?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo demonstrate the necessity of it...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo demonstrate the necessity of it...", "error": null}}
{"ID": 235, "Question": "### Background\n\nThe paper applies its framework to fiscal and monetary questions by contrasting the existence conditions for equilibria with positive transfers (e.g., fiat money) versus negative transfers (taxes).\n\n### Data / Model Specification\n\n- **Proposition 5** states that if the initial endowment `(ω^α)` is weakly Pareto optimal, no equilibrium with strictly positive transfers for all agents can exist.\n- **Proposition 6** states that a **taxation equilibrium**, where `p·x^α = (1-θ^α)p·ω^α` for tax rates `θ^α ∈ [0, 1)`, exists for any collection of tax rates, provided each agent has an endowment of only a finite number of goods.\n\n### Question\n\nSelect all statements that correctly characterize the paper's findings on equilibria with transfers.\n", "Answer": ["A", "C"], "qid": "235", "question": "### Background\n\nThe paper applies its framework to fiscal and monetary questions by contrasting the existence conditions for equilibria with positive transfers (e.g., fiat money) versus negative transfers (taxes).\n\n### Data / Model Specification\n\n- **Proposition 5** states that if the initial endowment `(ω^α)` is weakly Pareto optimal, no equilibrium with strictly positive transfers for all agents can exist.\n- **Proposition 6** states that a **taxation equilibrium**, where `p·x^α = (1-θ^α)p·ω^α` for tax rates `θ^α ∈ [0, 1)`, exists for any collection of tax rates, provided each agent has an endowment of only a finite number of goods.\n\n### Question\n\nSelect all statements that correctly characterize the paper's findings on equilibria with transfers.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper applies its framework to...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper applies its framework to...", "error": null}}
{"ID": 236, "Question": "### Background\n\nThe proof of Proposition 6 (existence of taxation equilibria) relies on the assumption that each agent `α` has an endowment of only a finite number of goods. This assumption is critical for ensuring that the limit of the value of endowments equals the value of the endowment at the limit prices, i.e., `lim_{κ→∞} p(κ)·ω^α = p·ω^α`.\n\n### Question\n\nSuppose the finite endowment assumption is violated for some agent `α`. Select all statements that are **incorrect** descriptions of the consequences, according to the paper's logic.\n", "Answer": ["A", "B"], "qid": "236", "question": "### Background\n\nThe proof of Proposition 6 (existence of taxation equilibria) relies on the assumption that each agent `α` has an endowment of only a finite number of goods. This assumption is critical for ensuring that the limit of the value of endowments equals the value of the endowment at the limit prices, i.e., `lim_{κ→∞} p(κ)·ω^α = p·ω^α`.\n\n### Question\n\nSuppose the finite endowment assumption is violated for some agent `α`. Select all statements that are **incorrect** descriptions of the consequences, according to the paper's logic.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe proof of Proposition 6 (existe...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe proof of Proposition 6 (existe...", "error": null}}
{"ID": 237, "Question": "### Background\n\n**Research Question.** This problem investigates alternative pathways to establishing Uniform Laws of Large Numbers (U-LLNs). It focuses on comparing the novel framework based on Termwise Stochastic Equicontinuity (TSE) and Domination (DM) against the more traditional approach based on a Lipschitz-type condition (W-LIP).\n\n**Setting.** We analyze a sequence of real-valued random functions, $\\{G_n(\\theta): n \\ge 1\\}$, representing the normalized and centered sample average of an objective function $q_t(Z_t, \\theta)$:\n\n  \nG_n(\\theta) = \\frac{1}{n} \\sum_{t=1}^n \\left( q_t(Z_t, \\theta) - E[q_t(Z_t, \\theta)] \\right)\n \n\nThe goal is to find verifiable, primitive conditions on the functions $\\{q_t\\}$ and the random variables $\\{Z_t\\}$ that ensure uniform convergence of $G_n(\\theta)$ to zero.\n\n### Data / Model Specification\n\nThe paper presents several sets of primitive conditions that are sufficient for establishing uniform convergence. Two key approaches are:\n\n1.  **The Lipschitz Approach (W-LIP):**\n    - **(a)** $|q_t(Z_t, \\theta') - q_t(Z_t, \\theta)| \\le B_t(Z_t) h(d(\\theta', \\theta))$ a.s., where $h(y) \\downarrow 0$ as $y \\downarrow 0$.\n    - **(b)** $\\sup_{n \\ge 1} (1/n) \\sum_{t=1}^n E[B_t(Z_t)] < \\infty$.\n\n2.  **The Termwise Stochastic Equicontinuity Approach (TSE-1):** This novel condition is designed for dependent, non-identically distributed (d.n.i.d.) data.\n    - **(a)** $q_t(z, \\theta)$ is continuous in $\\theta$ uniformly over $\\theta \\in \\Theta$ and $t \\ge 1$, for every $z$.\n    - **(b)** For every sequence of measurable sets $\\{A_m \\subset \\mathcal{Z}\\}$ with $A_m \\downarrow \\phi$, it holds that $\\lim_{m \\to \\infty} \\limsup_{n\\to\\infty} (1/n) \\sum_{t=1}^n P(Z_t \\in A_m) = 0$.\n\nA special case of TSE-1 for i.i.d. data is **TSE-1D (Jennrich's conditions)**, which requires $q_t(z, \\theta) = q(z, \\theta)$, a compact parameter space $\\Theta$, and i.i.d. data $\\{Z_t\\}$.\n\n### Question\n\nBased on the provided theory, select all statements that correctly describe the relationships and trade-offs between these different primitive assumptions for establishing uniform convergence.", "Answer": ["A", "D"], "qid": "237", "question": "### Background\n\n**Research Question.** This problem investigates alternative pathways to establishing Uniform Laws of Large Numbers (U-LLNs). It focuses on comparing the novel framework based on Termwise Stochastic Equicontinuity (TSE) and Domination (DM) against the more traditional approach based on a Lipschitz-type condition (W-LIP).\n\n**Setting.** We analyze a sequence of real-valued random functions, $\\{G_n(\\theta): n \\ge 1\\}$, representing the normalized and centered sample average of an objective function $q_t(Z_t, \\theta)$:\n\n  \nG_n(\\theta) = \\frac{1}{n} \\sum_{t=1}^n \\left( q_t(Z_t, \\theta) - E[q_t(Z_t, \\theta)] \\right)\n \n\nThe goal is to find verifiable, primitive conditions on the functions $\\{q_t\\}$ and the random variables $\\{Z_t\\}$ that ensure uniform convergence of $G_n(\\theta)$ to zero.\n\n### Data / Model Specification\n\nThe paper presents several sets of primitive conditions that are sufficient for establishing uniform convergence. Two key approaches are:\n\n1.  **The Lipschitz Approach (W-LIP):**\n    - **(a)** $|q_t(Z_t, \\theta') - q_t(Z_t, \\theta)| \\le B_t(Z_t) h(d(\\theta', \\theta))$ a.s., where $h(y) \\downarrow 0$ as $y \\downarrow 0$.\n    - **(b)** $\\sup_{n \\ge 1} (1/n) \\sum_{t=1}^n E[B_t(Z_t)] < \\infty$.\n\n2.  **The Termwise Stochastic Equicontinuity Approach (TSE-1):** This novel condition is designed for dependent, non-identically distributed (d.n.i.d.) data.\n    - **(a)** $q_t(z, \\theta)$ is continuous in $\\theta$ uniformly over $\\theta \\in \\Theta$ and $t \\ge 1$, for every $z$.\n    - **(b)** For every sequence of measurable sets $\\{A_m \\subset \\mathcal{Z}\\}$ with $A_m \\downarrow \\phi$, it holds that $\\lim_{m \\to \\infty} \\limsup_{n\\to\\infty} (1/n) \\sum_{t=1}^n P(Z_t \\in A_m) = 0$.\n\nA special case of TSE-1 for i.i.d. data is **TSE-1D (Jennrich's conditions)**, which requires $q_t(z, \\theta) = q(z, \\theta)$, a compact parameter space $\\Theta$, and i.i.d. data $\\{Z_t\\}$.\n\n### Question\n\nBased on the provided theory, select all statements that correctly describe the relationships and trade-offs between these different primitive assumptions for establishing uniform convergence.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 238, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational theory of generic uniform convergence, exploring the necessary and sufficient conditions to strengthen pointwise convergence of a sequence of random functions to uniform convergence.\n\n**Setting.** We consider a sequence of real-valued, measurable random functions, $\\{G_n(\\theta): n \\ge 1\\}$, defined on a probability space. The functions are indexed by a parameter $\\theta$ belonging to a metric space $(\\Theta, d)$.\n\n### Data / Model Specification\n\n**Key Concepts and Assumptions:**\n- **Assumption BD (Totally Bounded):** For any $\\delta > 0$, the parameter space $\\Theta$ can be covered by a finite number of closed balls $B(\\theta, \\delta)$ of radius $\\delta$.\n- **P-WCON (Pointwise Weak Convergence):** $G_n(\\theta) \\xrightarrow{p} 0$ for each $\\theta \\in \\Theta$.\n- **U-WCON (Uniform Weak Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\xrightarrow{p} 0$.\n- **SE (Stochastic Equicontinuity):** For all $\\epsilon > 0$, there exists a $\\delta > 0$ such that:\n    \n  \\limsup_{n\\to\\infty} P\\left( \\sup_{\\theta \\in \\Theta} \\sup_{\\theta' \\in B(\\theta, \\delta)} |G_n(\\theta') - G_n(\\theta)| > \\epsilon \\right) < \\epsilon \n   \n\n**Fundamental Theorem:**\n- **Theorem 1(a):** Under BD, (P-WCON & SE) $\\Rightarrow$ U-WCON.\n\n### Question\n\nAccording to the paper's proof of Theorem 1(a), select all statements that are logically correct components of the argument.", "Answer": ["A", "D"], "qid": "238", "question": "### Background\n\n**Research Question.** This problem investigates the foundational theory of generic uniform convergence, exploring the necessary and sufficient conditions to strengthen pointwise convergence of a sequence of random functions to uniform convergence.\n\n**Setting.** We consider a sequence of real-valued, measurable random functions, $\\{G_n(\\theta): n \\ge 1\\}$, defined on a probability space. The functions are indexed by a parameter $\\theta$ belonging to a metric space $(\\Theta, d)$.\n\n### Data / Model Specification\n\n**Key Concepts and Assumptions:**\n- **Assumption BD (Totally Bounded):** For any $\\delta > 0$, the parameter space $\\Theta$ can be covered by a finite number of closed balls $B(\\theta, \\delta)$ of radius $\\delta$.\n- **P-WCON (Pointwise Weak Convergence):** $G_n(\\theta) \\xrightarrow{p} 0$ for each $\\theta \\in \\Theta$.\n- **U-WCON (Uniform Weak Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\xrightarrow{p} 0$.\n- **SE (Stochastic Equicontinuity):** For all $\\epsilon > 0$, there exists a $\\delta > 0$ such that:\n    \n  \\limsup_{n\\to\\infty} P\\left( \\sup_{\\theta \\in \\Theta} \\sup_{\\theta' \\in B(\\theta, \\delta)} |G_n(\\theta') - G_n(\\theta)| > \\epsilon \\right) < \\epsilon \n   \n\n**Fundamental Theorem:**\n- **Theorem 1(a):** Under BD, (P-WCON & SE) $\\Rightarrow$ U-WCON.\n\n### Question\n\nAccording to the paper's proof of Theorem 1(a), select all statements that are logically correct components of the argument.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 239, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational theory of generic uniform convergence, exploring the extension from weak (in probability) to strong (almost sure) convergence and the practical implications of the paper's assumptions.\n\n**Setting.** We consider a sequence of real-valued, measurable random functions, $\\{G_n(\\theta): n \\ge 1\\}$, indexed by a parameter $\\theta$ from a metric space $(\\Theta, d)$.\n\n### Data / Model Specification\n\n**Key Concepts and Theorems:**\n- **U-WCON (Uniform Weak Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\xrightarrow{p} 0$.\n- **U-SCON (Uniform Strong Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\to 0$ a.s.\n- **SSE (Strong Stochastic Equicontinuity):** The sequence $\\{G_n(\\theta)\\}$ is SSE if the transformed sequence $\\{ \\sup_{m \\ge n} |G_m(\\theta)| : n \\ge 1 \\}$ is stochastically equicontinuous (SE).\n- **Theorem 1:** Under total boundedness (BD), (Pointwise Weak Convergence & SE) $\\Rightarrow$ U-WCON.\n- **Theorem 2:** Under BD, (Pointwise Strong Convergence & SSE) $\\Rightarrow$ U-SCON.\n- **A-S Convergence Equivalence:** For a sequence of random variables $\\{X_n\\}$,\n    \n  X_n \\to 0 \\text{ a.s.} \\quad \\iff \\quad \\sup_{m \\ge n} |X_m| \\xrightarrow{p} 0 \n   \n\n**Parameter Space Assumptions:**\n- **Compactness:** In $\\mathbb{R}^k$, a set is compact if it is closed and bounded.\n- **Total Boundedness:** In $\\mathbb{R}^k$, a set is totally bounded if it is bounded. This is weaker than compactness as it does not require the set to be closed.\n\n### Question\n\nSelect all statements that correctly describe the extension from weak to strong uniform convergence and the practical implications of using a totally bounded parameter space.", "Answer": ["A", "B"], "qid": "239", "question": "### Background\n\n**Research Question.** This problem investigates the foundational theory of generic uniform convergence, exploring the extension from weak (in probability) to strong (almost sure) convergence and the practical implications of the paper's assumptions.\n\n**Setting.** We consider a sequence of real-valued, measurable random functions, $\\{G_n(\\theta): n \\ge 1\\}$, indexed by a parameter $\\theta$ from a metric space $(\\Theta, d)$.\n\n### Data / Model Specification\n\n**Key Concepts and Theorems:**\n- **U-WCON (Uniform Weak Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\xrightarrow{p} 0$.\n- **U-SCON (Uniform Strong Convergence):** $\\sup_{\\theta \\in \\Theta} |G_n(\\theta)| \\to 0$ a.s.\n- **SSE (Strong Stochastic Equicontinuity):** The sequence $\\{G_n(\\theta)\\}$ is SSE if the transformed sequence $\\{ \\sup_{m \\ge n} |G_m(\\theta)| : n \\ge 1 \\}$ is stochastically equicontinuous (SE).\n- **Theorem 1:** Under total boundedness (BD), (Pointwise Weak Convergence & SE) $\\Rightarrow$ U-WCON.\n- **Theorem 2:** Under BD, (Pointwise Strong Convergence & SSE) $\\Rightarrow$ U-SCON.\n- **A-S Convergence Equivalence:** For a sequence of random variables $\\{X_n\\}$,\n    \n  X_n \\to 0 \\text{ a.s.} \\quad \\iff \\quad \\sup_{m \\ge n} |X_m| \\xrightarrow{p} 0 \n   \n\n**Parameter Space Assumptions:**\n- **Compactness:** In $\\mathbb{R}^k$, a set is compact if it is closed and bounded.\n- **Total Boundedness:** In $\\mathbb{R}^k$, a set is totally bounded if it is bounded. This is weaker than compactness as it does not require the set to be closed.\n\n### Question\n\nSelect all statements that correctly describe the extension from weak to strong uniform convergence and the practical implications of using a totally bounded parameter space.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 240, "Question": "### Background\n\n**Research Question.** This problem investigates a primary method for establishing Uniform Laws of Large Numbers (U-LLNs) by using primitive, verifiable Lipschitz-type conditions on the objective function.\n\n**Setting.** The analysis focuses on the sample average of a sequence of centered random functions, $G_n(\\theta) = (1/n) \\sum ( q_t(Z_t, \\theta) - E[q_t(Z_t, \\theta)] )$, with the goal of proving it converges uniformly to zero.\n\n### Data / Model Specification\n\nThe main theoretical result for generic uniform convergence requires establishing Stochastic Equicontinuity (SE). A powerful way to do this is via a primitive Lipschitz condition.\n\n**Primitive Lipschitz Assumptions:**\n- **Assumption W-LIP (for Weak LLN):**\n  (a) $|q_t(Z_t, \\theta') - q_t(Z_t, \\theta)| \\le B_t(Z_t) h(d(\\theta', \\theta))$ a.s., where $h(y) \\downarrow 0$ as $y \\downarrow 0$.\n  (b) $\\sup_{n \\ge 1} (1/n) \\sum_{t=1}^n E[B_t(Z_t)] < \\infty$.\n- **Assumption S-LIP (for Strong LLN):**\n  (a) W-LIP holds.\n  (b) $(1/n)\\sum_{t=1}^n(B_t(Z_t) - E B_t(Z_t)) \\to 0$ a.s.\n\n**Intermediate Assumptions:**\n- **Assumption SE-1:** Implies SE. It requires that for $\\hat{Q}_n(\\theta) = (1/n) \\sum q_t(Z_t, \\theta)$, we have $|\\hat{Q}_n(\\theta') - \\hat{Q}_n(\\theta)| \\le B_n h(d(\\theta', \\theta))$ where $B_n = O_p(1)$.\n- **Assumption SSE-1:** Implies Strong Stochastic Equicontinuity (SSE). It is similar to SE-1 but requires the stronger condition $B_n = O(1)$ a.s.\n\n### Question\n\nRegarding the Lipschitz-based approach to uniform convergence, select all statements that are correct.", "Answer": ["C", "D"], "qid": "240", "question": "### Background\n\n**Research Question.** This problem investigates a primary method for establishing Uniform Laws of Large Numbers (U-LLNs) by using primitive, verifiable Lipschitz-type conditions on the objective function.\n\n**Setting.** The analysis focuses on the sample average of a sequence of centered random functions, $G_n(\\theta) = (1/n) \\sum ( q_t(Z_t, \\theta) - E[q_t(Z_t, \\theta)] )$, with the goal of proving it converges uniformly to zero.\n\n### Data / Model Specification\n\nThe main theoretical result for generic uniform convergence requires establishing Stochastic Equicontinuity (SE). A powerful way to do this is via a primitive Lipschitz condition.\n\n**Primitive Lipschitz Assumptions:**\n- **Assumption W-LIP (for Weak LLN):**\n  (a) $|q_t(Z_t, \\theta') - q_t(Z_t, \\theta)| \\le B_t(Z_t) h(d(\\theta', \\theta))$ a.s., where $h(y) \\downarrow 0$ as $y \\downarrow 0$.\n  (b) $\\sup_{n \\ge 1} (1/n) \\sum_{t=1}^n E[B_t(Z_t)] < \\infty$.\n- **Assumption S-LIP (for Strong LLN):**\n  (a) W-LIP holds.\n  (b) $(1/n)\\sum_{t=1}^n(B_t(Z_t) - E B_t(Z_t)) \\to 0$ a.s.\n\n**Intermediate Assumptions:**\n- **Assumption SE-1:** Implies SE. It requires that for $\\hat{Q}_n(\\theta) = (1/n) \\sum q_t(Z_t, \\theta)$, we have $|\\hat{Q}_n(\\theta') - \\hat{Q}_n(\\theta)| \\le B_n h(d(\\theta', \\theta))$ where $B_n = O_p(1)$.\n- **Assumption SSE-1:** Implies Strong Stochastic Equicontinuity (SSE). It is similar to SE-1 but requires the stronger condition $B_n = O(1)$ a.s.\n\n### Question\n\nRegarding the Lipschitz-based approach to uniform convergence, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 241, "Question": "### Background\n\nAn experiment compares two all-pay auction formats to test predictions from dynamic evolutionary models against static fixed-point models (like Nash Equilibrium).\n\n*   **Auction 1:** Two bidders compete for one prize.\n*   **Auction 2:** Three bidders compete for two prizes.\n\nTwo key empirical measures are used:\n*   `Deviation from time-averaged mean`: Measures the instability of the bid distribution. Higher values mean more instability.\n*   `Cycle-rotation index (CRI)`: Measures clockwise cycling. A value of 0 indicates no systematic cycling; a value of 1 indicates perfect clockwise cycling.\n\n### Data / Model Specification\n\nFixed-point models (e.g., Nash Equilibrium) predict stable behavior, implying that both the deviation from mean and the CRI should be zero. In contrast, evolutionary models predict that the Nash Equilibrium of Auction 2 is unstable (`positive definite`) while the equilibrium of Auction 1 is stable (`globally neutrally stable`), leading to the hypothesis that both instability and cycling will be more pronounced in Auction 2.\n\n**Table 1: Empirical Results for Behavioral Dynamics**\n| | Nash Equilibrium | Empirical Behavior |\n| :--- | :---: | :---: | :---: | :---: |\n| | Auction 1 | Auction 2 | Auction 1 | Auction 2 |\n| Deviation from time-averaged mean | 0 | 0 | 0.193 | 0.283 |\n| Cycle-rotation index | 0 | 0 | 0.321 | 0.480 |\n\n---\n\nBased on the provided theory and data in Table 1, which of the following conclusions are validly supported?\n", "Answer": ["A", "C", "D"], "qid": "241", "question": "### Background\n\nAn experiment compares two all-pay auction formats to test predictions from dynamic evolutionary models against static fixed-point models (like Nash Equilibrium).\n\n*   **Auction 1:** Two bidders compete for one prize.\n*   **Auction 2:** Three bidders compete for two prizes.\n\nTwo key empirical measures are used:\n*   `Deviation from time-averaged mean`: Measures the instability of the bid distribution. Higher values mean more instability.\n*   `Cycle-rotation index (CRI)`: Measures clockwise cycling. A value of 0 indicates no systematic cycling; a value of 1 indicates perfect clockwise cycling.\n\n### Data / Model Specification\n\nFixed-point models (e.g., Nash Equilibrium) predict stable behavior, implying that both the deviation from mean and the CRI should be zero. In contrast, evolutionary models predict that the Nash Equilibrium of Auction 2 is unstable (`positive definite`) while the equilibrium of Auction 1 is stable (`globally neutrally stable`), leading to the hypothesis that both instability and cycling will be more pronounced in Auction 2.\n\n**Table 1: Empirical Results for Behavioral Dynamics**\n| | Nash Equilibrium | Empirical Behavior |\n| :--- | :---: | :---: | :---: | :---: |\n| | Auction 1 | Auction 2 | Auction 1 | Auction 2 |\n| Deviation from time-averaged mean | 0 | 0 | 0.193 | 0.283 |\n| Cycle-rotation index | 0 | 0 | 0.321 | 0.480 |\n\n---\n\nBased on the provided theory and data in Table 1, which of the following conclusions are validly supported?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn experiment compares two all-pay...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn experiment compares two all-pay...", "error": null}}
{"ID": 242, "Question": "### Background\n\nAn experiment compares two all-pay auction formats. A key assumption is that subjects were randomly assigned to treatments. Suppose this assumption was violated, and subjects assigned to Auction 2 coincidentally had a higher non-monetary 'utility of winning' (`U_w > 0`) than those in Auction 1. This `U_w` effectively increases the perceived value of the prize from `ν` to `ν + U_w`.\n\nEvolutionary cycles in these auctions are driven by a process of bid escalation up to the prize value, followed by a 'collapse' where it becomes optimal to bid zero. The instability of these cycles is measured by the 'Deviation from time-averaged mean'.\n\n### Data / Model Specification\n\n**Table 1: Empirical Results for Behavioral Dynamics**\n| | Auction 1 | Auction 2 |\n| :--- | :---: | :---: |\n| Deviation from time-averaged mean | 0.193 | 0.283 |\n\nThe empirically measured difference in instability is `0.283 - 0.193 = 0.090`.\n\n---\n\nGiven this scenario of failed randomization, which of the following statements correctly analyze the resulting bias?\n", "Answer": ["A", "B", "D"], "qid": "242", "question": "### Background\n\nAn experiment compares two all-pay auction formats. A key assumption is that subjects were randomly assigned to treatments. Suppose this assumption was violated, and subjects assigned to Auction 2 coincidentally had a higher non-monetary 'utility of winning' (`U_w > 0`) than those in Auction 1. This `U_w` effectively increases the perceived value of the prize from `ν` to `ν + U_w`.\n\nEvolutionary cycles in these auctions are driven by a process of bid escalation up to the prize value, followed by a 'collapse' where it becomes optimal to bid zero. The instability of these cycles is measured by the 'Deviation from time-averaged mean'.\n\n### Data / Model Specification\n\n**Table 1: Empirical Results for Behavioral Dynamics**\n| | Auction 1 | Auction 2 |\n| :--- | :---: | :---: |\n| Deviation from time-averaged mean | 0.193 | 0.283 |\n\nThe empirically measured difference in instability is `0.283 - 0.193 = 0.090`.\n\n---\n\nGiven this scenario of failed randomization, which of the following statements correctly analyze the resulting bias?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn experiment compares two all-pay...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn experiment compares two all-pay...", "error": null}}
{"ID": 243, "Question": "### Background\n\nAn analysis compares the symmetric mixed-strategy Nash Equilibria (NE) for two all-pay auction formats. In both auctions, the prize value is `ν`.\n\n*   **Auction 1:** Two bidders, one prize. The NE bidding distribution is `Φ_1(b) = b/ν`.\n*   **Auction 2:** Three bidders, two prizes. The NE bidding distribution is `Φ_2(b) = 1 - sqrt(1 - b/ν)`.\n\nBidding is considered more 'aggressive' in one auction if its bid distribution first-order stochastically dominates (FOSD) the other, which implies a higher mean bid and more probability mass on higher bids.\n\n### Data / Model Specification\n\nThe expected (mean) bid for a non-negative random variable with CDF `F(x)` over support `[0, M]` can be calculated as `E[X] = ∫[0 to M] (1 - F(x)) dx`.\n\nThe probability density function (pdf), `φ(b)`, is the derivative of the CDF, `Φ(b)`.\n\n---\n\nBased on the provided equilibrium strategies, which of the following statements are mathematically correct?\n", "Answer": ["A", "B", "C"], "qid": "243", "question": "### Background\n\nAn analysis compares the symmetric mixed-strategy Nash Equilibria (NE) for two all-pay auction formats. In both auctions, the prize value is `ν`.\n\n*   **Auction 1:** Two bidders, one prize. The NE bidding distribution is `Φ_1(b) = b/ν`.\n*   **Auction 2:** Three bidders, two prizes. The NE bidding distribution is `Φ_2(b) = 1 - sqrt(1 - b/ν)`.\n\nBidding is considered more 'aggressive' in one auction if its bid distribution first-order stochastically dominates (FOSD) the other, which implies a higher mean bid and more probability mass on higher bids.\n\n### Data / Model Specification\n\nThe expected (mean) bid for a non-negative random variable with CDF `F(x)` over support `[0, M]` can be calculated as `E[X] = ∫[0 to M] (1 - F(x)) dx`.\n\nThe probability density function (pdf), `φ(b)`, is the derivative of the CDF, `Φ(b)`.\n\n---\n\nBased on the provided equilibrium strategies, which of the following statements are mathematically correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis compares the symmetric...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis compares the symmetric...", "error": null}}
{"ID": 244, "Question": "### Background\n\nEvolutionary game theory provides criteria to assess the stability of a Nash Equilibrium (NE) strategy, `Φ`. The analysis focuses on what happens when a population playing `Φ` is perturbed by a small group of players adopting an alternative strategy, `G`.\n\n### Data / Model Specification\n\nTwo key stability concepts are defined based on the expected payoff `π(Strategy_Used | Opponent_Strategy)`:\n\n1.  **Positive Definiteness:** An equilibrium `Φ` is positive definite if `π(G|G) > π(Φ|G)` for any small deviation `G`. This implies the deviating strategy `G` earns more against the new population state `G` than the original equilibrium strategy `Φ` does. Such an equilibrium is evolutionarily **unstable**.\n\n2.  **Global Neutral Stability (GNS):** An equilibrium `Φ` is GNS if `π(Φ|G) ≥ π(G|G)` for any alternative strategy `G`. This implies the original equilibrium strategy `Φ` performs at least as well against the new population state `G` as the deviating strategy `G` does. Such an equilibrium is evolutionarily **stable**.\n\nThe paper states that the NE for Auction 1 is GNS, while the NE for Auction 2 is positive definite.\n\n---\n\nBased on these definitions and theoretical results, which of the following statements are **INCORRECT** characterizations of the auctions' stability properties or their implications?\n", "Answer": ["B", "D"], "qid": "244", "question": "### Background\n\nEvolutionary game theory provides criteria to assess the stability of a Nash Equilibrium (NE) strategy, `Φ`. The analysis focuses on what happens when a population playing `Φ` is perturbed by a small group of players adopting an alternative strategy, `G`.\n\n### Data / Model Specification\n\nTwo key stability concepts are defined based on the expected payoff `π(Strategy_Used | Opponent_Strategy)`:\n\n1.  **Positive Definiteness:** An equilibrium `Φ` is positive definite if `π(G|G) > π(Φ|G)` for any small deviation `G`. This implies the deviating strategy `G` earns more against the new population state `G` than the original equilibrium strategy `Φ` does. Such an equilibrium is evolutionarily **unstable**.\n\n2.  **Global Neutral Stability (GNS):** An equilibrium `Φ` is GNS if `π(Φ|G) ≥ π(G|G)` for any alternative strategy `G`. This implies the original equilibrium strategy `Φ` performs at least as well against the new population state `G` as the deviating strategy `G` does. Such an equilibrium is evolutionarily **stable**.\n\nThe paper states that the NE for Auction 1 is GNS, while the NE for Auction 2 is positive definite.\n\n---\n\nBased on these definitions and theoretical results, which of the following statements are **INCORRECT** characterizations of the auctions' stability properties or their implications?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nEvolutionary game theory provides ...", "model_answer": "This is a detailed response to the question about ### Background\n\nEvolutionary game theory provides ...", "error": null}}
{"ID": 245, "Question": "### Background\n\nThe paper's core empirical finding is that income inequality has opposing effects on economic growth over different time horizons. The analysis uses a System GMM estimation on a 5-year panel of up to 106 countries from 1965-2005. The dependent variable is the 5-year growth rate of real GDP per capita.\n\n### Data / Model Specification\n\nThe following table presents a selection of the core one-step System GMM results from the paper's Table 3, column (4).\n\n**Table 1: Selected System GMM Results**\n*Dependent Variable: 5-year growth rate in %: (y_t - y_{t-1}) x 100*\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `GINI(Q12)_{t-1}` | 0.143 | (0.179) |\n| `GINI(Q12)_{t-2}` | -0.454** | (0.212) |\n| **Diagnostic Tests** | **Statistic** | |\n| M2 test (t-value) | -0.678 | |\n| Hansen test (p-value) | 0.575 | |\n| Joint significance of GINI (p-value) | 0.051 | |\n\n*Notes: `**` denotes significance at the 5% level.*\n\n---\n\nBased on the results in Table 1, which of the following statements are valid interpretations or conclusions?\n\nSelect all that apply.", "Answer": ["B", "C", "D"], "qid": "245", "question": "### Background\n\nThe paper's core empirical finding is that income inequality has opposing effects on economic growth over different time horizons. The analysis uses a System GMM estimation on a 5-year panel of up to 106 countries from 1965-2005. The dependent variable is the 5-year growth rate of real GDP per capita.\n\n### Data / Model Specification\n\nThe following table presents a selection of the core one-step System GMM results from the paper's Table 3, column (4).\n\n**Table 1: Selected System GMM Results**\n*Dependent Variable: 5-year growth rate in %: (y_t - y_{t-1}) x 100*\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `GINI(Q12)_{t-1}` | 0.143 | (0.179) |\n| `GINI(Q12)_{t-2}` | -0.454** | (0.212) |\n| **Diagnostic Tests** | **Statistic** | |\n| M2 test (t-value) | -0.678 | |\n| Hansen test (p-value) | 0.575 | |\n| Joint significance of GINI (p-value) | 0.051 | |\n\n*Notes: `**` denotes significance at the 5% level.*\n\n---\n\nBased on the results in Table 1, which of the following statements are valid interpretations or conclusions?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's core empirical finding...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's core empirical finding...", "error": null}}
{"ID": 246, "Question": "### Background\n\nThe paper's theoretical model posits a trade-off between a short-run economic efficiency gain from inequality and a long-run productivity loss from reduced public goods. The model uses a political-economic equilibrium where public good investment in the next period, `G_{t+1}`, is high (`1`) if current inequality `D_t` is low (`L`), and low (`0`) if `D_t` is high (`H`).\n\n### Data / Model Specification\n\nAggregate output is given by:\n  \nY(D_t, G_t) = \\left(a^h - \\sigma(a^h - a^l)\\omega^P(D_t)\\right)X(G_t) \\quad \\text{(Eq. (1))}\n \nwhere `D_t` is asset inequality, `G_t` is the public good level, `a^h > a^l` are productivity levels, `σ` is the fraction of poor, `ω^P(D_t)` is the endowment of the poor, and `X(G_t)` is a productivity multiplier with `X(1) > X(0)`.\n\nConsider an economy that was in a low-inequality state (`D_{t-1}=L`), resulting in a high level of public goods (`G_t=1`). In period `t`, inequality unexpectedly rises to `H`.\n\nWhich of the following statements accurately describe the dynamic consequences of this shock according to the model?\n\nSelect all that apply.", "Answer": ["C", "D"], "qid": "246", "question": "### Background\n\nThe paper's theoretical model posits a trade-off between a short-run economic efficiency gain from inequality and a long-run productivity loss from reduced public goods. The model uses a political-economic equilibrium where public good investment in the next period, `G_{t+1}`, is high (`1`) if current inequality `D_t` is low (`L`), and low (`0`) if `D_t` is high (`H`).\n\n### Data / Model Specification\n\nAggregate output is given by:\n  \nY(D_t, G_t) = \\left(a^h - \\sigma(a^h - a^l)\\omega^P(D_t)\\right)X(G_t) \\quad \\text{(Eq. (1))}\n \nwhere `D_t` is asset inequality, `G_t` is the public good level, `a^h > a^l` are productivity levels, `σ` is the fraction of poor, `ω^P(D_t)` is the endowment of the poor, and `X(G_t)` is a productivity multiplier with `X(1) > X(0)`.\n\nConsider an economy that was in a low-inequality state (`D_{t-1}=L`), resulting in a high level of public goods (`G_t=1`). In period `t`, inequality unexpectedly rises to `H`.\n\nWhich of the following statements accurately describe the dynamic consequences of this shock according to the model?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical model posi...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical model posi...", "error": null}}
{"ID": 247, "Question": "### Background\n\nThe paper's theoretical model links asset inequality (`D_t`) to output levels. To create an empirical growth model, this is extended by assuming aggregate productivity `A_t` depends on past output (`Y_{t-1}`) via a learning-by-doing externality, `A_t = (Y_{t-1})^φ`.\n\n### Data / Model Specification\n\nThis extension transforms the model of log output levels into a specification for output growth:\n  \ny_{t}-y_{t-1}\\simeq\\gamma y_{t-1}+\\delta_{1}D_{t}^{\\mathrm{r}}+\\delta_{2}D_{t-1}^{\\mathrm{r}}+\\eta \\quad \\text{(Eq. (1))}\n \nwhere `y_t` is log output, `D_t^r` is income inequality, `γ = φ - 1`, `δ_1 > 0` (short-term effect), and `δ_2 < 0` (lagged effect).\n\nThis specification introduces econometric challenges. The presence of the lagged dependent variable `y_{t-1}` on the right-hand side means that standard estimators like Within-Groups (WG) or Random Effects (RE) would likely produce inconsistent estimates in a short panel.\n\nWhich of the following are valid reasons why the System GMM estimator is preferred over alternatives for estimating Eq. (1) in this context?\n\nSelect all that apply.", "Answer": ["B", "D"], "qid": "247", "question": "### Background\n\nThe paper's theoretical model links asset inequality (`D_t`) to output levels. To create an empirical growth model, this is extended by assuming aggregate productivity `A_t` depends on past output (`Y_{t-1}`) via a learning-by-doing externality, `A_t = (Y_{t-1})^φ`.\n\n### Data / Model Specification\n\nThis extension transforms the model of log output levels into a specification for output growth:\n  \ny_{t}-y_{t-1}\\simeq\\gamma y_{t-1}+\\delta_{1}D_{t}^{\\mathrm{r}}+\\delta_{2}D_{t-1}^{\\mathrm{r}}+\\eta \\quad \\text{(Eq. (1))}\n \nwhere `y_t` is log output, `D_t^r` is income inequality, `γ = φ - 1`, `δ_1 > 0` (short-term effect), and `δ_2 < 0` (lagged effect).\n\nThis specification introduces econometric challenges. The presence of the lagged dependent variable `y_{t-1}` on the right-hand side means that standard estimators like Within-Groups (WG) or Random Effects (RE) would likely produce inconsistent estimates in a short panel.\n\nWhich of the following are valid reasons why the System GMM estimator is preferred over alternatives for estimating Eq. (1) in this context?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical model link...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's theoretical model link...", "error": null}}
{"ID": 248, "Question": "### Background\n\n**Research Question.** This problem investigates the central flaw in a prior derivation of the limiting distribution for an inconsistent instrumental variables (IV) estimator and presents the logic behind the corrected formula.\n\n**Setting / Institutional Environment.** In a general linear model with inconsistent IV estimation (i.e., instruments are correlated with the error term), the derivation of the estimator's asymptotic variance is highly sensitive to the properties of certain residual terms. The critique focuses on an incorrect application of a convergence theorem by Hendry, which claimed that the limiting distribution of `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` could be found from a simplified control variate `\\(p^*\\)`. The key to the flaw lies in the properties of the residual vector `\\(e = u - Xp\\)`, where `\\(p\\)` is the vector of inconsistency.\n\n**Variables & Parameters.**\n- `\\(u, X, Z\\)`: Standard matrices for errors, regressors, and instruments.\n- `\\(p\\)`: `\\(k \\times 1\\)` vector of inconsistency, `\\(p = \\operatorname{plim}(\\widetilde{\\beta}) - \\beta\\)`.\n- `\\(e\\)`: A residual vector defined as `\\(e = u - Xp\\)`.\n- `\\(\\alpha = \\operatorname{plim}(Z'u/T)\\)`.\n- `\\(G = \\operatorname{plim}(Z'Z/T)\\)`.\n- `\\(A = \\operatorname{plim}\\{(X'Z/T)(Z'Z/T)^{-1}\\}\\)`.\n- `\\(K = \\operatorname{plim}(X'NX/T) = AGA'\\)`.\n\n---\n\n### Data / Model Specification\n\nHendry's flawed derivation relied on the argument that `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` has the same limiting distribution as `\\(\\sqrt{T}(p^*-p)\\)`, where `\\(p^* = p + T^{-1}K^{-1}AZ'e\\)`. This argument hinges on the term `\\(T^{-1/2}Z'e\\)` having a well-behaved limiting distribution. The critique in this paper demonstrates this is false by first deriving the expectation of `\\(Z'e\\)`:\n  \n\\mathrm{E}(Z'e) = T(I - G A'K^{-1}A)\\alpha \\quad \\text{(Eq. (1))}\n \nThis result implies that the correct limiting distribution must account for additional sources of variance. The corrected asymptotic variance `\\(C\\)` of `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` is shown to be `\\(C = H\\psi H'\\)`, where `\\(H\\)` is a transformation matrix that captures not only the variance from `\\(Z'e\\)` but also from the sampling variation in `\\(X'Z\\)` and `\\(Z'Z\\)`. The full expression for the statistic whose limiting distribution is sought is:\n  \n\\sqrt{T}(p^{*} - p) + K^{-1}\\sqrt{T}\\left\\{T^{-1}X'Z - \\mathrm{plim}(T^{-1}X'Z)\\right\\}G^{-1}\\alpha_e - K^{-1}A\\sqrt{T}\\{T^{-1}Z'Z - G\\}G^{-1}\\alpha_e \\quad \\text{(Eq. (2))}\n \nwhere `\\(\\alpha_e = E(T^{-1}Z'e)\\)`.\n\n---\n\n### Question\n\nBased on the provided model and definitions, select all statements that are mathematically correct and logically valid.", "Answer": ["A", "C"], "qid": "248", "question": "### Background\n\n**Research Question.** This problem investigates the central flaw in a prior derivation of the limiting distribution for an inconsistent instrumental variables (IV) estimator and presents the logic behind the corrected formula.\n\n**Setting / Institutional Environment.** In a general linear model with inconsistent IV estimation (i.e., instruments are correlated with the error term), the derivation of the estimator's asymptotic variance is highly sensitive to the properties of certain residual terms. The critique focuses on an incorrect application of a convergence theorem by Hendry, which claimed that the limiting distribution of `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` could be found from a simplified control variate `\\(p^*\\)`. The key to the flaw lies in the properties of the residual vector `\\(e = u - Xp\\)`, where `\\(p\\)` is the vector of inconsistency.\n\n**Variables & Parameters.**\n- `\\(u, X, Z\\)`: Standard matrices for errors, regressors, and instruments.\n- `\\(p\\)`: `\\(k \\times 1\\)` vector of inconsistency, `\\(p = \\operatorname{plim}(\\widetilde{\\beta}) - \\beta\\)`.\n- `\\(e\\)`: A residual vector defined as `\\(e = u - Xp\\)`.\n- `\\(\\alpha = \\operatorname{plim}(Z'u/T)\\)`.\n- `\\(G = \\operatorname{plim}(Z'Z/T)\\)`.\n- `\\(A = \\operatorname{plim}\\{(X'Z/T)(Z'Z/T)^{-1}\\}\\)`.\n- `\\(K = \\operatorname{plim}(X'NX/T) = AGA'\\)`.\n\n---\n\n### Data / Model Specification\n\nHendry's flawed derivation relied on the argument that `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` has the same limiting distribution as `\\(\\sqrt{T}(p^*-p)\\)`, where `\\(p^* = p + T^{-1}K^{-1}AZ'e\\)`. This argument hinges on the term `\\(T^{-1/2}Z'e\\)` having a well-behaved limiting distribution. The critique in this paper demonstrates this is false by first deriving the expectation of `\\(Z'e\\)`:\n  \n\\mathrm{E}(Z'e) = T(I - G A'K^{-1}A)\\alpha \\quad \\text{(Eq. (1))}\n \nThis result implies that the correct limiting distribution must account for additional sources of variance. The corrected asymptotic variance `\\(C\\)` of `\\(\\sqrt{T}(\\widetilde{\\beta}-\\beta_l)\\)` is shown to be `\\(C = H\\psi H'\\)`, where `\\(H\\)` is a transformation matrix that captures not only the variance from `\\(Z'e\\)` but also from the sampling variation in `\\(X'Z\\)` and `\\(Z'Z\\)`. The full expression for the statistic whose limiting distribution is sought is:\n  \n\\sqrt{T}(p^{*} - p) + K^{-1}\\sqrt{T}\\left\\{T^{-1}X'Z - \\mathrm{plim}(T^{-1}X'Z)\\right\\}G^{-1}\\alpha_e - K^{-1}A\\sqrt{T}\\{T^{-1}Z'Z - G\\}G^{-1}\\alpha_e \\quad \\text{(Eq. (2))}\n \nwhere `\\(\\alpha_e = E(T^{-1}Z'e)\\)`.\n\n---\n\n### Question\n\nBased on the provided model and definitions, select all statements that are mathematically correct and logically valid.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 249, "Question": "### Background\n\n**Research Question.** This problem concerns the formal derivation and economic interpretation of the asymptotic bias (inconsistency) of an instrumental variables (IV) estimator when the instruments are not exogenous.\n\n**Setting / Institutional Environment.** The analysis is conducted in a general linear model framework where some chosen instruments `\\(Z\\)` are correlated with the structural error term `\\(u\\)`, violating the standard exogeneity assumption for IV estimation. This leads to an estimator that does not converge to the true parameter value, even in infinitely large samples.\n\n**Variables & Parameters.**\n- `\\(y, X, u, Z, \\beta\\)`: Standard notation for dependent variable, regressors, error term, instruments, and true parameter vector.\n- `\\(\\widetilde{\\beta}\\)`: The IV estimator for `\\(\\beta\\)`.\n- `\\(N\\)`: The projection matrix `\\(Z(Z'Z)^{-1}Z'\\)`.\n- `\\(p\\)`: The vector of inconsistencies (asymptotic bias) in the estimator `\\(\\widetilde{\\beta}\\)`.\n- `\\(\\beta_l\\)`: The probability limit of `\\(\\widetilde{\\beta}\\)`.\n\n---\n\n### Data / Model Specification\n\nThe structural model and IV estimator are given by:\n  \ny = X\\beta + u \\quad \\text{(Eq. (1))}\n \n  \n\\widetilde{\\beta} = (X'N X)^{-1}X'N y, \\quad \\text{where } N = Z(Z'Z)^{-1}Z' \\quad \\text{(Eq. (2))}\n \nThe following probability limits are defined based on the model's assumptions:\n  \nA = \\operatorname*{plim}_{T\\to\\infty} \\left\\{ \\left(\\frac{X'Z}{T}\\right) \\left(\\frac{Z'Z}{T}\\right)^{-1} \\right\\} \\quad \\text{(Eq. (3))}\n \n  \nK = \\operatorname*{plim}_{T\\to\\infty} \\left( \\frac{X'NX}{T} \\right) \\quad \\text{(Eq. (4))}\n \n  \n\\alpha = \\operatorname*{plim}_{T\\to\\infty} \\left( \\frac{Z'u}{T} \\right) \\quad \\text{(Eq. (5))}\n \nThe probability limit of the estimator is:\n  \n\\operatorname*{plim}_{T\\to\\infty} \\widetilde{\\beta} = \\beta + K^{-1}A\\alpha = \\beta + p = \\beta_l \\quad \\text{(Eq. (6))}\n \n\n---\n\n### Question\n\nBased on the provided model and definitions, select all statements that correctly describe the inconsistency `\\(p\\)` of the IV estimator `\\(\\widetilde{\\beta}\\)`.", "Answer": ["C", "D"], "qid": "249", "question": "### Background\n\n**Research Question.** This problem concerns the formal derivation and economic interpretation of the asymptotic bias (inconsistency) of an instrumental variables (IV) estimator when the instruments are not exogenous.\n\n**Setting / Institutional Environment.** The analysis is conducted in a general linear model framework where some chosen instruments `\\(Z\\)` are correlated with the structural error term `\\(u\\)`, violating the standard exogeneity assumption for IV estimation. This leads to an estimator that does not converge to the true parameter value, even in infinitely large samples.\n\n**Variables & Parameters.**\n- `\\(y, X, u, Z, \\beta\\)`: Standard notation for dependent variable, regressors, error term, instruments, and true parameter vector.\n- `\\(\\widetilde{\\beta}\\)`: The IV estimator for `\\(\\beta\\)`.\n- `\\(N\\)`: The projection matrix `\\(Z(Z'Z)^{-1}Z'\\)`.\n- `\\(p\\)`: The vector of inconsistencies (asymptotic bias) in the estimator `\\(\\widetilde{\\beta}\\)`.\n- `\\(\\beta_l\\)`: The probability limit of `\\(\\widetilde{\\beta}\\)`.\n\n---\n\n### Data / Model Specification\n\nThe structural model and IV estimator are given by:\n  \ny = X\\beta + u \\quad \\text{(Eq. (1))}\n \n  \n\\widetilde{\\beta} = (X'N X)^{-1}X'N y, \\quad \\text{where } N = Z(Z'Z)^{-1}Z' \\quad \\text{(Eq. (2))}\n \nThe following probability limits are defined based on the model's assumptions:\n  \nA = \\operatorname*{plim}_{T\\to\\infty} \\left\\{ \\left(\\frac{X'Z}{T}\\right) \\left(\\frac{Z'Z}{T}\\right)^{-1} \\right\\} \\quad \\text{(Eq. (3))}\n \n  \nK = \\operatorname*{plim}_{T\\to\\infty} \\left( \\frac{X'NX}{T} \\right) \\quad \\text{(Eq. (4))}\n \n  \n\\alpha = \\operatorname*{plim}_{T\\to\\infty} \\left( \\frac{Z'u}{T} \\right) \\quad \\text{(Eq. (5))}\n \nThe probability limit of the estimator is:\n  \n\\operatorname*{plim}_{T\\to\\infty} \\widetilde{\\beta} = \\beta + K^{-1}A\\alpha = \\beta + p = \\beta_l \\quad \\text{(Eq. (6))}\n \n\n---\n\n### Question\n\nBased on the provided model and definitions, select all statements that correctly describe the inconsistency `\\(p\\)` of the IV estimator `\\(\\widetilde{\\beta}\\)`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 250, "Question": "### Background\n\n**Research Question.** This problem analyzes the properties of a Hausman-type test for fractional cointegration, focusing on its robustness to misspecification and its local power.\n\n**Setting / Institutional Environment.** A key challenge in testing for cointegration is the potential misspecification of the null hypothesis. The standard test, `X^*`, assumes all component series share a common integration order `\\delta`. If this assumption is violated (i.e., the series have heterogeneous integration orders `\\delta_i`), the `X^*` test can spuriously reject the null of no cointegration. A robustified statistic, `X^{**}`, is proposed to address this issue. This problem explores the failure mode of `X^*`, the solution provided by `X^{**}`, and the power of the robust test.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of observable time series.\n- `\\delta_i`: The integration order of the `i`-th series `z_{it}`.\n- `\\xi`: The `p x 1` vector of integration orders, `\\xi = (\\delta_1, ..., \\delta_p)'`.\n- `\\tilde{\\xi}`: The vector of univariate local Whittle estimates, `\\tilde{\\xi} = (\\tilde{\\delta}_{(1)}, ..., \\tilde{\\delta}_{(p)})'`.\n- `X^*`: The standard Hausman-type test statistic.\n- `X^{**}`: The robustified test statistic.\n\n---\n\n### Data / Model Specification\n\nThe standard test `X^*` can falsely reject the null of no cointegration if the series `z_{it}` are `I(\\delta_i)` where the `\\delta_i` differ. To address this, a robustified test `X^{**}` is proposed.\n\nIts construction relies on modified spectral matrices that use a vector of integration orders `x = (x_1, ..., x_p)'` to scale the periodogram `I_z(\\lambda_j)`:\n  \n\\hat{G}^{**}(x) = \\frac{1}{m}\\sum_{j=1}^{m} \\varPhi(\\lambda_{j};x) I_{z}(\\lambda_{j}) \\varPhi(\\lambda_{j};x) \n \nwhere `\\varPhi(\\lambda_j; x)` is a `p x p` diagonal matrix with `(i,i)`-th element `\\lambda_j^{x_i}`. The score `s^{**}(x) = \\mathrm{tr}\\{\\hat{G}^{**}(x)^{-1}\\hat{H}^{**}(x)\\}`, where `\\hat{H}^{**}` is similarly constructed, is evaluated at the vector of individual univariate estimates `x = \\tilde{\\xi}`. The final statistic is:\n  \nX^{**} = \\frac{m s^{**}(\\tilde{\\xi})^{2}}{\\mathrm{tr}\\{\\hat{R}^{**2}\\}-p} \\quad \\text{(Eq. (1))}\n \nUnder the null of no cointegration (even with heterogeneous `\\delta_i`), `X^{**}` converges to a `\\chi_1^2` distribution.\n\n---\n\n### Question\n\nBased on the paper's analysis of the standard test (`X^*`) and the robustified test (`X^{**}`), select all of the following statements that are correct.", "Answer": ["A", "B", "C", "D"], "qid": "250", "question": "### Background\n\n**Research Question.** This problem analyzes the properties of a Hausman-type test for fractional cointegration, focusing on its robustness to misspecification and its local power.\n\n**Setting / Institutional Environment.** A key challenge in testing for cointegration is the potential misspecification of the null hypothesis. The standard test, `X^*`, assumes all component series share a common integration order `\\delta`. If this assumption is violated (i.e., the series have heterogeneous integration orders `\\delta_i`), the `X^*` test can spuriously reject the null of no cointegration. A robustified statistic, `X^{**}`, is proposed to address this issue. This problem explores the failure mode of `X^*`, the solution provided by `X^{**}`, and the power of the robust test.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of observable time series.\n- `\\delta_i`: The integration order of the `i`-th series `z_{it}`.\n- `\\xi`: The `p x 1` vector of integration orders, `\\xi = (\\delta_1, ..., \\delta_p)'`.\n- `\\tilde{\\xi}`: The vector of univariate local Whittle estimates, `\\tilde{\\xi} = (\\tilde{\\delta}_{(1)}, ..., \\tilde{\\delta}_{(p)})'`.\n- `X^*`: The standard Hausman-type test statistic.\n- `X^{**}`: The robustified test statistic.\n\n---\n\n### Data / Model Specification\n\nThe standard test `X^*` can falsely reject the null of no cointegration if the series `z_{it}` are `I(\\delta_i)` where the `\\delta_i` differ. To address this, a robustified test `X^{**}` is proposed.\n\nIts construction relies on modified spectral matrices that use a vector of integration orders `x = (x_1, ..., x_p)'` to scale the periodogram `I_z(\\lambda_j)`:\n  \n\\hat{G}^{**}(x) = \\frac{1}{m}\\sum_{j=1}^{m} \\varPhi(\\lambda_{j};x) I_{z}(\\lambda_{j}) \\varPhi(\\lambda_{j};x) \n \nwhere `\\varPhi(\\lambda_j; x)` is a `p x p` diagonal matrix with `(i,i)`-th element `\\lambda_j^{x_i}`. The score `s^{**}(x) = \\mathrm{tr}\\{\\hat{G}^{**}(x)^{-1}\\hat{H}^{**}(x)\\}`, where `\\hat{H}^{**}` is similarly constructed, is evaluated at the vector of individual univariate estimates `x = \\tilde{\\xi}`. The final statistic is:\n  \nX^{**} = \\frac{m s^{**}(\\tilde{\\xi})^{2}}{\\mathrm{tr}\\{\\hat{R}^{**2}\\}-p} \\quad \\text{(Eq. (1))}\n \nUnder the null of no cointegration (even with heterogeneous `\\delta_i`), `X^{**}` converges to a `\\chi_1^2` distribution.\n\n---\n\n### Question\n\nBased on the paper's analysis of the standard test (`X^*`) and the robustified test (`X^{**}`), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 251, "Question": "### Background\n\n**Research Question.** This problem deconstructs the Hausman-type testing framework for fractional cointegration proposed in the paper, from its foundational estimators to the final test statistic and its asymptotic justification.\n\n**Setting / Institutional Environment.** The core of the paper's method is a specification test based on the ideas of Hausman. It involves comparing two estimators for the common integration order, `\\delta`: one that is efficient under the null hypothesis of no cointegration but inconsistent under the alternative (`\\hat{\\delta}`), and one that is consistent under both but inefficient under the null (`\\tilde{\\delta}`). The test is operationalized not by directly comparing the estimates, but through a computationally simpler score-based approach.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of `I(\\delta)` time series.\n- `\\delta`: The common integration order.\n- `\\hat{\\delta}`: The 'efficient' multivariate local Whittle estimator of `\\delta`.\n- `\\tilde{\\delta}`: The 'inefficient' estimator of `\\delta`, based on a weighted average of univariate estimates.\n- `S(d)`: The multivariate local Whittle objective function (a concentrated log-likelihood).\n- `s^*(d)`: A modified score (gradient) of the objective function `S(d)`.\n\n---\n\n### Data / Model Specification\n\nThe testing framework is built upon two estimators for `\\delta`:\n1.  **The 'Efficient' Estimator (`\\hat{\\delta}`):** This is the multivariate local Whittle estimator that minimizes a joint objective function `S(d)` using the full `p x p` periodogram matrix `I_z(\\lambda_j)`.\n      \n    \\hat{\\delta} = \\arg\\min_{d\\in U} S(d)\n     \n2.  **The 'Inefficient' Estimator (`\\tilde{\\delta}`):** This estimator first computes `p` separate univariate local Whittle estimates, `\\tilde{\\delta}_{(i)}`, using only the diagonal elements of the periodogram matrix, and then takes a weighted average.\n      \n    \\tilde{\\delta} = \\sum_{i=1}^{p} a_{i} \\tilde{\\delta}_{(i)}\n     \nInstead of computing the difference `\\hat{\\delta} - \\tilde{\\delta}`, the test is based on the score of the multivariate objective function, `s^*(d)`, evaluated at the inefficient estimate `\\tilde{\\delta}`.\n\n---\n\n### Question\n\nThe paper's test for cointegration is based on the Hausman principle. Select all statements that correctly describe the components and justification of this framework.", "Answer": ["A", "B", "C", "D"], "qid": "251", "question": "### Background\n\n**Research Question.** This problem deconstructs the Hausman-type testing framework for fractional cointegration proposed in the paper, from its foundational estimators to the final test statistic and its asymptotic justification.\n\n**Setting / Institutional Environment.** The core of the paper's method is a specification test based on the ideas of Hausman. It involves comparing two estimators for the common integration order, `\\delta`: one that is efficient under the null hypothesis of no cointegration but inconsistent under the alternative (`\\hat{\\delta}`), and one that is consistent under both but inefficient under the null (`\\tilde{\\delta}`). The test is operationalized not by directly comparing the estimates, but through a computationally simpler score-based approach.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of `I(\\delta)` time series.\n- `\\delta`: The common integration order.\n- `\\hat{\\delta}`: The 'efficient' multivariate local Whittle estimator of `\\delta`.\n- `\\tilde{\\delta}`: The 'inefficient' estimator of `\\delta`, based on a weighted average of univariate estimates.\n- `S(d)`: The multivariate local Whittle objective function (a concentrated log-likelihood).\n- `s^*(d)`: A modified score (gradient) of the objective function `S(d)`.\n\n---\n\n### Data / Model Specification\n\nThe testing framework is built upon two estimators for `\\delta`:\n1.  **The 'Efficient' Estimator (`\\hat{\\delta}`):** This is the multivariate local Whittle estimator that minimizes a joint objective function `S(d)` using the full `p x p` periodogram matrix `I_z(\\lambda_j)`.\n      \n    \\hat{\\delta} = \\arg\\min_{d\\in U} S(d)\n     \n2.  **The 'Inefficient' Estimator (`\\tilde{\\delta}`):** This estimator first computes `p` separate univariate local Whittle estimates, `\\tilde{\\delta}_{(i)}`, using only the diagonal elements of the periodogram matrix, and then takes a weighted average.\n      \n    \\tilde{\\delta} = \\sum_{i=1}^{p} a_{i} \\tilde{\\delta}_{(i)}\n     \nInstead of computing the difference `\\hat{\\delta} - \\tilde{\\delta}`, the test is based on the score of the multivariate objective function, `s^*(d)`, evaluated at the inefficient estimate `\\tilde{\\delta}`.\n\n---\n\n### Question\n\nThe paper's test for cointegration is based on the Hausman principle. Select all statements that correctly describe the components and justification of this framework.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 252, "Question": "### Background\n\n**Research Question.** This problem explores the foundational model of fractional cointegration, linking the spectral properties of time series to their structural relationships.\n\n**Setting / Institutional Environment.** In multivariate time series analysis, variables are often characterized by their order of integration, `\\delta`, which describes their long-run persistence. Fractional cointegration generalizes the standard `I(1)/I(0)` framework by allowing `\\delta` to be a real number. The theory connects the time-domain concept of a long-run equilibrium to frequency-domain properties, specifically the behavior of the spectral density matrix at frequency zero.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of observable time series.\n- `\\delta`: The common integration order of the series in `z_t`.\n- `r`: The cointegrating rank, or the number of cointegrating relationships (`r < p`).\n- `f_z(\\lambda)`: The `p x p` spectral density matrix of `z_t` at frequency `\\lambda`.\n- `G_0`: A `p x p` matrix characterizing the spectral behavior of `z_t` at `\\lambda=0`.\n- `B`: A `p x p` non-singular matrix defining the `r` cointegrating relationships.\n\n---\n\n### Data / Model Specification\n\nA `p`-dimensional vector process `z_t` is fractionally integrated of order `\\delta`, denoted `I(\\delta)`, if its spectral density matrix `f_z(\\lambda)` satisfies:\n  \nf_{z}(\\lambda) \\sim G_{0}\\lambda^{-2\\delta} \\quad \\text{as } \\lambda \\rightarrow 0+ \\quad \\text{(Eq. (1))}\n \nThe process `z_t` is cointegrated with rank `r` if there exists a `p x p` non-singular matrix `B` that transforms `z_t` into a vector `u_t` whose first `r` components are less persistent (i.e., `I(\\gamma_i)` with `\\gamma_i < \\delta`).\n  \nB z_{t} = u_{t} \\quad \\text{(Eq. (2))}\n \nThis structure implies that under cointegration, the matrix `G_0` from Eq. (1) has reduced rank `p-r`.\n\n---\n\n### Question\n\nAccording to the paper's theoretical framework for fractional cointegration, select all of the following statements that are correct.", "Answer": ["A", "B", "C", "D"], "qid": "252", "question": "### Background\n\n**Research Question.** This problem explores the foundational model of fractional cointegration, linking the spectral properties of time series to their structural relationships.\n\n**Setting / Institutional Environment.** In multivariate time series analysis, variables are often characterized by their order of integration, `\\delta`, which describes their long-run persistence. Fractional cointegration generalizes the standard `I(1)/I(0)` framework by allowing `\\delta` to be a real number. The theory connects the time-domain concept of a long-run equilibrium to frequency-domain properties, specifically the behavior of the spectral density matrix at frequency zero.\n\n**Variables & Parameters.**\n- `z_t`: A `p x 1` vector of observable time series.\n- `\\delta`: The common integration order of the series in `z_t`.\n- `r`: The cointegrating rank, or the number of cointegrating relationships (`r < p`).\n- `f_z(\\lambda)`: The `p x p` spectral density matrix of `z_t` at frequency `\\lambda`.\n- `G_0`: A `p x p` matrix characterizing the spectral behavior of `z_t` at `\\lambda=0`.\n- `B`: A `p x p` non-singular matrix defining the `r` cointegrating relationships.\n\n---\n\n### Data / Model Specification\n\nA `p`-dimensional vector process `z_t` is fractionally integrated of order `\\delta`, denoted `I(\\delta)`, if its spectral density matrix `f_z(\\lambda)` satisfies:\n  \nf_{z}(\\lambda) \\sim G_{0}\\lambda^{-2\\delta} \\quad \\text{as } \\lambda \\rightarrow 0+ \\quad \\text{(Eq. (1))}\n \nThe process `z_t` is cointegrated with rank `r` if there exists a `p x p` non-singular matrix `B` that transforms `z_t` into a vector `u_t` whose first `r` components are less persistent (i.e., `I(\\gamma_i)` with `\\gamma_i < \\delta`).\n  \nB z_{t} = u_{t} \\quad \\text{(Eq. (2))}\n \nThis structure implies that under cointegration, the matrix `G_0` from Eq. (1) has reduced rank `p-r`.\n\n---\n\n### Question\n\nAccording to the paper's theoretical framework for fractional cointegration, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 253, "Question": "### Background\n\n**Research Question.** This problem addresses the practical application and extension of the fractional cointegration testing framework, focusing on non-stationary series and the interpretation of finite-sample results.\n\n**Setting / Institutional Environment.** The local Whittle estimation methods at the core of the paper are theoretically designed for stationary series (`\\delta < 1/2`). To be useful for many applications, the framework is extended to non-stationary series (`\\delta > 1/2`) using technical modifications like data tapering and frequency skipping. The performance of these methods in finite samples is crucial for guiding applied research.\n\n**Variables & Parameters.**\n- `X_h^{**}`: The test statistic adapted for non-stationary series.\n- `n`: Sample size.\n- `m`: Bandwidth parameter.\n\n---\n\n### Data / Model Specification\n\nTo handle non-stationarity, the paper proposes a modified statistic, `X_h^{**}`, which incorporates data tapering and frequency skipping. The paper's Monte Carlo simulations show that this test can be severely undersized (i.e., reject the null far less often than the nominal size) in small samples and for small bandwidths `m`, leading to low power. The paper recommends against a single data-dependent bandwidth `m` and instead suggests computing the statistic over a grid of `m` values to assess sensitivity.\n\n---\n\n### Question\n\nA researcher analyzing a non-stationary series of length `n=500` computes the `X_h^{**}` test for cointegration across a range of bandwidths `m`. The p-value is insignificant for small `m` (`<60`), significant for a stable intermediate range of `m` (`60-90`), and insignificant again for large `m` (`>90`). Based on the principles of semi-parametric estimation and the paper's findings, select all valid conclusions.", "Answer": ["A", "B", "D"], "qid": "253", "question": "### Background\n\n**Research Question.** This problem addresses the practical application and extension of the fractional cointegration testing framework, focusing on non-stationary series and the interpretation of finite-sample results.\n\n**Setting / Institutional Environment.** The local Whittle estimation methods at the core of the paper are theoretically designed for stationary series (`\\delta < 1/2`). To be useful for many applications, the framework is extended to non-stationary series (`\\delta > 1/2`) using technical modifications like data tapering and frequency skipping. The performance of these methods in finite samples is crucial for guiding applied research.\n\n**Variables & Parameters.**\n- `X_h^{**}`: The test statistic adapted for non-stationary series.\n- `n`: Sample size.\n- `m`: Bandwidth parameter.\n\n---\n\n### Data / Model Specification\n\nTo handle non-stationarity, the paper proposes a modified statistic, `X_h^{**}`, which incorporates data tapering and frequency skipping. The paper's Monte Carlo simulations show that this test can be severely undersized (i.e., reject the null far less often than the nominal size) in small samples and for small bandwidths `m`, leading to low power. The paper recommends against a single data-dependent bandwidth `m` and instead suggests computing the statistic over a grid of `m` values to assess sensitivity.\n\n---\n\n### Question\n\nA researcher analyzing a non-stationary series of length `n=500` computes the `X_h^{**}` test for cointegration across a range of bandwidths `m`. The p-value is insignificant for small `m` (`<60`), significant for a stable intermediate range of `m` (`60-90`), and insignificant again for large `m` (`>90`). Based on the principles of semi-parametric estimation and the paper's findings, select all valid conclusions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 254, "Question": "### Background\n\nA researcher is evaluating the performance of the `DF-GLS` unit root test, a two-step procedure proposed in the paper for series with a linear time trend. The procedure involves: (1) estimating trend parameters `β` via a GLS-type regression on quasi-differenced data to create a 'locally detrended' series `y_t^d`; (2) running a standard augmented Dickey-Fuller regression on `y_t^d`.\n\n### Data / Model Specification\n\nThe researcher has a time series of `T=100` observations and calculates the `DF-GLS` statistic to be **-3.10**. The analysis is based on the following tables from the paper.\n\n**Table 1: Size and Size-Adjusted Power, Linear Trend, T=100, 5% Level**\n\n| Test Statistic | `α` | Asymptotic Power | MA(1), `θ` = 0.0 | MA(1), `θ` = 0.8 |\n| :--- | :--- | :--- | :--- | :--- |\n| **DF-GLS'(.5) AR(BIC)** | 1.00 | .05 | 0.07 | 0.11 |\n| | .90 | .27 | 0.36 | 0.39 |\n| | .80 | .81 | 0.72 | 0.77 |\n\n*Note: The `α=1.00` row shows the empirical size (rejection rate under the null) for a nominal 5% test.*\n\n**Table 2: Critical Values for DF-GLS Test (Linear Trend, c=-13.5)**\n\n| T | 1% | 2.5% | 5% | 10% |\n| :--- | :--- | :--- | :--- | :--- |\n| 50 | -3.77 | -3.46 | -3.19 | -2.89 |\n| 100 | -3.58 | -3.29 | -3.03 | -2.74 |\n| 200 | -3.46 | -3.18 | -2.93 | -2.64 |\n| `∞` | -3.48 | -3.15 | -2.89 | -2.57 |\n\n### Question\n\nBased on the provided data, which of the following statements are valid conclusions or interpretations?", "Answer": ["A", "B", "C", "D"], "qid": "254", "question": "### Background\n\nA researcher is evaluating the performance of the `DF-GLS` unit root test, a two-step procedure proposed in the paper for series with a linear time trend. The procedure involves: (1) estimating trend parameters `β` via a GLS-type regression on quasi-differenced data to create a 'locally detrended' series `y_t^d`; (2) running a standard augmented Dickey-Fuller regression on `y_t^d`.\n\n### Data / Model Specification\n\nThe researcher has a time series of `T=100` observations and calculates the `DF-GLS` statistic to be **-3.10**. The analysis is based on the following tables from the paper.\n\n**Table 1: Size and Size-Adjusted Power, Linear Trend, T=100, 5% Level**\n\n| Test Statistic | `α` | Asymptotic Power | MA(1), `θ` = 0.0 | MA(1), `θ` = 0.8 |\n| :--- | :--- | :--- | :--- | :--- |\n| **DF-GLS'(.5) AR(BIC)** | 1.00 | .05 | 0.07 | 0.11 |\n| | .90 | .27 | 0.36 | 0.39 |\n| | .80 | .81 | 0.72 | 0.77 |\n\n*Note: The `α=1.00` row shows the empirical size (rejection rate under the null) for a nominal 5% test.*\n\n**Table 2: Critical Values for DF-GLS Test (Linear Trend, c=-13.5)**\n\n| T | 1% | 2.5% | 5% | 10% |\n| :--- | :--- | :--- | :--- | :--- |\n| 50 | -3.77 | -3.46 | -3.19 | -2.89 |\n| 100 | -3.58 | -3.29 | -3.03 | -2.74 |\n| 200 | -3.46 | -3.18 | -2.93 | -2.64 |\n| `∞` | -3.48 | -3.15 | -2.89 | -2.57 |\n\n### Question\n\nBased on the provided data, which of the following statements are valid conclusions or interpretations?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA researcher is evaluating the per...", "model_answer": "This is a detailed response to the question about ### Background\n\nA researcher is evaluating the per...", "error": null}}
{"ID": 255, "Question": "### Background\n\nThe paper develops a theoretical upper bound on the power of unit root tests, known as the asymptotic power envelope. This envelope differs depending on whether the parameters of the deterministic trend component, `d_t`, are known or must be estimated.\n\n### Data / Model Specification\n\nConsider a time series `y_t = d_t + u_t` with `u_t = αu_{t-1} + v_t`. The analysis uses local-to-unity asymptotics where `c = T(α-1)`.\n\n1.  **Known Trend:** When `d_t` is known, the optimal test statistic's limiting distribution depends on the process `W_c(t)`, which is the continuous-time limit of the true stochastic component `u_t`. The resulting power envelope is `π(c)`.\n2.  **Unknown Linear Trend:** When `d_t = β₀ + β₁t` is unknown, feasible tests must be *invariant* to the values of `β₀` and `β₁`. The optimal invariant test statistic's limiting distribution depends on a 'detrended' process, `V_c(t, c̄) = W_c(t) - t × b₁`, where `b₁` is a functional of `W_c(·)`. The resulting invariant power envelope is `π^τ(c)`. The paper establishes that `π^τ(c) < π(c)`.\n\n### Question\n\nGiven the theoretical framework, which of the following statements accurately describe the reasons for, or consequences of, the power loss incurred when estimating a linear trend?", "Answer": ["A", "B", "D"], "qid": "255", "question": "### Background\n\nThe paper develops a theoretical upper bound on the power of unit root tests, known as the asymptotic power envelope. This envelope differs depending on whether the parameters of the deterministic trend component, `d_t`, are known or must be estimated.\n\n### Data / Model Specification\n\nConsider a time series `y_t = d_t + u_t` with `u_t = αu_{t-1} + v_t`. The analysis uses local-to-unity asymptotics where `c = T(α-1)`.\n\n1.  **Known Trend:** When `d_t` is known, the optimal test statistic's limiting distribution depends on the process `W_c(t)`, which is the continuous-time limit of the true stochastic component `u_t`. The resulting power envelope is `π(c)`.\n2.  **Unknown Linear Trend:** When `d_t = β₀ + β₁t` is unknown, feasible tests must be *invariant* to the values of `β₀` and `β₁`. The optimal invariant test statistic's limiting distribution depends on a 'detrended' process, `V_c(t, c̄) = W_c(t) - t × b₁`, where `b₁` is a functional of `W_c(·)`. The resulting invariant power envelope is `π^τ(c)`. The paper establishes that `π^τ(c) < π(c)`.\n\n### Question\n\nGiven the theoretical framework, which of the following statements accurately describe the reasons for, or consequences of, the power loss incurred when estimating a linear trend?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper develops a theoretical u...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper develops a theoretical u...", "error": null}}
{"ID": 256, "Question": "### Background\n\nThe paper proposes a feasible point-optimal test statistic, `P_T`, designed to have high power in the presence of unknown error covariance structures. Its construction and validity rely on a key set of assumptions.\n\n### Data / Model Specification\n\nThe feasible statistic is defined as:\n\n  \nP_{T} = [S(\\bar{\\alpha}) - \\bar{\\alpha}S(1)] / \\hat{\\omega}^{2} \n \n\nwhere `S(a)` is the sum of squared residuals from an OLS regression of quasi-differenced data, and `ω̂²` is a consistent estimator of the long-run variance of the errors `v_t`.\n\nTheorem 2 establishes that `P_T` attains the asymptotic power envelope under **Condition C**, which requires that the initial error `u₀` has a distribution with a bounded second moment for `α` in a neighborhood of unity.\n\nThe paper explicitly notes that an alternative hypothesis of *strict stationarity* for the `u_t` process would violate Condition C. For a stationary AR(1) process `u_t = αu_{t-1} + v_t` with `|α|<1` and `v_t` having variance `σ_v²`, the initial observation `u₀` is drawn from the stationary distribution with variance `Var(u₀) = σ_v² / (1 - α²)`. \n\n### Question\n\nWhich of the following statements are valid implications of the strict stationarity alternative for the paper's framework?", "Answer": ["A", "B", "D"], "qid": "256", "question": "### Background\n\nThe paper proposes a feasible point-optimal test statistic, `P_T`, designed to have high power in the presence of unknown error covariance structures. Its construction and validity rely on a key set of assumptions.\n\n### Data / Model Specification\n\nThe feasible statistic is defined as:\n\n  \nP_{T} = [S(\\bar{\\alpha}) - \\bar{\\alpha}S(1)] / \\hat{\\omega}^{2} \n \n\nwhere `S(a)` is the sum of squared residuals from an OLS regression of quasi-differenced data, and `ω̂²` is a consistent estimator of the long-run variance of the errors `v_t`.\n\nTheorem 2 establishes that `P_T` attains the asymptotic power envelope under **Condition C**, which requires that the initial error `u₀` has a distribution with a bounded second moment for `α` in a neighborhood of unity.\n\nThe paper explicitly notes that an alternative hypothesis of *strict stationarity* for the `u_t` process would violate Condition C. For a stationary AR(1) process `u_t = αu_{t-1} + v_t` with `|α|<1` and `v_t` having variance `σ_v²`, the initial observation `u₀` is drawn from the stationary distribution with variance `Var(u₀) = σ_v² / (1 - α²)`. \n\n### Question\n\nWhich of the following statements are valid implications of the strict stationarity alternative for the paper's framework?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper proposes a feasible poin...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper proposes a feasible poin...", "error": null}}
{"ID": 257, "Question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the heterogeneous impacts on primary school completion by household income.\n\n**Setting / Institutional Environment.** The study finds that the positive effect of the OB teacher provision program on girls' primary school completion was concentrated among the poorest households and was statistically zero for the richest households.\n\n### Question\n\nAccording to the economic reasoning presented in the paper, which of the following are plausible rationales for why the program's benefits were concentrated among poorer households? Select all that apply.", "Answer": ["C", "D"], "qid": "257", "question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the heterogeneous impacts on primary school completion by household income.\n\n**Setting / Institutional Environment.** The study finds that the positive effect of the OB teacher provision program on girls' primary school completion was concentrated among the poorest households and was statistically zero for the richest households.\n\n### Question\n\nAccording to the economic reasoning presented in the paper, which of the following are plausible rationales for why the program's benefits were concentrated among poorer households? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 258, "Question": "### Background\n\n**Research Question.** This problem examines how to construct a continuous measure of unobservable physician quality (ability or performance) from a series of discrete, observed actions, and how to interpret the parameters of such a model.\n\n**Setting / Institutional Environment.** The study measures physician quality in Tanzania. A doctor's underlying *ability* is measured using performance on vignettes (standardized, simulated patient cases), while their actual *performance* is measured using Direct Clinician Observation (DCO) of real patient encounters. The goal is to create aggregate quality scores from compliance with specific protocol items, rather than using a simple average of correct actions.\n\n### Data / Model Specification\n\nThe probability that doctor `i` with ability `A_i` correctly performs protocol item `j` is modeled using a logit function based on Item Response Theory (IRT):\n\n  \n\\mathrm{prob}(a_{ij}=1) = \\frac{\\exp(\\alpha_{j} A_{i} - \\beta_{j})}{1+\\exp(\\alpha_{j} A_{i} - \\beta_{j})} \\quad \\text{(Eq. (1))}\n \n\nWhere `a_ij` is an indicator for a correct action, `A_i` is the doctor's latent ability, `α_j` is the item's *discrimination* parameter, and `β_j` is the item's *difficulty* parameter. The paper notes that an item like \"asking about convulsions\" has a high, positive `α_j` and is performed by only 6% of doctors, while \"checking a patient's temperature\" has a small `α_j` and is performed by 88% of doctors.\n\n### Question\n\nBased on the model and examples provided, select all statements that are **true**.\n", "Answer": ["A", "D"], "qid": "258", "question": "### Background\n\n**Research Question.** This problem examines how to construct a continuous measure of unobservable physician quality (ability or performance) from a series of discrete, observed actions, and how to interpret the parameters of such a model.\n\n**Setting / Institutional Environment.** The study measures physician quality in Tanzania. A doctor's underlying *ability* is measured using performance on vignettes (standardized, simulated patient cases), while their actual *performance* is measured using Direct Clinician Observation (DCO) of real patient encounters. The goal is to create aggregate quality scores from compliance with specific protocol items, rather than using a simple average of correct actions.\n\n### Data / Model Specification\n\nThe probability that doctor `i` with ability `A_i` correctly performs protocol item `j` is modeled using a logit function based on Item Response Theory (IRT):\n\n  \n\\mathrm{prob}(a_{ij}=1) = \\frac{\\exp(\\alpha_{j} A_{i} - \\beta_{j})}{1+\\exp(\\alpha_{j} A_{i} - \\beta_{j})} \\quad \\text{(Eq. (1))}\n \n\nWhere `a_ij` is an indicator for a correct action, `A_i` is the doctor's latent ability, `α_j` is the item's *discrimination* parameter, and `β_j` is the item's *difficulty* parameter. The paper notes that an item like \"asking about convulsions\" has a high, positive `α_j` and is performed by only 6% of doctors, while \"checking a patient's temperature\" has a small `α_j` and is performed by 88% of doctors.\n\n### Question\n\nBased on the model and examples provided, select all statements that are **true**.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 259, "Question": "### Background\n\n**Research Question.** This problem examines how to construct a continuous measure of unobservable physician quality (ability or performance) from a series of discrete, observed actions, and how to interpret the parameters of such a model.\n\n**Setting / Institutional Environment.** The study measures physician quality in Tanzania. A doctor's underlying *ability* is measured using performance on vignettes (standardized, simulated patient cases), while their actual *performance* is measured using Direct Clinician Observation (DCO) of real patient encounters. The goal is to create aggregate quality scores from compliance with specific protocol items, rather than using a simple average of correct actions.\n\n### Data / Model Specification\n\nThe probability that doctor `i` with ability `A_i` correctly performs protocol item `j` is modeled using a logit function based on Item Response Theory (IRT):\n\n  \n\\mathrm{prob}(a_{ij}=1) = \\frac{\\exp(\\alpha_{j} A_{i} - \\beta_{j})}{1+\\exp(\\alpha_{j} A_{i} - \\beta_{j})} \\quad \\text{(Eq. (1))}\n \n\nWhere `a_ij` is an indicator for a correct action, `A_i` is the doctor's latent ability, `α_j` is the item's *discrimination* parameter, and `β_j` is the item's *difficulty* parameter.\n\n### Question\n\nSuppose that for the communication quality score, the authors had used a simple average of correctly performed items instead of the latent score from Eq. (1). This creates a noisy measure of true communication quality. Select all statements that are **incorrect** descriptions of the econometric consequences of this choice.\n", "Answer": ["A", "C"], "qid": "259", "question": "### Background\n\n**Research Question.** This problem examines how to construct a continuous measure of unobservable physician quality (ability or performance) from a series of discrete, observed actions, and how to interpret the parameters of such a model.\n\n**Setting / Institutional Environment.** The study measures physician quality in Tanzania. A doctor's underlying *ability* is measured using performance on vignettes (standardized, simulated patient cases), while their actual *performance* is measured using Direct Clinician Observation (DCO) of real patient encounters. The goal is to create aggregate quality scores from compliance with specific protocol items, rather than using a simple average of correct actions.\n\n### Data / Model Specification\n\nThe probability that doctor `i` with ability `A_i` correctly performs protocol item `j` is modeled using a logit function based on Item Response Theory (IRT):\n\n  \n\\mathrm{prob}(a_{ij}=1) = \\frac{\\exp(\\alpha_{j} A_{i} - \\beta_{j})}{1+\\exp(\\alpha_{j} A_{i} - \\beta_{j})} \\quad \\text{(Eq. (1))}\n \n\nWhere `a_ij` is an indicator for a correct action, `A_i` is the doctor's latent ability, `α_j` is the item's *discrimination* parameter, and `β_j` is the item's *difficulty* parameter.\n\n### Question\n\nSuppose that for the communication quality score, the authors had used a simple average of correctly performed items instead of the latent score from Eq. (1). This creates a noisy measure of true communication quality. Select all statements that are **incorrect** descriptions of the econometric consequences of this choice.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 260, "Question": "### Background\n\nAn analysis of UK child poverty between 1996/7 and 2000/1 seeks to explain why the average poverty gap (the average distance of a poor household's income from the poverty line) increased, even as the total number of poor children decreased. The poverty line is defined as 60% of the median household income.\n\n### Data / Model Specification\n\n**Table 1: The Composition of Different Income Groups**\n\n| Income group | 1996/7 | | 2000/01 | |\n| :--- | :---: | :---: | :---: | :---: |\n| (income as % of median) | **No. children (m)** | **% on MTBs** | **No. children (m)** | **% on MTBs** |\n| 0-20 | 0.3 | 37 | 0.4 | 32 |\n| 20-40 | 0.9 | 73 | 0.7 | 65 |\n| 40-60 | 3.2 | 80 | 2.7 | 78 |\n| 60-80 | 1.9 | 43 | 2.2 | 48 |\n| 80-100 | 1.7 | 15 | 1.8 | 21 |\n| 100+ | 4.9 | 4 | 4.9 | 7 |\n| **All** | **13.0** | **36** | **12.8** | **35** |\n\n*Notes: Income is measured After Housing Costs (AHC). MTBs are Means-Tested Benefits. The poverty line is at 60% of the median.*\n\n### Question\n\nBased on the data in Table 1, which of the following statements accurately describe changes between 1996/7 and 2000/01 that are consistent with the paper's explanation for the rising poverty gap? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "260", "question": "### Background\n\nAn analysis of UK child poverty between 1996/7 and 2000/1 seeks to explain why the average poverty gap (the average distance of a poor household's income from the poverty line) increased, even as the total number of poor children decreased. The poverty line is defined as 60% of the median household income.\n\n### Data / Model Specification\n\n**Table 1: The Composition of Different Income Groups**\n\n| Income group | 1996/7 | | 2000/01 | |\n| :--- | :---: | :---: | :---: | :---: |\n| (income as % of median) | **No. children (m)** | **% on MTBs** | **No. children (m)** | **% on MTBs** |\n| 0-20 | 0.3 | 37 | 0.4 | 32 |\n| 20-40 | 0.9 | 73 | 0.7 | 65 |\n| 40-60 | 3.2 | 80 | 2.7 | 78 |\n| 60-80 | 1.9 | 43 | 2.2 | 48 |\n| 80-100 | 1.7 | 15 | 1.8 | 21 |\n| 100+ | 4.9 | 4 | 4.9 | 7 |\n| **All** | **13.0** | **36** | **12.8** | **35** |\n\n*Notes: Income is measured After Housing Costs (AHC). MTBs are Means-Tested Benefits. The poverty line is at 60% of the median.*\n\n### Question\n\nBased on the data in Table 1, which of the following statements accurately describe changes between 1996/7 and 2000/01 that are consistent with the paper's explanation for the rising poverty gap? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of UK child poverty be...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of UK child poverty be...", "error": null}}
{"ID": 261, "Question": "### Background\n\nAn analysis of UK child poverty between 1996/7 and 2000/1 found that while the number of poor households with children decreased, the average poverty gap for those remaining in poverty actually increased. The poverty line is defined as a fraction of a central tendency of the national income distribution (e.g., 60% of the median).\n\n### Data / Model Specification\n\n**Table 1: Poverty Gap for Households with Children, 1996/7 and 2000/1**\n\n| Poverty line | Poverty gap | 1996/7 | 2000/1 |\n| :--- | :--- | :---: | :---: |\n| **50% mean** | Mean (£/wk) | 56.50 | 71.27 |\n| | Number of poor households (m) | 2.1 | 2.0 |\n| | Poverty gap ratio | 0.310 | 0.336 |\n| **60% median** | Mean (£/wk) | 55.82 | 68.49 |\n| | Number of poor households (m) | 2.1 | 1.9 |\n| | Poverty gap ratio | 0.310 | 0.334 |\n\n*Notes: Authors’ calculations from the HBAI data set. Income figures have been unequivalised.*\n\n### Question\n\nBased on the data in Table 1 and the paper's arguments, which of the following statements are valid conclusions or interpretations regarding the change in poverty intensity between 1996/7 and 2000/1? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "261", "question": "### Background\n\nAn analysis of UK child poverty between 1996/7 and 2000/1 found that while the number of poor households with children decreased, the average poverty gap for those remaining in poverty actually increased. The poverty line is defined as a fraction of a central tendency of the national income distribution (e.g., 60% of the median).\n\n### Data / Model Specification\n\n**Table 1: Poverty Gap for Households with Children, 1996/7 and 2000/1**\n\n| Poverty line | Poverty gap | 1996/7 | 2000/1 |\n| :--- | :--- | :---: | :---: |\n| **50% mean** | Mean (£/wk) | 56.50 | 71.27 |\n| | Number of poor households (m) | 2.1 | 2.0 |\n| | Poverty gap ratio | 0.310 | 0.336 |\n| **60% median** | Mean (£/wk) | 55.82 | 68.49 |\n| | Number of poor households (m) | 2.1 | 1.9 |\n| | Poverty gap ratio | 0.310 | 0.334 |\n\n*Notes: Authors’ calculations from the HBAI data set. Income figures have been unequivalised.*\n\n### Question\n\nBased on the data in Table 1 and the paper's arguments, which of the following statements are valid conclusions or interpretations regarding the change in poverty intensity between 1996/7 and 2000/1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of UK child poverty be...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of UK child poverty be...", "error": null}}
{"ID": 262, "Question": "### Background\n\nStatic micro-simulation models predicted that UK government policies from 1997-2001 would reduce child poverty by approximately 1.2 million. However, the actual observed reduction was only 0.5 million. The analysis focuses on a *relative* poverty line, set at 60% of the contemporary median household income.\n\n### Data / Model Specification\n\nBetween 1996/7 and 2000/1:\n- The observed reduction in the number of children in *relative* poverty was 0.5 million.\n- The predicted reduction from micro-simulations was 1.2 million.\n- Median household income grew in real terms by 13%.\n- The reduction in the number of children in *absolute* poverty (i.e., measured against a poverty line fixed at its 1996/7 real value) was 1.4 million.\n\n### Question\n\nGiven the discrepancy between predicted and actual poverty reduction, which of the following statements correctly identify a reason for the discrepancy or a valid implication of the data? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "262", "question": "### Background\n\nStatic micro-simulation models predicted that UK government policies from 1997-2001 would reduce child poverty by approximately 1.2 million. However, the actual observed reduction was only 0.5 million. The analysis focuses on a *relative* poverty line, set at 60% of the contemporary median household income.\n\n### Data / Model Specification\n\nBetween 1996/7 and 2000/1:\n- The observed reduction in the number of children in *relative* poverty was 0.5 million.\n- The predicted reduction from micro-simulations was 1.2 million.\n- Median household income grew in real terms by 13%.\n- The reduction in the number of children in *absolute* poverty (i.e., measured against a poverty line fixed at its 1996/7 real value) was 1.4 million.\n\n### Question\n\nGiven the discrepancy between predicted and actual poverty reduction, which of the following statements correctly identify a reason for the discrepancy or a valid implication of the data? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nStatic micro-simulation models pre...", "model_answer": "This is a detailed response to the question about ### Background\n\nStatic micro-simulation models pre...", "error": null}}
{"ID": 263, "Question": "### Background\n\n**Research Question.** This problem analyzes a recursive backward-induction procedure for refining a set of recommended continuation paths in a sequential game, and how the outcome depends on the order of moves.\n\n**Setting / Institutional Environment.** The setting is a \"spliced\" game `G_1 ⊕ G_2`, where `G_1` is a finite 3-player sequential game and `G_2` is a continuation game played after any terminal node of `G_1`. Players in `G_1` move in a fixed, sequential order.\n\n**Variables & Parameters.**\n- `σ(v^{**})`: The set of recommended continuation paths in the game `G_2`, starting from a representative root `v^{**}`.\n- `k`: Player index (mover number), `k ∈ {1, 2, 3}`.\n- `n`: The last player to move in the stage game `G_1`.\n- `U^k(y)`: Player `k`'s utility from a path `y` in the continuation game `G_2`.\n- `A_n^k(σ(v^{**}))`: The set of \"acceptable\" paths for players `k` through `n`, defined by a recursive refinement process.\n\n---\n\n### Data / Model Specification\n\nThe set of acceptable paths is defined by the following recursive procedure, starting with the last mover `n` and working backward to player `k`.\n\n**Base case (Player `n`):**\n  \nA_n^n(\\sigma(v^{**})) \\equiv \\operatorname*{Argmax}_{y \\in \\sigma(v^{**})} \\{U^n(y)\\}\n \n(Eq. (1))\n\n**Recursive step (Player `k < n`):**\n  \nA_n^k(\\sigma(v^{**})) \\equiv \\operatorname*{Argmax}_{y \\in A_n^{k+1}(\\sigma(v^{**}))} \\{U^k(y)\\}\n \n(Eq. (2))\n\nThe recommended set of paths in the continuation game is `σ(v^{**}) = {y_1, y_2, y_3, y_4}`, with corresponding utility vectors `U = (U^1, U^2, U^3)` given in Table 1.\n\n**Table 1: Utilities from Continuation Paths**\n| Path | `U^1` | `U^2` | `U^3` |\n|:----:|:-----:|:-----:|:-----:|\n| `y_1`  | 5     | 5     | 5     |\n| `y_2`  | 6     | 4     | 8     |\n| `y_3`  | 7     | 6     | 8     |\n| `y_4`  | 8     | 2     | 5     |\n\n---\n\n### Question\n\nAssume the players in game `G_1` move in the standard order: Player 1, then Player 2, then Player 3 (so `n=3`). Based on the recursive procedure in Eq. (1) and Eq. (2) and the data in Table 1, which of the following statements are correct?\n", "Answer": ["A", "B", "D"], "qid": "263", "question": "### Background\n\n**Research Question.** This problem analyzes a recursive backward-induction procedure for refining a set of recommended continuation paths in a sequential game, and how the outcome depends on the order of moves.\n\n**Setting / Institutional Environment.** The setting is a \"spliced\" game `G_1 ⊕ G_2`, where `G_1` is a finite 3-player sequential game and `G_2` is a continuation game played after any terminal node of `G_1`. Players in `G_1` move in a fixed, sequential order.\n\n**Variables & Parameters.**\n- `σ(v^{**})`: The set of recommended continuation paths in the game `G_2`, starting from a representative root `v^{**}`.\n- `k`: Player index (mover number), `k ∈ {1, 2, 3}`.\n- `n`: The last player to move in the stage game `G_1`.\n- `U^k(y)`: Player `k`'s utility from a path `y` in the continuation game `G_2`.\n- `A_n^k(σ(v^{**}))`: The set of \"acceptable\" paths for players `k` through `n`, defined by a recursive refinement process.\n\n---\n\n### Data / Model Specification\n\nThe set of acceptable paths is defined by the following recursive procedure, starting with the last mover `n` and working backward to player `k`.\n\n**Base case (Player `n`):**\n  \nA_n^n(\\sigma(v^{**})) \\equiv \\operatorname*{Argmax}_{y \\in \\sigma(v^{**})} \\{U^n(y)\\}\n \n(Eq. (1))\n\n**Recursive step (Player `k < n`):**\n  \nA_n^k(\\sigma(v^{**})) \\equiv \\operatorname*{Argmax}_{y \\in A_n^{k+1}(\\sigma(v^{**}))} \\{U^k(y)\\}\n \n(Eq. (2))\n\nThe recommended set of paths in the continuation game is `σ(v^{**}) = {y_1, y_2, y_3, y_4}`, with corresponding utility vectors `U = (U^1, U^2, U^3)` given in Table 1.\n\n**Table 1: Utilities from Continuation Paths**\n| Path | `U^1` | `U^2` | `U^3` |\n|:----:|:-----:|:-----:|:-----:|\n| `y_1`  | 5     | 5     | 5     |\n| `y_2`  | 6     | 4     | 8     |\n| `y_3`  | 7     | 6     | 8     |\n| `y_4`  | 8     | 2     | 5     |\n\n---\n\n### Question\n\nAssume the players in game `G_1` move in the standard order: Player 1, then Player 2, then Player 3 (so `n=3`). Based on the recursive procedure in Eq. (1) and Eq. (2) and the data in Table 1, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 264, "Question": "### Background\n\n**Research Question.** This problem examines the relationship between the solution to an infinite repeated game and the sequence of solutions to its finite-horizon counterparts under the nondiscriminating Optimistic Stable Standard of Behavior (OSSB) concept.\n\n**Setting / Institutional Environment.** The setting compares the nondiscriminating OSSB `σ` for an infinite Repeated Extensive Form (REF) game, `G^∞(δ)`, with the sequence of unique OSSBs `{σ_t}` for the corresponding finite `t`-period games, `G^t(δ)`.\n\n**Variables & Parameters.**\n- `G^∞(δ)`: The infinite REF game.\n- `G^t(δ)`: The finite `t`-period REF game.\n- `σ`: The nondiscriminating OSSB for `G^∞(δ)`.\n- `σ_t`: The unique OSSB for `G^t(δ)`.\n- `x`: An infinite path in `G^∞(δ)` starting at the root `v*`.\n- `x^t`: The truncation of path `x` to its first `t` stages.\n- `U^m(·)`: Player `m`'s utility function, which is continuous at infinity.\n\n---\n\n### Data / Model Specification\n\nProposition 4.6 provides a characterization of the paths in a nondiscriminating OSSB for an infinite game:\n\n  \n\\text{A path } x \\in \\sigma(v^*) \\iff x^t \\in \\sigma_t(v^*) \\text{ for all } t \\ge 1\n \n(Eq. (1))\n\nThe proof of the `(⇐)` direction proceeds by contradiction, assuming a path `x` satisfies the right-hand side condition but is not in `σ(v*)`.\n\n---\n\n### Question\n\nConsider the logic used to prove the `(⇐)` direction of Proposition 4.6. The proof assumes a path `x` satisfies `x^t ∈ σ_t(v*)` for all `t`, but is NOT in `σ(v*)`. This implies `x` is dominated in the infinite game by some path `y ∈ σ(w)`. Which of the following statements correctly describe valid steps or concepts used to complete this proof by contradiction?\n", "Answer": ["A", "B", "C"], "qid": "264", "question": "### Background\n\n**Research Question.** This problem examines the relationship between the solution to an infinite repeated game and the sequence of solutions to its finite-horizon counterparts under the nondiscriminating Optimistic Stable Standard of Behavior (OSSB) concept.\n\n**Setting / Institutional Environment.** The setting compares the nondiscriminating OSSB `σ` for an infinite Repeated Extensive Form (REF) game, `G^∞(δ)`, with the sequence of unique OSSBs `{σ_t}` for the corresponding finite `t`-period games, `G^t(δ)`.\n\n**Variables & Parameters.**\n- `G^∞(δ)`: The infinite REF game.\n- `G^t(δ)`: The finite `t`-period REF game.\n- `σ`: The nondiscriminating OSSB for `G^∞(δ)`.\n- `σ_t`: The unique OSSB for `G^t(δ)`.\n- `x`: An infinite path in `G^∞(δ)` starting at the root `v*`.\n- `x^t`: The truncation of path `x` to its first `t` stages.\n- `U^m(·)`: Player `m`'s utility function, which is continuous at infinity.\n\n---\n\n### Data / Model Specification\n\nProposition 4.6 provides a characterization of the paths in a nondiscriminating OSSB for an infinite game:\n\n  \n\\text{A path } x \\in \\sigma(v^*) \\iff x^t \\in \\sigma_t(v^*) \\text{ for all } t \\ge 1\n \n(Eq. (1))\n\nThe proof of the `(⇐)` direction proceeds by contradiction, assuming a path `x` satisfies the right-hand side condition but is not in `σ(v*)`.\n\n---\n\n### Question\n\nConsider the logic used to prove the `(⇐)` direction of Proposition 4.6. The proof assumes a path `x` satisfies `x^t ∈ σ_t(v*)` for all `t`, but is NOT in `σ(v*)`. This implies `x` is dominated in the infinite game by some path `y ∈ σ(w)`. Which of the following statements correctly describe valid steps or concepts used to complete this proof by contradiction?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 265, "Question": "### Background\n\n**Research Question.** This problem investigates the formal definition and behavioral foundations of the Optimistic Stable Standard of Behavior (OSSB) solution concept.\n\n**Setting / Institutional Environment.** The setting is a general `n`-person extensive form game of perfect information, represented as a game tree `G`.\n\n**Variables & Parameters.**\n- `v`: A node in the game tree `V(G)`.\n- `Π(v)`: The set of all continuation paths starting at node `v`.\n- `σ(v)`: A Standard of Behavior (SB), which is a subset of `Π(v)` representing the set of \"recommended\" paths at node `v`.\n- `Δ(σ, v)`: The Optimistic Dominion of `v` relative to `σ`. This is the set of paths `x ∈ Π(v)` that can be profitably rejected by some player `i` who can deviate to a new node `w` where there exists a recommended path `y ∈ σ(w)` that gives `i` a strictly higher payoff.\n\n---\n\n### Data / Model Specification\n\nA Standard of Behavior `σ` is an **Optimistic Stable Standard of Behavior (OSSB)** if it is both internally and externally stable for all nodes `v`:\n\n  \n\\text{Internal Stability: } \\sigma(v) \\cap \\Delta(\\sigma,v) = \\emptyset\n \n(Eq. (1))\n\n  \n\\text{External Stability: } \\Pi(v) \\setminus \\sigma(v) \\subseteq \\Delta(\\sigma,v)\n \n(Eq. (2))\n\nThese two conditions together imply `σ(v) = Π(v) \\setminus Δ(σ,v)`.\n\n---\n\n### Question\n\nBased on the definitions of Optimistic Internal and External Stability, which of the following statements are valid interpretations or consequences of the OSSB framework?\n", "Answer": ["A", "B", "D"], "qid": "265", "question": "### Background\n\n**Research Question.** This problem investigates the formal definition and behavioral foundations of the Optimistic Stable Standard of Behavior (OSSB) solution concept.\n\n**Setting / Institutional Environment.** The setting is a general `n`-person extensive form game of perfect information, represented as a game tree `G`.\n\n**Variables & Parameters.**\n- `v`: A node in the game tree `V(G)`.\n- `Π(v)`: The set of all continuation paths starting at node `v`.\n- `σ(v)`: A Standard of Behavior (SB), which is a subset of `Π(v)` representing the set of \"recommended\" paths at node `v`.\n- `Δ(σ, v)`: The Optimistic Dominion of `v` relative to `σ`. This is the set of paths `x ∈ Π(v)` that can be profitably rejected by some player `i` who can deviate to a new node `w` where there exists a recommended path `y ∈ σ(w)` that gives `i` a strictly higher payoff.\n\n---\n\n### Data / Model Specification\n\nA Standard of Behavior `σ` is an **Optimistic Stable Standard of Behavior (OSSB)** if it is both internally and externally stable for all nodes `v`:\n\n  \n\\text{Internal Stability: } \\sigma(v) \\cap \\Delta(\\sigma,v) = \\emptyset\n \n(Eq. (1))\n\n  \n\\text{External Stability: } \\Pi(v) \\setminus \\sigma(v) \\subseteq \\Delta(\\sigma,v)\n \n(Eq. (2))\n\nThese two conditions together imply `σ(v) = Π(v) \\setminus Δ(σ,v)`.\n\n---\n\n### Question\n\nBased on the definitions of Optimistic Internal and External Stability, which of the following statements are valid interpretations or consequences of the OSSB framework?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 266, "Question": "### Background\n\n**Research Question.** This problem asks you to formalize the practical guidelines for choosing between Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) when estimating a dynamic model with AR(1) errors, based on the findings of a Monte Carlo study.\n\n**Setting / Institutional Environment.** An applied econometrician is estimating a dynamic model and must choose an estimator. The choice depends on observable features of the data and estimated parameters: whether the exogenous variable `x_t` is trended, the sign of the error autocorrelation `ρ`, the degree of persistence `λ`, and the sample size `T`.\n\n**Variables & Parameters.**\n- `y_{t}`: The dependent variable at time `t`.\n- `x_{t}`: A fixed exogenous variable, which can be trended or non-trended.\n- `y_{t-1}`: The lagged dependent variable.\n- `u_{t}`: The AR(1) disturbance term.\n- `λ`: The autoregressive coefficient of the dependent variable (`|λ|<1`).\n- `ρ`: The first-order autocorrelation coefficient of the disturbances (`|ρ|<1`).\n\n---\n\n### Data / Model Specification\n\nThe data generating process is:\n\n  \ny_{t} = \\alpha + \\beta x_{t} + \\lambda y_{t-1} + u_{t} \\quad \\text{(Eq. (1))}\n \n\n  \nu_{t} = \\rho u_{t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the theoretical properties of the estimators for the model specified in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "Answer": ["A", "D"], "qid": "266", "question": "### Background\n\n**Research Question.** This problem asks you to formalize the practical guidelines for choosing between Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) when estimating a dynamic model with AR(1) errors, based on the findings of a Monte Carlo study.\n\n**Setting / Institutional Environment.** An applied econometrician is estimating a dynamic model and must choose an estimator. The choice depends on observable features of the data and estimated parameters: whether the exogenous variable `x_t` is trended, the sign of the error autocorrelation `ρ`, the degree of persistence `λ`, and the sample size `T`.\n\n**Variables & Parameters.**\n- `y_{t}`: The dependent variable at time `t`.\n- `x_{t}`: A fixed exogenous variable, which can be trended or non-trended.\n- `y_{t-1}`: The lagged dependent variable.\n- `u_{t}`: The AR(1) disturbance term.\n- `λ`: The autoregressive coefficient of the dependent variable (`|λ|<1`).\n- `ρ`: The first-order autocorrelation coefficient of the disturbances (`|ρ|<1`).\n\n---\n\n### Data / Model Specification\n\nThe data generating process is:\n\n  \ny_{t} = \\alpha + \\beta x_{t} + \\lambda y_{t-1} + u_{t} \\quad \\text{(Eq. (1))}\n \n\n  \nu_{t} = \\rho u_{t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the theoretical properties of the estimators for the model specified in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 267, "Question": "### Background\n\n**Research Question.** This problem asks you to critique the practical guidelines for choosing between Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) when estimating a dynamic model with AR(1) errors, based on the findings of a Monte Carlo study.\n\n**Setting / Institutional Environment.** An applied econometrician is estimating a dynamic model and must choose an estimator. The choice depends on observable features of the data and estimated parameters: whether the exogenous variable `x_t` is trended, the sign of the error autocorrelation `ρ`, the degree of persistence `λ`, and the sample size `T`.\n\n**Variables & Parameters.**\n- `y_{t}`: The dependent variable at time `t`.\n- `x_{t}`: A fixed exogenous variable, which can be trended or non-trended.\n- `y_{t-1}`: The lagged dependent variable.\n- `u_{t}`: The AR(1) disturbance term.\n- `λ`: The autoregressive coefficient of the dependent variable (`|λ|<1`).\n- `ρ`: The first-order autocorrelation coefficient of the disturbances (`|ρ|<1`).\n\n---\n\n### Data / Model Specification\n\nThe data generating process is:\n\n  \ny_{t} = \\alpha + \\beta x_{t} + \\lambda y_{t-1} + u_{t} \\quad \\text{(Eq. (1))}\n \n\n  \nu_{t} = \\rho u_{t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\nThe paper suggests a practical two-step procedure: first, run OLS to get estimates of λ and ρ, and second, use these estimates to decide whether to use OLS or a more sophisticated estimator (like GLS) for the final analysis.\n\n---\n\n### Question\n\nWhich of the following statements INCORRECTLY describe the statistical properties of, or valid inferential approaches for, this two-step pre-testing procedure?", "Answer": ["A", "B"], "qid": "267", "question": "### Background\n\n**Research Question.** This problem asks you to critique the practical guidelines for choosing between Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) when estimating a dynamic model with AR(1) errors, based on the findings of a Monte Carlo study.\n\n**Setting / Institutional Environment.** An applied econometrician is estimating a dynamic model and must choose an estimator. The choice depends on observable features of the data and estimated parameters: whether the exogenous variable `x_t` is trended, the sign of the error autocorrelation `ρ`, the degree of persistence `λ`, and the sample size `T`.\n\n**Variables & Parameters.**\n- `y_{t}`: The dependent variable at time `t`.\n- `x_{t}`: A fixed exogenous variable, which can be trended or non-trended.\n- `y_{t-1}`: The lagged dependent variable.\n- `u_{t}`: The AR(1) disturbance term.\n- `λ`: The autoregressive coefficient of the dependent variable (`|λ|<1`).\n- `ρ`: The first-order autocorrelation coefficient of the disturbances (`|ρ|<1`).\n\n---\n\n### Data / Model Specification\n\nThe data generating process is:\n\n  \ny_{t} = \\alpha + \\beta x_{t} + \\lambda y_{t-1} + u_{t} \\quad \\text{(Eq. (1))}\n \n\n  \nu_{t} = \\rho u_{t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\nThe paper suggests a practical two-step procedure: first, run OLS to get estimates of λ and ρ, and second, use these estimates to decide whether to use OLS or a more sophisticated estimator (like GLS) for the final analysis.\n\n---\n\n### Question\n\nWhich of the following statements INCORRECTLY describe the statistical properties of, or valid inferential approaches for, this two-step pre-testing procedure?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 268, "Question": "### Background\n\nAn agent's preferences are defined over a set of decision problems, `Z`. Each decision problem `z ∈ Z` is a compact set of alternatives. An alternative is a triplet `(c, d, x)`, specifying current non-drug consumption `c`, current drug consumption `d`, and the continuation decision problem `x`. The agent's preferences are indexed by a state `s ∈ [0,1]`, representing their drug consumption in the last period.\n\n### Data / Model Specification\n\nThe agent's actual choice is an element of the set `D(s,z)`:\n  \n\\mathcal{D}(s,z) := \\{\\mu \\in z \\mid E_{\\mu}[u+\\sigma(s)\\upsilon+\\delta W] \\geq E_{\\nu}[u+\\sigma(s)\\upsilon+\\delta W], \\forall\\nu \\in z\\}\n \nThe commitment-optimal choices (those that maximize long-term welfare without temptation) are in the set `C(z)`:\n  \n\\mathcal{C}(z) := \\{\\mu \\in z \\mid E_{\\mu}[u+\\delta W] \\geq E_{\\nu}[u+\\delta W], \\forall\\nu \\in z\\}\n \n\n### Question\n\nAn agent's choices are governed by the model of harmful addiction. Which of the following statements accurately describe the core concepts of the model? (Select all that apply)", "Answer": ["A", "B"], "qid": "268", "question": "### Background\n\nAn agent's preferences are defined over a set of decision problems, `Z`. Each decision problem `z ∈ Z` is a compact set of alternatives. An alternative is a triplet `(c, d, x)`, specifying current non-drug consumption `c`, current drug consumption `d`, and the continuation decision problem `x`. The agent's preferences are indexed by a state `s ∈ [0,1]`, representing their drug consumption in the last period.\n\n### Data / Model Specification\n\nThe agent's actual choice is an element of the set `D(s,z)`:\n  \n\\mathcal{D}(s,z) := \\{\\mu \\in z \\mid E_{\\mu}[u+\\sigma(s)\\upsilon+\\delta W] \\geq E_{\\nu}[u+\\sigma(s)\\upsilon+\\delta W], \\forall\\nu \\in z\\}\n \nThe commitment-optimal choices (those that maximize long-term welfare without temptation) are in the set `C(z)`:\n  \n\\mathcal{C}(z) := \\{\\mu \\in z \\mid E_{\\mu}[u+\\delta W] \\geq E_{\\nu}[u+\\delta W], \\forall\\nu \\in z\\}\n \n\n### Question\n\nAn agent's choices are governed by the model of harmful addiction. Which of the following statements accurately describe the core concepts of the model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn agent's preferences are defined...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn agent's preferences are defined...", "error": null}}
{"ID": 269, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical conflict between two models of decision-making that account for between-prospect comparisons: Regret Aversion (RA) and Probability Dominance (PD). The goal is to understand their formal definitions and how they can generate opposing predictions in a specifically constructed choice scenario.\n\n**Setting.** A decision-maker (DM) chooses between two prospects, A and B, with outcomes contingent on six equally likely events.\n\n### Data / Model Specification\n\n**Regret Theory:** A DM prefers Prospect A to B if the following condition holds:\n  \n\\sum_{i=1}^{n}p_{i}Q(u(x_{i}^{\\mathrm{A}})-u(x_{i}^{\\mathrm{B}})) \\geq 0 \\quad \\text{(Eq. 1)}\n \nwhere `u(·)` is a utility function (assume `u(x)=x` for simplicity), `pᵢ` is the probability of event `i`, and `Q(·)` is a regret function. The `Q` function is strictly increasing and antisymmetric, meaning `Q(-z) = -Q(z)`.\n\n**Regret Aversion (RA):** A DM is regret-averse if their `Q` function is convex. Convexity of `Q` implies that for any `c > b > a`:\n  \nQ(c-a) > Q(c-b) + Q(b-a) \\quad \\text{(Eq. 2)}\n \nThis property implies that one large positive utility difference is valued more than several smaller differences that sum to the same amount.\n\n**Probability Dominance (PD):** Prospect B dominates Prospect A by PD if and only if:\n  \n\\operatorname*{Pr}(x_{\\mathrm{B}}>x_{\\mathrm{A}}) > \\operatorname*{Pr}(x_{\\mathrm{A}}>x_{\\mathrm{B}}) \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Prospects for Task 5**\n\n| Event | Probability | Prospect A (€) | Prospect B (€) |\n|:-----:|:-----------:|:--------------:|:--------------:|\n| 1     | 1/6         | 5              | 6              |\n| 2     | 1/6         | 12             | 14             |\n| 3     | 1/6         | 14             | 3              |\n| 4     | 1/6         | 9              | 12             |\n| 5     | 1/6         | 3              | 5              |\n| 6     | 1/6         | 6              | 9              |\n\n*Note: The univariate distributions of outcomes for A and B are identical: {3, 5, 6, 9, 12, 14}.*\n\nBased on the provided definitions and data for Task 5, select all statements that are correct.", "Answer": ["A", "B"], "qid": "269", "question": "### Background\n\n**Research Question.** This problem explores the theoretical conflict between two models of decision-making that account for between-prospect comparisons: Regret Aversion (RA) and Probability Dominance (PD). The goal is to understand their formal definitions and how they can generate opposing predictions in a specifically constructed choice scenario.\n\n**Setting.** A decision-maker (DM) chooses between two prospects, A and B, with outcomes contingent on six equally likely events.\n\n### Data / Model Specification\n\n**Regret Theory:** A DM prefers Prospect A to B if the following condition holds:\n  \n\\sum_{i=1}^{n}p_{i}Q(u(x_{i}^{\\mathrm{A}})-u(x_{i}^{\\mathrm{B}})) \\geq 0 \\quad \\text{(Eq. 1)}\n \nwhere `u(·)` is a utility function (assume `u(x)=x` for simplicity), `pᵢ` is the probability of event `i`, and `Q(·)` is a regret function. The `Q` function is strictly increasing and antisymmetric, meaning `Q(-z) = -Q(z)`.\n\n**Regret Aversion (RA):** A DM is regret-averse if their `Q` function is convex. Convexity of `Q` implies that for any `c > b > a`:\n  \nQ(c-a) > Q(c-b) + Q(b-a) \\quad \\text{(Eq. 2)}\n \nThis property implies that one large positive utility difference is valued more than several smaller differences that sum to the same amount.\n\n**Probability Dominance (PD):** Prospect B dominates Prospect A by PD if and only if:\n  \n\\operatorname*{Pr}(x_{\\mathrm{B}}>x_{\\mathrm{A}}) > \\operatorname*{Pr}(x_{\\mathrm{A}}>x_{\\mathrm{B}}) \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Prospects for Task 5**\n\n| Event | Probability | Prospect A (€) | Prospect B (€) |\n|:-----:|:-----------:|:--------------:|:--------------:|\n| 1     | 1/6         | 5              | 6              |\n| 2     | 1/6         | 12             | 14             |\n| 3     | 1/6         | 14             | 3              |\n| 4     | 1/6         | 9              | 12             |\n| 5     | 1/6         | 3              | 5              |\n| 6     | 1/6         | 6              | 9              |\n\n*Note: The univariate distributions of outcomes for A and B are identical: {3, 5, 6, 9, 12, 14}.*\n\nBased on the provided definitions and data for Task 5, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 270, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical conflict between two models of decision-making that account for between-prospect comparisons: Regret Aversion (RA) and Probability Dominance (PD). The goal is to understand their formal definitions and how they can generate opposing predictions in a specifically constructed choice scenario.\n\n**Setting.** A decision-maker (DM) chooses between two prospects, A and B, with outcomes contingent on six equally likely events.\n\n### Data / Model Specification\n\n**Regret Theory:** A DM prefers Prospect A to B if the following condition holds:\n  \n\\sum_{i=1}^{n}p_{i}Q(u(x_{i}^{\\mathrm{A}})-u(x_{i}^{\\mathrm{B}})) \\geq 0 \\quad \\text{(Eq. 1)}\n \nwhere `u(·)` is a utility function (assume `u(x)=x` for simplicity), `pᵢ` is the probability of event `i`, and `Q(·)` is a regret function. The `Q` function is strictly increasing and antisymmetric, meaning `Q(-z) = -Q(z)`.\n\n**Regret Aversion (RA):** A DM is regret-averse if their `Q` function is convex. Convexity of `Q` implies that for any `c > b > a`:\n  \nQ(c-a) > Q(c-b) + Q(b-a) \\quad \\text{(Eq. 2)}\n \nThis property implies that one large positive utility difference is valued more than several smaller differences that sum to the same amount.\n\n**Probability Dominance (PD):** Prospect B dominates Prospect A by PD if and only if:\n  \n\\operatorname*{Pr}(x_{\\mathrm{B}}>x_{\\mathrm{A}}) > \\operatorname*{Pr}(x_{\\mathrm{A}}>x_{\\mathrm{B}}) \\quad \\text{(Eq. 3)}\n \n\nAccording to the paper's definitions, select all statements that correctly describe the formal properties and psychological intuition of Regret Aversion (RA).", "Answer": ["A", "B"], "qid": "270", "question": "### Background\n\n**Research Question.** This problem explores the theoretical conflict between two models of decision-making that account for between-prospect comparisons: Regret Aversion (RA) and Probability Dominance (PD). The goal is to understand their formal definitions and how they can generate opposing predictions in a specifically constructed choice scenario.\n\n**Setting.** A decision-maker (DM) chooses between two prospects, A and B, with outcomes contingent on six equally likely events.\n\n### Data / Model Specification\n\n**Regret Theory:** A DM prefers Prospect A to B if the following condition holds:\n  \n\\sum_{i=1}^{n}p_{i}Q(u(x_{i}^{\\mathrm{A}})-u(x_{i}^{\\mathrm{B}})) \\geq 0 \\quad \\text{(Eq. 1)}\n \nwhere `u(·)` is a utility function (assume `u(x)=x` for simplicity), `pᵢ` is the probability of event `i`, and `Q(·)` is a regret function. The `Q` function is strictly increasing and antisymmetric, meaning `Q(-z) = -Q(z)`.\n\n**Regret Aversion (RA):** A DM is regret-averse if their `Q` function is convex. Convexity of `Q` implies that for any `c > b > a`:\n  \nQ(c-a) > Q(c-b) + Q(b-a) \\quad \\text{(Eq. 2)}\n \nThis property implies that one large positive utility difference is valued more than several smaller differences that sum to the same amount.\n\n**Probability Dominance (PD):** Prospect B dominates Prospect A by PD if and only if:\n  \n\\operatorname*{Pr}(x_{\\mathrm{B}}>x_{\\mathrm{A}}) > \\operatorname*{Pr}(x_{\\mathrm{A}}>x_{\\mathrm{B}}) \\quad \\text{(Eq. 3)}\n \n\nAccording to the paper's definitions, select all statements that correctly describe the formal properties and psychological intuition of Regret Aversion (RA).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 271, "Question": "### Background\n\n**Research Question.** This problem analyzes the relative importance of government wage policies versus market forces in determining nominal wage growth in Brazilian manufacturing during a period of high inflation and labor market flexibility (1966-1976).\n\n**Setting / Institutional Environment.** The study uses a time-series regression model on monthly data for the textile and rubber industries. The key policy variable is a government-mandated collective wage adjustment. The author notes that unmeasured seasonality in economic activity could be a source of noise or bias, and that simultaneity between wage changes and the labor demand proxy is a concern.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in nominal mean wage.\n- `COLWGADi`: Percent collective wage adjustment (coefficient `B1`).\n- `LGCLWGAi`: One-month lagged `COLWGADi` (coefficient `B2`).\n- `DEMANDi`: Percent change in nominal value of production per worker.\n- `CONINFLi`: Monthly consumer price inflation.\n\n---\n\n### Data / Model Specification\n\nThe wage determination model is specified as:\n  \nWi = B0 + B1*COLWGADi + B2*LGCLWGAi + B5*DEMANDi + B6*CONINFLi + ... + ei\n \nThe model is estimated using OLS with a twelfth-order autocorrelation correction to account for seasonality. Key results are summarized in Table 1.\n\n**Table 1: Selected Regression Results for `Wi` (Autocorrelation Corrected)**\n| Variable | Textile Industry | Rubber Industry (Sample 1) |\n| :--- | :--- | :--- |\n| `COLWGADi` (B1) | 0.36*** | 0.26*** |\n| `LGCLWGAi` (B2) | not significant | 0.10** |\n| `DEMANDi` | 0.148*** | 0.277*** |\n| `CONINFLi` | 0.238** | 0.655*** |\n\n*Notes: *** p<0.01, ** p<0.05. \"not significant\" implies p>0.10.*\n\nThe text states that the null hypothesis `H0: B1 + B2 = 1` was rejected for both the textile and first rubber samples at the 1% significance level.\n\n---\n\nBased on the provided model and results, select all of the following statements that are valid interpretations or conclusions.\n", "Answer": ["B", "C"], "qid": "271", "question": "### Background\n\n**Research Question.** This problem analyzes the relative importance of government wage policies versus market forces in determining nominal wage growth in Brazilian manufacturing during a period of high inflation and labor market flexibility (1966-1976).\n\n**Setting / Institutional Environment.** The study uses a time-series regression model on monthly data for the textile and rubber industries. The key policy variable is a government-mandated collective wage adjustment. The author notes that unmeasured seasonality in economic activity could be a source of noise or bias, and that simultaneity between wage changes and the labor demand proxy is a concern.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in nominal mean wage.\n- `COLWGADi`: Percent collective wage adjustment (coefficient `B1`).\n- `LGCLWGAi`: One-month lagged `COLWGADi` (coefficient `B2`).\n- `DEMANDi`: Percent change in nominal value of production per worker.\n- `CONINFLi`: Monthly consumer price inflation.\n\n---\n\n### Data / Model Specification\n\nThe wage determination model is specified as:\n  \nWi = B0 + B1*COLWGADi + B2*LGCLWGAi + B5*DEMANDi + B6*CONINFLi + ... + ei\n \nThe model is estimated using OLS with a twelfth-order autocorrelation correction to account for seasonality. Key results are summarized in Table 1.\n\n**Table 1: Selected Regression Results for `Wi` (Autocorrelation Corrected)**\n| Variable | Textile Industry | Rubber Industry (Sample 1) |\n| :--- | :--- | :--- |\n| `COLWGADi` (B1) | 0.36*** | 0.26*** |\n| `LGCLWGAi` (B2) | not significant | 0.10** |\n| `DEMANDi` | 0.148*** | 0.277*** |\n| `CONINFLi` | 0.238** | 0.655*** |\n\n*Notes: *** p<0.01, ** p<0.05. \"not significant\" implies p>0.10.*\n\nThe text states that the null hypothesis `H0: B1 + B2 = 1` was rejected for both the textile and first rubber samples at the 1% significance level.\n\n---\n\nBased on the provided model and results, select all of the following statements that are valid interpretations or conclusions.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 272, "Question": "### Background\n\n**Research Question.** This problem concerns the correct measurement of the total, multi-period impact of a policy when its effects are distributed over time.\n\n**Setting / Institutional Environment.** A government wage policy mandates a collective wage adjustment in a specific month (`t`). The effect of this policy on average nominal wages is captured by a regression model using monthly data. The model allows for both an immediate impact in month `t` and a lagged impact in the following month, `t+1`.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in the nominal mean wage.\n- `B1`: Impact elasticity of wages with respect to the current wage adjustment.\n- `B2`: Impact elasticity of wages with respect to the one-month lagged wage adjustment.\n- `Ep`: The two-month joint impact elasticity.\n\n---\n\n### Data / Model Specification\n\nThe total two-month joint impact elasticity is given by the compounding formula:\n  \nEp = (1 + B1)(1 + B2) - 1 \n \nThe author notes that a test of the linear sum `B1 + B2 = 1` is used as an approximation for a test of `Ep = 1`.\n\n---\n\nConsidering the relationship between the true compounded elasticity `Ep` and its linear approximation `B1 + B2`, select all of the following statements that are mathematically or economically correct.\n", "Answer": ["A", "B", "C"], "qid": "272", "question": "### Background\n\n**Research Question.** This problem concerns the correct measurement of the total, multi-period impact of a policy when its effects are distributed over time.\n\n**Setting / Institutional Environment.** A government wage policy mandates a collective wage adjustment in a specific month (`t`). The effect of this policy on average nominal wages is captured by a regression model using monthly data. The model allows for both an immediate impact in month `t` and a lagged impact in the following month, `t+1`.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in the nominal mean wage.\n- `B1`: Impact elasticity of wages with respect to the current wage adjustment.\n- `B2`: Impact elasticity of wages with respect to the one-month lagged wage adjustment.\n- `Ep`: The two-month joint impact elasticity.\n\n---\n\n### Data / Model Specification\n\nThe total two-month joint impact elasticity is given by the compounding formula:\n  \nEp = (1 + B1)(1 + B2) - 1 \n \nThe author notes that a test of the linear sum `B1 + B2 = 1` is used as an approximation for a test of `Ep = 1`.\n\n---\n\nConsidering the relationship between the true compounded elasticity `Ep` and its linear approximation `B1 + B2`, select all of the following statements that are mathematically or economically correct.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 273, "Question": "### Background\n\n**Research Question.** This problem analyzes the relative importance of government wage policies versus market forces in determining nominal wage growth in Brazilian manufacturing during a period of high inflation and labor market flexibility (1966-1976).\n\n**Setting / Institutional Environment.** The study uses a time-series regression model on monthly data. The author is concerned about unmeasured seasonality in the monthly data (e.g., work intensity changes around Carnival) and potential simultaneity between wage changes and the labor demand proxy.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in nominal mean wage.\n- `DEMANDi`: Percent change in nominal value of production per worker.\n- `ei`: The error term of the regression model.\n\n---\n\n### Data / Model Specification\n\nThe author uses a twelfth-order autocorrelation correction to address seasonality, which assumes the error in a given month is correlated with the error from the same month in the previous year. However, the author also notes that the estimated coefficient on `DEMANDi` may suffer from simultaneity bias, as causality could run from wages to prices (part of `DEMANDi`), or both could be jointly determined.\n\n---\n\nWhich of the following statements accurately describe the econometric challenges and procedures in this study? Select all that apply.\n", "Answer": ["A", "C", "D"], "qid": "273", "question": "### Background\n\n**Research Question.** This problem analyzes the relative importance of government wage policies versus market forces in determining nominal wage growth in Brazilian manufacturing during a period of high inflation and labor market flexibility (1966-1976).\n\n**Setting / Institutional Environment.** The study uses a time-series regression model on monthly data. The author is concerned about unmeasured seasonality in the monthly data (e.g., work intensity changes around Carnival) and potential simultaneity between wage changes and the labor demand proxy.\n\n**Variables & Parameters.**\n- `Wi`: Month-to-month percent change in nominal mean wage.\n- `DEMANDi`: Percent change in nominal value of production per worker.\n- `ei`: The error term of the regression model.\n\n---\n\n### Data / Model Specification\n\nThe author uses a twelfth-order autocorrelation correction to address seasonality, which assumes the error in a given month is correlated with the error from the same month in the previous year. However, the author also notes that the estimated coefficient on `DEMANDi` may suffer from simultaneity bias, as causality could run from wages to prices (part of `DEMANDi`), or both could be jointly determined.\n\n---\n\nWhich of the following statements accurately describe the econometric challenges and procedures in this study? Select all that apply.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 274, "Question": "### Background\n\n**Setting / Institutional Environment.** A regulator compares two audit mechanisms: a Random Audit (RA) mechanism and a competitive Tournament (T) mechanism. In the RA mechanism, a firm's audit probability `p` is fixed. In the T mechanism, the audit probability is endogenous, as firms compete to avoid being audited. A key policy lever is the fixed cost of being audited, `γ`.\n\n**Theoretical Predictions.**\n- **Hypothesis 3:** Under random audits, a firm's optimal disclosure is invariant to the fixed audit cost `γ`.\n- **Hypothesis 4:** Under competitive endogenous audits, increasing the fixed audit cost `γ` increases disclosure.\n\n---\n\n### Data / Model Specification\n\nAn experiment was conducted to test these predictions. The table below shows selected results from a regression model where the dependent variable is the firm's disclosure rate.\n\n**Table 1: Selected Regression Results for Disclosure Rate**\n\n| Variable | Random | Tournament |\n| :--- | :--- | :--- |\n| High β | 0.298* | 0.490* |\n| | (0.060) | (0.073) |\n| High γ | 0.056 | 0.453* |\n| | (0.062) | (0.065) |\n| High p | 0.399* | 0.376* |\n| | (0.061) | (0.051) |\n| Constant | 0.004 | 0.208* |\n| | (0.060) | (0.069) |\n\n*Note: Cluster-robust standard errors in parentheses. * indicates significance at the 5% level.* \n\n---\n\nBased on the theoretical predictions and the empirical results in Table 1, which of the following statements are valid conclusions? Select all that apply.", "Answer": ["B", "C", "D"], "qid": "274", "question": "### Background\n\n**Setting / Institutional Environment.** A regulator compares two audit mechanisms: a Random Audit (RA) mechanism and a competitive Tournament (T) mechanism. In the RA mechanism, a firm's audit probability `p` is fixed. In the T mechanism, the audit probability is endogenous, as firms compete to avoid being audited. A key policy lever is the fixed cost of being audited, `γ`.\n\n**Theoretical Predictions.**\n- **Hypothesis 3:** Under random audits, a firm's optimal disclosure is invariant to the fixed audit cost `γ`.\n- **Hypothesis 4:** Under competitive endogenous audits, increasing the fixed audit cost `γ` increases disclosure.\n\n---\n\n### Data / Model Specification\n\nAn experiment was conducted to test these predictions. The table below shows selected results from a regression model where the dependent variable is the firm's disclosure rate.\n\n**Table 1: Selected Regression Results for Disclosure Rate**\n\n| Variable | Random | Tournament |\n| :--- | :--- | :--- |\n| High β | 0.298* | 0.490* |\n| | (0.060) | (0.073) |\n| High γ | 0.056 | 0.453* |\n| | (0.062) | (0.065) |\n| High p | 0.399* | 0.376* |\n| | (0.061) | (0.051) |\n| Constant | 0.004 | 0.208* |\n| | (0.060) | (0.069) |\n\n*Note: Cluster-robust standard errors in parentheses. * indicates significance at the 5% level.* \n\n---\n\nBased on the theoretical predictions and the empirical results in Table 1, which of the following statements are valid conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "error": null}}
{"ID": 275, "Question": "### Background\n\n**Research Question.** This problem analyzes the systematic deviations between observed behavior in a compliance experiment and the predictions of the rational-agent Nash equilibrium (NE) model.\n\n**Setting / Institutional Environment.** Experimental data on disclosure rates are compared against the theoretical NE predictions for various treatments. A consistent pattern of deviation emerges across different audit mechanisms and parameter settings.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Nash Equilibrium and Mean Disclosure Rate by Treatment**\n\n| Treatment | NE (Tourn./GRE) | Mean (Tournament) |\n| :--- | :--- | :--- |\n| 1: Low β,γ,p | 0.19 | 0.208 |\n| 2: Low β,γ; High p | 0.51 | 0.583 |\n| 4: High β; Low γ,p | 1.03 | 0.698* |\n| 6: High β,p; Low γ | 1.30 | 1.073* |\n| 8: High β,γ,p | 1.70 | 1.527* |\n\n*Note: Abridged table. * indicates the mean is statistically different from the NE at the 5% level.* \n\n---\n\nBased on the data in Table 1 and behavioral economic theory, which of the following statements accurately describe the observed patterns or provide plausible explanations? Select all that apply.", "Answer": ["A", "B", "C"], "qid": "275", "question": "### Background\n\n**Research Question.** This problem analyzes the systematic deviations between observed behavior in a compliance experiment and the predictions of the rational-agent Nash equilibrium (NE) model.\n\n**Setting / Institutional Environment.** Experimental data on disclosure rates are compared against the theoretical NE predictions for various treatments. A consistent pattern of deviation emerges across different audit mechanisms and parameter settings.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Nash Equilibrium and Mean Disclosure Rate by Treatment**\n\n| Treatment | NE (Tourn./GRE) | Mean (Tournament) |\n| :--- | :--- | :--- |\n| 1: Low β,γ,p | 0.19 | 0.208 |\n| 2: Low β,γ; High p | 0.51 | 0.583 |\n| 4: High β; Low γ,p | 1.03 | 0.698* |\n| 6: High β,p; Low γ | 1.30 | 1.073* |\n| 8: High β,γ,p | 1.70 | 1.527* |\n\n*Note: Abridged table. * indicates the mean is statistically different from the NE at the 5% level.* \n\n---\n\nBased on the data in Table 1 and behavioral economic theory, which of the following statements accurately describe the observed patterns or provide plausible explanations? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 276, "Question": "### Background\n\n**Setting / Institutional Environment.** A regulator uses a rank-order tournament to select `k` firms from a peer group of `N` for audit. A firm's probability of being audited, `p_i`, is endogenous to its disclosure level `z_i` relative to its peers. The paper states that the effect of increasing the number of firms `N` (while holding `k` fixed) on the equilibrium disclosure level `z^T` is ambiguous.\n\n---\n\n### Data / Model Specification\n\nThe effect of an increase in `N` on `z^T` is determined by its effect on the marginal incentives to disclose. This total effect can be decomposed into two distinct economic forces:\n\n1.  **Probability Dilution Effect:** As `N` increases, the baseline audit probability `k/N` for any given firm decreases. This reduces the expected cost of non-compliance, weakening the incentive to disclose.\n2.  **Competition Intensity Effect:** A change in `N` may alter the marginal effectiveness of an additional unit of disclosure in reducing the audit probability, `∂p_i/∂z_i`. This effect's direction depends on the distribution of the regulator's estimation error, `g(ε)`.\n\n---\n\nBased on this framework, which of the following statements about the effect of increasing the peer group size `N` are correct? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "276", "question": "### Background\n\n**Setting / Institutional Environment.** A regulator uses a rank-order tournament to select `k` firms from a peer group of `N` for audit. A firm's probability of being audited, `p_i`, is endogenous to its disclosure level `z_i` relative to its peers. The paper states that the effect of increasing the number of firms `N` (while holding `k` fixed) on the equilibrium disclosure level `z^T` is ambiguous.\n\n---\n\n### Data / Model Specification\n\nThe effect of an increase in `N` on `z^T` is determined by its effect on the marginal incentives to disclose. This total effect can be decomposed into two distinct economic forces:\n\n1.  **Probability Dilution Effect:** As `N` increases, the baseline audit probability `k/N` for any given firm decreases. This reduces the expected cost of non-compliance, weakening the incentive to disclose.\n2.  **Competition Intensity Effect:** A change in `N` may alter the marginal effectiveness of an additional unit of disclosure in reducing the audit probability, `∂p_i/∂z_i`. This effect's direction depends on the distribution of the regulator's estimation error, `g(ε)`.\n\n---\n\nBased on this framework, which of the following statements about the effect of increasing the peer group size `N` are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "error": null}}
{"ID": 277, "Question": "### Background\n\n**Setting / Institutional Environment.** A firm's cost minimization problem in a tournament audit mechanism is given below. The firm chooses its disclosure level `z_i` to minimize expected costs. The probability of being audited, `p_i`, depends on `z_i` and the choices of rivals, `z_{-i}`.\n\n---\n\n### Data / Model Specification\n\nThe optimization problem for firm `i` is:\n  \n\\min_{z_{i}} \\alpha(e_{i}+z_{i})+p_i(z_{i},z_{-i})\\left\\{\\gamma+\\beta\\int_{z_{i}}^{b}(t-z_{i})f(t)d t\\right\\} \n\\quad \\text{(Eq. (1))}\n \nThe first-order condition (FOC) for this problem is:\n  \n\\alpha + \\frac{\\partial p_i}{\\partial z_i} \\left\\{\\gamma+\\beta\\int_{z_{i}}^{b}(t-z_{i})f(t)d t\\right\\} - p_i \\beta \\int_{z_{i}}^{b}f(t)d t = 0\n\\quad \\text{(Eq. (2))}\n \nwhere `α` is the marginal cost of disclosure, and the other two terms represent marginal benefits.\n\n---\n\nWhich of the following statements are valid interpretations of the components of the first-order condition in Eq. (2)? Select all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "277", "question": "### Background\n\n**Setting / Institutional Environment.** A firm's cost minimization problem in a tournament audit mechanism is given below. The firm chooses its disclosure level `z_i` to minimize expected costs. The probability of being audited, `p_i`, depends on `z_i` and the choices of rivals, `z_{-i}`.\n\n---\n\n### Data / Model Specification\n\nThe optimization problem for firm `i` is:\n  \n\\min_{z_{i}} \\alpha(e_{i}+z_{i})+p_i(z_{i},z_{-i})\\left\\{\\gamma+\\beta\\int_{z_{i}}^{b}(t-z_{i})f(t)d t\\right\\} \n\\quad \\text{(Eq. (1))}\n \nThe first-order condition (FOC) for this problem is:\n  \n\\alpha + \\frac{\\partial p_i}{\\partial z_i} \\left\\{\\gamma+\\beta\\int_{z_{i}}^{b}(t-z_{i})f(t)d t\\right\\} - p_i \\beta \\int_{z_{i}}^{b}f(t)d t = 0\n\\quad \\text{(Eq. (2))}\n \nwhere `α` is the marginal cost of disclosure, and the other two terms represent marginal benefits.\n\n---\n\nWhich of the following statements are valid interpretations of the components of the first-order condition in Eq. (2)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Setting / Institutional Environm...", "error": null}}
{"ID": 278, "Question": "### Background\n\nTo rationalize the empirical findings of state-dependent information rigidity and heterogeneous forecaster behavior, the paper develops a theoretical model of optimal information acquisition. Agents in the model choose how much costly private information to acquire to supplement a free but noisy public signal. This choice is driven by a trade-off between the cost of information and the benefit of forecast accuracy.\n\n### Data / Model Specification\n\nThe core of the model rests on several key components:\n\n1.  **Agent's Cost Function:** Each agent `i` seeks to minimize a cost function at time `t` by choosing the optimal number of private information units, `l`:\n    \n      \n    C_t(l) = β_i \\operatorname{tr}[M_t(l)] + αl \\quad \\text{(Eq. (1))}\n     \n    \n    where `β_i` is the agent's preference for accuracy, `M_t(l)` is the forecast Mean Squared Error (MSE) matrix which decreases in `l`, and `α` is the per-unit cost of private information.\n\n2.  **Disaster Shock:** A disaster shock is modeled as a large, temporary increase in the variance of the public noise, `Σ_t`.\n\n### Question\n\nBased on the model's framework, select all of the following statements that correctly describe the predicted behavior of agents in response to a large, temporary increase in public signal variance (`Σ_t`).", "Answer": ["B", "C"], "qid": "278", "question": "### Background\n\nTo rationalize the empirical findings of state-dependent information rigidity and heterogeneous forecaster behavior, the paper develops a theoretical model of optimal information acquisition. Agents in the model choose how much costly private information to acquire to supplement a free but noisy public signal. This choice is driven by a trade-off between the cost of information and the benefit of forecast accuracy.\n\n### Data / Model Specification\n\nThe core of the model rests on several key components:\n\n1.  **Agent's Cost Function:** Each agent `i` seeks to minimize a cost function at time `t` by choosing the optimal number of private information units, `l`:\n    \n      \n    C_t(l) = β_i \\operatorname{tr}[M_t(l)] + αl \\quad \\text{(Eq. (1))}\n     \n    \n    where `β_i` is the agent's preference for accuracy, `M_t(l)` is the forecast Mean Squared Error (MSE) matrix which decreases in `l`, and `α` is the per-unit cost of private information.\n\n2.  **Disaster Shock:** A disaster shock is modeled as a large, temporary increase in the variance of the public noise, `Σ_t`.\n\n### Question\n\nBased on the model's framework, select all of the following statements that correctly describe the predicted behavior of agents in response to a large, temporary increase in public signal variance (`Σ_t`).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo rationalize the empirical findi...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo rationalize the empirical findi...", "error": null}}
{"ID": 279, "Question": "### Background\n\nTo rationalize its empirical findings, the paper develops a theoretical model of optimal information acquisition. Agents use a Kalman filter to process a combination of public and private signals to forecast a true economic state `x_t`.\n\n### Data / Model Specification\n\nTo analyze aggregate forecast properties like dispersion, it is necessary to compute the covariance of prediction errors between any two agents, `j` and `k`. Let `Q_{t+1|t}^{(jk)} = \\operatorname{Cov}[x_{t+1} - \\hat{x}_{t+1|t}^{(j)}, x_{t+1} - \\hat{x}_{t+1|t}^{(k)}]`. The paper provides the following recursive formula:\n\n**Proposition 1:**\n\n  \nQ_{t+1|t}^{(jk)} = [Φ-K_t^{(j)}H^{(j)}]Q_{t|t-1}^{(jk)}[Φ-K_t^{(k)}H^{(k)}]' + Σ^ε + K_t^{(j)} \\begin{bmatrix} 0 & 0 \\\\ 0 & Σ_t \\end{bmatrix} K_t^{(k)'} \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ` is the state transition matrix, `K_t` is the Kalman gain, `H` is the observation matrix, `Σ^ε` is the variance of the true state innovation, and `Σ_t` is the variance of the common public noise.\n\n### Question\n\nProposition 1 (Eq. (1)) provides the formula for the covariance of prediction errors between two agents, `j` and `k`. Select all of the following statements that provide a correct economic interpretation of the components of this formula.", "Answer": ["A", "C"], "qid": "279", "question": "### Background\n\nTo rationalize its empirical findings, the paper develops a theoretical model of optimal information acquisition. Agents use a Kalman filter to process a combination of public and private signals to forecast a true economic state `x_t`.\n\n### Data / Model Specification\n\nTo analyze aggregate forecast properties like dispersion, it is necessary to compute the covariance of prediction errors between any two agents, `j` and `k`. Let `Q_{t+1|t}^{(jk)} = \\operatorname{Cov}[x_{t+1} - \\hat{x}_{t+1|t}^{(j)}, x_{t+1} - \\hat{x}_{t+1|t}^{(k)}]`. The paper provides the following recursive formula:\n\n**Proposition 1:**\n\n  \nQ_{t+1|t}^{(jk)} = [Φ-K_t^{(j)}H^{(j)}]Q_{t|t-1}^{(jk)}[Φ-K_t^{(k)}H^{(k)}]' + Σ^ε + K_t^{(j)} \\begin{bmatrix} 0 & 0 \\\\ 0 & Σ_t \\end{bmatrix} K_t^{(k)'} \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ` is the state transition matrix, `K_t` is the Kalman gain, `H` is the observation matrix, `Σ^ε` is the variance of the true state innovation, and `Σ_t` is the variance of the common public noise.\n\n### Question\n\nProposition 1 (Eq. (1)) provides the formula for the covariance of prediction errors between two agents, `j` and `k`. Select all of the following statements that provide a correct economic interpretation of the components of this formula.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo rationalize its empirical findi...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo rationalize its empirical findi...", "error": null}}
{"ID": 280, "Question": "### Background\n\n**Research Question.** This problem explores the formal microeconomic model underpinning the paper's central hypothesis: that the relationship between an individual's trust and their economic performance is hump-shaped.\n\n**Setting / Institutional Environment.** An investor with a fixed endowment is randomly matched with a partner who is either honest or a cheater. The investor chooses how much of their endowment to invest with the partner based on their personal, subjective belief about the partner's trustworthiness. The actual expected return, however, depends on the true proportion of honest partners in the population.\n\n**Variables & Parameters.**\n*   `E`: Investor's initial endowment.\n*   `S`: Amount of endowment invested, `S ≤ E`.\n*   `f(S)`: Production function for the surplus created by investment `S`. Assume `f(S) > S`, and `f(S)` is twice-differentiable and strictly concave (`f'(S) > 0`, `f''(S) < 0`).\n*   `γ`: Fraction of the surplus `γf(S)` that an honest partner returns, `0 < γ < 1`.\n*   `π`: True probability that a randomly matched partner is honest (true population trustworthiness), `π ∈ [0, 1]`.\n*   `τ`: Investor's subjective belief about the probability that a partner is honest (perceived trustworthiness), `τ ∈ [0, 1]`.\n\n---\n\n### Data / Model Specification\n\nAn investor chooses the amount to invest, `S`, to maximize their *perceived* expected utility, as shown in Eq. (1):\n\n  \n\\max_{0 \\le S \\le E} \\quad E - S + \\tau \\gamma f(S) \\quad \\text{(Eq. 1)}\n \n\nLet `S*(τ)` be the optimal investment level chosen by an individual with trust belief `τ`. The *true* expected income for this individual, `Y(τ)`, is determined by their choice `S*(τ)` and the true trustworthiness of the population, `π`, as shown in Eq. (2):\n\n  \nY(\\tau) = E - S^*(\\tau) + \\pi \\gamma f(S^*(\\tau)) \\quad \\text{(Eq. 2)}\n \n\n---\n\n### Question\n\nBased on the model, select all of the following statements that are mathematically correct.", "Answer": ["A", "B"], "qid": "280", "question": "### Background\n\n**Research Question.** This problem explores the formal microeconomic model underpinning the paper's central hypothesis: that the relationship between an individual's trust and their economic performance is hump-shaped.\n\n**Setting / Institutional Environment.** An investor with a fixed endowment is randomly matched with a partner who is either honest or a cheater. The investor chooses how much of their endowment to invest with the partner based on their personal, subjective belief about the partner's trustworthiness. The actual expected return, however, depends on the true proportion of honest partners in the population.\n\n**Variables & Parameters.**\n*   `E`: Investor's initial endowment.\n*   `S`: Amount of endowment invested, `S ≤ E`.\n*   `f(S)`: Production function for the surplus created by investment `S`. Assume `f(S) > S`, and `f(S)` is twice-differentiable and strictly concave (`f'(S) > 0`, `f''(S) < 0`).\n*   `γ`: Fraction of the surplus `γf(S)` that an honest partner returns, `0 < γ < 1`.\n*   `π`: True probability that a randomly matched partner is honest (true population trustworthiness), `π ∈ [0, 1]`.\n*   `τ`: Investor's subjective belief about the probability that a partner is honest (perceived trustworthiness), `τ ∈ [0, 1]`.\n\n---\n\n### Data / Model Specification\n\nAn investor chooses the amount to invest, `S`, to maximize their *perceived* expected utility, as shown in Eq. (1):\n\n  \n\\max_{0 \\le S \\le E} \\quad E - S + \\tau \\gamma f(S) \\quad \\text{(Eq. 1)}\n \n\nLet `S*(τ)` be the optimal investment level chosen by an individual with trust belief `τ`. The *true* expected income for this individual, `Y(τ)`, is determined by their choice `S*(τ)` and the true trustworthiness of the population, `π`, as shown in Eq. (2):\n\n  \nY(\\tau) = E - S^*(\\tau) + \\pi \\gamma f(S^*(\\tau)) \\quad \\text{(Eq. 2)}\n \n\n---\n\n### Question\n\nBased on the model, select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 281, "Question": "### Background\n\n**Research Question.** This problem explores the formal microeconomic model underpinning the paper's central hypothesis: that the relationship between an individual's trust and their economic performance is hump-shaped.\n\n**Setting / Institutional Environment.** An investor with a fixed endowment is randomly matched with a partner who is either honest or a cheater. The investor chooses how much of their endowment to invest with the partner based on their personal, subjective belief about the partner's trustworthiness. The actual expected return, however, depends on the true proportion of honest partners in the population.\n\n**Variables & Parameters.**\n*   `E`: Investor's initial endowment.\n*   `S`: Amount of endowment invested, `S ≤ E`.\n*   `f(S)`: Production function for the surplus created by investment `S`. Assume `f(S) > S`, and `f(S)` is twice-differentiable and strictly concave (`f'(S) > 0`, `f''(S) < 0`).\n*   `γ`: Fraction of the surplus `γf(S)` that an honest partner returns, `0 < γ < 1`.\n*   `π`: True probability that a randomly matched partner is honest (true population trustworthiness), `π ∈ [0, 1]`.\n*   `τ`: Investor's subjective belief about the probability that a partner is honest (perceived trustworthiness), `τ ∈ [0, 1]`.\n\n---\n\n### Data / Model Specification\n\nAn investor chooses the amount to invest, `S`, to maximize their *perceived* expected utility, as shown in Eq. (1):\n\n  \n\\max_{0 \\le S \\le E} \\quad E - S + \\tau \\gamma f(S) \\quad \\text{(Eq. 1)}\n \n\nLet `S*(τ)` be the optimal investment level chosen by an individual with trust belief `τ`. The *true* expected income for this individual, `Y(τ)`, is determined by their choice `S*(τ)` and the true trustworthiness of the population, `π`, as shown in Eq. (2):\n\n  \nY(\\tau) = E - S^*(\\tau) + \\pi \\gamma f(S^*(\\tau)) \\quad \\text{(Eq. 2)}\n \n\n---\n\n### Question\n\nBased on the model, select all of the following statements that correctly characterize the true expected income function `Y(τ)` and the optimal trust level `τ*`.", "Answer": ["A", "D"], "qid": "281", "question": "### Background\n\n**Research Question.** This problem explores the formal microeconomic model underpinning the paper's central hypothesis: that the relationship between an individual's trust and their economic performance is hump-shaped.\n\n**Setting / Institutional Environment.** An investor with a fixed endowment is randomly matched with a partner who is either honest or a cheater. The investor chooses how much of their endowment to invest with the partner based on their personal, subjective belief about the partner's trustworthiness. The actual expected return, however, depends on the true proportion of honest partners in the population.\n\n**Variables & Parameters.**\n*   `E`: Investor's initial endowment.\n*   `S`: Amount of endowment invested, `S ≤ E`.\n*   `f(S)`: Production function for the surplus created by investment `S`. Assume `f(S) > S`, and `f(S)` is twice-differentiable and strictly concave (`f'(S) > 0`, `f''(S) < 0`).\n*   `γ`: Fraction of the surplus `γf(S)` that an honest partner returns, `0 < γ < 1`.\n*   `π`: True probability that a randomly matched partner is honest (true population trustworthiness), `π ∈ [0, 1]`.\n*   `τ`: Investor's subjective belief about the probability that a partner is honest (perceived trustworthiness), `τ ∈ [0, 1]`.\n\n---\n\n### Data / Model Specification\n\nAn investor chooses the amount to invest, `S`, to maximize their *perceived* expected utility, as shown in Eq. (1):\n\n  \n\\max_{0 \\le S \\le E} \\quad E - S + \\tau \\gamma f(S) \\quad \\text{(Eq. 1)}\n \n\nLet `S*(τ)` be the optimal investment level chosen by an individual with trust belief `τ`. The *true* expected income for this individual, `Y(τ)`, is determined by their choice `S*(τ)` and the true trustworthiness of the population, `π`, as shown in Eq. (2):\n\n  \nY(\\tau) = E - S^*(\\tau) + \\pi \\gamma f(S^*(\\tau)) \\quad \\text{(Eq. 2)}\n \n\n---\n\n### Question\n\nBased on the model, select all of the following statements that correctly characterize the true expected income function `Y(τ)` and the optimal trust level `τ*`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 282, "Question": "### Background\n\n**Research Question.** This problem investigates the efficiency of Pseudo Maximum Likelihood Estimators (PMLEs) and the conditions under which the theoretical lower bound on their asymptotic variance can be achieved.\n\n**Setting.** We consider the class of all consistent PMLEs based on linear exponential families for estimating the parameters `θ₀` of a conditional mean function `f(x, θ₀)`. The true conditional variance is given by `Var(yₜ|xₜ) = Ω₀(xₜ)`.\n\n### Data / Model Specification\n\nThe asymptotic covariance matrix of a PMLE based on a linear exponential family is `V = J⁻¹IJ⁻¹`, where:\n  \nJ = E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Sigma_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg) \n \n  \nI = E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Sigma_{0}^{-1}\\Omega_{0}\\Sigma_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg) \n \nHere, `Ω₀` is the true conditional covariance matrix of the data, and `Σ₀` is the conditional covariance matrix implied by the chosen pseudo-likelihood. Property 5 states that for any choice of linear exponential family, `V` is bounded below by the efficiency bound `H`:\n  \n\\mathcal{H} = \\bigg[E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Omega_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg)\\bigg]^{-1} \\quad \\text{(Eq. (1))}\n \nThe Quasi-Generalized PMLE (QGPML) is a two-step procedure designed to achieve this bound by effectively setting `Σ₀ = Ω₀` in the second step.\n\n---\n\nBased on the provided framework, select all of the following statements that are mathematically correct consequences of the theory.\n", "Answer": ["A", "B", "C"], "qid": "282", "question": "### Background\n\n**Research Question.** This problem investigates the efficiency of Pseudo Maximum Likelihood Estimators (PMLEs) and the conditions under which the theoretical lower bound on their asymptotic variance can be achieved.\n\n**Setting.** We consider the class of all consistent PMLEs based on linear exponential families for estimating the parameters `θ₀` of a conditional mean function `f(x, θ₀)`. The true conditional variance is given by `Var(yₜ|xₜ) = Ω₀(xₜ)`.\n\n### Data / Model Specification\n\nThe asymptotic covariance matrix of a PMLE based on a linear exponential family is `V = J⁻¹IJ⁻¹`, where:\n  \nJ = E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Sigma_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg) \n \n  \nI = E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Sigma_{0}^{-1}\\Omega_{0}\\Sigma_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg) \n \nHere, `Ω₀` is the true conditional covariance matrix of the data, and `Σ₀` is the conditional covariance matrix implied by the chosen pseudo-likelihood. Property 5 states that for any choice of linear exponential family, `V` is bounded below by the efficiency bound `H`:\n  \n\\mathcal{H} = \\bigg[E_{x}\\bigg(\\frac{\\partial f}{\\partial\\theta}\\Omega_{0}^{-1}\\frac{\\partial f'}{\\partial\\theta}\\bigg)\\bigg]^{-1} \\quad \\text{(Eq. (1))}\n \nThe Quasi-Generalized PMLE (QGPML) is a two-step procedure designed to achieve this bound by effectively setting `Σ₀ = Ω₀` in the second step.\n\n---\n\nBased on the provided framework, select all of the following statements that are mathematically correct consequences of the theory.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 283, "Question": "### Background\n\n**Research Question.** This problem examines the proof of consistency for the Pseudo Maximum Likelihood Estimator (PMLE) when the chosen likelihood belongs to a linear exponential family.\n\n**Setting.** An econometrician estimates `θ₀` in the model `E[yₜ|xₜ] = f(xₜ, θ₀)` by maximizing a pseudo-log-likelihood `Σ log l(yₜ, f(xₜ, θ))`, where `l` is from a linear exponential family.\n\n### Data / Model Specification\n\nThe density of the chosen linear exponential family, parameterized by its mean `m`, is `l(u, m) = exp{A(m) + B(u) + C(m)u}`. Consistency of the PMLE `hat(θ)ₜ` requires showing that the limiting objective function `φ_∞(θ) = EₓE₀[log l(y, f(x, θ))]` is uniquely maximized at `θ = θ₀`.\n\nThe proof of Theorem 1 relies on two key elements:\n1.  **Property 4 (from Kullback's Inequality):** For any `m, m₀` in the parameter space, `A(m) + C(m)m₀ ≤ A(m₀) + C(m₀)m₀`, with equality holding if and only if `m = m₀`.\n2.  **First-Order Identification Assumption:** `f(x, θ₁) = f(x, θ₂)` for almost all `x` implies `θ₁ = θ₂`.\n\n---\n\nBased on the provided theory, which of the following statements are valid steps or conclusions in the proof of consistency for the PMLE?\n", "Answer": ["B", "C", "D"], "qid": "283", "question": "### Background\n\n**Research Question.** This problem examines the proof of consistency for the Pseudo Maximum Likelihood Estimator (PMLE) when the chosen likelihood belongs to a linear exponential family.\n\n**Setting.** An econometrician estimates `θ₀` in the model `E[yₜ|xₜ] = f(xₜ, θ₀)` by maximizing a pseudo-log-likelihood `Σ log l(yₜ, f(xₜ, θ))`, where `l` is from a linear exponential family.\n\n### Data / Model Specification\n\nThe density of the chosen linear exponential family, parameterized by its mean `m`, is `l(u, m) = exp{A(m) + B(u) + C(m)u}`. Consistency of the PMLE `hat(θ)ₜ` requires showing that the limiting objective function `φ_∞(θ) = EₓE₀[log l(y, f(x, θ))]` is uniquely maximized at `θ = θ₀`.\n\nThe proof of Theorem 1 relies on two key elements:\n1.  **Property 4 (from Kullback's Inequality):** For any `m, m₀` in the parameter space, `A(m) + C(m)m₀ ≤ A(m₀) + C(m₀)m₀`, with equality holding if and only if `m = m₀`.\n2.  **First-Order Identification Assumption:** `f(x, θ₁) = f(x, θ₂)` for almost all `x` implies `θ₁ = θ₂`.\n\n---\n\nBased on the provided theory, which of the following statements are valid steps or conclusions in the proof of consistency for the PMLE?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 284, "Question": "### Background\n\n**Research Question.** This problem explores the necessity of the linear exponential family structure for ensuring the robust consistency of a Pseudo Maximum Likelihood Estimator (PMLE).\n\n**Setting.** Theorem 2 states that for a PMLE of the mean to be strongly consistent for *any* true distribution `λ₀` (satisfying regularity conditions), the chosen family of densities `l(u, m)` *must* be a linear exponential family.\n\n### Data / Model Specification\n\nThe proof of Theorem 2 (Appendix 2) relies on the following logic: if the PMLE is consistent for any `λ₀`, then the population score must be zero at the true mean `m₀` for any `λ₀` that has `m₀` as its mean. Formally, for the score `s(y, m₀) = ∂log l(y, m₀)/∂m`:\n  \n\\text{For any } \\lambda_0 \\text{ such that } \\int (y - m_0) d\\lambda_0(y) = 0, \\text{ it must be that } \\int s(y, m_0) d\\lambda_0(y) = 0 \\quad \\text{(Eq. (1))}\n \nThe proof strategy involves testing this condition against a challenging class of two-point distributions, which ultimately forces the score function `s(y, m₀)` to be linear in `(y - m₀)`.\n\n---\n\nAccording to the logic of Theorem 2 and its proof, which of the following statements are valid conclusions or required assumptions?\n", "Answer": ["A", "C", "D"], "qid": "284", "question": "### Background\n\n**Research Question.** This problem explores the necessity of the linear exponential family structure for ensuring the robust consistency of a Pseudo Maximum Likelihood Estimator (PMLE).\n\n**Setting.** Theorem 2 states that for a PMLE of the mean to be strongly consistent for *any* true distribution `λ₀` (satisfying regularity conditions), the chosen family of densities `l(u, m)` *must* be a linear exponential family.\n\n### Data / Model Specification\n\nThe proof of Theorem 2 (Appendix 2) relies on the following logic: if the PMLE is consistent for any `λ₀`, then the population score must be zero at the true mean `m₀` for any `λ₀` that has `m₀` as its mean. Formally, for the score `s(y, m₀) = ∂log l(y, m₀)/∂m`:\n  \n\\text{For any } \\lambda_0 \\text{ such that } \\int (y - m_0) d\\lambda_0(y) = 0, \\text{ it must be that } \\int s(y, m_0) d\\lambda_0(y) = 0 \\quad \\text{(Eq. (1))}\n \nThe proof strategy involves testing this condition against a challenging class of two-point distributions, which ultimately forces the score function `s(y, m₀)` to be linear in `(y - m₀)`.\n\n---\n\nAccording to the logic of Theorem 2 and its proof, which of the following statements are valid conclusions or required assumptions?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 285, "Question": "### Background\n\nThis problem examines the comparison of alternative measures of market concentration. The author introduces the conventional Herfindahl index as a theoretically superior alternative to simple N-firm concentration ratios.\n\n### Data / Model Specification\n\nThe standard definition of the Herfindahl index for a market with `N` firms is:\n\n  \nH = \\sum_{j=1}^{N} s_j^2 \\quad \\text{(Eq. (1))}\n \n\nwhere `s_j` is the market share of firm `j`.\n\n### Question\n\nThe Herfindahl index (H) is often considered theoretically superior to an N-firm concentration ratio. According to industrial organization theory, which of the following are valid reasons for this superiority? Select all that apply.", "Answer": ["A", "D"], "qid": "285", "question": "### Background\n\nThis problem examines the comparison of alternative measures of market concentration. The author introduces the conventional Herfindahl index as a theoretically superior alternative to simple N-firm concentration ratios.\n\n### Data / Model Specification\n\nThe standard definition of the Herfindahl index for a market with `N` firms is:\n\n  \nH = \\sum_{j=1}^{N} s_j^2 \\quad \\text{(Eq. (1))}\n \n\nwhere `s_j` is the market share of firm `j`.\n\n### Question\n\nThe Herfindahl index (H) is often considered theoretically superior to an N-firm concentration ratio. According to industrial organization theory, which of the following are valid reasons for this superiority? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem examines the comparis...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem examines the comparis...", "error": null}}
{"ID": 286, "Question": "### Background\n\n**Research Question.** This problem addresses the central claim of the paper: that for a bivariate system with integrated variables measured with error, the presence of a cointegrating relationship provides the key to completely identify the underlying dynamic transfer function, even when that function is non-minimum phase.\n\n**Setting / Institutional Environment.** The analysis synthesizes two major econometric concepts. The first is the Beveridge-Nelson (BN) decomposition, a time-series tool for separating a process's long-run trend from its short-run dynamics. The second is frequency-domain analysis, which allows for the identification of a system's dynamic properties from its phase and gain characteristics.\n\n### Data / Model Specification\n\nThe dynamic errors-in-variables (EIV) model relates observed series (`y_t`, `x_t`) to latent true series (`\\hat{y}_t`, `\\hat{x}_t`) and stationary measurement errors (`\\varepsilon_{yt}`, `\\varepsilon_{xt}`):\n  \ny_{t} = \\hat{y}_{t} + \\varepsilon_{yt} \n \n  \nx_{t} = \\hat{x}_{t} + \\varepsilon_{xt} \n \n  \n\\hat{y}_{t} = T(q^{-1})\\hat{x}_{t} \n \nThe true transfer function `T(q^{-1})` is stable and causal. It can be expressed as the product of a scale factor `\\mu` and a normalized component `T_o(q^{-1})`:\n  \nT(q^{-1}) = \\mu T_o(q^{-1}) \\quad \\text{(Eq. (1))}\n \nThe zero-frequency or long-run gain of the transfer function is denoted `T(1)`.\n\n### Question\n\nThe paper's central result (Theorem 5) is that the transfer function `T(q^{-1})` is completely identifiable by synthesizing results from time-domain cointegration analysis and frequency-domain spectral analysis. Assume that the normalized transfer function `T_o(q^{-1})` has already been identified from the cross-spectrum of the differenced data.\n\nSelect all statements that correctly describe the remaining steps to completely identify `T(q^{-1})`.", "Answer": ["B", "C"], "qid": "286", "question": "### Background\n\n**Research Question.** This problem addresses the central claim of the paper: that for a bivariate system with integrated variables measured with error, the presence of a cointegrating relationship provides the key to completely identify the underlying dynamic transfer function, even when that function is non-minimum phase.\n\n**Setting / Institutional Environment.** The analysis synthesizes two major econometric concepts. The first is the Beveridge-Nelson (BN) decomposition, a time-series tool for separating a process's long-run trend from its short-run dynamics. The second is frequency-domain analysis, which allows for the identification of a system's dynamic properties from its phase and gain characteristics.\n\n### Data / Model Specification\n\nThe dynamic errors-in-variables (EIV) model relates observed series (`y_t`, `x_t`) to latent true series (`\\hat{y}_t`, `\\hat{x}_t`) and stationary measurement errors (`\\varepsilon_{yt}`, `\\varepsilon_{xt}`):\n  \ny_{t} = \\hat{y}_{t} + \\varepsilon_{yt} \n \n  \nx_{t} = \\hat{x}_{t} + \\varepsilon_{xt} \n \n  \n\\hat{y}_{t} = T(q^{-1})\\hat{x}_{t} \n \nThe true transfer function `T(q^{-1})` is stable and causal. It can be expressed as the product of a scale factor `\\mu` and a normalized component `T_o(q^{-1})`:\n  \nT(q^{-1}) = \\mu T_o(q^{-1}) \\quad \\text{(Eq. (1))}\n \nThe zero-frequency or long-run gain of the transfer function is denoted `T(1)`.\n\n### Question\n\nThe paper's central result (Theorem 5) is that the transfer function `T(q^{-1})` is completely identifiable by synthesizing results from time-domain cointegration analysis and frequency-domain spectral analysis. Assume that the normalized transfer function `T_o(q^{-1})` has already been identified from the cross-spectrum of the differenced data.\n\nSelect all statements that correctly describe the remaining steps to completely identify `T(q^{-1})`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 287, "Question": "### Background\n\n**Research Question.** This problem explores the technical challenge of identifying a dynamic transfer function from spectral data, contrasting the simple but restrictive \"minimum phase\" case with the general and more realistic \"non-minimum phase\" case.\n\n**Setting / Institutional Environment.** The analysis is conducted in the frequency domain. For a linear system, the transfer function `T(e^{-j\\omega})` can be represented by its gain and phase. A key result in signal processing states that for minimum phase systems, the gain and phase are uniquely related, but this relationship breaks down for non-minimum phase systems, creating an identification challenge.\n\n### Data / Model Specification\n\nThe relationship between the relevant spectra is:\n  \nF_{\\Delta y,\\Delta x}(e^{-j\\omega}) = T(e^{-j\\omega})F_{\\Delta \\hat{x}}(e^{-j\\omega}) \\quad \\text{(Eq. (1))}\n \nSince `F_{\\Delta \\hat{x}}(e^{-j\\omega})` is real, the phase of the cross-spectrum is equal to the phase of the transfer function, denoted `p(\\omega)`.\n\nA transfer function `T(q^{-1})` is **non-minimum phase** if its numerator polynomial has `m \\ge 1` roots (zeros) with modulus greater than 1. The normalized transfer function can be written as `T_o(e^{-j\\omega}) = \\exp(A(\\omega) + j p(\\omega))`, where `A(\\omega)` is the log-gain and `p(\\omega)` is the phase. These can be represented by Fourier series with coefficients `{A_s}` and `{P_s}`.\n\n### Question\n\nConsider a dynamic EIV system where the true transfer function `T(q^{-1})` is potentially non-minimum phase. According to the paper's general identification procedure (Theorem 4), which of the following statements correctly describe how `T(q^{-1})` can be identified (to within a scale factor) from the phase `p(\\omega)` of the cross-spectrum?", "Answer": ["A", "D"], "qid": "287", "question": "### Background\n\n**Research Question.** This problem explores the technical challenge of identifying a dynamic transfer function from spectral data, contrasting the simple but restrictive \"minimum phase\" case with the general and more realistic \"non-minimum phase\" case.\n\n**Setting / Institutional Environment.** The analysis is conducted in the frequency domain. For a linear system, the transfer function `T(e^{-j\\omega})` can be represented by its gain and phase. A key result in signal processing states that for minimum phase systems, the gain and phase are uniquely related, but this relationship breaks down for non-minimum phase systems, creating an identification challenge.\n\n### Data / Model Specification\n\nThe relationship between the relevant spectra is:\n  \nF_{\\Delta y,\\Delta x}(e^{-j\\omega}) = T(e^{-j\\omega})F_{\\Delta \\hat{x}}(e^{-j\\omega}) \\quad \\text{(Eq. (1))}\n \nSince `F_{\\Delta \\hat{x}}(e^{-j\\omega})` is real, the phase of the cross-spectrum is equal to the phase of the transfer function, denoted `p(\\omega)`.\n\nA transfer function `T(q^{-1})` is **non-minimum phase** if its numerator polynomial has `m \\ge 1` roots (zeros) with modulus greater than 1. The normalized transfer function can be written as `T_o(e^{-j\\omega}) = \\exp(A(\\omega) + j p(\\omega))`, where `A(\\omega)` is the log-gain and `p(\\omega)` is the phase. These can be represented by Fourier series with coefficients `{A_s}` and `{P_s}`.\n\n### Question\n\nConsider a dynamic EIV system where the true transfer function `T(q^{-1})` is potentially non-minimum phase. According to the paper's general identification procedure (Theorem 4), which of the following statements correctly describe how `T(q^{-1})` can be identified (to within a scale factor) from the phase `p(\\omega)` of the cross-spectrum?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 288, "Question": "### Background\n\n**Research Question.** This problem examines the paper's key methodological choices and their implications for identifying the sources of the productivity slowdown. It focuses on the decision to treat capital as a produced input.\n\n**Setting.** The analysis contrasts the \"Standard\" Leontief model, where capital is a primary factor, with an augmented \"Marxian labor value\" model that endogenizes capital production. This choice has significant consequences for the empirical decomposition of the productivity slowdown.\n\n### Data / Model Specification\n\nIn the augmented model, capital is treated as a produced good. This modeling choice alters the calculation of sectoral and aggregate TFP growth. The paper finds that this change significantly magnifies the contribution of shifts in the economy's composition to the overall productivity slowdown.\n\n**Table 1. The Effect of Modeling Choice on the Value Share Effect's Contribution to the Slowdown**\n| Model | Value Share Effect (% of Δρ) |\n| :--- | :--- |\n| 1. Standard | 6.0% |\n| 2. Marxian labor value | 21.7% |\n\n### Question\n\nBased on the paper's framework and the results in Table 1, select all statements that correctly explain the conceptual basis of the augmented model and the reason for its larger measured composition effect.", "Answer": ["A", "B"], "qid": "288", "question": "### Background\n\n**Research Question.** This problem examines the paper's key methodological choices and their implications for identifying the sources of the productivity slowdown. It focuses on the decision to treat capital as a produced input.\n\n**Setting.** The analysis contrasts the \"Standard\" Leontief model, where capital is a primary factor, with an augmented \"Marxian labor value\" model that endogenizes capital production. This choice has significant consequences for the empirical decomposition of the productivity slowdown.\n\n### Data / Model Specification\n\nIn the augmented model, capital is treated as a produced good. This modeling choice alters the calculation of sectoral and aggregate TFP growth. The paper finds that this change significantly magnifies the contribution of shifts in the economy's composition to the overall productivity slowdown.\n\n**Table 1. The Effect of Modeling Choice on the Value Share Effect's Contribution to the Slowdown**\n| Model | Value Share Effect (% of Δρ) |\n| :--- | :--- |\n| 1. Standard | 6.0% |\n| 2. Marxian labor value | 21.7% |\n\n### Question\n\nBased on the paper's framework and the results in Table 1, select all statements that correctly explain the conceptual basis of the augmented model and the reason for its larger measured composition effect.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 289, "Question": "### Background\n\n**Research Question.** This problem critically examines the paper's core identification strategy for separating the sources of the productivity slowdown.\n\n### Data / Model Specification\n\nThe paper's core decomposition of the slowdown in aggregate TFP growth (`Δρ`) is:\n  \n\\Delta\\rho \\cong \\underbrace{\\pi s(\\Delta\\beta)}_{\\text{Value Share Effect}} + \\underbrace{\\pi(\\Delta s)\\beta}_{\\text{Interindustry Effect}} + \\underbrace{(\\Delta\\pi)s\\beta}_{\\text{Sectoral Change Effect}} \n \n- `π`: Vector of direct sectoral TFP growth rates.\n- `s`: Matrix of inter-industry linkages in value terms.\n- `β`: Vector of final output value shares.\n\nA key methodological choice is to classify the \"Interindustry Effect\" (`π(Δs)β`) as a form of technical change, distinct from the \"Value Share\" composition effect. This separation is crucial for the paper's claim that composition effects are larger than previously thought.\n\n### Question\n\nThe author's classification of the 'Interindustry Effect' as purely technical change can be challenged. Select all statements that accurately describe a valid challenge to this identifying assumption.", "Answer": ["B", "D"], "qid": "289", "question": "### Background\n\n**Research Question.** This problem critically examines the paper's core identification strategy for separating the sources of the productivity slowdown.\n\n### Data / Model Specification\n\nThe paper's core decomposition of the slowdown in aggregate TFP growth (`Δρ`) is:\n  \n\\Delta\\rho \\cong \\underbrace{\\pi s(\\Delta\\beta)}_{\\text{Value Share Effect}} + \\underbrace{\\pi(\\Delta s)\\beta}_{\\text{Interindustry Effect}} + \\underbrace{(\\Delta\\pi)s\\beta}_{\\text{Sectoral Change Effect}} \n \n- `π`: Vector of direct sectoral TFP growth rates.\n- `s`: Matrix of inter-industry linkages in value terms.\n- `β`: Vector of final output value shares.\n\nA key methodological choice is to classify the \"Interindustry Effect\" (`π(Δs)β`) as a form of technical change, distinct from the \"Value Share\" composition effect. This separation is crucial for the paper's claim that composition effects are larger than previously thought.\n\n### Question\n\nThe author's classification of the 'Interindustry Effect' as purely technical change can be challenged. Select all statements that accurately describe a valid challenge to this identifying assumption.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 290, "Question": "### Background\n\n**Research Question.** This problem explores the core theoretical framework of the paper, which links aggregate Total Factor Productivity (TFP) growth to technological change at the sectoral level within a Leontief input-output model.\n\n### Data / Model Specification\n\nThe model defines two distinct measures of sectoral TFP:\n- The **direct** rate of TFP growth (`π_j`) captures productivity gains originating *within* sector `j`'s own production process.\n- The **\"inverse\"** rate of TFP growth (`π*_j`) captures the productivity gains in the *entire vertically integrated supply chain* required to produce one unit of final output for sector `j`.\n\nThese two measures are linked by the transformation `π* = πs`, where `s = p̂qp̂⁻¹`.\n- `q = (I-a)⁻¹` is the Leontief inverse, where `a` is the matrix of technical coefficients.\n- `p̂` is a diagonal matrix of prices.\n\n### Question\n\nBased on the definitions provided, select all statements that correctly interpret these theoretical concepts.", "Answer": ["A", "D"], "qid": "290", "question": "### Background\n\n**Research Question.** This problem explores the core theoretical framework of the paper, which links aggregate Total Factor Productivity (TFP) growth to technological change at the sectoral level within a Leontief input-output model.\n\n### Data / Model Specification\n\nThe model defines two distinct measures of sectoral TFP:\n- The **direct** rate of TFP growth (`π_j`) captures productivity gains originating *within* sector `j`'s own production process.\n- The **\"inverse\"** rate of TFP growth (`π*_j`) captures the productivity gains in the *entire vertically integrated supply chain* required to produce one unit of final output for sector `j`.\n\nThese two measures are linked by the transformation `π* = πs`, where `s = p̂qp̂⁻¹`.\n- `q = (I-a)⁻¹` is the Leontief inverse, where `a` is the matrix of technical coefficients.\n- `p̂` is a diagonal matrix of prices.\n\n### Question\n\nBased on the definitions provided, select all statements that correctly interpret these theoretical concepts.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 291, "Question": "### Background\n\n**Research Question.** This problem examines the empirical strategy used to identify 'disparate treatment' in a peer-to-peer lending market and critiques its core identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis uses a sample of 110,333 loan listings from Prosper.com. Lenders observe a rich set of financial data for each borrower, in addition to optional, unverified information like pictures. A key institutional feature is that borrowers whose listings expire without funding are able to relist their requests, often with modified terms (e.g., a higher maximum interest rate). The platform also allows borrowers to join 'groups' organized around a theme (e.g., university alumni), which provides social pressure to repay.\n\n**Variables & Parameters.**\n- `Funded_i`: An indicator variable equal to 1 if listing `i` was funded, 0 otherwise.\n- `X_i`: A vector of characteristics for listing `i` coded from pictures (e.g., an indicator for the borrower being perceived as black) and the one-line text description.\n- `Z_i`: A vector of other observable characteristics of the listing and borrower, including a comprehensive set of credit controls (credit grade, debt-to-income ratio, delinquencies, etc.) and loan parameters.\n\n---\n\n### Data / Model Specification\n\nThe researchers' basic empirical strategy involves estimating the probability that a loan listing gets funded as a function of the listing characteristics observed by lenders. The baseline specification is the following linear probability model (LPM):\n\n  \nFunded_i = \\alpha + X_i\\beta + Z_i\\theta + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n\nThe primary goal is to obtain an unbiased estimate of the parameter vector `β`, which captures the effect of picture and text characteristics on the funding outcome.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the identification strategy of Eq. (1) and potential sources of bias.\n", "Answer": ["A", "C"], "qid": "291", "question": "### Background\n\n**Research Question.** This problem examines the empirical strategy used to identify 'disparate treatment' in a peer-to-peer lending market and critiques its core identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis uses a sample of 110,333 loan listings from Prosper.com. Lenders observe a rich set of financial data for each borrower, in addition to optional, unverified information like pictures. A key institutional feature is that borrowers whose listings expire without funding are able to relist their requests, often with modified terms (e.g., a higher maximum interest rate). The platform also allows borrowers to join 'groups' organized around a theme (e.g., university alumni), which provides social pressure to repay.\n\n**Variables & Parameters.**\n- `Funded_i`: An indicator variable equal to 1 if listing `i` was funded, 0 otherwise.\n- `X_i`: A vector of characteristics for listing `i` coded from pictures (e.g., an indicator for the borrower being perceived as black) and the one-line text description.\n- `Z_i`: A vector of other observable characteristics of the listing and borrower, including a comprehensive set of credit controls (credit grade, debt-to-income ratio, delinquencies, etc.) and loan parameters.\n\n---\n\n### Data / Model Specification\n\nThe researchers' basic empirical strategy involves estimating the probability that a loan listing gets funded as a function of the listing characteristics observed by lenders. The baseline specification is the following linear probability model (LPM):\n\n  \nFunded_i = \\alpha + X_i\\beta + Z_i\\theta + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n\nThe primary goal is to obtain an unbiased estimate of the parameter vector `β`, which captures the effect of picture and text characteristics on the funding outcome.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the identification strategy of Eq. (1) and potential sources of bias.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 292, "Question": "### Background\n\n**Research Question.** This problem examines the empirical strategy used to identify 'disparate treatment' in a peer-to-peer lending market and critiques its core identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis uses a sample of 110,333 loan listings from Prosper.com. Lenders observe a rich set of financial data for each borrower, in addition to optional, unverified information like pictures. A key institutional feature is that borrowers whose listings expire without funding are able to relist their requests, often with modified terms (e.g., a higher maximum interest rate). The platform also allows borrowers to join 'groups' organized around a theme (e.g., university alumni), which provides social pressure to repay.\n\n**Variables & Parameters.**\n- `Funded_i`: An indicator variable equal to 1 if listing `i` was funded, 0 otherwise.\n- `X_i`: A vector of characteristics for listing `i` coded from pictures (e.g., an indicator for the borrower being perceived as black) and the one-line text description.\n- `Z_i`: A vector of other observable characteristics of the listing and borrower, including a comprehensive set of credit controls (credit grade, debt-to-income ratio, delinquencies, etc.) and loan parameters.\n\n---\n\n### Data / Model Specification\n\nThe researchers' basic empirical strategy involves estimating the probability that a loan listing gets funded as a function of the listing characteristics observed by lenders. The baseline specification is the following linear probability model (LPM):\n\n  \nFunded_i = \\alpha + X_i\\beta + Z_i\\theta + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n\nThe primary goal is to obtain an unbiased estimate of the parameter vector `β`, which captures the effect of picture and text characteristics on the funding outcome.\n\n---\n\n### Question\n\nThe paper performs a robustness check by re-estimating Eq. (1) on a sample restricted to only the first loan posted by each borrower. This is done to address potential selection bias from relisting behavior. Select all statements that are **NOT** valid reasons for this analytical choice or **INCORRECTLY** describe its implications.", "Answer": ["A", "C"], "qid": "292", "question": "### Background\n\n**Research Question.** This problem examines the empirical strategy used to identify 'disparate treatment' in a peer-to-peer lending market and critiques its core identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis uses a sample of 110,333 loan listings from Prosper.com. Lenders observe a rich set of financial data for each borrower, in addition to optional, unverified information like pictures. A key institutional feature is that borrowers whose listings expire without funding are able to relist their requests, often with modified terms (e.g., a higher maximum interest rate). The platform also allows borrowers to join 'groups' organized around a theme (e.g., university alumni), which provides social pressure to repay.\n\n**Variables & Parameters.**\n- `Funded_i`: An indicator variable equal to 1 if listing `i` was funded, 0 otherwise.\n- `X_i`: A vector of characteristics for listing `i` coded from pictures (e.g., an indicator for the borrower being perceived as black) and the one-line text description.\n- `Z_i`: A vector of other observable characteristics of the listing and borrower, including a comprehensive set of credit controls (credit grade, debt-to-income ratio, delinquencies, etc.) and loan parameters.\n\n---\n\n### Data / Model Specification\n\nThe researchers' basic empirical strategy involves estimating the probability that a loan listing gets funded as a function of the listing characteristics observed by lenders. The baseline specification is the following linear probability model (LPM):\n\n  \nFunded_i = \\alpha + X_i\\beta + Z_i\\theta + \\varepsilon_i \\quad \\text{(Eq. (1))}\n \n\nThe primary goal is to obtain an unbiased estimate of the parameter vector `β`, which captures the effect of picture and text characteristics on the funding outcome.\n\n---\n\n### Question\n\nThe paper performs a robustness check by re-estimating Eq. (1) on a sample restricted to only the first loan posted by each borrower. This is done to address potential selection bias from relisting behavior. Select all statements that are **NOT** valid reasons for this analytical choice or **INCORRECTLY** describe its implications.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 293, "Question": "### Background\n\nThe analysis of normal form games often focuses on a specific class of equilibria: **totally mixed** (all pure strategies are played with positive probability) and **regular** (the equilibrium is stable and isolated). An equilibrium is regular if the system of indifference equations that define it has a non-singular Jacobian matrix at the solution. This property is economically meaningful because, by the Implicit Function Theorem, it ensures the equilibrium is robust to small perturbations in the game's payoffs.\n\n### Data / Model Specification\n\nThe table below presents the maximum possible number of regular totally mixed Nash equilibria (TMNE) for an `n`-person game where each player has `k` pure strategies.\n\n**Table 1.** Maximum Number of Regular TMNE\n\n| n (Players) | k=2 | k=3 | k=4 | k=5 |\n|:---:|:---:|:---:|:---:|:---:|\n| 2 | 1 | 1 | 1 | 1 |\n| 3 | 2 | 10 | 56 | 346 |\n| 4 | 9 | 297 | 13833 | 748521 |\n| 5 | 44 | 13756 | 6.7e6 | 4.0e9 |\n\n*Note: Some values are rounded for presentation.*\n\n---\n\nBased on the provided background and data in Table 1, which of the following statements are valid conclusions?", "Answer": ["A", "B", "C"], "qid": "293", "question": "### Background\n\nThe analysis of normal form games often focuses on a specific class of equilibria: **totally mixed** (all pure strategies are played with positive probability) and **regular** (the equilibrium is stable and isolated). An equilibrium is regular if the system of indifference equations that define it has a non-singular Jacobian matrix at the solution. This property is economically meaningful because, by the Implicit Function Theorem, it ensures the equilibrium is robust to small perturbations in the game's payoffs.\n\n### Data / Model Specification\n\nThe table below presents the maximum possible number of regular totally mixed Nash equilibria (TMNE) for an `n`-person game where each player has `k` pure strategies.\n\n**Table 1.** Maximum Number of Regular TMNE\n\n| n (Players) | k=2 | k=3 | k=4 | k=5 |\n|:---:|:---:|:---:|:---:|:---:|\n| 2 | 1 | 1 | 1 | 1 |\n| 3 | 2 | 10 | 56 | 346 |\n| 4 | 9 | 297 | 13833 | 748521 |\n| 5 | 44 | 13756 | 6.7e6 | 4.0e9 |\n\n*Note: Some values are rounded for presentation.*\n\n---\n\nBased on the provided background and data in Table 1, which of the following statements are valid conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe analysis of normal form games ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe analysis of normal form games ...", "error": null}}
{"ID": 294, "Question": "### Background\n\nThe maximum number of regular totally mixed Nash equilibria (TMNE) in an `n`-person game is equal to a combinatorial quantity, `L_n(d, d)`. This function counts the number of ways to partition the players' non-reference strategies subject to certain constraints. The number of non-reference strategies (or strategic degrees of freedom) for player `i` is `d_i = |S_i| - 1`, where `S_i` is the set of pure strategies.\n\n### Data / Model Specification\n\nThe function `L_n(d, d)` is characterized by the following recurrence relation, which holds for any player `i` with `d_i > 0`:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) = \\sum_{j \\neq i} \\mathcal{L}_{n}(\\mathbf{d}-\\mathbf{e}_{j}, \\mathbf{d}-\\mathbf{e}_{i}) \\quad \\text{(Eq. (1))}\n \nwhere `d` is the vector `(d_1, ..., d_n)` and `e_j` is the `j`-th standard basis vector.\n\nThis recurrence relation can be used to derive an upper bound on the number of equilibria:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) \\le (n-1)^{\\sum d_i} \\quad \\text{(Eq. (2))}\n \n\n---\n\nBased on the recurrence relation in Eq. (1) and its properties, which of the following statements are correct?", "Answer": ["A", "B", "D"], "qid": "294", "question": "### Background\n\nThe maximum number of regular totally mixed Nash equilibria (TMNE) in an `n`-person game is equal to a combinatorial quantity, `L_n(d, d)`. This function counts the number of ways to partition the players' non-reference strategies subject to certain constraints. The number of non-reference strategies (or strategic degrees of freedom) for player `i` is `d_i = |S_i| - 1`, where `S_i` is the set of pure strategies.\n\n### Data / Model Specification\n\nThe function `L_n(d, d)` is characterized by the following recurrence relation, which holds for any player `i` with `d_i > 0`:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) = \\sum_{j \\neq i} \\mathcal{L}_{n}(\\mathbf{d}-\\mathbf{e}_{j}, \\mathbf{d}-\\mathbf{e}_{i}) \\quad \\text{(Eq. (1))}\n \nwhere `d` is the vector `(d_1, ..., d_n)` and `e_j` is the `j`-th standard basis vector.\n\nThis recurrence relation can be used to derive an upper bound on the number of equilibria:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) \\le (n-1)^{\\sum d_i} \\quad \\text{(Eq. (2))}\n \n\n---\n\nBased on the recurrence relation in Eq. (1) and its properties, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe maximum number of regular tota...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe maximum number of regular tota...", "error": null}}
{"ID": 295, "Question": "### Background\n\nFor a regular totally mixed Nash equilibrium (TMNE) to exist for generic payoffs in an `n`-person game, a specific condition on the game's dimensions must be met. This condition relates the number of strategic degrees of freedom for each player, where `d_i = |S_i| - 1` is the number of degrees of freedom for player `i`.\n\n### Data / Model Specification\n\nThe existence of a regular TMNE is equivalent to the maximum number of such equilibria, `L_n(d, d)`, being greater than zero. The paper establishes the following necessary and sufficient condition:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) > 0 \\iff d_{i} \\le \\sum_{j \\neq i} d_{j} \\quad \\text{for all } i \\in \\{1,...,n\\} \\quad \\text{(Eq. (1))}\n \nThis means that for any player, their number of strategic degrees of freedom must not exceed the sum of the degrees of freedom of all other players.\n\n---\n\nConsider a game with `n=3` players. The numbers of pure strategies for the players are `|S_1| = 5`, `|S_2| = 3`, and `|S_3| = 3`. According to the condition in Eq. (1), which of the following statements are **INVALID** conclusions?", "Answer": ["B", "C"], "qid": "295", "question": "### Background\n\nFor a regular totally mixed Nash equilibrium (TMNE) to exist for generic payoffs in an `n`-person game, a specific condition on the game's dimensions must be met. This condition relates the number of strategic degrees of freedom for each player, where `d_i = |S_i| - 1` is the number of degrees of freedom for player `i`.\n\n### Data / Model Specification\n\nThe existence of a regular TMNE is equivalent to the maximum number of such equilibria, `L_n(d, d)`, being greater than zero. The paper establishes the following necessary and sufficient condition:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) > 0 \\iff d_{i} \\le \\sum_{j \\neq i} d_{j} \\quad \\text{for all } i \\in \\{1,...,n\\} \\quad \\text{(Eq. (1))}\n \nThis means that for any player, their number of strategic degrees of freedom must not exceed the sum of the degrees of freedom of all other players.\n\n---\n\nConsider a game with `n=3` players. The numbers of pure strategies for the players are `|S_1| = 5`, `|S_2| = 3`, and `|S_3| = 3`. According to the condition in Eq. (1), which of the following statements are **INVALID** conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nFor a regular totally mixed Nash e...", "model_answer": "This is a detailed response to the question about ### Background\n\nFor a regular totally mixed Nash e...", "error": null}}
{"ID": 296, "Question": "### Background\n\nFor a regular totally mixed Nash equilibrium (TMNE) to exist for generic payoffs in an `n`-person game, a specific condition on the game's dimensions must be met. This condition relates the number of strategic degrees of freedom for each player, where `d_i = |S_i| - 1` is the number of degrees of freedom for player `i`.\n\n### Data / Model Specification\n\nThe existence of a regular TMNE is equivalent to the maximum number of such equilibria, `L_n(d, d)`, being greater than zero. The paper establishes the following necessary and sufficient condition:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) > 0 \\iff d_{i} \\le \\sum_{j \\neq i} d_{j} \\quad \\text{for all } i \\in \\{1,...,n\\} \\quad \\text{(Eq. (1))}\n \nThis means that for any player, their number of strategic degrees of freedom must not exceed the sum of the degrees of freedom of all other players.\n\n---\n\nConsider a 2-player game where Player 1 has 10 pure strategies (`|S_1|=10`) and Player 2 has 3 pure strategies (`|S_2|=3`). Based on the condition in Eq. (1) and its implications, which of the following statements are correct?", "Answer": ["A", "B", "C"], "qid": "296", "question": "### Background\n\nFor a regular totally mixed Nash equilibrium (TMNE) to exist for generic payoffs in an `n`-person game, a specific condition on the game's dimensions must be met. This condition relates the number of strategic degrees of freedom for each player, where `d_i = |S_i| - 1` is the number of degrees of freedom for player `i`.\n\n### Data / Model Specification\n\nThe existence of a regular TMNE is equivalent to the maximum number of such equilibria, `L_n(d, d)`, being greater than zero. The paper establishes the following necessary and sufficient condition:\n  \n\\mathcal{L}_{n}(\\mathbf{d}, \\mathbf{d}) > 0 \\iff d_{i} \\le \\sum_{j \\neq i} d_{j} \\quad \\text{for all } i \\in \\{1,...,n\\} \\quad \\text{(Eq. (1))}\n \nThis means that for any player, their number of strategic degrees of freedom must not exceed the sum of the degrees of freedom of all other players.\n\n---\n\nConsider a 2-player game where Player 1 has 10 pure strategies (`|S_1|=10`) and Player 2 has 3 pure strategies (`|S_2|=3`). Based on the condition in Eq. (1) and its implications, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nFor a regular totally mixed Nash e...", "model_answer": "This is a detailed response to the question about ### Background\n\nFor a regular totally mixed Nash e...", "error": null}}
{"ID": 297, "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n  \nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n \nBy using the numeraire good `x_1` to substitute for `u`, one can derive the estimable 'generalized Ashenfelter-Heckman' (GAH) system:\n  \nx_i = g_i(x_1 - ψ^1(p)) + ψ^i(p) \n\n\\quad\\text{for } i = 2, ..., N \\quad\\text{(Eq. 2)}\n \nwhere `g_i(·) = f_i(f_1^{-1}(·))`.\n\n---\n\n### Question\n\nTo address the endogeneity of `x_1` in the GAH system, a researcher proposes using exogenous non-wage income (`y`) as an instrumental variable (IV). Which of the following conditions must be met for `y` to be a valid instrument in this context? (Select all that apply)", "Answer": ["A", "C"], "qid": "297", "question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n  \nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n \nBy using the numeraire good `x_1` to substitute for `u`, one can derive the estimable 'generalized Ashenfelter-Heckman' (GAH) system:\n  \nx_i = g_i(x_1 - ψ^1(p)) + ψ^i(p) \n\n\\quad\\text{for } i = 2, ..., N \\quad\\text{(Eq. 2)}\n \nwhere `g_i(·) = f_i(f_1^{-1}(·))`.\n\n---\n\n### Question\n\nTo address the endogeneity of `x_1` in the GAH system, a researcher proposes using exogenous non-wage income (`y`) as an instrumental variable (IV). Which of the following conditions must be met for `y` to be a valid instrument in this context? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 298, "Question": "### Background\n\n**Research Question.** This problem investigates the connection between the general, model-free factorization of the pricing kernel developed in this paper and the classic eigen-factorization of Hansen and Scheinkman that applies in time-homogeneous Markovian environments. The goal is to understand the mathematical properties of the Markovian framework and the conditions under which its components are uniquely identified.\n\n**Setting.** The economy's uncertainty is driven by a time-homogeneous, conservative Borel right process `X_t` taking values in a space `E`. The pricing kernel `S_t` is a positive multiplicative functional of `X_t`.\n\n### Data / Model Specification\n\nIn this Markovian setting, a family of pricing operators `(\\mathcal{P}_t)_{t \\ge 0}` is defined as `\\mathcal{P}_{t}f(x) := \\mathbb{E}_{x}^{\\mathbb{P}}[S_{t}f(X_{t})]`, which gives the time-0 price of a payoff `f(X_t)` when the initial state is `X_0=x`.\n\n1.  **The Hansen-Scheinkman (HS) Framework:** The HS theory relies on the existence of a positive eigenfunction `\\pi(x)` and a corresponding real eigenvalue `\\lambda` that solve the eigen-equation for all `t>0`:\n      \n    \\mathcal{P}_{t}\\pi(x)=e^{-\\lambda t}\\pi(x)\n    \\quad \\text{(Eq. (1))}\n     \n    This allows for the factorization of the pricing kernel `S_t` into a martingale component `M_t^\\pi` and a state-dependent component:\n      \n    S_{t}=M_{t}^{\\pi}e^{-\\lambda t}\\frac{\\pi(X_{0})}{\\pi(X_{t})}, \\quad \\text{where} \\quad M_{t}^{\\pi} := S_{t}e^{\\lambda t}\\frac{\\pi(X_{t})}{\\pi(X_{0})}\n    \\quad \\text{(Eq. (2))}\n     \n2.  **Stochastic Stability:** A key condition for uniqueness is **Exponential Ergodicity**. This assumes the existence of a unique *recurrent* eigenfunction `\\pi_R` and that under its associated measure `\\mathbb{Q}^{\\pi_R}`, the state process converges to a stationary distribution at an exponential rate `\\alpha > 0`:\n      \n    \\left|\\mathbb{E}_{x}^{\\mathbb{Q}^{\\pi_{R}}}\\big[f(X_{t})/\\pi_{R}(X_{t})\\big]-c_{f}\\right| \\le c e^{-\\alpha t}/\\pi_{R}(x)\n    \\quad \\text{(Eq. (3))}\n     \n    where `c_f` is the long-run average of `f/\\pi_R` and `f` is any bounded function.\n\n### Question\n\nBased on the provided specifications, select all statements that are mathematically correct derivations or consequences of the Hansen-Scheinkman framework and the exponential ergodicity assumption.", "Answer": ["B", "C"], "qid": "298", "question": "### Background\n\n**Research Question.** This problem investigates the connection between the general, model-free factorization of the pricing kernel developed in this paper and the classic eigen-factorization of Hansen and Scheinkman that applies in time-homogeneous Markovian environments. The goal is to understand the mathematical properties of the Markovian framework and the conditions under which its components are uniquely identified.\n\n**Setting.** The economy's uncertainty is driven by a time-homogeneous, conservative Borel right process `X_t` taking values in a space `E`. The pricing kernel `S_t` is a positive multiplicative functional of `X_t`.\n\n### Data / Model Specification\n\nIn this Markovian setting, a family of pricing operators `(\\mathcal{P}_t)_{t \\ge 0}` is defined as `\\mathcal{P}_{t}f(x) := \\mathbb{E}_{x}^{\\mathbb{P}}[S_{t}f(X_{t})]`, which gives the time-0 price of a payoff `f(X_t)` when the initial state is `X_0=x`.\n\n1.  **The Hansen-Scheinkman (HS) Framework:** The HS theory relies on the existence of a positive eigenfunction `\\pi(x)` and a corresponding real eigenvalue `\\lambda` that solve the eigen-equation for all `t>0`:\n      \n    \\mathcal{P}_{t}\\pi(x)=e^{-\\lambda t}\\pi(x)\n    \\quad \\text{(Eq. (1))}\n     \n    This allows for the factorization of the pricing kernel `S_t` into a martingale component `M_t^\\pi` and a state-dependent component:\n      \n    S_{t}=M_{t}^{\\pi}e^{-\\lambda t}\\frac{\\pi(X_{0})}{\\pi(X_{t})}, \\quad \\text{where} \\quad M_{t}^{\\pi} := S_{t}e^{\\lambda t}\\frac{\\pi(X_{t})}{\\pi(X_{0})}\n    \\quad \\text{(Eq. (2))}\n     \n2.  **Stochastic Stability:** A key condition for uniqueness is **Exponential Ergodicity**. This assumes the existence of a unique *recurrent* eigenfunction `\\pi_R` and that under its associated measure `\\mathbb{Q}^{\\pi_R}`, the state process converges to a stationary distribution at an exponential rate `\\alpha > 0`:\n      \n    \\left|\\mathbb{E}_{x}^{\\mathbb{Q}^{\\pi_{R}}}\\big[f(X_{t})/\\pi_{R}(X_{t})\\big]-c_{f}\\right| \\le c e^{-\\alpha t}/\\pi_{R}(x)\n    \\quad \\text{(Eq. (3))}\n     \n    where `c_f` is the long-run average of `f/\\pi_R` and `f` is any bounded function.\n\n### Question\n\nBased on the provided specifications, select all statements that are mathematically correct derivations or consequences of the Hansen-Scheinkman framework and the exponential ergodicity assumption.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 299, "Question": "### Background\n\n**Research Question.** This problem investigates the connection between the general, model-free factorization of the pricing kernel developed in this paper and the classic eigen-factorization of Hansen and Scheinkman that applies in time-homogeneous Markovian environments. The goal is to understand the conditions required to uniquely identify the components of the general theory with the concrete objects from the Markovian framework.\n\n### Data / Model Specification\n\n-   **General Framework:** Under general conditions, the pricing kernel `S_t` has a long-term factorization `S_t = (1/B_t^\\infty) M_t^\\infty`, where `B_t^\\infty` is the long bond and `M_t^\\infty` defines the long forward measure `\\mathbb{L}`. This can be refined to `S_t = e^{-\\lambda_L t} (1/\\pi_t) M_t^\\infty`.\n-   **Markovian (HS) Framework:** In a Markovian setting, the pricing operator `\\mathcal{P}_t` may admit a positive eigenfunction `\\pi(x)` and eigenvalue `e^{-\\lambda t}` satisfying `\\mathcal{P}_{t}\\pi(x)=e^{-\\lambda t}\\pi(x)`. This implies a factorization `S_t = M_t^\\pi e^{-\\lambda t} \\pi(X_0) / \\pi(X_t)`.\n-   **The Bridge:** The paper shows that in a Markovian setting, the abstract process `\\pi_t` from the general framework is identified with `\\pi_L(X_t) / \\pi_L(X_0)`, where `\\pi_L` is the specific eigenfunction associated with the long-term limit.\n\n### Question\n\nRegarding the relationship between the general and Markovian frameworks, and the conditions for a unique identification, select all statements that are **NOT** supported by the paper.", "Answer": ["B", "D"], "qid": "299", "question": "### Background\n\n**Research Question.** This problem investigates the connection between the general, model-free factorization of the pricing kernel developed in this paper and the classic eigen-factorization of Hansen and Scheinkman that applies in time-homogeneous Markovian environments. The goal is to understand the conditions required to uniquely identify the components of the general theory with the concrete objects from the Markovian framework.\n\n### Data / Model Specification\n\n-   **General Framework:** Under general conditions, the pricing kernel `S_t` has a long-term factorization `S_t = (1/B_t^\\infty) M_t^\\infty`, where `B_t^\\infty` is the long bond and `M_t^\\infty` defines the long forward measure `\\mathbb{L}`. This can be refined to `S_t = e^{-\\lambda_L t} (1/\\pi_t) M_t^\\infty`.\n-   **Markovian (HS) Framework:** In a Markovian setting, the pricing operator `\\mathcal{P}_t` may admit a positive eigenfunction `\\pi(x)` and eigenvalue `e^{-\\lambda t}` satisfying `\\mathcal{P}_{t}\\pi(x)=e^{-\\lambda t}\\pi(x)`. This implies a factorization `S_t = M_t^\\pi e^{-\\lambda t} \\pi(X_0) / \\pi(X_t)`.\n-   **The Bridge:** The paper shows that in a Markovian setting, the abstract process `\\pi_t` from the general framework is identified with `\\pi_L(X_t) / \\pi_L(X_0)`, where `\\pi_L` is the specific eigenfunction associated with the long-term limit.\n\n### Question\n\nRegarding the relationship between the general and Markovian frameworks, and the conditions for a unique identification, select all statements that are **NOT** supported by the paper.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 300, "Question": "### Background\n\n**Research Question.** This problem explores the paper's central contribution: the construction of the long-term factorization of the arbitrage-free pricing kernel in a general semimartingale setting. The goal is to understand the mathematical properties of the components of this factorization.\n\n### Data / Model Specification\n\nIn a continuous-time, arbitrage-free model, the following components are defined:\n-   `S_t`: The pricing kernel, a positive semimartingale with `S_0=1`.\n-   `P_t^T = \\mathbb{E}_t^{\\mathbb{P}}[S_T/S_t]`: The time-`t` price of a `T`-maturity zero-coupon bond.\n-   `M_t^T`: The `T`-forward measure martingale, defined for `t \\in [0, T]` as:\n      \n    M_{t}^{T} := S_{t}P_{t}^{T}/P_{0}^{T}\n    \\quad \\text{(Eq. (1))}\n     \n-   `B_t^T`: The value of a self-financing strategy that perpetually rolls over `T`-maturity bonds. For any finite `T`, `S_t = (1/B_t^T) M_t^T`.\n\nUnder sufficient regularity conditions, as `T \\to \\infty`, these components converge to long-term counterparts `M_t^\\infty` and `B_t^\\infty`. The long bond itself is factorized as `B_t^\\infty = e^{\\lambda t} \\pi_t`, leading to the paper's main decomposition:\n  \nS_{t}=e^{-\\lambda t}\\frac{1}{\\pi_{t}}M_{t}^{\\infty}\n\\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the provided definitions and results, select all statements that are mathematically correct.", "Answer": ["C", "D"], "qid": "300", "question": "### Background\n\n**Research Question.** This problem explores the paper's central contribution: the construction of the long-term factorization of the arbitrage-free pricing kernel in a general semimartingale setting. The goal is to understand the mathematical properties of the components of this factorization.\n\n### Data / Model Specification\n\nIn a continuous-time, arbitrage-free model, the following components are defined:\n-   `S_t`: The pricing kernel, a positive semimartingale with `S_0=1`.\n-   `P_t^T = \\mathbb{E}_t^{\\mathbb{P}}[S_T/S_t]`: The time-`t` price of a `T`-maturity zero-coupon bond.\n-   `M_t^T`: The `T`-forward measure martingale, defined for `t \\in [0, T]` as:\n      \n    M_{t}^{T} := S_{t}P_{t}^{T}/P_{0}^{T}\n    \\quad \\text{(Eq. (1))}\n     \n-   `B_t^T`: The value of a self-financing strategy that perpetually rolls over `T`-maturity bonds. For any finite `T`, `S_t = (1/B_t^T) M_t^T`.\n\nUnder sufficient regularity conditions, as `T \\to \\infty`, these components converge to long-term counterparts `M_t^\\infty` and `B_t^\\infty`. The long bond itself is factorized as `B_t^\\infty = e^{\\lambda t} \\pi_t`, leading to the paper's main decomposition:\n  \nS_{t}=e^{-\\lambda t}\\frac{1}{\\pi_{t}}M_{t}^{\\infty}\n\\quad \\text{(Eq. (2))}\n \n\n### Question\n\nBased on the provided definitions and results, select all statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 301, "Question": "### Background\n\n**Research Question.** This problem explores the economic interpretation of the paper's central result: the long-term factorization of the pricing kernel. The goal is to understand what each component represents and the implications for pricing long-term risk.\n\n### Data / Model Specification\n\nThe paper's main result is the factorization of the pricing kernel `S_t` into three components:\n  \nS_{t}=e^{-\\lambda t}\\frac{1}{\\pi_{t}}M_{t}^{\\infty}\n\\quad \\text{(Eq. (1))}\n \n-   `\\lambda` is a constant, the long-term discount rate.\n-   `\\pi_t` is a positive semimartingale.\n-   `M_t^\\infty` is a positive martingale that defines a change of measure from the physical measure `\\mathbb{P}` to the **long forward measure** `\\mathbb{L}`.\n\nA key finding (Theorem 3.5) is that for a stochastically growing cash flow `G_t`, its long-term expected yield under the measure `\\mathbb{L}` converges to `\\lambda`.\n\n### Question\n\nBased on the factorization in Eq. (1) and its implications, select all statements that provide a correct economic interpretation.", "Answer": ["A", "C"], "qid": "301", "question": "### Background\n\n**Research Question.** This problem explores the economic interpretation of the paper's central result: the long-term factorization of the pricing kernel. The goal is to understand what each component represents and the implications for pricing long-term risk.\n\n### Data / Model Specification\n\nThe paper's main result is the factorization of the pricing kernel `S_t` into three components:\n  \nS_{t}=e^{-\\lambda t}\\frac{1}{\\pi_{t}}M_{t}^{\\infty}\n\\quad \\text{(Eq. (1))}\n \n-   `\\lambda` is a constant, the long-term discount rate.\n-   `\\pi_t` is a positive semimartingale.\n-   `M_t^\\infty` is a positive martingale that defines a change of measure from the physical measure `\\mathbb{P}` to the **long forward measure** `\\mathbb{L}`.\n\nA key finding (Theorem 3.5) is that for a stochastically growing cash flow `G_t`, its long-term expected yield under the measure `\\mathbb{L}` converges to `\\lambda`.\n\n### Question\n\nBased on the factorization in Eq. (1) and its implications, select all statements that provide a correct economic interpretation.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 302, "Question": "### Background\n\n**Research Question:** This problem investigates two mechanisms that may explain firm responses to R&D tax incentives: the importance of credit refundability for firms without tax liability, and the role of sunk fixed costs in enabling R&D expansion.\n\n**Setting / Institutional Environment:** A triple-difference (DDD) model is used to test for heterogeneous treatment effects of a 2004 Canadian R&D tax policy change. The analysis compares the policy's impact on two distinct dimensions: (1) zero-tax liability firms versus tax-paying firms, and (2) firms with pre-policy R&D capital investments versus those without.\n\n### Data / Model Specification\n\nThe analysis uses a Poisson QML specification where coefficients can be interpreted as approximate percentage changes. The key coefficients from the DDD models are presented in Table 1 below.\n\n**Table 1: Triple-Difference Estimates of Policy Impact**\n\n| | (1) | (2) | (4) | (5) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Sample** | **All Firms** | **All Firms** | **Non-NAICS 541** | **Non-NAICS 541** |\n| **R&D Outcome** | **Total** | **Wages** | **Total** | **Wages** |\n| `Eligible × policy × zero-tax` | 0.16 | 0.11 | | |\n| `Eligible × policy` (for zero-tax model) | 0.04 | 0.02 | | |\n| `Eligible × policy × capital` | | | 0.24 | 0.22 |\n| `Eligible × policy` (for capital model) | | | -0.01 | -0.07 |\n\n*Notes: The model includes all lower-order interactions. `Eligible × policy` is the baseline DiD effect for the reference group (tax-paying firms or firms without pre-policy capital). The total effect for the subgroup is the sum of the baseline DiD and the triple-interaction term.* \n\n### Question\n\nBased on the results in Table 1, which of the following statements are valid interpretations of the heterogeneous policy effects? Select all that apply.", "Answer": ["A", "D"], "qid": "302", "question": "### Background\n\n**Research Question:** This problem investigates two mechanisms that may explain firm responses to R&D tax incentives: the importance of credit refundability for firms without tax liability, and the role of sunk fixed costs in enabling R&D expansion.\n\n**Setting / Institutional Environment:** A triple-difference (DDD) model is used to test for heterogeneous treatment effects of a 2004 Canadian R&D tax policy change. The analysis compares the policy's impact on two distinct dimensions: (1) zero-tax liability firms versus tax-paying firms, and (2) firms with pre-policy R&D capital investments versus those without.\n\n### Data / Model Specification\n\nThe analysis uses a Poisson QML specification where coefficients can be interpreted as approximate percentage changes. The key coefficients from the DDD models are presented in Table 1 below.\n\n**Table 1: Triple-Difference Estimates of Policy Impact**\n\n| | (1) | (2) | (4) | (5) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Sample** | **All Firms** | **All Firms** | **Non-NAICS 541** | **Non-NAICS 541** |\n| **R&D Outcome** | **Total** | **Wages** | **Total** | **Wages** |\n| `Eligible × policy × zero-tax` | 0.16 | 0.11 | | |\n| `Eligible × policy` (for zero-tax model) | 0.04 | 0.02 | | |\n| `Eligible × policy × capital` | | | 0.24 | 0.22 |\n| `Eligible × policy` (for capital model) | | | -0.01 | -0.07 |\n\n*Notes: The model includes all lower-order interactions. `Eligible × policy` is the baseline DiD effect for the reference group (tax-paying firms or firms without pre-policy capital). The total effect for the subgroup is the sum of the baseline DiD and the triple-interaction term.* \n\n### Question\n\nBased on the results in Table 1, which of the following statements are valid interpretations of the heterogeneous policy effects? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "error": null}}
{"ID": 303, "Question": "### Background\n\n**Research Question:** This problem requires translating a reduced-form policy impact estimate into a structural economic parameter: the user-cost elasticity of R&D.\n\n**Setting / Institutional Environment:** A 2004 policy change in Canada provided a more generous R&D tax credit. The user cost of R&D is `U = (r+δ)(1-ρ)`. The policy effectively increases the marginal credit rate `ρ` for some firms. For firms with positive tax liability, the effective `ρ` increases by 0.15 (from 0.20 to 0.35). For firms with zero tax liability, the effective `ρ` increases by 0.35 (from 0.00 to 0.35). The baseline credit rate for firms receiving the top credit is 0.35, so the pre-treatment user cost factor is `1-ρ = 1 - 0.35 = 0.65`.\n\n### Data / Model Specification\n\nThe formula for the implied user-cost elasticity of R&D is given by:\n\n  \n\\varepsilon = \\frac{0.65 \\times \\beta_1 / s}{0.20p - 0.35} \\quad \\text{(Eq. (1))}\n \n\nwhere:\n- `β_1`: The reduced-form intent-to-treat (ITT) estimate from the main DiD model.\n- `s`: The share of eligible firms that are actually treated by the policy change.\n- `p`: The share of treated firms (those in group `s`) that have positive current tax liability.\n\n### Question\n\nConsider two scenarios for calculating the elasticity for Total R&D, where `β_1 = 0.17`.\n- **Scenario 1 (Lower Bound):** `s = 0.59`, `p = 0.43`\n- **Scenario 2 (Upper Bound):** `s = 0.11`, `p = 0.65`\n\nWhich of the following statements about the user-cost elasticity are correct? Select all that apply.", "Answer": ["B", "C"], "qid": "303", "question": "### Background\n\n**Research Question:** This problem requires translating a reduced-form policy impact estimate into a structural economic parameter: the user-cost elasticity of R&D.\n\n**Setting / Institutional Environment:** A 2004 policy change in Canada provided a more generous R&D tax credit. The user cost of R&D is `U = (r+δ)(1-ρ)`. The policy effectively increases the marginal credit rate `ρ` for some firms. For firms with positive tax liability, the effective `ρ` increases by 0.15 (from 0.20 to 0.35). For firms with zero tax liability, the effective `ρ` increases by 0.35 (from 0.00 to 0.35). The baseline credit rate for firms receiving the top credit is 0.35, so the pre-treatment user cost factor is `1-ρ = 1 - 0.35 = 0.65`.\n\n### Data / Model Specification\n\nThe formula for the implied user-cost elasticity of R&D is given by:\n\n  \n\\varepsilon = \\frac{0.65 \\times \\beta_1 / s}{0.20p - 0.35} \\quad \\text{(Eq. (1))}\n \n\nwhere:\n- `β_1`: The reduced-form intent-to-treat (ITT) estimate from the main DiD model.\n- `s`: The share of eligible firms that are actually treated by the policy change.\n- `p`: The share of treated firms (those in group `s`) that have positive current tax liability.\n\n### Question\n\nConsider two scenarios for calculating the elasticity for Total R&D, where `β_1 = 0.17`.\n- **Scenario 1 (Lower Bound):** `s = 0.59`, `p = 0.43`\n- **Scenario 2 (Upper Bound):** `s = 0.11`, `p = 0.65`\n\nWhich of the following statements about the user-cost elasticity are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "error": null}}
{"ID": 304, "Question": "### Background\n\n**Research Question:** This problem evaluates the causal impact of an expanded R&D tax credit on the R&D spending of small firms using a difference-in-differences (DiD) framework and addresses the key identifying assumption of parallel trends.\n\n**Setting / Institutional Environment:** The study examines a 2004 policy change in Canada that made the SRED R&D tax credit more generous for firms with prior-year taxable income between C$200,000 and C$500,000. This group is considered 'eligible' for the enhanced incentive.\n\n### Data / Model Specification\n\nThe main DiD model for R&D spending is:\n\n  \nE[R_{it} | \\dots] = \\exp\\{T_{it} \\cdot \\text{Post-Policy}_t \\cdot \\beta_1 + T_{it} \\cdot \\beta_2 + \\gamma_i + \\lambda_{jt} + X_{it}\\theta\\} \\quad \\text{(Eq. (1))}\n \n\nwhere `T_it` is an indicator for being 'Eligible', `Post-Policy_t` is an indicator for years after 2003, `β_1` is the DiD coefficient of interest, `γ_i` are firm fixed effects, and `λ_jt` are industry-by-year fixed effects.\n\n### Question\n\nWhich of the following statements represent valid threats to the causal interpretation of the `β_1` coefficient from Eq. (1), or valid ways the research design attempts to mitigate them? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "304", "question": "### Background\n\n**Research Question:** This problem evaluates the causal impact of an expanded R&D tax credit on the R&D spending of small firms using a difference-in-differences (DiD) framework and addresses the key identifying assumption of parallel trends.\n\n**Setting / Institutional Environment:** The study examines a 2004 policy change in Canada that made the SRED R&D tax credit more generous for firms with prior-year taxable income between C$200,000 and C$500,000. This group is considered 'eligible' for the enhanced incentive.\n\n### Data / Model Specification\n\nThe main DiD model for R&D spending is:\n\n  \nE[R_{it} | \\dots] = \\exp\\{T_{it} \\cdot \\text{Post-Policy}_t \\cdot \\beta_1 + T_{it} \\cdot \\beta_2 + \\gamma_i + \\lambda_{jt} + X_{it}\\theta\\} \\quad \\text{(Eq. (1))}\n \n\nwhere `T_it` is an indicator for being 'Eligible', `Post-Policy_t` is an indicator for years after 2003, `β_1` is the DiD coefficient of interest, `γ_i` are firm fixed effects, and `λ_jt` are industry-by-year fixed effects.\n\n### Question\n\nWhich of the following statements represent valid threats to the causal interpretation of the `β_1` coefficient from Eq. (1), or valid ways the research design attempts to mitigate them? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "error": null}}
{"ID": 305, "Question": "### Background\n\n**Research Question.** This problem examines the principles of experimental design and causal inference in the context of comparing willingness-to-pay (WTP) elicitation mechanisms.\n\n**Setting & Institutional Environment.** An experiment in rural Uganda randomly assigned 200 participants to one of four treatments to test three design features: response mode (BDM vs. Multiple Price List, MPL), price revelation (Onsite vs. Preassigned), and price distribution information (Uniform vs. Unstated). A key concern is that features offering statistical advantages (e.g., Preassigned prices allowing for stratification) might reduce participant comprehension or trust compared to more transparent alternatives (e.g., Onsite randomization).\n\n---\n\n### Data / Model Specification\n\nThe study's identification strategy relies on the random assignment of 50 participants to each of the four treatment arms. Comprehension was measured via a series of test questions, and a key finding is that comprehension levels were statistically indistinguishable across treatments.\n\n**Key Statistical Result:** For the voucher elicitation task, an F-test for the joint null hypothesis of equal mean comprehension scores across all four treatment arms yields a p-value of 0.63.\n\n---\n\n### Question\n\nBased on the study's design and findings, select all statements that are correct.", "Answer": ["A", "C"], "qid": "305", "question": "### Background\n\n**Research Question.** This problem examines the principles of experimental design and causal inference in the context of comparing willingness-to-pay (WTP) elicitation mechanisms.\n\n**Setting & Institutional Environment.** An experiment in rural Uganda randomly assigned 200 participants to one of four treatments to test three design features: response mode (BDM vs. Multiple Price List, MPL), price revelation (Onsite vs. Preassigned), and price distribution information (Uniform vs. Unstated). A key concern is that features offering statistical advantages (e.g., Preassigned prices allowing for stratification) might reduce participant comprehension or trust compared to more transparent alternatives (e.g., Onsite randomization).\n\n---\n\n### Data / Model Specification\n\nThe study's identification strategy relies on the random assignment of 50 participants to each of the four treatment arms. Comprehension was measured via a series of test questions, and a key finding is that comprehension levels were statistically indistinguishable across treatments.\n\n**Key Statistical Result:** For the voucher elicitation task, an F-test for the joint null hypothesis of equal mean comprehension scores across all four treatment arms yields a p-value of 0.63.\n\n---\n\n### Question\n\nBased on the study's design and findings, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 306, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the Becker-DeGroot-Marschak (BDM) mechanism and explores how optimal bidding strategy adapts when core assumptions are relaxed.\n\n**Setting & Institutional Environment.** In a field experiment, participants state their willingness-to-pay (WTP) for an induced-value voucher. This voucher is immediately redeemable for a fixed amount of cash. A random price is then drawn from a discrete set of possible prices to determine if a transaction occurs.\n\n**Variables & Parameters.**\n- `V`: The true value of a good to a participant.\n- `W`: The participant's stated WTP for the good.\n- `P`: The randomly drawn price for the good.\n- `π(W, P, V)`: The participant's monetary profit from the transaction.\n\n---\n\n### Data / Model Specification\n\nThe transaction rule is that the participant buys the good at price `P` if their stated WTP, `W`, is greater than or equal to `P`. The profit function is therefore:\n\n  \n\\pi(W, P, V) = \n\\begin{cases} \nV - P & \\text{if } W \\geq P \\\\\n0 & \\text{if } W < P \n\\end{cases}\n\\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nConsider the BDM mechanism described. Based on an analysis of a participant's optimal bidding strategy `W*` relative to their true value `V`, select all of the following conclusions that are correct.", "Answer": ["B", "D"], "qid": "306", "question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the Becker-DeGroot-Marschak (BDM) mechanism and explores how optimal bidding strategy adapts when core assumptions are relaxed.\n\n**Setting & Institutional Environment.** In a field experiment, participants state their willingness-to-pay (WTP) for an induced-value voucher. This voucher is immediately redeemable for a fixed amount of cash. A random price is then drawn from a discrete set of possible prices to determine if a transaction occurs.\n\n**Variables & Parameters.**\n- `V`: The true value of a good to a participant.\n- `W`: The participant's stated WTP for the good.\n- `P`: The randomly drawn price for the good.\n- `π(W, P, V)`: The participant's monetary profit from the transaction.\n\n---\n\n### Data / Model Specification\n\nThe transaction rule is that the participant buys the good at price `P` if their stated WTP, `W`, is greater than or equal to `P`. The profit function is therefore:\n\n  \n\\pi(W, P, V) = \n\\begin{cases} \nV - P & \\text{if } W \\geq P \\\\\n0 & \\text{if } W < P \n\\end{cases}\n\\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nConsider the BDM mechanism described. Based on an analysis of a participant's optimal bidding strategy `W*` relative to their true value `V`, select all of the following conclusions that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 307, "Question": "### Background\n\nAn analysis of income in Nigeria from 1997-1999 uses a Mincer-style wage equation to estimate the return to schooling. The endogeneity of schooling is addressed using a two-stage least squares (2SLS) approach, with length of exposure to the Universal Primary Education (UPE) program as the instrument.\n\n### Data / Model Specification\n\nResults from various Ordinary Least Squares (OLS) and 2SLS specifications are presented in Table 1. The preferred 2SLS specification in column (7) includes a full set of controls (Age, Age², Sex, Sector, Cohort, and State fixed effects). The first-stage coefficient on `UPEexposure` for this specification is 0.15, meaning each year of UPE exposure increased schooling by 0.15 years.\n\n**Table 1: Summary of OLS vs IV Results (1997–1999)**\n\n| Variable of interest | (OLS) (5) | (IV) (6) | (IV) (7) |\n| :--- | :---: | :---: | :---: |\n| **Panel B. 2nd stage (Dep. Var: log income)** | | | |\n| Yrs of sch | 0.026* | 0.052** | 0.027* |\n| | (0.001) | (0.030) | (0.013) |\n| **Controls** | | | |\n| Age, Age2 | Yes | Yes | Yes |\n| Sex, Sector | Yes | Yes | Yes |\n| Cohort | Yes | Yes | Yes |\n| Region | No | Yes | No |\n| State | Yes | No | Yes |\n\n*Notes: Standard errors in parentheses. * Significant at 5%. ** Significant at 10%.*\n\n### Question\n\nBased on the information provided, which of the following statements are valid interpretations or derivations from the econometric results?\n", "Answer": ["A", "B", "C"], "qid": "307", "question": "### Background\n\nAn analysis of income in Nigeria from 1997-1999 uses a Mincer-style wage equation to estimate the return to schooling. The endogeneity of schooling is addressed using a two-stage least squares (2SLS) approach, with length of exposure to the Universal Primary Education (UPE) program as the instrument.\n\n### Data / Model Specification\n\nResults from various Ordinary Least Squares (OLS) and 2SLS specifications are presented in Table 1. The preferred 2SLS specification in column (7) includes a full set of controls (Age, Age², Sex, Sector, Cohort, and State fixed effects). The first-stage coefficient on `UPEexposure` for this specification is 0.15, meaning each year of UPE exposure increased schooling by 0.15 years.\n\n**Table 1: Summary of OLS vs IV Results (1997–1999)**\n\n| Variable of interest | (OLS) (5) | (IV) (6) | (IV) (7) |\n| :--- | :---: | :---: | :---: |\n| **Panel B. 2nd stage (Dep. Var: log income)** | | | |\n| Yrs of sch | 0.026* | 0.052** | 0.027* |\n| | (0.001) | (0.030) | (0.013) |\n| **Controls** | | | |\n| Age, Age2 | Yes | Yes | Yes |\n| Sex, Sector | Yes | Yes | Yes |\n| Cohort | Yes | Yes | Yes |\n| Region | No | Yes | No |\n| State | Yes | No | Yes |\n\n*Notes: Standard errors in parentheses. * Significant at 5%. ** Significant at 10%.*\n\n### Question\n\nBased on the information provided, which of the following statements are valid interpretations or derivations from the econometric results?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of income in Nigeria f...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of income in Nigeria f...", "error": null}}
{"ID": 308, "Question": "### Background\n\nTo validate the relevance of the Universal Primary Education (UPE) program as an instrument for schooling, a difference-in-differences (DiD) analysis is performed. The design compares educational attainment for cohorts exposed to the 1976 nationwide UPE rollout versus those too old to be exposed. It also compares regions with high intensity of exposure (treated in 1976) versus low intensity (treated before 1976).\n\n### Data / Model Specification\n\n**Table 1: Mean of education in a high vs low intensity region**\n\n| | **Mean years of education** | | |\n| :--- | :---: | :---: | :---: |\n| | **High (1)** | **Low (2)** | **Difference (3)** |\n| **Panel A: Main Experiment** | | | |\n| Aged 2 to 11 in 1976 (Exposed) | 9.97 | 10.50 | -0.53 |\n| Aged 12-18 in 1976 (Unexposed) | 7.53 | 8.41 | -0.88 |\n| **Difference** | **2.44** | **2.09** | **0.35** |\n| **Panel B: Placebo Test** | | | |\n| Aged 12-18 in 1976 (Unexposed) | 7.53 | 8.41 | -0.88 |\n| Aged 19-26 in 1976 (Unexposed) | 6.42 | 7.19 | -0.77 |\n| **Difference** | **1.11** | **1.22** | **-0.11** |\n\n### Question\n\nBased on the DiD analysis in Table 1, which of the following statements are correct?\n", "Answer": ["A", "C", "D"], "qid": "308", "question": "### Background\n\nTo validate the relevance of the Universal Primary Education (UPE) program as an instrument for schooling, a difference-in-differences (DiD) analysis is performed. The design compares educational attainment for cohorts exposed to the 1976 nationwide UPE rollout versus those too old to be exposed. It also compares regions with high intensity of exposure (treated in 1976) versus low intensity (treated before 1976).\n\n### Data / Model Specification\n\n**Table 1: Mean of education in a high vs low intensity region**\n\n| | **Mean years of education** | | |\n| :--- | :---: | :---: | :---: |\n| | **High (1)** | **Low (2)** | **Difference (3)** |\n| **Panel A: Main Experiment** | | | |\n| Aged 2 to 11 in 1976 (Exposed) | 9.97 | 10.50 | -0.53 |\n| Aged 12-18 in 1976 (Unexposed) | 7.53 | 8.41 | -0.88 |\n| **Difference** | **2.44** | **2.09** | **0.35** |\n| **Panel B: Placebo Test** | | | |\n| Aged 12-18 in 1976 (Unexposed) | 7.53 | 8.41 | -0.88 |\n| Aged 19-26 in 1976 (Unexposed) | 6.42 | 7.19 | -0.77 |\n| **Difference** | **1.11** | **1.22** | **-0.11** |\n\n### Question\n\nBased on the DiD analysis in Table 1, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nTo validate the relevance of the U...", "model_answer": "This is a detailed response to the question about ### Background\n\nTo validate the relevance of the U...", "error": null}}
{"ID": 309, "Question": "### Background\n\nA researcher specifies a two-stage least squares (2SLS) model to estimate the causal return to schooling (S) on log income (log(y)). The instrument (Z) is exposure to a free education program. The core identification assumptions are instrument relevance (Cov(Z, S) ≠ 0) and the exclusion restriction (the instrument affects income only through schooling).\n\n### Question\n\nIn the context of this 2SLS model, which of the following scenarios would result in an INCONSISTENT or BIASED estimate of the return to schooling?\n", "Answer": ["A", "B"], "qid": "309", "question": "### Background\n\nA researcher specifies a two-stage least squares (2SLS) model to estimate the causal return to schooling (S) on log income (log(y)). The instrument (Z) is exposure to a free education program. The core identification assumptions are instrument relevance (Cov(Z, S) ≠ 0) and the exclusion restriction (the instrument affects income only through schooling).\n\n### Question\n\nIn the context of this 2SLS model, which of the following scenarios would result in an INCONSISTENT or BIASED estimate of the return to schooling?\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA researcher specifies a two-stage...", "model_answer": "This is a detailed response to the question about ### Background\n\nA researcher specifies a two-stage...", "error": null}}
{"ID": 310, "Question": "### Background\n\n**Research Question.** This question examines the descriptive evidence supporting the paper's central theory that the make-or-buy decision for clinical trials is driven by the trade-off between 'knowledge production' and 'data production'.\n\n**Setting / Institutional Environment.** The paper's theory posits that firms insource knowledge-intensive projects and outsource data-intensive projects. Knowledge intensity is proxied by factors like the involvement of academic medical centers (`%AMC`), while data intensity is proxied by logistical complexity (`#SITES`, `#PROCEDURES`).\n\n### Data / Model Specification\n\n**Table 1: Project-Level Descriptive Statistics**\n\n| Variable | Mean, Insourced Projects | Mean, Outsourced Projects |\n|:---|---:|---:|\n| %AMC | 0.653 | 0.454 |\n| #PROCEDURES | 15.615 | 17.079 |\n| #SITES | 7.386 | 10.121 |\n\n**Table 2: Distribution of Outsourcing Across Trial Phase**\n\n| Trial Phase | Mean Outsourcing Level |\n|:---|---:|\n| PHASE I—Oncology | 0.094 |\n| PHASE II | 0.139 |\n| PHASE III | 0.148 |\n| PHASE IV | 0.204 |\n\n**Table 3: Distribution of Outsourcing Across Therapeutic Areas**\n\n| Therapeutic Area | Mean Outsourcing Level |\n|:---|---:|\n| Pain/anesthesia | 0.329 |\n| Oncology | 0.103 |\n\n### Question\n\nBased on the descriptive evidence in Tables 1, 2, and 3, which of the following statements are valid inferences that support the paper's multitask theory of outsourcing? Select all that apply.", "Answer": ["A", "C"], "qid": "310", "question": "### Background\n\n**Research Question.** This question examines the descriptive evidence supporting the paper's central theory that the make-or-buy decision for clinical trials is driven by the trade-off between 'knowledge production' and 'data production'.\n\n**Setting / Institutional Environment.** The paper's theory posits that firms insource knowledge-intensive projects and outsource data-intensive projects. Knowledge intensity is proxied by factors like the involvement of academic medical centers (`%AMC`), while data intensity is proxied by logistical complexity (`#SITES`, `#PROCEDURES`).\n\n### Data / Model Specification\n\n**Table 1: Project-Level Descriptive Statistics**\n\n| Variable | Mean, Insourced Projects | Mean, Outsourced Projects |\n|:---|---:|---:|\n| %AMC | 0.653 | 0.454 |\n| #PROCEDURES | 15.615 | 17.079 |\n| #SITES | 7.386 | 10.121 |\n\n**Table 2: Distribution of Outsourcing Across Trial Phase**\n\n| Trial Phase | Mean Outsourcing Level |\n|:---|---:|\n| PHASE I—Oncology | 0.094 |\n| PHASE II | 0.139 |\n| PHASE III | 0.148 |\n| PHASE IV | 0.204 |\n\n**Table 3: Distribution of Outsourcing Across Therapeutic Areas**\n\n| Therapeutic Area | Mean Outsourcing Level |\n|:---|---:|\n| Pain/anesthesia | 0.329 |\n| Oncology | 0.103 |\n\n### Question\n\nBased on the descriptive evidence in Tables 1, 2, and 3, which of the following statements are valid inferences that support the paper's multitask theory of outsourcing? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 311, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's core theoretical mechanism: how communication, when combined with different organizational structures, can foster “in-group bias” among insiders.\n\n**Setting / Institutional Environment.** The analysis uses data from three experimental treatments where communication between insiders (A and B) is allowed: a Vertical structure (VwC), a Horizontal-Consensus structure (H_co wC), and a Horizontal-Averaging structure (H_av wC). The key data source is the content of the electronic chats between insiders, which is coded for evidence of “bonding.”\n\n**Variables & Parameters.**\n*   `Structure_g`: The organizational structure (VwC, H_co wC, H_av wC) for group `g`.\n*   `Bonding_g`: An indicator variable, coded from chat data, equal to 1 if insiders in group `g` showed evidence of bonding, and 0 otherwise.\n*   `y_g`: The final production plan chosen by group `g`, representing the level of kindness to an outsider.\n*   Unit of Observation: A two-insider group (`g`) in a treatment with communication.\n\n---\n\n### Data / Model Specification\n\nThe paper proposes a two-stage causal chain to explain why flat, collaborative structures with communication might lead to less ethical outcomes:\n\n1.  **Stage 1 (Structure → Bonding):** Horizontal structures (H_co wC, H_av wC) are hypothesized to be more conducive to bonding than the vertical structure (VwC).\n2.  **Stage 2 (Bonding → Unethical Outcomes):** Groups where bonding occurs are hypothesized to choose a lower `y` due to in-group bias.\n\nThe empirical strategy involves coding the chat data to create the `Bonding_g` variable and then using statistical tests to check for the relationships predicted in both stages.\n\n---\n\n### Question\n\nBased on the paper's proposed causal chain and experimental design, select all statements that are correct.", "Answer": ["B", "C"], "qid": "311", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's core theoretical mechanism: how communication, when combined with different organizational structures, can foster “in-group bias” among insiders.\n\n**Setting / Institutional Environment.** The analysis uses data from three experimental treatments where communication between insiders (A and B) is allowed: a Vertical structure (VwC), a Horizontal-Consensus structure (H_co wC), and a Horizontal-Averaging structure (H_av wC). The key data source is the content of the electronic chats between insiders, which is coded for evidence of “bonding.”\n\n**Variables & Parameters.**\n*   `Structure_g`: The organizational structure (VwC, H_co wC, H_av wC) for group `g`.\n*   `Bonding_g`: An indicator variable, coded from chat data, equal to 1 if insiders in group `g` showed evidence of bonding, and 0 otherwise.\n*   `y_g`: The final production plan chosen by group `g`, representing the level of kindness to an outsider.\n*   Unit of Observation: A two-insider group (`g`) in a treatment with communication.\n\n---\n\n### Data / Model Specification\n\nThe paper proposes a two-stage causal chain to explain why flat, collaborative structures with communication might lead to less ethical outcomes:\n\n1.  **Stage 1 (Structure → Bonding):** Horizontal structures (H_co wC, H_av wC) are hypothesized to be more conducive to bonding than the vertical structure (VwC).\n2.  **Stage 2 (Bonding → Unethical Outcomes):** Groups where bonding occurs are hypothesized to choose a lower `y` due to in-group bias.\n\nThe empirical strategy involves coding the chat data to create the `Bonding_g` variable and then using statistical tests to check for the relationships predicted in both stages.\n\n---\n\n### Question\n\nBased on the paper's proposed causal chain and experimental design, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 312, "Question": "### Background\n\n**Research Question.** This problem analyzes the econometric challenge of identifying the causal effect of the paper's core theoretical mechanism: “in-group bias.”\n\n**Setting / Institutional Environment.** The analysis uses data from three experimental treatments where communication between insiders (A and B) is allowed: a Vertical structure (VwC), a Horizontal-Consensus structure (H_co wC), and a Horizontal-Averaging structure (H_av wC).\n\n**Variables & Parameters.**\n*   `Structure_g`: The organizational structure for group `g`.\n*   `Bonding_g`: An indicator variable, coded from chat data, equal to 1 if insiders in group `g` showed evidence of bonding, and 0 otherwise. This variable is considered endogenous because the decision to bond is a choice.\n*   `y_g`: The final production plan chosen by group `g`, representing the level of kindness to an outsider.\n\n---\n\n### Data / Model Specification\n\nTo estimate the causal effect of `Bonding_g` on the ethical outcome `y_g`, a researcher proposes an instrumental variable (IV) strategy. The proposed instrument, `Z_g`, is an indicator variable for the randomly assigned treatment: `Z_g = 1` if group `g` was assigned to a horizontal structure (H_co wC or H_av wC), and `Z_g = 0` if assigned to the vertical structure (VwC).\n\n---\n\n### Question\n\nSelect all statements that correctly describe the assumptions required for this IV strategy to be valid in this context.", "Answer": ["B", "D"], "qid": "312", "question": "### Background\n\n**Research Question.** This problem analyzes the econometric challenge of identifying the causal effect of the paper's core theoretical mechanism: “in-group bias.”\n\n**Setting / Institutional Environment.** The analysis uses data from three experimental treatments where communication between insiders (A and B) is allowed: a Vertical structure (VwC), a Horizontal-Consensus structure (H_co wC), and a Horizontal-Averaging structure (H_av wC).\n\n**Variables & Parameters.**\n*   `Structure_g`: The organizational structure for group `g`.\n*   `Bonding_g`: An indicator variable, coded from chat data, equal to 1 if insiders in group `g` showed evidence of bonding, and 0 otherwise. This variable is considered endogenous because the decision to bond is a choice.\n*   `y_g`: The final production plan chosen by group `g`, representing the level of kindness to an outsider.\n\n---\n\n### Data / Model Specification\n\nTo estimate the causal effect of `Bonding_g` on the ethical outcome `y_g`, a researcher proposes an instrumental variable (IV) strategy. The proposed instrument, `Z_g`, is an indicator variable for the randomly assigned treatment: `Z_g = 1` if group `g` was assigned to a horizontal structure (H_co wC or H_av wC), and `Z_g = 0` if assigned to the vertical structure (VwC).\n\n---\n\n### Question\n\nSelect all statements that correctly describe the assumptions required for this IV strategy to be valid in this context.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 313, "Question": "### Background\n\n**Research Question.** This problem applies the paper's core theoretical model of agent behavior, which incorporates psychological factors into economic decision-making, to a strategic setting.\n\n**Setting / Institutional Environment.** The model is applied to the Vertical (V) structure, where a superior (S) unilaterally sets a plan `y`, and a subordinate (P) can only accept it or quit (in which case all payoffs are zero).\n\n**Variables & Parameters.**\n*   `y`: The firm’s production plan, an integer `y ∈ {0, 1, ..., 10}`.\n*   `u_i`: The utility of an insider `i ∈ {A, B}`.\n*   `π_i`: Material payoff for player `i`.\n*   `θ_i = r_i w_i`: An insider's “effective social concern.”\n\n---\n\n### Data / Model Specification\n\nThe material payoffs are determined by the choice of `y`:\n  \nπ_A = π_B = 5 - 0.5y \\quad \\text{(Eq. (1))}\n \n  \nπ_C = 1.2 \\times \\min\\{y, 6\\} - 6 \\quad \\text{(Eq. (2))}\n \nThe insider's utility function is given by:\n  \nu_i = π_i + θ_i π_C \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nConsider the Vertical (V) structure where preferences are heterogeneous and common knowledge: Player A is highly ethical (`θ_A = 1`), while Player B is purely self-interested (`θ_B = 0`). Based on a Subgame Perfect Nash Equilibrium (SPNE) analysis, select all correct outcomes.", "Answer": ["A", "D"], "qid": "313", "question": "### Background\n\n**Research Question.** This problem applies the paper's core theoretical model of agent behavior, which incorporates psychological factors into economic decision-making, to a strategic setting.\n\n**Setting / Institutional Environment.** The model is applied to the Vertical (V) structure, where a superior (S) unilaterally sets a plan `y`, and a subordinate (P) can only accept it or quit (in which case all payoffs are zero).\n\n**Variables & Parameters.**\n*   `y`: The firm’s production plan, an integer `y ∈ {0, 1, ..., 10}`.\n*   `u_i`: The utility of an insider `i ∈ {A, B}`.\n*   `π_i`: Material payoff for player `i`.\n*   `θ_i = r_i w_i`: An insider's “effective social concern.”\n\n---\n\n### Data / Model Specification\n\nThe material payoffs are determined by the choice of `y`:\n  \nπ_A = π_B = 5 - 0.5y \\quad \\text{(Eq. (1))}\n \n  \nπ_C = 1.2 \\times \\min\\{y, 6\\} - 6 \\quad \\text{(Eq. (2))}\n \nThe insider's utility function is given by:\n  \nu_i = π_i + θ_i π_C \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nConsider the Vertical (V) structure where preferences are heterogeneous and common knowledge: Player A is highly ethical (`θ_A = 1`), while Player B is purely self-interested (`θ_B = 0`). Based on a Subgame Perfect Nash Equilibrium (SPNE) analysis, select all correct outcomes.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 314, "Question": "### Background\n\n**Research Question.** This problem analyzes the bargaining dynamics within a consensus-based group, focusing on the empirical test for “downward convergence,” a phenomenon where agreements tend to gravitate toward the less ethical initial proposal.\n\n**Setting / Institutional Environment.** The analysis focuses on the Horizontal-Consensus (H_co) treatment, where two insiders must bargain to reach a unanimous agreement on the production plan `y`.\n\n**Variables & Parameters.**\n*   `y_{i1}`: Player `i`'s first proposal for `y`.\n*   `y_N`: The final plan upon which consensus is reached.\n*   `y_{K1}`, `y_{U1}`: The first proposals of the “kind” (higher `y`) and “unkind” (lower `y`) players, respectively.\n*   `Δ = ρ_{unkind} - ρ_{kind}`: The test statistic for downward convergence.\n\n---\n\n### Data / Model Specification\n\nTo measure bargaining behavior, the authors define a resistance index for player `i`:\n  \nρ_i = \\frac{|y_N - y_{j1}|}{|y_{i1} - y_{j1}|} \\quad \\text{for } i \\neq j \\quad \\text{(Eq. (1))}\n \nBy construction, `ρ_{kind} + ρ_{unkind} = 1`. The test for downward convergence is a test of whether `Δ` is significantly greater than zero.\n\n---\n\n### Question\n\nIn the H_co treatment, the kind player (K) initially proposes `y_K1` and the unkind player (U) proposes `y_U1` (where `y_K1 > y_U1`). They reach a consensus `y_N`. Based on the definitions provided, select all statements that are mathematically correct.", "Answer": ["A", "B"], "qid": "314", "question": "### Background\n\n**Research Question.** This problem analyzes the bargaining dynamics within a consensus-based group, focusing on the empirical test for “downward convergence,” a phenomenon where agreements tend to gravitate toward the less ethical initial proposal.\n\n**Setting / Institutional Environment.** The analysis focuses on the Horizontal-Consensus (H_co) treatment, where two insiders must bargain to reach a unanimous agreement on the production plan `y`.\n\n**Variables & Parameters.**\n*   `y_{i1}`: Player `i`'s first proposal for `y`.\n*   `y_N`: The final plan upon which consensus is reached.\n*   `y_{K1}`, `y_{U1}`: The first proposals of the “kind” (higher `y`) and “unkind” (lower `y`) players, respectively.\n*   `Δ = ρ_{unkind} - ρ_{kind}`: The test statistic for downward convergence.\n\n---\n\n### Data / Model Specification\n\nTo measure bargaining behavior, the authors define a resistance index for player `i`:\n  \nρ_i = \\frac{|y_N - y_{j1}|}{|y_{i1} - y_{j1}|} \\quad \\text{for } i \\neq j \\quad \\text{(Eq. (1))}\n \nBy construction, `ρ_{kind} + ρ_{unkind} = 1`. The test for downward convergence is a test of whether `Δ` is significantly greater than zero.\n\n---\n\n### Question\n\nIn the H_co treatment, the kind player (K) initially proposes `y_K1` and the unkind player (U) proposes `y_U1` (where `y_K1 > y_U1`). They reach a consensus `y_N`. Based on the definitions provided, select all statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 315, "Question": "### Background\n\n**Research Question.** This problem analyzes a potential methodological issue in the paper's analysis of bargaining dynamics: sample selection bias.\n\n**Setting / Institutional Environment.** The analysis of bargaining dynamics in the Horizontal-Consensus (H_co) treatment is conducted on the subsample of groups that did not reach an immediate consensus. This procedure excludes homogeneous pairs who agree on their first proposal (e.g., two very kind players who both propose `y=6`, or two very unkind players who both propose `y=0`).\n\n---\n\n### Question\n\nRegarding the potential for sample selection bias from excluding these homogeneous pairs, select all statements that are INCORRECT.", "Answer": ["C", "D"], "qid": "315", "question": "### Background\n\n**Research Question.** This problem analyzes a potential methodological issue in the paper's analysis of bargaining dynamics: sample selection bias.\n\n**Setting / Institutional Environment.** The analysis of bargaining dynamics in the Horizontal-Consensus (H_co) treatment is conducted on the subsample of groups that did not reach an immediate consensus. This procedure excludes homogeneous pairs who agree on their first proposal (e.g., two very kind players who both propose `y=6`, or two very unkind players who both propose `y=0`).\n\n---\n\n### Question\n\nRegarding the potential for sample selection bias from excluding these homogeneous pairs, select all statements that are INCORRECT.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 316, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical properties of the Lasso-type GMM estimator for the standard, well-identified case. It investigates the estimator's construction, its consistency, and its key asymptotic feature—the 'oracle property,' which allows for simultaneous model selection and efficient estimation.\n\n### Data / Model Specification\n\nThe Lasso-type GMM estimator `θ̂_T` minimizes a penalized objective function over the compact parameter space `Θ`:\n\n  \nU_{T}(\\theta)=\\left[T^{-1/2}\\sum_{t=1}^{T}\\psi_{t}(\\theta)\\right]^{\\prime}W_{T}(\\theta)\\left[T^{-1/2}\\sum_{t=1}^{T}\\psi_{t}(\\theta)\\right]+\\lambda_{T}\\sum_{j=1}^{p}|\\theta_{j}|^{\\gamma}\n\n\t(Eq. 1)\n \n\nwhere `λ_T` is a positive regularization parameter and `0 < γ < 1`. The analysis relies on the following key results:\n\n- **Theorem 1 (Consistency):** If `λ_T = o(T)`, then `θ̂_T ᵖ→ θ₀`.\n- **Theorem 2 (Asymptotic Distribution & Oracle Property):** If the tuning parameter grows at the rate `λ_T / T^(γ/2) → λ₀ ≥ 0`, the estimator correctly identifies zero-valued parameters (`θ_j₀ = 0`) while estimating non-zero parameters (`θ_j₀ ≠ 0`) efficiently and without asymptotic bias.\n\n### Question\n\nBased on the theoretical results presented, select all statements that **INCORRECTLY** describe the properties of the Lasso-type GMM estimator or the role of the regularization parameter `λ_T`.", "Answer": ["B", "D"], "qid": "316", "question": "### Background\n\n**Research Question.** This problem examines the theoretical properties of the Lasso-type GMM estimator for the standard, well-identified case. It investigates the estimator's construction, its consistency, and its key asymptotic feature—the 'oracle property,' which allows for simultaneous model selection and efficient estimation.\n\n### Data / Model Specification\n\nThe Lasso-type GMM estimator `θ̂_T` minimizes a penalized objective function over the compact parameter space `Θ`:\n\n  \nU_{T}(\\theta)=\\left[T^{-1/2}\\sum_{t=1}^{T}\\psi_{t}(\\theta)\\right]^{\\prime}W_{T}(\\theta)\\left[T^{-1/2}\\sum_{t=1}^{T}\\psi_{t}(\\theta)\\right]+\\lambda_{T}\\sum_{j=1}^{p}|\\theta_{j}|^{\\gamma}\n\n\t(Eq. 1)\n \n\nwhere `λ_T` is a positive regularization parameter and `0 < γ < 1`. The analysis relies on the following key results:\n\n- **Theorem 1 (Consistency):** If `λ_T = o(T)`, then `θ̂_T ᵖ→ θ₀`.\n- **Theorem 2 (Asymptotic Distribution & Oracle Property):** If the tuning parameter grows at the rate `λ_T / T^(γ/2) → λ₀ ≥ 0`, the estimator correctly identifies zero-valued parameters (`θ_j₀ = 0`) while estimating non-zero parameters (`θ_j₀ ≠ 0`) efficiently and without asymptotic bias.\n\n### Question\n\nBased on the theoretical results presented, select all statements that **INCORRECTLY** describe the properties of the Lasso-type GMM estimator or the role of the regularization parameter `λ_T`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 317, "Question": "### Background\n\n**Research Question.** This problem analyzes the performance of the Lasso-type GMM estimator under conditions of weak identification, where moment conditions provide limited information for parameter estimation. The key finding is that the estimator's 'oracle property' is preserved, but at the cost of a slower rate of convergence.\n\n### Data / Model Specification\n\nThe strength of identification is modeled by a sequence `a_T` that grows with the sample size `T`. The standard case corresponds to `a_T = 1`, while weak identification corresponds to `a_T → ∞` with `a_T = o(T¹/²)`. Under weak identification, the estimator `θ̂_T` converges at a slower rate `r_T`, defined as:\n\n  \nr_T = T^{1/2} / a_T\n \n\n**Theorem 3.** Under weak identification, the estimator `r_T(θ̂_T - θ₀)` converges to a well-defined distribution that preserves the oracle property. The paper notes that the case `a_T = T¹/²` corresponds to the weak identification setup of Stock and Wright, under which the Lasso-type GMM estimator is not consistent.\n\n### Question\n\nBased on the provided framework for weak identification, select all statements that are correct.", "Answer": ["C", "D"], "qid": "317", "question": "### Background\n\n**Research Question.** This problem analyzes the performance of the Lasso-type GMM estimator under conditions of weak identification, where moment conditions provide limited information for parameter estimation. The key finding is that the estimator's 'oracle property' is preserved, but at the cost of a slower rate of convergence.\n\n### Data / Model Specification\n\nThe strength of identification is modeled by a sequence `a_T` that grows with the sample size `T`. The standard case corresponds to `a_T = 1`, while weak identification corresponds to `a_T → ∞` with `a_T = o(T¹/²)`. Under weak identification, the estimator `θ̂_T` converges at a slower rate `r_T`, defined as:\n\n  \nr_T = T^{1/2} / a_T\n \n\n**Theorem 3.** Under weak identification, the estimator `r_T(θ̂_T - θ₀)` converges to a well-defined distribution that preserves the oracle property. The paper notes that the case `a_T = T¹/²` corresponds to the weak identification setup of Stock and Wright, under which the Lasso-type GMM estimator is not consistent.\n\n### Question\n\nBased on the provided framework for weak identification, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 318, "Question": "### Background\n\nAn analysis of Canadian youth employment from 1988-1990 investigates how the minimum wage's effect differs between 'permanent' low-wage workers (those with persistent low-wage histories) and 'transitory' low-wage workers (e.g., students in summer jobs). The 'treatment group' consists of individuals whose wage at time $t-1$ was between the old and new minimum wage during a provincial wage hike at time $t$. The 'control group' consists of low-wage workers in provinces with no wage hike.\n\n### Data / Model Specification\n\nThe re-employment probability is estimated using OLS on different subsamples of teenage workers. The key coefficient, $\\beta_1$, measures the effect of being 'at-risk' of a minimum wage increase on the probability of re-employment.\n\n**Table 1: Minimum Wage Effect on Re-employment Probability (Teens, Low-Wage Sample)**\n| | (1) OLS | (2) OLS | (3) OLS |\n|:---|:---:|:---:|:---:|\n| **Sample Used** | Full Low-Wage Sample | 'FE Sample' (Permanent) | 'Excluded Sample' (Transitory) |\n| **Control Group** | Low-Wage | Low-Wage | Low-Wage |\n| $AtRisk_{it}$ ($\"\\beta_1\"$) | -0.005 | -0.068** | +0.090** |\n| | (0.025) | (0.029) | (0.040) |\n\n*Notes: Standard errors in parentheses. ** p<0.05. The 'FE Sample' (permanent workers) constitutes 60% of the full low-wage sample, while the 'Excluded Sample' (transitory workers) is the remaining 40%.*\n\nBased on the results in Table 1, which of the following statements are valid interpretations or calculations?", "Answer": ["A", "B", "D"], "qid": "318", "question": "### Background\n\nAn analysis of Canadian youth employment from 1988-1990 investigates how the minimum wage's effect differs between 'permanent' low-wage workers (those with persistent low-wage histories) and 'transitory' low-wage workers (e.g., students in summer jobs). The 'treatment group' consists of individuals whose wage at time $t-1$ was between the old and new minimum wage during a provincial wage hike at time $t$. The 'control group' consists of low-wage workers in provinces with no wage hike.\n\n### Data / Model Specification\n\nThe re-employment probability is estimated using OLS on different subsamples of teenage workers. The key coefficient, $\\beta_1$, measures the effect of being 'at-risk' of a minimum wage increase on the probability of re-employment.\n\n**Table 1: Minimum Wage Effect on Re-employment Probability (Teens, Low-Wage Sample)**\n| | (1) OLS | (2) OLS | (3) OLS |\n|:---|:---:|:---:|:---:|\n| **Sample Used** | Full Low-Wage Sample | 'FE Sample' (Permanent) | 'Excluded Sample' (Transitory) |\n| **Control Group** | Low-Wage | Low-Wage | Low-Wage |\n| $AtRisk_{it}$ ($\"\\beta_1\"$) | -0.005 | -0.068** | +0.090** |\n| | (0.025) | (0.029) | (0.040) |\n\n*Notes: Standard errors in parentheses. ** p<0.05. The 'FE Sample' (permanent workers) constitutes 60% of the full low-wage sample, while the 'Excluded Sample' (transitory workers) is the remaining 40%.*\n\nBased on the results in Table 1, which of the following statements are valid interpretations or calculations?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn analysis of Canadian youth empl...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn analysis of Canadian youth empl...", "error": null}}
{"ID": 319, "Question": "### Background\n\nA study of Canadian youth employment (1988-1990) investigates the sensitivity of minimum wage effect estimates to the choice of control group. The analysis focuses on 'transitory' low-wage workers (e.g., students in summer jobs), who have fewer than three quarters of low-wage employment history.\n\n### Data / Model Specification\n\nTwo OLS models are estimated for the same group of 'at-risk' transitory teenage workers. The models differ only in the control group used for comparison.\n\n**Table 1: Minimum Wage Effect on 'Transitory' Low-Wage Workers (Teens)**\n| | (1) Low-Wage Controls | (2) High-Wage Controls |\n|:---|:---:|:---:|\n| $AtRisk_{it}$ | +0.090** | -0.085*** |\n| | (0.040) | (0.025) |\n\n*Notes: *** p<0.01, ** p<0.05. The treatment group is identical in both models.*\n\nThe true model for re-employment is assumed to be $E_{it} = \\beta_{true} AtRisk_{it} + \\gamma H_i + u_{it}$, where $H_i$ is an unobserved indicator for being a 'high-wage type' with higher employment stability ($\"\\gamma > 0\"$). The regression in Column (2) omits $H_i$, leading to potential omitted variable bias.\n\nGiven this information, which of the following statements are correct conclusions about the results in Table 1?", "Answer": ["A", "C", "D"], "qid": "319", "question": "### Background\n\nA study of Canadian youth employment (1988-1990) investigates the sensitivity of minimum wage effect estimates to the choice of control group. The analysis focuses on 'transitory' low-wage workers (e.g., students in summer jobs), who have fewer than three quarters of low-wage employment history.\n\n### Data / Model Specification\n\nTwo OLS models are estimated for the same group of 'at-risk' transitory teenage workers. The models differ only in the control group used for comparison.\n\n**Table 1: Minimum Wage Effect on 'Transitory' Low-Wage Workers (Teens)**\n| | (1) Low-Wage Controls | (2) High-Wage Controls |\n|:---|:---:|:---:|\n| $AtRisk_{it}$ | +0.090** | -0.085*** |\n| | (0.040) | (0.025) |\n\n*Notes: *** p<0.01, ** p<0.05. The treatment group is identical in both models.*\n\nThe true model for re-employment is assumed to be $E_{it} = \\beta_{true} AtRisk_{it} + \\gamma H_i + u_{it}$, where $H_i$ is an unobserved indicator for being a 'high-wage type' with higher employment stability ($\"\\gamma > 0\"$). The regression in Column (2) omits $H_i$, leading to potential omitted variable bias.\n\nGiven this information, which of the following statements are correct conclusions about the results in Table 1?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study of Canadian youth employme...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study of Canadian youth employme...", "error": null}}
{"ID": 320, "Question": "### Background\n\nA study uses Canadian panel data (1988-1990) to estimate the effect of minimum wage increases on youth re-employment probability. The 'traditional' approach compares 'at-risk' low-wage workers to a control group that includes high-wage workers.\n\n### Data / Model Specification\n\nThe re-employment probability is modeled as:\n\n  \nE_{it} = \\beta_1 AtRisk_{it} + X_{it}'\\Gamma + \\alpha_i + \\varepsilon_{it}\n \n\nwhere:\n- $E_{it}$: Binary re-employment outcome for individual $i$ at time $t$.\n- $AtRisk_{it}$: Binary treatment indicator.\n- $X_{it}$: Observable time-varying controls.\n- $\\alpha_i$: An unobserved, time-invariant individual-specific effect (e.g., ability, motivation).\n\nThis model can be estimated using Pooled OLS, Fixed Effects (FE), or Random Effects (RE).\n\nWhich of the following statements about the estimation of this model are methodologically correct in the context of this research?", "Answer": ["A", "C", "D"], "qid": "320", "question": "### Background\n\nA study uses Canadian panel data (1988-1990) to estimate the effect of minimum wage increases on youth re-employment probability. The 'traditional' approach compares 'at-risk' low-wage workers to a control group that includes high-wage workers.\n\n### Data / Model Specification\n\nThe re-employment probability is modeled as:\n\n  \nE_{it} = \\beta_1 AtRisk_{it} + X_{it}'\\Gamma + \\alpha_i + \\varepsilon_{it}\n \n\nwhere:\n- $E_{it}$: Binary re-employment outcome for individual $i$ at time $t$.\n- $AtRisk_{it}$: Binary treatment indicator.\n- $X_{it}$: Observable time-varying controls.\n- $\\alpha_i$: An unobserved, time-invariant individual-specific effect (e.g., ability, motivation).\n\nThis model can be estimated using Pooled OLS, Fixed Effects (FE), or Random Effects (RE).\n\nWhich of the following statements about the estimation of this model are methodologically correct in the context of this research?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA study uses Canadian panel data (...", "model_answer": "This is a detailed response to the question about ### Background\n\nA study uses Canadian panel data (...", "error": null}}
{"ID": 321, "Question": "### Background\n\n**Research Question.** This problem investigates the novel equilibrium phenomena that arise in a search market where objects have two attributes, but searchers can only observe one. Specifically, it explores how this informational friction generates a discontinuous payoff function.\n\n**Setting / Institutional Environment.** The market is in a symmetric, diversified steady-state equilibrium. This means the entry distributions of attributes are identical (`H_1=H_2=H`), searchers are split equally between inspecting attribute 1 and 2 (`g_1=g_2=1/2`), and all searchers employ the same acceptance threshold `\\underline{x}`. An object is accepted if the observed attribute `x_i` satisfies `x_i \\ge \\underline{x}`.\n\n### Data / Model Specification\n\nA key result of the model (Claim 1) is that the expected value of the unobserved attribute is discontinuous at the threshold `\\underline{x}`. The steady-state distribution of attributes is denoted by `F`, which differs from the entry distribution `H`.\n\n  \nE_{F}(x_{2}\\mid x_{1}<\\underline{x}) < E_{F}(x_{2}\\mid x_{1}>\\underline{x})\n \n\nThis means the expected payoff to a searcher, `x_1 + E_F(x_2|x_1)`, jumps up at `x_1 = \\underline{x}`.\n\n### Question\n\nIn the symmetric threshold equilibrium described, which of the following statements correctly describe the mechanism causing the discontinuity in the conditional expectation `E_F(x_2|x_1)` at the threshold `\\underline{x}`?", "Answer": ["B", "C"], "qid": "321", "question": "### Background\n\n**Research Question.** This problem investigates the novel equilibrium phenomena that arise in a search market where objects have two attributes, but searchers can only observe one. Specifically, it explores how this informational friction generates a discontinuous payoff function.\n\n**Setting / Institutional Environment.** The market is in a symmetric, diversified steady-state equilibrium. This means the entry distributions of attributes are identical (`H_1=H_2=H`), searchers are split equally between inspecting attribute 1 and 2 (`g_1=g_2=1/2`), and all searchers employ the same acceptance threshold `\\underline{x}`. An object is accepted if the observed attribute `x_i` satisfies `x_i \\ge \\underline{x}`.\n\n### Data / Model Specification\n\nA key result of the model (Claim 1) is that the expected value of the unobserved attribute is discontinuous at the threshold `\\underline{x}`. The steady-state distribution of attributes is denoted by `F`, which differs from the entry distribution `H`.\n\n  \nE_{F}(x_{2}\\mid x_{1}<\\underline{x}) < E_{F}(x_{2}\\mid x_{1}>\\underline{x})\n \n\nThis means the expected payoff to a searcher, `x_1 + E_F(x_2|x_1)`, jumps up at `x_1 = \\underline{x}`.\n\n### Question\n\nIn the symmetric threshold equilibrium described, which of the following statements correctly describe the mechanism causing the discontinuity in the conditional expectation `E_F(x_2|x_1)` at the threshold `\\underline{x}`?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 322, "Question": "### Background\n\n**Research Question.** This problem investigates the novel equilibrium phenomena that arise in a search market where objects have two attributes, but searchers can only observe one. Specifically, it explores how an informational friction generates a discontinuous payoff function, leading to a multiplicity of equilibria and the possibility of non-monotonic acceptance strategies.\n\n**Setting / Institutional Environment.** The market is in a symmetric, diversified steady-state equilibrium. This means the entry distributions of attributes are identical (`H_1=H_2=H`), searchers are split equally between inspecting attribute 1 and 2 (`g_1=g_2=1/2`), and all searchers employ the same acceptance threshold `\\underline{x}`.\n\n### Data / Model Specification\n\nThe expected payoff to a searcher, `x_1 + E_F(x_2|x_1)`, jumps up at `x_1 = \\underline{x}`. For a threshold `\\underline{x}` to be part of a symmetric equilibrium, the continuation value of searching, `(1-d)V`, must satisfy:\n\n  \n\\underline{x}+E_{F}(x_{j}|x_{i}<\\underline{x}) \\leq (1-d)V \\leq \\underline{x}+E_{F}(x_{j}|x_{i}>\\underline{x})\n \n\nProposition 2 in the paper states that for a low enough death rate (`d < 1/2`), there exists a non-degenerate interval of equilibrium thresholds `[x^-, x^+]` where these conditions hold. For any `\\underline{x}` in the interior of this interval, both inequalities are strict.\n\n### Question\n\nAccording to the paper, which of the following are direct consequences of the payoff discontinuity at the equilibrium threshold `\\underline{x}`?", "Answer": ["B", "C"], "qid": "322", "question": "### Background\n\n**Research Question.** This problem investigates the novel equilibrium phenomena that arise in a search market where objects have two attributes, but searchers can only observe one. Specifically, it explores how an informational friction generates a discontinuous payoff function, leading to a multiplicity of equilibria and the possibility of non-monotonic acceptance strategies.\n\n**Setting / Institutional Environment.** The market is in a symmetric, diversified steady-state equilibrium. This means the entry distributions of attributes are identical (`H_1=H_2=H`), searchers are split equally between inspecting attribute 1 and 2 (`g_1=g_2=1/2`), and all searchers employ the same acceptance threshold `\\underline{x}`.\n\n### Data / Model Specification\n\nThe expected payoff to a searcher, `x_1 + E_F(x_2|x_1)`, jumps up at `x_1 = \\underline{x}`. For a threshold `\\underline{x}` to be part of a symmetric equilibrium, the continuation value of searching, `(1-d)V`, must satisfy:\n\n  \n\\underline{x}+E_{F}(x_{j}|x_{i}<\\underline{x}) \\leq (1-d)V \\leq \\underline{x}+E_{F}(x_{j}|x_{i}>\\underline{x})\n \n\nProposition 2 in the paper states that for a low enough death rate (`d < 1/2`), there exists a non-degenerate interval of equilibrium thresholds `[x^-, x^+]` where these conditions hold. For any `\\underline{x}` in the interior of this interval, both inequalities are strict.\n\n### Question\n\nAccording to the paper, which of the following are direct consequences of the payoff discontinuity at the equilibrium threshold `\\underline{x}`?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 323, "Question": "### Background\n\n**Research Question.** This problem analyzes the benchmark search model where all searchers observe the same attribute, focusing on the welfare properties of the equilibrium.\n\n**Setting / Institutional Environment.** In a steady-state search market, all searchers observe attribute `x_1` and employ a common threshold strategy `A^{\\underline{x}}`. Unmatched participants face a per-period probability of exogenous exit (\"death\") `d`. Social welfare is defined as the expected lifetime utility of a representative searcher.\n\n### Data / Model Specification\n\nThe social welfare as a function of a common threshold `\\underline{x}` is given by `W(\\underline{x}) = V_1(A^{\\underline{x}}; F^{\\underline{x}})`:\n\n  \nW(\\underline{x}) = \\int_{\\underline{x}}^1 x_{1}d H_{1}(x_{1})+[1-H_{1}(\\underline{{x}})]E_{H_{2}}(x_{2}) \\quad \\text{(Eq. (1))}\n \n\nAn equilibrium threshold `\\underline{x}^e` is determined by the searcher's private optimality condition, while the socially optimal threshold `\\underline{x}^w` maximizes `W(\\underline{x})`.\n\n### Question\n\nIn the benchmark model where all searchers observe attribute `x_1`, which of the following statements about the equilibrium and its welfare properties are INCORRECT?", "Answer": ["A", "B"], "qid": "323", "question": "### Background\n\n**Research Question.** This problem analyzes the benchmark search model where all searchers observe the same attribute, focusing on the welfare properties of the equilibrium.\n\n**Setting / Institutional Environment.** In a steady-state search market, all searchers observe attribute `x_1` and employ a common threshold strategy `A^{\\underline{x}}`. Unmatched participants face a per-period probability of exogenous exit (\"death\") `d`. Social welfare is defined as the expected lifetime utility of a representative searcher.\n\n### Data / Model Specification\n\nThe social welfare as a function of a common threshold `\\underline{x}` is given by `W(\\underline{x}) = V_1(A^{\\underline{x}}; F^{\\underline{x}})`:\n\n  \nW(\\underline{x}) = \\int_{\\underline{x}}^1 x_{1}d H_{1}(x_{1})+[1-H_{1}(\\underline{{x}})]E_{H_{2}}(x_{2}) \\quad \\text{(Eq. (1))}\n \n\nAn equilibrium threshold `\\underline{x}^e` is determined by the searcher's private optimality condition, while the socially optimal threshold `\\underline{x}^w` maximizes `W(\\underline{x})`.\n\n### Question\n\nIn the benchmark model where all searchers observe attribute `x_1`, which of the following statements about the equilibrium and its welfare properties are INCORRECT?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 324, "Question": "### Background\n\n**Research Question.** This problem investigates the decomposition of sovereign credit risk into systemic and sovereign-specific components, and how these components differ between the U.S. and Eurozone.\n\n**Setting / Institutional Environment.** The analysis uses a multivariate credit risk model to estimate the components of sovereign default risk from Credit Default Swap (CDS) spreads. The model assumes a sovereign's total default risk is the sum of a sovereign-specific component and its exposure to a common systemic component. For the U.S. states, the systemic component is identified from U.S. Treasury CDS spreads. For Eurozone countries, it is identified from German Bund CDS spreads.\n\n**Variables & Parameters.**\n- `Total Default Intensity`: The instantaneous risk-neutral probability of default for a sovereign, `γ_i λ_t + ξ_it`.\n- `Systemic Component`: The portion of total risk attributable to the common factor, `γ_i λ_t`.\n- `Sovereign-Specific Component`: The portion of total risk attributable to idiosyncratic factors, `ξ_it`.\n- `Systemic Index (γ_i)`: A sovereign's sensitivity or vulnerability to the common systemic shock. It is normalized to 1.000 for the U.S. Treasury and Germany.\n- `Percentage Systemic Component`: The ratio of the systemic component to the total default intensity, `(γ_i λ_t) / (γ_i λ_t + ξ_it)`, averaged over the sample period.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Systemic Default Indexes (γ_i)**\n\n| Sovereign   | Systemic Index |\n|-------------|----------------|\n| California  | 2.647          |\n| Illinois    | 0.000          |\n| New York    | 0.000          |\n| USA         | 1.000          |\n| Greece      | 4.688          |\n| Italy       | 1.710          |\n| Germany     | 1.000          |\n\n*Source: Adapted from original paper's Table 4. The index measures systemic sensitivity relative to the U.S. Treasury (for states) or Germany (for Eurozone countries).* \n\n**Table 2: Average Percentage Systemic Component of Total Default Risk**\n\n| Sovereign   | Mean (%) |\n|-------------|----------|\n| California  | 36.78    |\n| Illinois    | 0.00     |\n| New York    | 0.00     |\n| USA         | 100.00   |\n| Greece      | 44.48    |\n| Italy       | 31.84    |\n| Germany     | 100.00   |\n\n*Source: Adapted from original paper's Table 5. This is the time-series average of the systemic component as a percentage of total credit risk.* \n\n---\n\nBased on the provided data and model, which of the following statements are valid interpretations or conclusions?", "Answer": ["A", "B", "D"], "qid": "324", "question": "### Background\n\n**Research Question.** This problem investigates the decomposition of sovereign credit risk into systemic and sovereign-specific components, and how these components differ between the U.S. and Eurozone.\n\n**Setting / Institutional Environment.** The analysis uses a multivariate credit risk model to estimate the components of sovereign default risk from Credit Default Swap (CDS) spreads. The model assumes a sovereign's total default risk is the sum of a sovereign-specific component and its exposure to a common systemic component. For the U.S. states, the systemic component is identified from U.S. Treasury CDS spreads. For Eurozone countries, it is identified from German Bund CDS spreads.\n\n**Variables & Parameters.**\n- `Total Default Intensity`: The instantaneous risk-neutral probability of default for a sovereign, `γ_i λ_t + ξ_it`.\n- `Systemic Component`: The portion of total risk attributable to the common factor, `γ_i λ_t`.\n- `Sovereign-Specific Component`: The portion of total risk attributable to idiosyncratic factors, `ξ_it`.\n- `Systemic Index (γ_i)`: A sovereign's sensitivity or vulnerability to the common systemic shock. It is normalized to 1.000 for the U.S. Treasury and Germany.\n- `Percentage Systemic Component`: The ratio of the systemic component to the total default intensity, `(γ_i λ_t) / (γ_i λ_t + ξ_it)`, averaged over the sample period.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Systemic Default Indexes (γ_i)**\n\n| Sovereign   | Systemic Index |\n|-------------|----------------|\n| California  | 2.647          |\n| Illinois    | 0.000          |\n| New York    | 0.000          |\n| USA         | 1.000          |\n| Greece      | 4.688          |\n| Italy       | 1.710          |\n| Germany     | 1.000          |\n\n*Source: Adapted from original paper's Table 4. The index measures systemic sensitivity relative to the U.S. Treasury (for states) or Germany (for Eurozone countries).* \n\n**Table 2: Average Percentage Systemic Component of Total Default Risk**\n\n| Sovereign   | Mean (%) |\n|-------------|----------|\n| California  | 36.78    |\n| Illinois    | 0.00     |\n| New York    | 0.00     |\n| USA         | 100.00   |\n| Greece      | 44.48    |\n| Italy       | 31.84    |\n| Germany     | 100.00   |\n\n*Source: Adapted from original paper's Table 5. This is the time-series average of the systemic component as a percentage of total credit risk.* \n\n---\n\nBased on the provided data and model, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 325, "Question": "### Background\n\n**Research Question.** This problem concerns the theoretical modeling of a sovereign's default probability, which is composed of both idiosyncratic and systemic risk components.\n\n**Setting / Institutional Environment.** The setting is a continuous-time, reduced-form credit risk model. A sovereign `i` can default due to two independent types of shocks, modeled as arrivals of Poisson processes. An idiosyncratic shock triggers default with certainty. A systemic shock triggers default with a sovereign-specific probability.\n\n**Variables & Parameters.**\n- `ξ_it`: The intensity (arrival rate) of the sovereign-specific Poisson process for sovereign `i` at time `t`.\n- `λ_t`: The intensity (arrival rate) of the common systemic Poisson process at time `t`.\n- `γ_i`: A constant, sovereign-specific parameter representing the probability of default for sovereign `i`, conditional on the arrival of a systemic shock.\n\n---\n\n### Data / Model Specification\n\nThe total instantaneous probability of default for sovereign `i` at time `t`, also known as the total intensity `h_it`, is given by:\n  \nh_{it} = \\gamma_{i}\\lambda_{t} + \\xi_{it} \n\\quad \\text{(Eq. (1))}\n \nThe model is estimated by identifying the systemic intensity `λ_t` with the default intensity of a benchmark sovereign (U.S. Treasury for states, Germany for Eurozone countries), which is assumed to have zero idiosyncratic risk (`ξ_benchmark,t = 0`).\n\n---\n\nBased on the model's structure and identification strategy, which of the following statements are valid limitations or interpretations of the model's findings?", "Answer": ["B", "C", "D"], "qid": "325", "question": "### Background\n\n**Research Question.** This problem concerns the theoretical modeling of a sovereign's default probability, which is composed of both idiosyncratic and systemic risk components.\n\n**Setting / Institutional Environment.** The setting is a continuous-time, reduced-form credit risk model. A sovereign `i` can default due to two independent types of shocks, modeled as arrivals of Poisson processes. An idiosyncratic shock triggers default with certainty. A systemic shock triggers default with a sovereign-specific probability.\n\n**Variables & Parameters.**\n- `ξ_it`: The intensity (arrival rate) of the sovereign-specific Poisson process for sovereign `i` at time `t`.\n- `λ_t`: The intensity (arrival rate) of the common systemic Poisson process at time `t`.\n- `γ_i`: A constant, sovereign-specific parameter representing the probability of default for sovereign `i`, conditional on the arrival of a systemic shock.\n\n---\n\n### Data / Model Specification\n\nThe total instantaneous probability of default for sovereign `i` at time `t`, also known as the total intensity `h_it`, is given by:\n  \nh_{it} = \\gamma_{i}\\lambda_{t} + \\xi_{it} \n\\quad \\text{(Eq. (1))}\n \nThe model is estimated by identifying the systemic intensity `λ_t` with the default intensity of a benchmark sovereign (U.S. Treasury for states, Germany for Eurozone countries), which is assumed to have zero idiosyncratic risk (`ξ_benchmark,t = 0`).\n\n---\n\nBased on the model's structure and identification strategy, which of the following statements are valid limitations or interpretations of the model's findings?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 326, "Question": "### Background\n\nIn econometric forecasting, the ideal predictive density, `$f(y_f|\\theta)$`, is unattainable because the true parameters `$\\theta$` are unknown. A common practice is to use a \"certainty equivalence\" (CEQ) predictor, `$f(y_f|\\hat{\\theta}_d)$`, which simply substitutes a point estimate `$\\hat{\\theta}_d$` for `$\\theta$`. However, this approach ignores the uncertainty inherent in the estimate `$\\hat{\\theta}_d$`. The paper proposes an asymptotic predictive likelihood function that corrects the CEQ predictor for this uncertainty.\n\n### Data / Model Specification\n\nThe proposed asymptotic predictive likelihood function is defined as:\n  \n\\mathrm{plik}^{a}\\left(y_{f}\\vert\\hat{\\theta}_{d}\\right)=f\\big(y_{f}\\vert\\hat{\\theta}_{d}\\big)\\cdot\\mathrm{exp}\\left\\{w_{1}\\big(y_{f};\\hat{\\theta}_{d}\\big)+w_{2}\\big(y_{f};\\hat{\\theta}_{d}\\big)\\right\\} \\quad \\text{(Eq. 1)}\n \nwhere the correction terms are:\n  \nw_{1}\\big(y_{f};\\widehat{\\theta}_{d}\\big)=-\\frac{1}{2}\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)H\\big(y_{d+f};\\widehat{\\theta}_{d}\\big)^{-1}\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)^{\\prime} \\quad \\text{(Eq. 2)}\n \n  \nw_{2}\\big(y_{f};\\widehat{\\theta}_{d}\\big)=\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)\\psi\\big(\\widehat{\\theta}_{d}\\big)-\\frac{1}{2}\\operatorname{tr}\\Big[H\\big(y_{f};\\widehat{\\theta}_{d}\\big)H\\big(y_{d};\\widehat{\\theta}_{d}\\big)^{-1}\\Big] \\quad \\text{(Eq. 3)}\n \nHere, `$\\nabla(y_f; \\hat{\\theta}_d)$` is the log-gradient (score vector) of the future density, `$H(\\cdot)$` is the log-Hessian, and `$\\psi(\\hat{\\theta}_d)$` is the `$O(m^{-1})$` asymptotic bias in the MLE `$\\hat{\\theta}_d$`.\n\n---\n\nBased on the provided model, select all of the following statements that provide a correct interpretation of the components of the asymptotic predictive likelihood function.", "Answer": ["B", "C", "D"], "qid": "326", "question": "### Background\n\nIn econometric forecasting, the ideal predictive density, `$f(y_f|\\theta)$`, is unattainable because the true parameters `$\\theta$` are unknown. A common practice is to use a \"certainty equivalence\" (CEQ) predictor, `$f(y_f|\\hat{\\theta}_d)$`, which simply substitutes a point estimate `$\\hat{\\theta}_d$` for `$\\theta$`. However, this approach ignores the uncertainty inherent in the estimate `$\\hat{\\theta}_d$`. The paper proposes an asymptotic predictive likelihood function that corrects the CEQ predictor for this uncertainty.\n\n### Data / Model Specification\n\nThe proposed asymptotic predictive likelihood function is defined as:\n  \n\\mathrm{plik}^{a}\\left(y_{f}\\vert\\hat{\\theta}_{d}\\right)=f\\big(y_{f}\\vert\\hat{\\theta}_{d}\\big)\\cdot\\mathrm{exp}\\left\\{w_{1}\\big(y_{f};\\hat{\\theta}_{d}\\big)+w_{2}\\big(y_{f};\\hat{\\theta}_{d}\\big)\\right\\} \\quad \\text{(Eq. 1)}\n \nwhere the correction terms are:\n  \nw_{1}\\big(y_{f};\\widehat{\\theta}_{d}\\big)=-\\frac{1}{2}\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)H\\big(y_{d+f};\\widehat{\\theta}_{d}\\big)^{-1}\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)^{\\prime} \\quad \\text{(Eq. 2)}\n \n  \nw_{2}\\big(y_{f};\\widehat{\\theta}_{d}\\big)=\\nabla\\big(y_{f};\\widehat{\\theta}_{d}\\big)\\psi\\big(\\widehat{\\theta}_{d}\\big)-\\frac{1}{2}\\operatorname{tr}\\Big[H\\big(y_{f};\\widehat{\\theta}_{d}\\big)H\\big(y_{d};\\widehat{\\theta}_{d}\\big)^{-1}\\Big] \\quad \\text{(Eq. 3)}\n \nHere, `$\\nabla(y_f; \\hat{\\theta}_d)$` is the log-gradient (score vector) of the future density, `$H(\\cdot)$` is the log-Hessian, and `$\\psi(\\hat{\\theta}_d)$` is the `$O(m^{-1})$` asymptotic bias in the MLE `$\\hat{\\theta}_d$`.\n\n---\n\nBased on the provided model, select all of the following statements that provide a correct interpretation of the components of the asymptotic predictive likelihood function.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nIn econometric forecasting, the id...", "model_answer": "This is a detailed response to the question about ### Background\n\nIn econometric forecasting, the id...", "error": null}}
{"ID": 327, "Question": "### Background\n\nThe paper introduces measures of predictive efficiency to compare different prediction functions. The expected information loss, `$\\bar{I}(f,f^{*})$`, quantifies how much a candidate prediction function `$f^*$` deviates from the true density `$f$`. For a simple \"certainty equivalence\" (CEQ) predictor `$\\hat{f} = f(y_f|\\hat{\\theta}_d)$`, this loss is of order `$O(m^{-1})$`, where `$m$` is the sample size. Any well-behaved correction for parameter uncertainty can only improve this by a smaller, second-order term of `$O(m^{-2})$`.\n\n### Data / Model Specification\n\n**Proposition 1** states that the first-order predictive efficiency of the CEQ predictor is given by:\n  \n\\lambda_{1}\\big(f,\\hat{f}\\big)=-\\frac{1}{2}\\operatorname{tr}\\big[V\\big(\\hat{\\theta}_{d}\\big)E_{Y}\\big(H\\big(y_{f};\\theta\\big)\\big)\\big] \\quad \\text{(Eq. 1)}\n \nwhere `$V(\\hat{\\theta}_d)$` is the asymptotic variance-covariance matrix of the parameter estimator and `$E_Y[H(y_f; \\theta)]$` is the expected log-Hessian (curvature) of the future density.\n\n**Proposition 2** implies that the efficiency gain from any well-behaved correction for parameter uncertainty is of order `$O(m^{-2})$`.\n\n---\n\nConsider two forecasting scenarios, both with a large sample size `$m$`:\n*   **Scenario A:** A linear regression model `$y_i = x_i\\beta + \\varepsilon_i$` with `$\\varepsilon_i \\sim N(0, \\sigma^2)$` is used to predict a future observation `$y_f$`. The regressors `$x_d$` are orthogonal, leading to very precise, low-variance estimates of `$\\beta$`.\n*   **Scenario B:** An ARCH model is used to predict the variance of a financial return `$y_f$`. The model is highly non-linear, and due to market conditions, the parameter estimates for the ARCH effects are known to have high variance.\n\nBased on the theory presented, select all of the following conclusions that are valid.", "Answer": ["A", "D"], "qid": "327", "question": "### Background\n\nThe paper introduces measures of predictive efficiency to compare different prediction functions. The expected information loss, `$\\bar{I}(f,f^{*})$`, quantifies how much a candidate prediction function `$f^*$` deviates from the true density `$f$`. For a simple \"certainty equivalence\" (CEQ) predictor `$\\hat{f} = f(y_f|\\hat{\\theta}_d)$`, this loss is of order `$O(m^{-1})$`, where `$m$` is the sample size. Any well-behaved correction for parameter uncertainty can only improve this by a smaller, second-order term of `$O(m^{-2})$`.\n\n### Data / Model Specification\n\n**Proposition 1** states that the first-order predictive efficiency of the CEQ predictor is given by:\n  \n\\lambda_{1}\\big(f,\\hat{f}\\big)=-\\frac{1}{2}\\operatorname{tr}\\big[V\\big(\\hat{\\theta}_{d}\\big)E_{Y}\\big(H\\big(y_{f};\\theta\\big)\\big)\\big] \\quad \\text{(Eq. 1)}\n \nwhere `$V(\\hat{\\theta}_d)$` is the asymptotic variance-covariance matrix of the parameter estimator and `$E_Y[H(y_f; \\theta)]$` is the expected log-Hessian (curvature) of the future density.\n\n**Proposition 2** implies that the efficiency gain from any well-behaved correction for parameter uncertainty is of order `$O(m^{-2})$`.\n\n---\n\nConsider two forecasting scenarios, both with a large sample size `$m$`:\n*   **Scenario A:** A linear regression model `$y_i = x_i\\beta + \\varepsilon_i$` with `$\\varepsilon_i \\sim N(0, \\sigma^2)$` is used to predict a future observation `$y_f$`. The regressors `$x_d$` are orthogonal, leading to very precise, low-variance estimates of `$\\beta$`.\n*   **Scenario B:** An ARCH model is used to predict the variance of a financial return `$y_f$`. The model is highly non-linear, and due to market conditions, the parameter estimates for the ARCH effects are known to have high variance.\n\nBased on the theory presented, select all of the following conclusions that are valid.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper introduces measures of p...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper introduces measures of p...", "error": null}}
{"ID": 328, "Question": "### Background\n\n**Research Question.** This problem examines the core methodological contribution of the paper: the use of a 'primal' system (the production function and its first-order conditions) to estimate inefficiencies with a flexible functional form. This approach is motivated by the analytical intractability of the alternative 'dual' (cost function) approach.\n\n**Setting / Institutional Environment.** The theoretical setting is a cost-minimizing firm whose technology is described by a flexible translog production function. The firm is subject to random production shocks (`v`), technical inefficiency (`u`), and allocative inefficiency (`\\xi`).\n\n### Data / Model Specification\n\nThe dual approach begins with a translog cost function. When inefficiencies are incorporated, the log of actual cost (`\\ln c^a`) becomes a highly complex, non-linear function of the unobserved error components `u`, `v`, and `\\xi`, making Maximum Likelihood (ML) estimation infeasible.\n\nThe paper's proposed primal approach avoids this by estimating a system of equations directly derived from the production side.\n\n**The Primal System:**\n1.  The translog stochastic production function:\n      \n    \\ln y_{i t}=\\alpha_{0}+\\sum_{j}\\alpha_{j}\\ln x_{j i t}+\\alpha_{t}t+\\frac{1}{2}\\sum_{j}\\sum_{k}\\alpha_{j k}\\ln x_{j i t}\\ln x_{k i t} + \\sum_{j}\\alpha_{j t}\\ln x_{j i t}t+\\frac{1}{2}\\alpha_{t t}t^{2} + v_{i t}-u_{i t}\n    \\quad\\quad\text{(Eq. (1))}\n     \n2.  The first-order conditions (FOCs) for cost minimization:\n      \n    \\ln s_{j i t}-\\ln s_{1i t}-\\ln(w_{j i t}x_{j i t})+\\ln(w_{1i t}x_{1i t})=\\xi_{j i t}\n    \\quad\\quad\text{(Eq. (2))}\n     \n    where `s_{jit}` is the output elasticity of input `j`, `s_{j i t}=\\alpha_{j}+\\sum_{k}\\alpha_{j k}\\ln x_{k i t}+\\alpha_{j t}t`.\n\n**Maximum Likelihood Estimation:**\nThe log-likelihood function for this system is constructed from the joint probability density of the structural errors, `g(v-u)` and `h(\\xi)`, and includes a Jacobian term `|J|` because the inputs `x` are endogenous.\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the rationale and identification strategy of the primal estimation approach for the translog model? Select all that apply.", "Answer": ["C", "D"], "qid": "328", "question": "### Background\n\n**Research Question.** This problem examines the core methodological contribution of the paper: the use of a 'primal' system (the production function and its first-order conditions) to estimate inefficiencies with a flexible functional form. This approach is motivated by the analytical intractability of the alternative 'dual' (cost function) approach.\n\n**Setting / Institutional Environment.** The theoretical setting is a cost-minimizing firm whose technology is described by a flexible translog production function. The firm is subject to random production shocks (`v`), technical inefficiency (`u`), and allocative inefficiency (`\\xi`).\n\n### Data / Model Specification\n\nThe dual approach begins with a translog cost function. When inefficiencies are incorporated, the log of actual cost (`\\ln c^a`) becomes a highly complex, non-linear function of the unobserved error components `u`, `v`, and `\\xi`, making Maximum Likelihood (ML) estimation infeasible.\n\nThe paper's proposed primal approach avoids this by estimating a system of equations directly derived from the production side.\n\n**The Primal System:**\n1.  The translog stochastic production function:\n      \n    \\ln y_{i t}=\\alpha_{0}+\\sum_{j}\\alpha_{j}\\ln x_{j i t}+\\alpha_{t}t+\\frac{1}{2}\\sum_{j}\\sum_{k}\\alpha_{j k}\\ln x_{j i t}\\ln x_{k i t} + \\sum_{j}\\alpha_{j t}\\ln x_{j i t}t+\\frac{1}{2}\\alpha_{t t}t^{2} + v_{i t}-u_{i t}\n    \\quad\\quad\text{(Eq. (1))}\n     \n2.  The first-order conditions (FOCs) for cost minimization:\n      \n    \\ln s_{j i t}-\\ln s_{1i t}-\\ln(w_{j i t}x_{j i t})+\\ln(w_{1i t}x_{1i t})=\\xi_{j i t}\n    \\quad\\quad\text{(Eq. (2))}\n     \n    where `s_{jit}` is the output elasticity of input `j`, `s_{j i t}=\\alpha_{j}+\\sum_{k}\\alpha_{j k}\\ln x_{k i t}+\\alpha_{j t}t`.\n\n**Maximum Likelihood Estimation:**\nThe log-likelihood function for this system is constructed from the joint probability density of the structural errors, `g(v-u)` and `h(\\xi)`, and includes a Jacobian term `|J|` because the inputs `x` are endogenous.\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the rationale and identification strategy of the primal estimation approach for the translog model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 329, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical definitions of technical inefficiency, the relationship between different measurement orientations (input vs. output), and the procedure for translating these physical inefficiency measures into a monetary cost.\n\n**Setting / Institutional Environment.** A firm's performance is benchmarked against an efficient production frontier. Its technical inefficiency can be measured in two ways: as an output shortfall for a given set of inputs (Output-Oriented), or as an excess of inputs for a given level of output (Input-Oriented).\n\n### Data / Model Specification\n\nThe two specifications for technical inefficiency are:\n\n**Output-Oriented (OO) Model:** The firm produces output `y` using inputs `x`.\n  \ny = f(x) \\cdot \\mathrm{e}^{v-u}\n\\quad\\quad\text{(Eq. (1))}\n \nwhere `u \\ge 0` is related to the proportional shortfall of output from the frontier `f(x)`.\n\n**Input-Oriented (IO) Model:**\n  \ny = f(x \\mathrm{e}^{-\\eta}) \\cdot \\mathrm{e}^{v}\n\\quad\\quad\text{(Eq. (2))}\n \nwhere `\\eta \\ge 0` is the proportional over-use of all inputs.\n\nFor a translog production function, the cost impact of these inefficiencies cannot be derived analytically. It is computed numerically. The paper notes that for a translog model, the total cost of inefficiency is not simply the sum of the costs of technical and allocative inefficiency due to interaction effects.\n\n### Question\n\nBased on the paper's theoretical discussion of inefficiency, select all statements that are correct.", "Answer": ["B", "D"], "qid": "329", "question": "### Background\n\n**Research Question.** This problem explores the theoretical definitions of technical inefficiency, the relationship between different measurement orientations (input vs. output), and the procedure for translating these physical inefficiency measures into a monetary cost.\n\n**Setting / Institutional Environment.** A firm's performance is benchmarked against an efficient production frontier. Its technical inefficiency can be measured in two ways: as an output shortfall for a given set of inputs (Output-Oriented), or as an excess of inputs for a given level of output (Input-Oriented).\n\n### Data / Model Specification\n\nThe two specifications for technical inefficiency are:\n\n**Output-Oriented (OO) Model:** The firm produces output `y` using inputs `x`.\n  \ny = f(x) \\cdot \\mathrm{e}^{v-u}\n\\quad\\quad\text{(Eq. (1))}\n \nwhere `u \\ge 0` is related to the proportional shortfall of output from the frontier `f(x)`.\n\n**Input-Oriented (IO) Model:**\n  \ny = f(x \\mathrm{e}^{-\\eta}) \\cdot \\mathrm{e}^{v}\n\\quad\\quad\text{(Eq. (2))}\n \nwhere `\\eta \\ge 0` is the proportional over-use of all inputs.\n\nFor a translog production function, the cost impact of these inefficiencies cannot be derived analytically. It is computed numerically. The paper notes that for a translog model, the total cost of inefficiency is not simply the sum of the costs of technical and allocative inefficiency due to interaction effects.\n\n### Question\n\nBased on the paper's theoretical discussion of inefficiency, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 330, "Question": "### Background\n\n**Research Question.** This problem examines the foundational principles of causal inference in an experimental setting, exploring both the ideal identification strategy and the complications that arise from real-world implementation issues like imperfect compliance.\n\n**Setting / Institutional Environment.** The Greater Avenues for Independence (GAIN) program was evaluated via a randomized experiment. A sample of welfare recipients were randomly assigned to either the treatment group (required to participate in GAIN services) or the control group (remaining in the AFDC program). However, compliance was imperfect: some individuals assigned to GAIN did not participate, and others were exempted from GAIN activities if they found work on their own.\n\n**Variables & Parameters.**\n*   `Y_{i1}`, `Y_{i0}`: Potential earnings of individual `i` under treatment and control, respectively.\n*   `Y_i`: Observed earnings for individual `i`.\n*   `Z_i`: Random assignment indicator (`Z_i=1` for treatment assignment, `Z_i=0` for control).\n*   `D_i`: Actual participation indicator (`D_i=1` if individual `i` participates in GAIN).\n\n---\n\n### Data / Model Specification\n\nThe potential outcomes framework defines the causal effect for an individual `i` as `Y_{i1} - Y_{i0}`. The relationship between observed and potential outcomes is given by the switching equation:\n\n  \nY_i = Z_i Y_{i1} + (1-Z_i) Y_{i0} \n \n\n---\n\n### The Question\n\nBased on the provided framework and institutional details, select all statements that are correct regarding the causal analysis of the GAIN program.", "Answer": ["B", "D"], "qid": "330", "question": "### Background\n\n**Research Question.** This problem examines the foundational principles of causal inference in an experimental setting, exploring both the ideal identification strategy and the complications that arise from real-world implementation issues like imperfect compliance.\n\n**Setting / Institutional Environment.** The Greater Avenues for Independence (GAIN) program was evaluated via a randomized experiment. A sample of welfare recipients were randomly assigned to either the treatment group (required to participate in GAIN services) or the control group (remaining in the AFDC program). However, compliance was imperfect: some individuals assigned to GAIN did not participate, and others were exempted from GAIN activities if they found work on their own.\n\n**Variables & Parameters.**\n*   `Y_{i1}`, `Y_{i0}`: Potential earnings of individual `i` under treatment and control, respectively.\n*   `Y_i`: Observed earnings for individual `i`.\n*   `Z_i`: Random assignment indicator (`Z_i=1` for treatment assignment, `Z_i=0` for control).\n*   `D_i`: Actual participation indicator (`D_i=1` if individual `i` participates in GAIN).\n\n---\n\n### Data / Model Specification\n\nThe potential outcomes framework defines the causal effect for an individual `i` as `Y_{i1} - Y_{i0}`. The relationship between observed and potential outcomes is given by the switching equation:\n\n  \nY_i = Z_i Y_{i1} + (1-Z_i) Y_{i0} \n \n\n---\n\n### The Question\n\nBased on the provided framework and institutional details, select all statements that are correct regarding the causal analysis of the GAIN program.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 331, "Question": "### Background\n\n**Research Question.** This problem examines the econometric modeling strategy used to analyze the GAIN program's impact, focusing on how the model specification addresses key features of the data, such as the high prevalence of zero-earnings observations and the potential for heterogeneous treatment effects.\n\n**Setting / Institutional Environment.** The analysis uses data from the GAIN experiment. A key feature of this dataset is that a large fraction of individuals report zero earnings in any given quarter. To move beyond simple mean comparisons and predict individual-level outcomes, a formal econometric model is required.\n\n**Variables & Parameters.**\n*   `y_{it}`: Observed quarterly earnings for individual `i` at time `t`.\n*   `y_{it}^*`: A latent (unobserved) variable representing underlying earnings potential.\n*   `T_i`: Treatment indicator (=1 if assigned to GAIN).\n*   `1_{kit}`: An indicator variable for quarter `k`.\n*   `Z_i`: A vector of pre-treatment individual characteristics (e.g., education, test scores).\n\n---\n\n### Data / Model Specification\n\nThe study employs a Tobit model to account for the censoring of earnings at zero. The model is specified as follows:\n\n  \ny_{it}^* = x_{it}'\\beta + \\epsilon_{it}, \\quad \\text{where } \\epsilon_{it} \\sim N(0, \\sigma^2) \\quad \\text{(Eq. (1))}\n \n\nObserved earnings `y_{it}` are related to the latent variable `y_{it}^*` by `y_{it} = \\max(0, y_{it}^*)`. The vector of explanatory variables `x_{it}` is specified to have a flexible structure, captured by the following index function:\n\n  \nx_{it}'\\beta = \\sum_{k=1}^{13} \\alpha_k 1_{kit} + \\sum_{k=1}^{13} \\delta_k (1_{kit} \\cdot T_i) + Z_i'\\gamma + (Z_i' \\cdot T_i)\\lambda \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nBased on the econometric model specified in Eq. (1) and Eq. (2), select all correct interpretations of the model's components and purpose.", "Answer": ["A", "D"], "qid": "331", "question": "### Background\n\n**Research Question.** This problem examines the econometric modeling strategy used to analyze the GAIN program's impact, focusing on how the model specification addresses key features of the data, such as the high prevalence of zero-earnings observations and the potential for heterogeneous treatment effects.\n\n**Setting / Institutional Environment.** The analysis uses data from the GAIN experiment. A key feature of this dataset is that a large fraction of individuals report zero earnings in any given quarter. To move beyond simple mean comparisons and predict individual-level outcomes, a formal econometric model is required.\n\n**Variables & Parameters.**\n*   `y_{it}`: Observed quarterly earnings for individual `i` at time `t`.\n*   `y_{it}^*`: A latent (unobserved) variable representing underlying earnings potential.\n*   `T_i`: Treatment indicator (=1 if assigned to GAIN).\n*   `1_{kit}`: An indicator variable for quarter `k`.\n*   `Z_i`: A vector of pre-treatment individual characteristics (e.g., education, test scores).\n\n---\n\n### Data / Model Specification\n\nThe study employs a Tobit model to account for the censoring of earnings at zero. The model is specified as follows:\n\n  \ny_{it}^* = x_{it}'\\beta + \\epsilon_{it}, \\quad \\text{where } \\epsilon_{it} \\sim N(0, \\sigma^2) \\quad \\text{(Eq. (1))}\n \n\nObserved earnings `y_{it}` are related to the latent variable `y_{it}^*` by `y_{it} = \\max(0, y_{it}^*)`. The vector of explanatory variables `x_{it}` is specified to have a flexible structure, captured by the following index function:\n\n  \nx_{it}'\\beta = \\sum_{k=1}^{13} \\alpha_k 1_{kit} + \\sum_{k=1}^{13} \\delta_k (1_{kit} \\cdot T_i) + Z_i'\\gamma + (Z_i' \\cdot T_i)\\lambda \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nBased on the econometric model specified in Eq. (1) and Eq. (2), select all correct interpretations of the model's components and purpose.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 332, "Question": "### Background\n\n**Research Question.** This question examines the paper's multi-layered empirical strategy for identifying the causal effect of Employment Protection Legislation (EPL) deregulation on the labor share.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013. A key part of the strategy is a country-industry-level analysis to sharpen identification and explore mechanisms.\n\n### Data / Model Specification\n\nThe paper's identification strategy is motivated by the idea that the impact of EPL is most pronounced in industries with high *intrinsic job instability* (e.g., high 'natural' layoff rates). To identify the effect via this cross-industry variation, the paper uses the following difference-in-differences (DiD) model:\n\n  \ny_{i,j,t+k} - y_{i,j,t-1} = \\tau_{j,t} + \\alpha_{i,j} + \\mu_{i,t} + \\beta_{k}(\\vartheta_{i} \\times R_{j,t}) + \\dots + \\epsilon_{i,j,t} \\quad \\text{(Eq. 1)}\n \nwhere `y` is the labor share, `i` is industry, `j` is country, and `t` is time. `R_jt` is a country-level reform, and `ϑ_i` is an industry characteristic like the layoff rate. The model includes country-time fixed effects (`τ_jt`), country-industry fixed effects (`α_ij`), and industry-time fixed effects (`μ_it`).\n\n### Question\n\nWhich of the following statements accurately describe the role of the fixed effects in the DiD specification in Eq. (1)? Select all that apply.", "Answer": ["A", "B"], "qid": "332", "question": "### Background\n\n**Research Question.** This question examines the paper's multi-layered empirical strategy for identifying the causal effect of Employment Protection Legislation (EPL) deregulation on the labor share.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013. A key part of the strategy is a country-industry-level analysis to sharpen identification and explore mechanisms.\n\n### Data / Model Specification\n\nThe paper's identification strategy is motivated by the idea that the impact of EPL is most pronounced in industries with high *intrinsic job instability* (e.g., high 'natural' layoff rates). To identify the effect via this cross-industry variation, the paper uses the following difference-in-differences (DiD) model:\n\n  \ny_{i,j,t+k} - y_{i,j,t-1} = \\tau_{j,t} + \\alpha_{i,j} + \\mu_{i,t} + \\beta_{k}(\\vartheta_{i} \\times R_{j,t}) + \\dots + \\epsilon_{i,j,t} \\quad \\text{(Eq. 1)}\n \nwhere `y` is the labor share, `i` is industry, `j` is country, and `t` is time. `R_jt` is a country-level reform, and `ϑ_i` is an industry characteristic like the layoff rate. The model includes country-time fixed effects (`τ_jt`), country-industry fixed effects (`α_ij`), and industry-time fixed effects (`μ_it`).\n\n### Question\n\nWhich of the following statements accurately describe the role of the fixed effects in the DiD specification in Eq. (1)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 333, "Question": "### Background\n\n**Research Question.** This problem explores the asymptotic theory of the Ordinary Least Squares (OLS) estimator in a cointegrating regression and the logic behind the Fully Modified OLS (FM-OLS) corrections.\n\n**Setting.** In a standard cointegrating regression with endogenous regressors, the OLS estimator is biased and has a non-standard limiting distribution. The FM-OLS estimator of Phillips and Hansen (1990) is designed to correct these issues through two distinct transformations: a serial correlation correction ($y_t \\to y_t^+$) and an endogeneity bias correction (subtracting a term $\\mathcal{M}^*$).\n\n### Data / Model Specification\n\nThe limiting distribution for the OLS estimator $\\widehat{\\theta} = [\\widehat{\\mu}, \\widehat{\\beta}']'$ is:\n\n  \n\\binom{T^{1/2}(\\widehat{\\mu}-\\mu)}{T(\\widehat{\\beta}-\\beta)} \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1}\\left(\\int B_{v}^{*}(r)d B_{u}(r)+{\\varDelta}_{v u}^{*}\\right) \\quad \\text{(Eq. (1))}\n \n\nThis distribution is contaminated by two distinct issues stemming from the terms $\\int B_{v}^{*}(r)d B_{u}(r)$ and ${\\varDelta}_{v u}^{*}$.\n\n### Question\n\nWhich of the following statements correctly pairs a problem in the OLS limiting distribution (Eq. (1)) with the corresponding FM-OLS transformation designed to solve it? (Select all that apply)", "Answer": ["A", "D"], "qid": "333", "question": "### Background\n\n**Research Question.** This problem explores the asymptotic theory of the Ordinary Least Squares (OLS) estimator in a cointegrating regression and the logic behind the Fully Modified OLS (FM-OLS) corrections.\n\n**Setting.** In a standard cointegrating regression with endogenous regressors, the OLS estimator is biased and has a non-standard limiting distribution. The FM-OLS estimator of Phillips and Hansen (1990) is designed to correct these issues through two distinct transformations: a serial correlation correction ($y_t \\to y_t^+$) and an endogeneity bias correction (subtracting a term $\\mathcal{M}^*$).\n\n### Data / Model Specification\n\nThe limiting distribution for the OLS estimator $\\widehat{\\theta} = [\\widehat{\\mu}, \\widehat{\\beta}']'$ is:\n\n  \n\\binom{T^{1/2}(\\widehat{\\mu}-\\mu)}{T(\\widehat{\\beta}-\\beta)} \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1}\\left(\\int B_{v}^{*}(r)d B_{u}(r)+{\\varDelta}_{v u}^{*}\\right) \\quad \\text{(Eq. (1))}\n \n\nThis distribution is contaminated by two distinct issues stemming from the terms $\\int B_{v}^{*}(r)d B_{u}(r)$ and ${\\varDelta}_{v u}^{*}$.\n\n### Question\n\nWhich of the following statements correctly pairs a problem in the OLS limiting distribution (Eq. (1)) with the corresponding FM-OLS transformation designed to solve it? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 334, "Question": "### Background\n\nConventional evaluations of a presidential administration's economic performance are often criticized for two primary flaws: (i) they fail to account for the difficulty of controlling the economy (e.g., unfavorable initial conditions or external shocks), and (ii) they ignore the intertemporal effects of policies, where actions taken during one term have consequences for the next.\n\nThis paper proposes a new measure of performance, `M`, based on optimal control theory, designed to address these flaws. A lower value of `M` indicates better performance.\n\n### Data / Model Specification\n\nThe performance measure `M` for an administration (Admin 1) is defined as:\n\n`M` = (Expected actual loss in Admin 1's term) - (Expected optimal loss in Admin 1's term) \n      + (Expected optimal loss for Admin 2, given Admin 1's actual policies) \n      - (Expected optimal loss for Admin 2, had Admin 1 also acted optimally).\n\nLet's denote these four components as `a`, `b`, `c`, and `d` respectively, so `M = (a - b) + (c - d)`.\n\n### Question\n\nBased on the definition of the performance measure `M`, select all statements that are factually correct.", "Answer": ["C", "D"], "qid": "334", "question": "### Background\n\nConventional evaluations of a presidential administration's economic performance are often criticized for two primary flaws: (i) they fail to account for the difficulty of controlling the economy (e.g., unfavorable initial conditions or external shocks), and (ii) they ignore the intertemporal effects of policies, where actions taken during one term have consequences for the next.\n\nThis paper proposes a new measure of performance, `M`, based on optimal control theory, designed to address these flaws. A lower value of `M` indicates better performance.\n\n### Data / Model Specification\n\nThe performance measure `M` for an administration (Admin 1) is defined as:\n\n`M` = (Expected actual loss in Admin 1's term) - (Expected optimal loss in Admin 1's term) \n      + (Expected optimal loss for Admin 2, given Admin 1's actual policies) \n      - (Expected optimal loss for Admin 2, had Admin 1 also acted optimally).\n\nLet's denote these four components as `a`, `b`, `c`, and `d` respectively, so `M = (a - b) + (c - d)`.\n\n### Question\n\nBased on the definition of the performance measure `M`, select all statements that are factually correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nConventional evaluations of a pres...", "model_answer": "This is a detailed response to the question about ### Background\n\nConventional evaluations of a pres...", "error": null}}
{"ID": 335, "Question": "### Background\n\nA policymaker's single-period loss in quarter `t` is given by the function:\n  \nL_{t} = \\gamma g_t^2 + \\pi_t^2\n \nwhere `g_t` is the output gap, `π_t` is the inflation rate, and `γ > 0` is the relative weight on the output gap. The policymaker faces a linear Phillips Curve constraint: `π_t = π^e + α g_t`, where `π^e > 0` is expected inflation and `α > 0`.\n\n### Question\n\nIn the simplified static economy described, a policymaker chooses the output gap `g_t` to minimize the loss `L_t`. Select all statements that correctly describe the optimal policy and its properties.", "Answer": ["A", "D"], "qid": "335", "question": "### Background\n\nA policymaker's single-period loss in quarter `t` is given by the function:\n  \nL_{t} = \\gamma g_t^2 + \\pi_t^2\n \nwhere `g_t` is the output gap, `π_t` is the inflation rate, and `γ > 0` is the relative weight on the output gap. The policymaker faces a linear Phillips Curve constraint: `π_t = π^e + α g_t`, where `π^e > 0` is expected inflation and `α > 0`.\n\n### Question\n\nIn the simplified static economy described, a policymaker chooses the output gap `g_t` to minimize the loss `L_t`. Select all statements that correctly describe the optimal policy and its properties.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA policymaker's single-period loss...", "model_answer": "This is a detailed response to the question about ### Background\n\nA policymaker's single-period loss...", "error": null}}
{"ID": 336, "Question": "### Background\n\n**Research Question.** This problem investigates the efficiency losses from market specialization in the presence of adverse selection. It questions whether the optimal screening contract offered by a global, multi-product monopoly can be replicated by the sum of contracts from specialized, single-product monopolies.\n\n**Setting / Institutional Environment.** A market for insurance on two independent risks, `$\\tilde{x}$` and `$\\tilde{y}$`, is populated by two unobservable consumer types, `a` and `b`, who differ in their loss distributions. A global monopoly can offer an integrated insurance contract `$I_B(x,y)$` that depends on the joint realization of both losses. This is compared to a market with two specialized monopolies offering separate contracts `$I_X(x)$` and `$I_Y(y)$`. The central issue is whether the specialized firms can collectively achieve the same profit as the global monopoly.\n\n### Data / Model Specification\n\nIn a separating equilibrium, one consumer type is partially insured. The optimal indemnity for this type depends only on the likelihood ratio of the observed loss. \n\nFor a **global monopoly**, the indemnity takes the form:\n  \nI_B(x,y) = -x-y + J_B\\left[\\frac{l_B^a(x,y)}{l_B^b(x,y)}\\right]\n \nwhere `$l_B^i(x,y)$` is the joint probability density of losses `(x,y)` for type `i`.\n\nFor **specialized monopolies**, the sum of indemnities is necessarily separable:\n  \nI_X(x)+I_Y(y) = -x-y + J_X\\left[\\frac{l_X^a(x)}{l_X^b(x)}\\right] + J_Y\\left[\\frac{l_Y^a(y)}{l_Y^b(y)}\\right]\n \nwhere `$l_X^i(x)$` and `$l_Y^i(y)$` are the marginal densities.\n\nSince the risks `$\\tilde{x}$` and `$\\tilde{y}$` are independent, the joint density is the product of the marginals: `$l_B^i(x,y) = l_X^i(x)l_Y^i(y)$`. Therefore, for the specialized firms to replicate the global monopoly's outcome, it must be possible to find functions `$J_X$` and `$J_Y$` such that for all `x` and `y`:\n  \nJ_{B}\\left[\\frac{l_{X}^{a}(x)}{l_{X}^{b}(x)} \\cdot \\frac{l_{Y}^{a}(y)}{l_{Y}^{b}(y)}\\right] = J_{X}\\left[\\frac{l_{X}^{a}(x)}{l_{X}^{b}(x)}\\right] + J_{Y}\\left[\\frac{l_{Y}^{a}(y)}{l_{Y}^{b}(y)}\\right] \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the model, select all statements that correctly explain why the optimal screening contract of a global monopoly is generically non-replicable by specialized monopolies, even when risks are independent.", "Answer": ["A", "C"], "qid": "336", "question": "### Background\n\n**Research Question.** This problem investigates the efficiency losses from market specialization in the presence of adverse selection. It questions whether the optimal screening contract offered by a global, multi-product monopoly can be replicated by the sum of contracts from specialized, single-product monopolies.\n\n**Setting / Institutional Environment.** A market for insurance on two independent risks, `$\\tilde{x}$` and `$\\tilde{y}$`, is populated by two unobservable consumer types, `a` and `b`, who differ in their loss distributions. A global monopoly can offer an integrated insurance contract `$I_B(x,y)$` that depends on the joint realization of both losses. This is compared to a market with two specialized monopolies offering separate contracts `$I_X(x)$` and `$I_Y(y)$`. The central issue is whether the specialized firms can collectively achieve the same profit as the global monopoly.\n\n### Data / Model Specification\n\nIn a separating equilibrium, one consumer type is partially insured. The optimal indemnity for this type depends only on the likelihood ratio of the observed loss. \n\nFor a **global monopoly**, the indemnity takes the form:\n  \nI_B(x,y) = -x-y + J_B\\left[\\frac{l_B^a(x,y)}{l_B^b(x,y)}\\right]\n \nwhere `$l_B^i(x,y)$` is the joint probability density of losses `(x,y)` for type `i`.\n\nFor **specialized monopolies**, the sum of indemnities is necessarily separable:\n  \nI_X(x)+I_Y(y) = -x-y + J_X\\left[\\frac{l_X^a(x)}{l_X^b(x)}\\right] + J_Y\\left[\\frac{l_Y^a(y)}{l_Y^b(y)}\\right]\n \nwhere `$l_X^i(x)$` and `$l_Y^i(y)$` are the marginal densities.\n\nSince the risks `$\\tilde{x}$` and `$\\tilde{y}$` are independent, the joint density is the product of the marginals: `$l_B^i(x,y) = l_X^i(x)l_Y^i(y)$`. Therefore, for the specialized firms to replicate the global monopoly's outcome, it must be possible to find functions `$J_X$` and `$J_Y$` such that for all `x` and `y`:\n  \nJ_{B}\\left[\\frac{l_{X}^{a}(x)}{l_{X}^{b}(x)} \\cdot \\frac{l_{Y}^{a}(y)}{l_{Y}^{b}(y)}\\right] = J_{X}\\left[\\frac{l_{X}^{a}(x)}{l_{X}^{b}(x)}\\right] + J_{Y}\\left[\\frac{l_{Y}^{a}(y)}{l_{Y}^{b}(y)}\\right] \\quad \\text{(Eq. (1))}\n \n\n### Question\n\nBased on the model, select all statements that correctly explain why the optimal screening contract of a global monopoly is generically non-replicable by specialized monopolies, even when risks are independent.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 337, "Question": "### Background\n\n**Research Question.** This problem investigates how the relationship between a consumer's aversion to multiple risks determines the nature of strategic interaction between specialized insurers and the resulting equilibrium outcomes under symmetric information.\n\n**Setting / Institutional Environment.** Two specialized monopoly insurers offer full-insurance contracts for two independent risks, `$\\tilde{x}$` and `$\\tilde{y}$`. The game is simplified to insurers choosing premiums `$\\pi_X$` and `$\\pi_Y$`. The nature of competition depends on whether insuring one risk makes the consumer more or less willing to pay for insuring the other. This distinction leads to two different types of market equilibria with different welfare consequences.\n\n### Data / Model Specification\n\nThe analysis relies on three key parameters representing the maximum premiums a consumer is willing to pay:\n- `$\\bar{\\pi}_X$`: The maximum premium for insuring risk `$\\tilde{x}$` alone, leaving `$\\tilde{y}$` uninsured.\n- `$\\bar{\\pi}_Y$`: The maximum premium for insuring risk `$\\tilde{y}$` alone, leaving `$\\tilde{x}$` uninsured.\n- `$\\bar{\\pi}_B$`: The maximum total premium for insuring both risks `$\\tilde{x}$` and `$\\tilde{y}$`.\n\nThese are defined by making the consumer indifferent between the insured outcome and the fully uninsured baseline `$E u(w+\\tilde{x}+\\tilde{y})$`:\n  \nE u(w-\\bar{\\pi}_{X}+\\tilde{y}) = E u(w+\\tilde{x}-\\bar{\\pi}_{Y}) = u(w-\\bar{\\pi}_{B}) = E u(w+\\tilde{x}+\\tilde{y}) \\quad \\text{(Eq. (1))}\n \n**Strategic complementarity** is defined as `$\\bar{\\pi}_X + \\bar{\\pi}_Y < \\bar{\\pi}_B$`. \n**Strategic substitutability** is defined as `$\\bar{\\pi}_X + \\bar{\\pi}_Y > \\bar{\\pi}_B$`. \n\nThe paper identifies two sets of possible Nash Equilibria:\n- **Type 1 Equilibria:** `$\\mathcal{E}_{1}=\\left\\{(\\pi_{X},\\pi_{Y}) | \\pi_{X}+\\pi_{Y}={\\bar{\\pi}}_{B}; \\pi_{X}\\geq{\\bar{\\pi}}_{X}; \\pi_{Y}\\geq{\\bar{\\pi}}_{Y}\\right\\}`\n- **Type 2 Equilibria:** `$\\mathcal{E}_{2}=\\left\\{(\\pi_{X},\\pi_{Y}) | B \\approx X \\approx Y \\succeq N\\right\\}` (where B, X, Y, N are the choices to buy Both, X only, Y only, or None).\n\n### Question\n\nGiven the definitions of strategic complementarity and substitutability, select all statements that correctly characterize the resulting market equilibria.", "Answer": ["A", "D"], "qid": "337", "question": "### Background\n\n**Research Question.** This problem investigates how the relationship between a consumer's aversion to multiple risks determines the nature of strategic interaction between specialized insurers and the resulting equilibrium outcomes under symmetric information.\n\n**Setting / Institutional Environment.** Two specialized monopoly insurers offer full-insurance contracts for two independent risks, `$\\tilde{x}$` and `$\\tilde{y}$`. The game is simplified to insurers choosing premiums `$\\pi_X$` and `$\\pi_Y$`. The nature of competition depends on whether insuring one risk makes the consumer more or less willing to pay for insuring the other. This distinction leads to two different types of market equilibria with different welfare consequences.\n\n### Data / Model Specification\n\nThe analysis relies on three key parameters representing the maximum premiums a consumer is willing to pay:\n- `$\\bar{\\pi}_X$`: The maximum premium for insuring risk `$\\tilde{x}$` alone, leaving `$\\tilde{y}$` uninsured.\n- `$\\bar{\\pi}_Y$`: The maximum premium for insuring risk `$\\tilde{y}$` alone, leaving `$\\tilde{x}$` uninsured.\n- `$\\bar{\\pi}_B$`: The maximum total premium for insuring both risks `$\\tilde{x}$` and `$\\tilde{y}$`.\n\nThese are defined by making the consumer indifferent between the insured outcome and the fully uninsured baseline `$E u(w+\\tilde{x}+\\tilde{y})$`:\n  \nE u(w-\\bar{\\pi}_{X}+\\tilde{y}) = E u(w+\\tilde{x}-\\bar{\\pi}_{Y}) = u(w-\\bar{\\pi}_{B}) = E u(w+\\tilde{x}+\\tilde{y}) \\quad \\text{(Eq. (1))}\n \n**Strategic complementarity** is defined as `$\\bar{\\pi}_X + \\bar{\\pi}_Y < \\bar{\\pi}_B$`. \n**Strategic substitutability** is defined as `$\\bar{\\pi}_X + \\bar{\\pi}_Y > \\bar{\\pi}_B$`. \n\nThe paper identifies two sets of possible Nash Equilibria:\n- **Type 1 Equilibria:** `$\\mathcal{E}_{1}=\\left\\{(\\pi_{X},\\pi_{Y}) | \\pi_{X}+\\pi_{Y}={\\bar{\\pi}}_{B}; \\pi_{X}\\geq{\\bar{\\pi}}_{X}; \\pi_{Y}\\geq{\\bar{\\pi}}_{Y}\\right\\}`\n- **Type 2 Equilibria:** `$\\mathcal{E}_{2}=\\left\\{(\\pi_{X},\\pi_{Y}) | B \\approx X \\approx Y \\succeq N\\right\\}` (where B, X, Y, N are the choices to buy Both, X only, Y only, or None).\n\n### Question\n\nGiven the definitions of strategic complementarity and substitutability, select all statements that correctly characterize the resulting market equilibria.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 338, "Question": "### Background\n\n**Research Question.** This problem explores the methodological foundation for analyzing games with discontinuous payoffs, a common issue in models of insurance where firms' optimal pricing makes consumers exactly indifferent between multiple choices.\n\n**Setting / Institutional Environment.** A risk-averse consumer faces two independent risks. Two risk-neutral, specialized monopoly insurers (X and Y) simultaneously make take-it-or-leave-it contract offers. The consumer then chooses the option that maximizes their expected utility. Because profit-maximizing insurers aim to extract all consumer surplus, they create situations of consumer indifference, which leads to discontinuities in their profit functions and makes standard existence theorems for Nash Equilibrium inapplicable.\n\n### Data / Model Specification\n\nThe consumer's problem is to choose from four options: purchase from both insurers (B), purchase from X only (X), purchase from Y only (Y), or purchase from none (N). The utility is the supremum over these options.\n\nThe discontinuity in the profit function `$P_X(\\mathcal{O}_X, \\mathcal{O}_Y)$` motivates the creation of an **Auxiliary Game (AG)**. The AG uses a modified profit function, `$\\overline{P}_X$`, which is the upper semi-continuous (u.s.c.) envelope of `$P_X$`:\n  \n\\overline{P}_{X}(\\mathcal{O}_{X},\\mathcal{O}_{Y}) = \\lim_{\\mathcal{O} \\to \\mathcal{O}_{X}} \\sup P_{X}(\\mathcal{O}, \\mathcal{O}_{Y}) \\quad \\text{(Eq. (1))}\n \nThis `$\\overline{P}_X$` represents an optimistic view where the insurer assumes they will win any tie-break in consumer choice.\n\nA key result connects the equilibria of the two games:\n**Proposition 1:** `$(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` is a Nash Equilibrium (NE) of the initial game if and only if:\n(i) `$(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` is a NE of the AG, AND\n(ii) `$P_X(\\mathcal{O}_X^*, \\mathcal{O}_Y^*) = \\overline{P}_X(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` and `$P_Y(\\mathcal{O}_X^*, \\mathcal{O}_Y^*) = \\overline{P}_Y(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$`.\n\n### Question\n\nRegarding the model's methodological framework for handling discontinuous payoffs, select all correct statements.", "Answer": ["B", "D"], "qid": "338", "question": "### Background\n\n**Research Question.** This problem explores the methodological foundation for analyzing games with discontinuous payoffs, a common issue in models of insurance where firms' optimal pricing makes consumers exactly indifferent between multiple choices.\n\n**Setting / Institutional Environment.** A risk-averse consumer faces two independent risks. Two risk-neutral, specialized monopoly insurers (X and Y) simultaneously make take-it-or-leave-it contract offers. The consumer then chooses the option that maximizes their expected utility. Because profit-maximizing insurers aim to extract all consumer surplus, they create situations of consumer indifference, which leads to discontinuities in their profit functions and makes standard existence theorems for Nash Equilibrium inapplicable.\n\n### Data / Model Specification\n\nThe consumer's problem is to choose from four options: purchase from both insurers (B), purchase from X only (X), purchase from Y only (Y), or purchase from none (N). The utility is the supremum over these options.\n\nThe discontinuity in the profit function `$P_X(\\mathcal{O}_X, \\mathcal{O}_Y)$` motivates the creation of an **Auxiliary Game (AG)**. The AG uses a modified profit function, `$\\overline{P}_X$`, which is the upper semi-continuous (u.s.c.) envelope of `$P_X$`:\n  \n\\overline{P}_{X}(\\mathcal{O}_{X},\\mathcal{O}_{Y}) = \\lim_{\\mathcal{O} \\to \\mathcal{O}_{X}} \\sup P_{X}(\\mathcal{O}, \\mathcal{O}_{Y}) \\quad \\text{(Eq. (1))}\n \nThis `$\\overline{P}_X$` represents an optimistic view where the insurer assumes they will win any tie-break in consumer choice.\n\nA key result connects the equilibria of the two games:\n**Proposition 1:** `$(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` is a Nash Equilibrium (NE) of the initial game if and only if:\n(i) `$(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` is a NE of the AG, AND\n(ii) `$P_X(\\mathcal{O}_X^*, \\mathcal{O}_Y^*) = \\overline{P}_X(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$` and `$P_Y(\\mathcal{O}_X^*, \\mathcal{O}_Y^*) = \\overline{P}_Y(\\mathcal{O}_X^*, \\mathcal{O}_Y^*)$`.\n\n### Question\n\nRegarding the model's methodological framework for handling discontinuous payoffs, select all correct statements.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 339, "Question": "### Background\n\n**Research Question.** This problem provides a comprehensive examination of the paper's central contribution: a novel general equilibrium model designed to resolve the market failures associated with externalities. The analysis covers the model's core mechanics (Lindahl pricing), its equilibrium concept, and its fundamental welfare properties (the First and Second Welfare Theorems).\n\n**Setting / Institutional Environment.** The setting is a private ownership economy with `I` consumers and `F` firms. The model extends the standard general equilibrium framework to include private goods, directed externalities (private bads), public goods, and a market for public externality rights. These rights can be used by firms to generate public bads (e.g., pollution) or held by the public as a public good (abatement).\n\n### Data / Model Specification\n\n**Commodities and Agents:**\n- `I` consumers, `F` firms.\n- `N^c` private consumer goods, `N^d` directed externalities, `N^g` public goods, `N^r` public externality rights.\n- Consumer `i` has an endowment `ω_i = (ω_i^c, 0, 0, ω_i^r)` and a preference relation `≿_i` over a consumption set `X_i`.\n- Firm `f` has a production set `Y_f` and an endowment of property rights `η_f = (0, 0, 0, η_f^r)`.\n\n**Key Assumptions:**\n- **Preferences (A1-A4):** Complete, transitive, continuous, weakly convex, and locally non-satiated.\n- **Production (B1-B3):** Individual production sets `Y_f` are non-empty, closed, and convex. The aggregate production set `Y` is closed.\n\n**Prices and Budgets:**\n- The price space includes personalized prices for public commodities. The price vector for consumer `i` is `p_i = (p^c, p^d, p_i^g, p_i^r)`.\n- The price vector faced by firms is `p = (p^c, p^d, ∑_i p_i^g, ∑_i p_i^r)`. (Eq. (1))\n- The budget set for consumer `i` with profit shares `θ_{if}` is:\n  \nB_{i}(\\omega_{i},\\theta_{i},\\pi,\\mathbf{p})\\equiv\\left\\{x_{i}\\in X_{i} \\mid p_{i}x_{i}\\leqslant p\\omega_{i}+\\sum_{f}\\theta_{i f}\\pi_{f}\\right\\}\n \n\n**Coasian Equilibrium Definition:** A feasible allocation `a` and price vector `p` is a Coasian equilibrium if:\n(a) For all `i`, `x_i` maximizes `≿_i` on `B_i`.\n(b) For all `f`, `y_f` maximizes profits `p · y_f` over `Y_f`.\n(c) Profits are defined as `π_f = p · (y_f + η_f)`.\n\n### Question\n\nSelect all statements that correctly describe the pricing and efficiency properties of the Coasian equilibrium defined in the model.", "Answer": ["A", "B"], "qid": "339", "question": "### Background\n\n**Research Question.** This problem provides a comprehensive examination of the paper's central contribution: a novel general equilibrium model designed to resolve the market failures associated with externalities. The analysis covers the model's core mechanics (Lindahl pricing), its equilibrium concept, and its fundamental welfare properties (the First and Second Welfare Theorems).\n\n**Setting / Institutional Environment.** The setting is a private ownership economy with `I` consumers and `F` firms. The model extends the standard general equilibrium framework to include private goods, directed externalities (private bads), public goods, and a market for public externality rights. These rights can be used by firms to generate public bads (e.g., pollution) or held by the public as a public good (abatement).\n\n### Data / Model Specification\n\n**Commodities and Agents:**\n- `I` consumers, `F` firms.\n- `N^c` private consumer goods, `N^d` directed externalities, `N^g` public goods, `N^r` public externality rights.\n- Consumer `i` has an endowment `ω_i = (ω_i^c, 0, 0, ω_i^r)` and a preference relation `≿_i` over a consumption set `X_i`.\n- Firm `f` has a production set `Y_f` and an endowment of property rights `η_f = (0, 0, 0, η_f^r)`.\n\n**Key Assumptions:**\n- **Preferences (A1-A4):** Complete, transitive, continuous, weakly convex, and locally non-satiated.\n- **Production (B1-B3):** Individual production sets `Y_f` are non-empty, closed, and convex. The aggregate production set `Y` is closed.\n\n**Prices and Budgets:**\n- The price space includes personalized prices for public commodities. The price vector for consumer `i` is `p_i = (p^c, p^d, p_i^g, p_i^r)`.\n- The price vector faced by firms is `p = (p^c, p^d, ∑_i p_i^g, ∑_i p_i^r)`. (Eq. (1))\n- The budget set for consumer `i` with profit shares `θ_{if}` is:\n  \nB_{i}(\\omega_{i},\\theta_{i},\\pi,\\mathbf{p})\\equiv\\left\\{x_{i}\\in X_{i} \\mid p_{i}x_{i}\\leqslant p\\omega_{i}+\\sum_{f}\\theta_{i f}\\pi_{f}\\right\\}\n \n\n**Coasian Equilibrium Definition:** A feasible allocation `a` and price vector `p` is a Coasian equilibrium if:\n(a) For all `i`, `x_i` maximizes `≿_i` on `B_i`.\n(b) For all `f`, `y_f` maximizes profits `p · y_f` over `Y_f`.\n(c) Profits are defined as `π_f = p · (y_f + η_f)`.\n\n### Question\n\nSelect all statements that correctly describe the pricing and efficiency properties of the Coasian equilibrium defined in the model.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 340, "Question": "### Background\n\n**Research Question.** This problem investigates the paper's central critique of Starrett's \"fundamental nonconvexity\" argument. It contrasts Starrett's model of market failure, based on an implicit assumption of unbounded pollution rights, with the paper's alternative Coasian model where bounded, tradable property rights restore market equilibrium.\n\n**Setting / Institutional Environment.** The analysis begins with Starrett's classic example of a steel mill (polluter) and a laundry (victim). The focus is on the Arrow market for the \"laundry's observation of steel production,\" which functions as a market for pollution rights. This is then contrasted with the paper's two-industry model where the total stock of externality rights is explicitly bounded.\n\n### Data / Model Specification\n\n**Part 1: Starrett's Arrow Market**\n- Let `p_e` be the price of the Arrow commodity (the right to pollute).\n- **Supply of rights by the laundry, `S(p_e)`:** The laundry is endowed with the right to a clean environment.\n  - If `p_e > 0`, the laundry can earn infinite revenue by shutting down and selling an infinite number of rights. Thus, `S(p_e) = ∞`.\n  - If `p_e = 0`, the laundry supplies zero rights. Thus, `S(p_e) = 0`.\n- **Demand for rights by the steel mill, `D(p_e)`:** The mill demands a finite, positive number of rights, `D(p_e) > 0`.\n\n**Part 2: The Paper's Coasian Model with Bounded Rights**\n- Two industries: laundry (`l`) and steel (`s`). Inputs are labor (`x_l`, `x_s`) and externality rights (`e_l`, `e_s`).\n- Total rights are fixed: `η_l + η_s = 100`, where `η_i` is the initial endowment for firm `i`.\n- **Production & Externality:**\n  \n\\ell = x_{\\ell} + e_{\\ell} \\quad \\text{(Eq. (1))}\n \n  \ns = x_{s} \\quad \\text{(Eq. (2))}\n \n  \ne_{s} = 0.5s \\quad \\text{(Eq. (3))}\n \n- **Equilibrium Prices:** Price of labor is 1. `p_l = 1`, `p_s = 1.5`, `p_e = 1`.\n\n### Question\n\nSelect all statements that correctly describe the market failure in the Arrow-Starrett model and its resolution in the paper's Coasian model.", "Answer": ["C", "D"], "qid": "340", "question": "### Background\n\n**Research Question.** This problem investigates the paper's central critique of Starrett's \"fundamental nonconvexity\" argument. It contrasts Starrett's model of market failure, based on an implicit assumption of unbounded pollution rights, with the paper's alternative Coasian model where bounded, tradable property rights restore market equilibrium.\n\n**Setting / Institutional Environment.** The analysis begins with Starrett's classic example of a steel mill (polluter) and a laundry (victim). The focus is on the Arrow market for the \"laundry's observation of steel production,\" which functions as a market for pollution rights. This is then contrasted with the paper's two-industry model where the total stock of externality rights is explicitly bounded.\n\n### Data / Model Specification\n\n**Part 1: Starrett's Arrow Market**\n- Let `p_e` be the price of the Arrow commodity (the right to pollute).\n- **Supply of rights by the laundry, `S(p_e)`:** The laundry is endowed with the right to a clean environment.\n  - If `p_e > 0`, the laundry can earn infinite revenue by shutting down and selling an infinite number of rights. Thus, `S(p_e) = ∞`.\n  - If `p_e = 0`, the laundry supplies zero rights. Thus, `S(p_e) = 0`.\n- **Demand for rights by the steel mill, `D(p_e)`:** The mill demands a finite, positive number of rights, `D(p_e) > 0`.\n\n**Part 2: The Paper's Coasian Model with Bounded Rights**\n- Two industries: laundry (`l`) and steel (`s`). Inputs are labor (`x_l`, `x_s`) and externality rights (`e_l`, `e_s`).\n- Total rights are fixed: `η_l + η_s = 100`, where `η_i` is the initial endowment for firm `i`.\n- **Production & Externality:**\n  \n\\ell = x_{\\ell} + e_{\\ell} \\quad \\text{(Eq. (1))}\n \n  \ns = x_{s} \\quad \\text{(Eq. (2))}\n \n  \ne_{s} = 0.5s \\quad \\text{(Eq. (3))}\n \n- **Equilibrium Prices:** Price of labor is 1. `p_l = 1`, `p_s = 1.5`, `p_e = 1`.\n\n### Question\n\nSelect all statements that correctly describe the market failure in the Arrow-Starrett model and its resolution in the paper's Coasian model.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 341, "Question": "### Background\n\n**Research Question.** This problem investigates the paper's refutation of Starrett's concern that Local Pareto Satiation (LPS) can cause market decentralization to fail. The core of the argument rests on the geometric properties of the feasible production set when viewed in the extended Arrow commodity space.\n\n**Setting / Institutional Environment.** The analysis contrasts the *ordinary commodity space* (e.g., lawns for Jones and Smith) with the *extended Arrow commodity space*, which includes \"observation\" commodities to internalize externalities. An allocation is a point of Local Pareto Satiation (LPS) if it is in the interior of the ordinary feasible set but is Pareto dominated by another allocation.\n\n### Data / Model Specification\n\nConsider a general economy with `A` agents (consumers and firms) and `N` private goods.\n\n- The dimension of the ordinary commodity space is `A * N`.\n- The Arrow commodity space includes a commodity `x_{ijk}` for each agent `i` observing the production/consumption of each good `k` by each agent `j`. The dimension of this space is `A^2 * N`.\n- The Arrovian joint production technology is defined by the constraints that for any good `k` produced by agent `j`, all other agents `i` observe the same amount: `x_{ijk} = x_{jjk}` for all `i=1,...,A`.\n\n### Question\n\nSelect all correct statements regarding the geometry of the Arrow commodity space and its implications for Local Pareto Satiation (LPS).", "Answer": ["C", "D"], "qid": "341", "question": "### Background\n\n**Research Question.** This problem investigates the paper's refutation of Starrett's concern that Local Pareto Satiation (LPS) can cause market decentralization to fail. The core of the argument rests on the geometric properties of the feasible production set when viewed in the extended Arrow commodity space.\n\n**Setting / Institutional Environment.** The analysis contrasts the *ordinary commodity space* (e.g., lawns for Jones and Smith) with the *extended Arrow commodity space*, which includes \"observation\" commodities to internalize externalities. An allocation is a point of Local Pareto Satiation (LPS) if it is in the interior of the ordinary feasible set but is Pareto dominated by another allocation.\n\n### Data / Model Specification\n\nConsider a general economy with `A` agents (consumers and firms) and `N` private goods.\n\n- The dimension of the ordinary commodity space is `A * N`.\n- The Arrow commodity space includes a commodity `x_{ijk}` for each agent `i` observing the production/consumption of each good `k` by each agent `j`. The dimension of this space is `A^2 * N`.\n- The Arrovian joint production technology is defined by the constraints that for any good `k` produced by agent `j`, all other agents `i` observe the same amount: `x_{ijk} = x_{jjk}` for all `i=1,...,A`.\n\n### Question\n\nSelect all correct statements regarding the geometry of the Arrow commodity space and its implications for Local Pareto Satiation (LPS).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 342, "Question": "### Background\n\nAn influential test in asset pricing, known as the Daniel-Titman test, performs a \"horse race\" to see whether firm characteristics (like book-to-market ratio) or risk covariances (like factor loadings) are better predictors of stock returns. The common finding that characteristics dominate is often interpreted as evidence of market mispricing.\n\nThis paper investigates whether this conclusion is warranted by comparing results from real-world data to data simulated from a rational asset pricing model where risk, by construction, determines expected returns.\n\n### Data / Model Specification\n\nThe following table presents results from double-sorted portfolios. Stocks are first sorted into quintiles based on their book-to-market (B/M) ratio, and then within each B/M quintile, they are further sorted into sub-portfolios based on a measure of risk.\n\n*   **Panel A** uses historical U.S. stock market data (July 1973 - June 2010), with risk measured by pre-formation HML factor loadings.\n*   **Panel B** uses data simulated from a rational model, with risk measured by the same noisy HML factor loadings.\n*   **Panel C** uses the same simulated data, but with risk measured by the true, perfectly-measured conditional beta (`β^M`).\n\n**Table 1. Mean Monthly Percentage Excess Returns of 25 Portfolios**\n\n| | Low B/M | 2 | 3 | 4 | High B/M | All (B/M Spread) |\n|:---|---:|---:|---:|---:|---:|---:|\n| **Panel A: Data, HML loadings** | | | | | | |\n| Low HML | 0.28 | 0.58 | 0.84 | 0.50 | 1.05 | 0.65 |\n| High HML | 0.42 | 0.92 | 0.93 | 0.83 | 1.34 | 0.89 |\n| **All (HML Spread)** | **0.70** | **0.66** | **0.75** | **0.66** | **1.07** | **0.24** |\n| | | | | | | |\n| **Panel B: Model, HML loadings** | | | | | | |\n| Low HML | 0.65 | 0.71 | 0.76 | 0.82 | 0.96 | 0.78 |\n| High HML | 0.66 | 0.72 | 0.77 | 0.85 | 1.01 | 0.80 |\n| **All (HML Spread)** | **0.31** | **0.22** | **0.18** | **0.13** | **0.05** | **0.02** |\n| | | | | | | |\n| **Panel C: Model, true betas** | | | | | | |\n| Low Beta | 0.58 | 0.63 | 0.67 | 0.70 | 0.78 | 0.67 |\n| High Beta | 0.78 | 0.86 | 0.90 | 1.04 | 1.40 | 1.00 |\n| **All (Beta Spread)** | **0.36** | **0.47** | **0.47** | **0.54** | **0.62** | **0.33** |\n\n*Note: The value in the top-right of the \"All\" row is the High-minus-Low B/M spread, averaged across risk quintiles. The value in the bottom-right of each panel is the High-minus-Low risk spread, averaged across B/M quintiles.*\n\nBased on the provided data, which of the following statements are valid conclusions supported by the analysis? Select all that apply.", "Answer": ["A", "B", "D"], "qid": "342", "question": "### Background\n\nAn influential test in asset pricing, known as the Daniel-Titman test, performs a \"horse race\" to see whether firm characteristics (like book-to-market ratio) or risk covariances (like factor loadings) are better predictors of stock returns. The common finding that characteristics dominate is often interpreted as evidence of market mispricing.\n\nThis paper investigates whether this conclusion is warranted by comparing results from real-world data to data simulated from a rational asset pricing model where risk, by construction, determines expected returns.\n\n### Data / Model Specification\n\nThe following table presents results from double-sorted portfolios. Stocks are first sorted into quintiles based on their book-to-market (B/M) ratio, and then within each B/M quintile, they are further sorted into sub-portfolios based on a measure of risk.\n\n*   **Panel A** uses historical U.S. stock market data (July 1973 - June 2010), with risk measured by pre-formation HML factor loadings.\n*   **Panel B** uses data simulated from a rational model, with risk measured by the same noisy HML factor loadings.\n*   **Panel C** uses the same simulated data, but with risk measured by the true, perfectly-measured conditional beta (`β^M`).\n\n**Table 1. Mean Monthly Percentage Excess Returns of 25 Portfolios**\n\n| | Low B/M | 2 | 3 | 4 | High B/M | All (B/M Spread) |\n|:---|---:|---:|---:|---:|---:|---:|\n| **Panel A: Data, HML loadings** | | | | | | |\n| Low HML | 0.28 | 0.58 | 0.84 | 0.50 | 1.05 | 0.65 |\n| High HML | 0.42 | 0.92 | 0.93 | 0.83 | 1.34 | 0.89 |\n| **All (HML Spread)** | **0.70** | **0.66** | **0.75** | **0.66** | **1.07** | **0.24** |\n| | | | | | | |\n| **Panel B: Model, HML loadings** | | | | | | |\n| Low HML | 0.65 | 0.71 | 0.76 | 0.82 | 0.96 | 0.78 |\n| High HML | 0.66 | 0.72 | 0.77 | 0.85 | 1.01 | 0.80 |\n| **All (HML Spread)** | **0.31** | **0.22** | **0.18** | **0.13** | **0.05** | **0.02** |\n| | | | | | | |\n| **Panel C: Model, true betas** | | | | | | |\n| Low Beta | 0.58 | 0.63 | 0.67 | 0.70 | 0.78 | 0.67 |\n| High Beta | 0.78 | 0.86 | 0.90 | 1.04 | 1.40 | 1.00 |\n| **All (Beta Spread)** | **0.36** | **0.47** | **0.47** | **0.54** | **0.62** | **0.33** |\n\n*Note: The value in the top-right of the \"All\" row is the High-minus-Low B/M spread, averaged across risk quintiles. The value in the bottom-right of each panel is the High-minus-Low risk spread, averaged across B/M quintiles.*\n\nBased on the provided data, which of the following statements are valid conclusions supported by the analysis? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nAn influential test in asset prici...", "model_answer": "This is a detailed response to the question about ### Background\n\nAn influential test in asset prici...", "error": null}}
{"ID": 343, "Question": "### Background\n\nThe paper investigates two potential sources of bias in the Daniel-Titman test that may cause firm characteristics to appear to dominate risk covariances in predicting returns: (1) the high correlation between characteristics and true risk, and (2) the sequential nature of the portfolio sorting procedure.\n\n### Data / Model Specification\n\nThe analysis uses data simulated from a rational model where expected returns are determined solely by a firm's true conditional beta (`β^M`). To isolate the component of Book-to-Market (B/M) that is orthogonal to risk, a \"risk-adjusted\" B/M is constructed. The following table summarizes results from modified Daniel-Titman tests on the simulated data.\n\n*   **Panel A** sorts first on risk-adjusted B/M, then on true beta.\n*   **Panel B** reverses the standard procedure, sorting first on true beta, then on B/M.\n\n**Table 1. Mean Monthly Percentage Excess Returns of 25 Portfolios in the Model**\n\n| | Low | 2 | 3 | 4 | High | All |\n|:---|---:|---:|---:|---:|---:|---:|\n| **Panel A: Risk-Adjusted B/M, then True Betas** | | | | | | |\n| Low B/M | 0.64 | 0.62 | 0.62 | 0.63 | 0.67 | 0.64 |\n| High B/M | 1.13 | 1.00 | 0.99 | 1.01 | 1.16 | 1.06 |\n| **All** | **0.03** | **0.22** | **0.24** | **0.24** | **0.29** | **0.23** |\n| | | | | | | |\n| **Panel B: True Betas, then B/M** | | | | | | |\n| Low Beta | 0.60 | 0.62 | 0.62 | 0.65 | 0.63 | 0.63 |\n| High Beta | 1.02 | 1.02 | 1.06 | 1.09 | 1.23 | 1.08 |\n| **All** | **0.42** | **0.40** | **0.44** | **0.44** | **0.60** | **0.45** |\n\n*Note: The value in the bottom-left of each panel is the High-minus-Low spread for the first sorting variable. The value in the top-right of the \"All\" row is the High-minus-Low spread for the second sorting variable.*\n\nBased on the provided data and the paper's logic, which of the following conclusions about biases in the Daniel-Titman test are valid? Select all that apply.", "Answer": ["C", "D"], "qid": "343", "question": "### Background\n\nThe paper investigates two potential sources of bias in the Daniel-Titman test that may cause firm characteristics to appear to dominate risk covariances in predicting returns: (1) the high correlation between characteristics and true risk, and (2) the sequential nature of the portfolio sorting procedure.\n\n### Data / Model Specification\n\nThe analysis uses data simulated from a rational model where expected returns are determined solely by a firm's true conditional beta (`β^M`). To isolate the component of Book-to-Market (B/M) that is orthogonal to risk, a \"risk-adjusted\" B/M is constructed. The following table summarizes results from modified Daniel-Titman tests on the simulated data.\n\n*   **Panel A** sorts first on risk-adjusted B/M, then on true beta.\n*   **Panel B** reverses the standard procedure, sorting first on true beta, then on B/M.\n\n**Table 1. Mean Monthly Percentage Excess Returns of 25 Portfolios in the Model**\n\n| | Low | 2 | 3 | 4 | High | All |\n|:---|---:|---:|---:|---:|---:|---:|\n| **Panel A: Risk-Adjusted B/M, then True Betas** | | | | | | |\n| Low B/M | 0.64 | 0.62 | 0.62 | 0.63 | 0.67 | 0.64 |\n| High B/M | 1.13 | 1.00 | 0.99 | 1.01 | 1.16 | 1.06 |\n| **All** | **0.03** | **0.22** | **0.24** | **0.24** | **0.29** | **0.23** |\n| | | | | | | |\n| **Panel B: True Betas, then B/M** | | | | | | |\n| Low Beta | 0.60 | 0.62 | 0.62 | 0.65 | 0.63 | 0.63 |\n| High Beta | 1.02 | 1.02 | 1.06 | 1.09 | 1.23 | 1.08 |\n| **All** | **0.42** | **0.40** | **0.44** | **0.44** | **0.60** | **0.45** |\n\n*Note: The value in the bottom-left of each panel is the High-minus-Low spread for the first sorting variable. The value in the top-right of the \"All\" row is the High-minus-Low spread for the second sorting variable.*\n\nBased on the provided data and the paper's logic, which of the following conclusions about biases in the Daniel-Titman test are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper investigates two potenti...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper investigates two potenti...", "error": null}}
{"ID": 344, "Question": "### Background\n\nThe paper develops a two-period general equilibrium model to demonstrate the theoretical equivalence of the consumption-based (risk-focused) and investment-based (characteristics-focused) approaches to asset pricing.\n\n### Data / Model Specification\n\nThe household's consumption first-order condition yields the standard beta-pricing model:\n\n  \nE_{0}[r_{i1}^{S}] = r_{f}+\\beta_{i}^{M}\\lambda_{M} \\quad \\text{(Eq. 1)}\n \n\nwhere `r_{i1}^S` is the stock return, `r_f` is the risk-free rate, `β_i^M` is the firm's systematic risk, and `λ_M` is the price of risk.\n\nThe firm's investment first-order condition implies that the expected stock return is determined by its observable characteristics:\n\n  \nE_{0}[r_{i1}^{S}]=\\frac{E_{0}[\\Pi_{i1}]}{1+a(I_{i0}/K_{i0})} \\quad \\text{(Eq. 2)}\n \n\nwhere `E_0[Π_{i1}]` is expected future profitability, `I_{i0}/K_{i0}` is the investment-to-capital ratio, and `a > 0` is an adjustment cost parameter.\n\nBy equating these two expressions, one can derive a formula for a firm's systematic risk as a function of its characteristics:\n\n  \n\\beta_{i}^{M} = \\frac{1}{\\lambda_M} \\left[ \\frac{E_{0}[\\Pi_{i1}]}{1+a(I_{i0}/K_{i0})} - r_f \\right] \\quad \\text{(Eq. 3)}\n \n\nBased on this theoretical framework, which of the following statements are **INCORRECT** interpretations or implications of the model?", "Answer": ["A", "C", "D"], "qid": "344", "question": "### Background\n\nThe paper develops a two-period general equilibrium model to demonstrate the theoretical equivalence of the consumption-based (risk-focused) and investment-based (characteristics-focused) approaches to asset pricing.\n\n### Data / Model Specification\n\nThe household's consumption first-order condition yields the standard beta-pricing model:\n\n  \nE_{0}[r_{i1}^{S}] = r_{f}+\\beta_{i}^{M}\\lambda_{M} \\quad \\text{(Eq. 1)}\n \n\nwhere `r_{i1}^S` is the stock return, `r_f` is the risk-free rate, `β_i^M` is the firm's systematic risk, and `λ_M` is the price of risk.\n\nThe firm's investment first-order condition implies that the expected stock return is determined by its observable characteristics:\n\n  \nE_{0}[r_{i1}^{S}]=\\frac{E_{0}[\\Pi_{i1}]}{1+a(I_{i0}/K_{i0})} \\quad \\text{(Eq. 2)}\n \n\nwhere `E_0[Π_{i1}]` is expected future profitability, `I_{i0}/K_{i0}` is the investment-to-capital ratio, and `a > 0` is an adjustment cost parameter.\n\nBy equating these two expressions, one can derive a formula for a firm's systematic risk as a function of its characteristics:\n\n  \n\\beta_{i}^{M} = \\frac{1}{\\lambda_M} \\left[ \\frac{E_{0}[\\Pi_{i1}]}{1+a(I_{i0}/K_{i0})} - r_f \\right] \\quad \\text{(Eq. 3)}\n \n\nBased on this theoretical framework, which of the following statements are **INCORRECT** interpretations or implications of the model?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper develops a two-period ge...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper develops a two-period ge...", "error": null}}
{"ID": 345, "Question": "### Background\n\nThe paper tests the nutrition-based efficiency wage model (EWM) using data from Indian agriculture. In one case, the author analyzes wage data from Ferozepur, Punjab.\n\n### Data / Model Specification\n\n**Table 1: Daily wage rates of adult male casual and permanent labor in Ferozepur, 1967-70**\n| Labor Type                                               | Rupees | Calories obtained from spending 35% of daily wage on wheat |\n| :------------------------------------------------------- | :----- | :--------------------------------------------------------- |\n| Lowest daily wage rate for male casual laborers (August) | 5.08   | 7246                                                       |\n| Daily wage rate of permanent laborers                    | 4.56   | 6170                                                       |\n\nThe author states that the calorie estimates in Table 1 are deliberately constructed as underestimates: \"This will give us an underestimate of the calorie equivalent of the wage for two reasons.\"\n\n### Question\n\nBased on the paper's description of its methodology, which of the following are the two reasons provided for why the calorie estimates in Table 1 are likely underestimates?", "Answer": ["A", "D"], "qid": "345", "question": "### Background\n\nThe paper tests the nutrition-based efficiency wage model (EWM) using data from Indian agriculture. In one case, the author analyzes wage data from Ferozepur, Punjab.\n\n### Data / Model Specification\n\n**Table 1: Daily wage rates of adult male casual and permanent labor in Ferozepur, 1967-70**\n| Labor Type                                               | Rupees | Calories obtained from spending 35% of daily wage on wheat |\n| :------------------------------------------------------- | :----- | :--------------------------------------------------------- |\n| Lowest daily wage rate for male casual laborers (August) | 5.08   | 7246                                                       |\n| Daily wage rate of permanent laborers                    | 4.56   | 6170                                                       |\n\nThe author states that the calorie estimates in Table 1 are deliberately constructed as underestimates: \"This will give us an underestimate of the calorie equivalent of the wage for two reasons.\"\n\n### Question\n\nBased on the paper's description of its methodology, which of the following are the two reasons provided for why the calorie estimates in Table 1 are likely underestimates?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper tests the nutrition-base...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper tests the nutrition-base...", "error": null}}
{"ID": 346, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical framework used to link urban concentration to national productivity growth, providing a formal basis for an empirical test of the Williamson hypothesis.\n\n**Setting.** The analysis begins with a standard aggregate production function for a national economy and then specifies a model for the evolution of technology or Total Factor Productivity (TFP).\n\n**Variables & Parameters.**\n- `Y, K, N, A`: Aggregate output, capital, labor, and TFP, respectively.\n- `α`: The output elasticity of capital.\n- `primacy`: The share of the largest metro area in a country's national urban population.\n- `ln(Y/N)`: Log of output per worker, a proxy for the level of economic development.\n- `α₀, α₂, β₀`: Structural parameters to be estimated.\n\n### Data / Model Specification\n\nThe economy's output is described by a Cobb-Douglas production function:\n  \nY_{i}(t) = K_{i}(t)^{\\alpha} (A_{i}(t) N_{i}(t))^{1-\\alpha}\n \n(Eq. 1)\n\nThe paper hypothesizes that TFP growth is itself a function of urban concentration. The specific functional form for the contribution of primacy to TFP growth is:\n  \nf(\\mathrm{primacy}, \\ln(Y/N)) = \\left(\\alpha_{0}+\\alpha_{2}\\ln\\left({\\frac{Y}{N}}\\right)\\right)\\mathrm{primacy} + \\beta_{0}\\mathrm{primacy}^{2}\n \n(Eq. 2)\n\n### Question\n\nBased on the provided model, select all statements that correctly describe the model's assumptions, properties, and consistency with the Williamson hypothesis.", "Answer": ["B", "C"], "qid": "346", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical framework used to link urban concentration to national productivity growth, providing a formal basis for an empirical test of the Williamson hypothesis.\n\n**Setting.** The analysis begins with a standard aggregate production function for a national economy and then specifies a model for the evolution of technology or Total Factor Productivity (TFP).\n\n**Variables & Parameters.**\n- `Y, K, N, A`: Aggregate output, capital, labor, and TFP, respectively.\n- `α`: The output elasticity of capital.\n- `primacy`: The share of the largest metro area in a country's national urban population.\n- `ln(Y/N)`: Log of output per worker, a proxy for the level of economic development.\n- `α₀, α₂, β₀`: Structural parameters to be estimated.\n\n### Data / Model Specification\n\nThe economy's output is described by a Cobb-Douglas production function:\n  \nY_{i}(t) = K_{i}(t)^{\\alpha} (A_{i}(t) N_{i}(t))^{1-\\alpha}\n \n(Eq. 1)\n\nThe paper hypothesizes that TFP growth is itself a function of urban concentration. The specific functional form for the contribution of primacy to TFP growth is:\n  \nf(\\mathrm{primacy}, \\ln(Y/N)) = \\left(\\alpha_{0}+\\alpha_{2}\\ln\\left({\\frac{Y}{N}}\\right)\\right)\\mathrm{primacy} + \\beta_{0}\\mathrm{primacy}^{2}\n \n(Eq. 2)\n\n### Question\n\nBased on the provided model, select all statements that correctly describe the model's assumptions, properties, and consistency with the Williamson hypothesis.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 347, "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of identifying the causal effect of urban primacy on productivity growth using panel data, in the presence of unobserved heterogeneity and endogeneity.\n\n**Setting.** The analysis uses a dynamic panel data model for a set of countries over multiple time periods.\n\n**Variables & Parameters.**\n- `Δln(A_it)`: TFP growth in country `i` from `t-1` to `t`.\n- `X_{i,t-1}`: A vector of lagged covariates, including measures of urban primacy.\n- `μ_i`: A time-invariant, unobserved country-specific effect (e.g., geography, deep-rooted institutions, culture).\n- `δ_t`: A time-specific effect common to all countries (e.g., global business cycles).\n- `ε_it`: A time-varying, country-specific idiosyncratic error term.\n\n### Data / Model Specification\n\nThe underlying structural model for TFP growth is:\n  \n\\ln A_{i}(t)-\\ln A_{i}(t-1) = f(X_{i,t-1}) + \\mu_{i} + \\delta_{t} + \\varepsilon_{i t}\n \n(Eq. 1)\n\nTo identify the causal effect of the variables in `X`, the paper employs a two-step strategy:\n1.  **First-Differencing:** The equation is first-differenced to eliminate the country fixed effect `μ_i`.\n2.  **Instrumental Variables (GMM):** The resulting equation in first-differences is estimated via GMM, using lagged *levels* of the covariates (e.g., `X_{i,t-3}`, `X_{i,t-4}`) as instruments for the first-differenced covariates (e.g., `X_{i,t-1} - X_{i,t-2}`).\n\n### Question\n\nRegarding the paper's two-step identification strategy (first-differencing followed by GMM with lagged-level instruments), select all statements that are correct.", "Answer": ["A", "B"], "qid": "347", "question": "### Background\n\n**Research Question.** This problem addresses the core challenge of identifying the causal effect of urban primacy on productivity growth using panel data, in the presence of unobserved heterogeneity and endogeneity.\n\n**Setting.** The analysis uses a dynamic panel data model for a set of countries over multiple time periods.\n\n**Variables & Parameters.**\n- `Δln(A_it)`: TFP growth in country `i` from `t-1` to `t`.\n- `X_{i,t-1}`: A vector of lagged covariates, including measures of urban primacy.\n- `μ_i`: A time-invariant, unobserved country-specific effect (e.g., geography, deep-rooted institutions, culture).\n- `δ_t`: A time-specific effect common to all countries (e.g., global business cycles).\n- `ε_it`: A time-varying, country-specific idiosyncratic error term.\n\n### Data / Model Specification\n\nThe underlying structural model for TFP growth is:\n  \n\\ln A_{i}(t)-\\ln A_{i}(t-1) = f(X_{i,t-1}) + \\mu_{i} + \\delta_{t} + \\varepsilon_{i t}\n \n(Eq. 1)\n\nTo identify the causal effect of the variables in `X`, the paper employs a two-step strategy:\n1.  **First-Differencing:** The equation is first-differenced to eliminate the country fixed effect `μ_i`.\n2.  **Instrumental Variables (GMM):** The resulting equation in first-differences is estimated via GMM, using lagged *levels* of the covariates (e.g., `X_{i,t-3}`, `X_{i,t-4}`) as instruments for the first-differenced covariates (e.g., `X_{i,t-1} - X_{i,t-2}`).\n\n### Question\n\nRegarding the paper's two-step identification strategy (first-differencing followed by GMM with lagged-level instruments), select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 348, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical foundations of an implicit wage-insurance contract between a risk-neutral firm and a risk-averse worker, and how this contract can generate a gender wage gap when social norms dictate gendered caregiving responsibilities.\n\n**Setting / Institutional Environment.** The model considers a competitive labor market with two states of nature for a worker: healthy ($s=0$) or ill ($s=1$). A worker's productivity is `f'_0` when healthy and zero when ill. Firms are risk-neutral and workers are risk-averse. Firms offer wage contracts specifying payments `w_s` in each state.\n\n### Data / Model Specification\n\nThe optimal contract maximizes the worker's expected utility subject to the firm's participation constraint. In a competitive market, this constraint holds with equality (zero expected profit).\n\n**Firm's Zero-Profit Condition:**\n  \nV=(1-q)(f_{0}^{\\prime}-w_{0})+q(-w_{1}) = 0 \n\\quad \\text{(Eq. (1))}\n \nwhere `q` is the probability of the worker being unable to work.\n\n**Worker's Optimality Condition:**\nThe first-order condition for optimal consumption smoothing from the worker's constrained maximization problem implies equal marginal utility of consumption across states:\n  \nU_{c_0}^{\\prime} = U_{c_1}^{\\prime}\n\\quad \\text{(Eq. (2))}\n \n\n**Behavioral Assumption on Caregiving:**\nIt is assumed that women, but not men, stay home from work to care for other sick family members (e.g., `n` children). This differential responsibility affects their total probability of being absent from work.\n\n### Question\n\nBased on the model, select all of the following statements that are mathematically correct derivations.", "Answer": ["C", "D"], "qid": "348", "question": "### Background\n\n**Research Question.** This problem examines the theoretical foundations of an implicit wage-insurance contract between a risk-neutral firm and a risk-averse worker, and how this contract can generate a gender wage gap when social norms dictate gendered caregiving responsibilities.\n\n**Setting / Institutional Environment.** The model considers a competitive labor market with two states of nature for a worker: healthy ($s=0$) or ill ($s=1$). A worker's productivity is `f'_0` when healthy and zero when ill. Firms are risk-neutral and workers are risk-averse. Firms offer wage contracts specifying payments `w_s` in each state.\n\n### Data / Model Specification\n\nThe optimal contract maximizes the worker's expected utility subject to the firm's participation constraint. In a competitive market, this constraint holds with equality (zero expected profit).\n\n**Firm's Zero-Profit Condition:**\n  \nV=(1-q)(f_{0}^{\\prime}-w_{0})+q(-w_{1}) = 0 \n\\quad \\text{(Eq. (1))}\n \nwhere `q` is the probability of the worker being unable to work.\n\n**Worker's Optimality Condition:**\nThe first-order condition for optimal consumption smoothing from the worker's constrained maximization problem implies equal marginal utility of consumption across states:\n  \nU_{c_0}^{\\prime} = U_{c_1}^{\\prime}\n\\quad \\text{(Eq. (2))}\n \n\n**Behavioral Assumption on Caregiving:**\nIt is assumed that women, but not men, stay home from work to care for other sick family members (e.g., `n` children). This differential responsibility affects their total probability of being absent from work.\n\n### Question\n\nBased on the model, select all of the following statements that are mathematically correct derivations.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 349, "Question": "### Background\n\n**Research Question.** This problem applies the paper's theoretical model of wage insurance to a hypothetical policy counterfactual.\n\n**Setting / Institutional Environment.** Recall from the model that the optimal wage for a worker of gender `i` is `w_i = f'_0(1-P_i)`, where `P_i` is their total probability of absence. A man's probability of absence is `P_male = q_ind`. A woman is the sole caregiver for `n` family members, and her original probability of absence is `P_female = 1 - (1-q_ind)^{n+1}`.\n\n### Data / Model Specification\n\nConsider a government policy that provides public nursing care. This policy reduces the probability that a mother must miss work for a sick child from `q_ind` to `αq_ind`, where `α ∈ [0, 1)` is a policy effectiveness parameter. The policy is funded by a flat payroll tax `τ` on all salaried workers' wages, so the after-tax wage is `w_i^{net} = w_i(1-τ)`.\n\n### Question\n\nGiven this new policy, select all of the following statements that are correct.", "Answer": ["A", "C"], "qid": "349", "question": "### Background\n\n**Research Question.** This problem applies the paper's theoretical model of wage insurance to a hypothetical policy counterfactual.\n\n**Setting / Institutional Environment.** Recall from the model that the optimal wage for a worker of gender `i` is `w_i = f'_0(1-P_i)`, where `P_i` is their total probability of absence. A man's probability of absence is `P_male = q_ind`. A woman is the sole caregiver for `n` family members, and her original probability of absence is `P_female = 1 - (1-q_ind)^{n+1}`.\n\n### Data / Model Specification\n\nConsider a government policy that provides public nursing care. This policy reduces the probability that a mother must miss work for a sick child from `q_ind` to `αq_ind`, where `α ∈ [0, 1)` is a policy effectiveness parameter. The policy is funded by a flat payroll tax `τ` on all salaried workers' wages, so the after-tax wage is `w_i^{net} = w_i(1-τ)`.\n\n### Question\n\nGiven this new policy, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 350, "Question": "### Background\n\n**Research Question.** This problem explores three competing theoretical models of social learning in a continuous action space: the rational benchmark (Perfect Bayesian Equilibrium), a model of information redundancy neglect (Best Response Trailing Naïve Inference), and a model of relative overconfidence.\n\n**Setting / Institutional Environment.** A sequence of agents `t=1, 2, ..., T` make decisions. Agent `t` observes the actions of all `t-1` predecessors and receives a private binary signal `s_t ∈ {0, 1}` with precision `q_t > 0.5` about a true state `V ∈ {0, 100}`. The agent's action `a_t ∈ [0, 100]` is their belief that `V=100`.\n\n### Data / Model Specification\n\nThe models make distinct predictions about an agent's posterior belief, expressed as a log-odds ratio, `ln(a_t / (100-a_t))`. Let `L_i = (2s_i - 1)ln(q_i / (1-q_i))` be the log-likelihood ratio of signal `s_i`.\n\n1.  **Perfect Bayesian Equilibrium (PBE):** Agents are fully rational and perfectly infer all past signals from past actions. The belief is the sum of all log-likelihood ratios up to time `t`.\n      \n    \\ln\\left(\\frac{a_{t}^{PBE}}{100-a_{t}^{PBE}}\\right) = \\sum_{i=1}^{t} L_i\n     \n\n2.  **Best Response Trailing Naïve Inference (BRTNI):** Agents naively believe predecessors act only on their private signal, leading to redundant information being repeatedly counted. The weight on a predecessor's signal `s_i` is `2^{t-i-1}`.\n      \n    \\ln\\left(\\frac{a_{t}^{BRTNI}}{100-a_{t}^{BRTNI}}\\right) = \\sum_{i=1}^{t-1} 2^{t-i-1} L_i + L_t\n     \n\n3.  **Overconfidence (OC):** Agents believe predecessors are less capable. They discount the inferred signals of all predecessors by a constant factor `k ∈ (0, 1)` but use their own signal correctly. This belief is common knowledge.\n      \n    \\ln\\left(\\frac{a_{t}^{OC}}{100-a_{t}^{OC}}\\right) = \\sum_{i=1}^{t-1} k L_i + L_t\n     \n\nBased on the theoretical descriptions, which of the following statements accurately describe the core assumptions or mechanisms of the models? Select all that apply.", "Answer": ["A", "C", "D"], "qid": "350", "question": "### Background\n\n**Research Question.** This problem explores three competing theoretical models of social learning in a continuous action space: the rational benchmark (Perfect Bayesian Equilibrium), a model of information redundancy neglect (Best Response Trailing Naïve Inference), and a model of relative overconfidence.\n\n**Setting / Institutional Environment.** A sequence of agents `t=1, 2, ..., T` make decisions. Agent `t` observes the actions of all `t-1` predecessors and receives a private binary signal `s_t ∈ {0, 1}` with precision `q_t > 0.5` about a true state `V ∈ {0, 100}`. The agent's action `a_t ∈ [0, 100]` is their belief that `V=100`.\n\n### Data / Model Specification\n\nThe models make distinct predictions about an agent's posterior belief, expressed as a log-odds ratio, `ln(a_t / (100-a_t))`. Let `L_i = (2s_i - 1)ln(q_i / (1-q_i))` be the log-likelihood ratio of signal `s_i`.\n\n1.  **Perfect Bayesian Equilibrium (PBE):** Agents are fully rational and perfectly infer all past signals from past actions. The belief is the sum of all log-likelihood ratios up to time `t`.\n      \n    \\ln\\left(\\frac{a_{t}^{PBE}}{100-a_{t}^{PBE}}\\right) = \\sum_{i=1}^{t} L_i\n     \n\n2.  **Best Response Trailing Naïve Inference (BRTNI):** Agents naively believe predecessors act only on their private signal, leading to redundant information being repeatedly counted. The weight on a predecessor's signal `s_i` is `2^{t-i-1}`.\n      \n    \\ln\\left(\\frac{a_{t}^{BRTNI}}{100-a_{t}^{BRTNI}}\\right) = \\sum_{i=1}^{t-1} 2^{t-i-1} L_i + L_t\n     \n\n3.  **Overconfidence (OC):** Agents believe predecessors are less capable. They discount the inferred signals of all predecessors by a constant factor `k ∈ (0, 1)` but use their own signal correctly. This belief is common knowledge.\n      \n    \\ln\\left(\\frac{a_{t}^{OC}}{100-a_{t}^{OC}}\\right) = \\sum_{i=1}^{t-1} k L_i + L_t\n     \n\nBased on the theoretical descriptions, which of the following statements accurately describe the core assumptions or mechanisms of the models? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 351, "Question": "### Background\n\nThis paper models a relational contract where a principal has private information about future productivity. To prevent the principal from opportunistically understating future prospects to save on current bonus payments, the optimal contract may involve imposing costs on the principal when she announces bad news. This is achieved by deliberately reducing the agent's effort below the first-best level, a mechanism referred to as \"overshooting.\"\n\n### Data / Model Specification\n\nIn the model, the principal's incentive to truthfully report the next period's type (`θ_{t+1}`) is captured by a set of constraints. When the discount factor `δ` is in an intermediate range, the constraint to deter the principal from falsely reporting a low type when the true type is high becomes binding. This combined **Enforcement Constraint (EC)** for a period with a high-productivity state is:\n\n  \n-n^{h}c+\\delta(q\\Pi^{h}+(1-q)\\Pi_{0}^{l}) \\geq \\delta q g(n_{0}^{l})(\\theta^{h}-\\theta^{l}) \\quad \\text{(Eq. 1)}\n \n\nHere, `n^h` is the effort in a high state, `n_0^l` is the effort in a low state that immediately follows a high state, `c` is the agent's effort cost, `Π` represents continuation profits, and `g(n)` is the production function. The term on the right is the principal's **information rent** from lying.\n\nThe first-best effort level in a low state, `n_l^{FB}`, is defined by `θ^l g'(n_l^{FB}) = c`.\n\n### Question\n\nAccording to the paper's central argument (Proposition 4), when the constraint in Eq. (1) binds, it is optimal to set `n_0^l` inefficiently low (i.e., `n_0^l < n_l^{FB}`). Select all statements that correctly describe the logic behind this \"overshooting\" strategy.", "Answer": ["B", "C"], "qid": "351", "question": "### Background\n\nThis paper models a relational contract where a principal has private information about future productivity. To prevent the principal from opportunistically understating future prospects to save on current bonus payments, the optimal contract may involve imposing costs on the principal when she announces bad news. This is achieved by deliberately reducing the agent's effort below the first-best level, a mechanism referred to as \"overshooting.\"\n\n### Data / Model Specification\n\nIn the model, the principal's incentive to truthfully report the next period's type (`θ_{t+1}`) is captured by a set of constraints. When the discount factor `δ` is in an intermediate range, the constraint to deter the principal from falsely reporting a low type when the true type is high becomes binding. This combined **Enforcement Constraint (EC)** for a period with a high-productivity state is:\n\n  \n-n^{h}c+\\delta(q\\Pi^{h}+(1-q)\\Pi_{0}^{l}) \\geq \\delta q g(n_{0}^{l})(\\theta^{h}-\\theta^{l}) \\quad \\text{(Eq. 1)}\n \n\nHere, `n^h` is the effort in a high state, `n_0^l` is the effort in a low state that immediately follows a high state, `c` is the agent's effort cost, `Π` represents continuation profits, and `g(n)` is the production function. The term on the right is the principal's **information rent** from lying.\n\nThe first-best effort level in a low state, `n_l^{FB}`, is defined by `θ^l g'(n_l^{FB}) = c`.\n\n### Question\n\nAccording to the paper's central argument (Proposition 4), when the constraint in Eq. (1) binds, it is optimal to set `n_0^l` inefficiently low (i.e., `n_0^l < n_l^{FB}`). Select all statements that correctly describe the logic behind this \"overshooting\" strategy.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis paper models a relational con...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis paper models a relational con...", "error": null}}
{"ID": 352, "Question": "### Background\n\nThe paper's main model assumes that the principal's productivity shocks (`θ_t`) are independent and identically distributed (i.i.d.) over time. An extension considers the case where shocks are persistent, meaning the type in one period is correlated with the type in the next.\n\n### Data / Model Specification\n\nIn the main **i.i.d. model**, the optimal contract may feature an inefficiently low effort level (`n_0^l < n_l^{FB}`) for one period following a high-to-low state announcement. This serves as an \"implicit downsizing cost\" to deter the principal from lying.\n\nIn the **persistent shocks model**, a switch from a high to a low state is permanent. The principal's informational advantage from a lie (claiming the state is low when it is still high) extends over multiple future periods, as she knows the state will remain high with probability `q` each period.\n\n### Question\n\nSelect all statements that correctly describe the duration and nature of the inefficient effort distortion (`n_i^l < n_l^{FB}`) under different assumptions about the shock process.", "Answer": ["A", "B"], "qid": "352", "question": "### Background\n\nThe paper's main model assumes that the principal's productivity shocks (`θ_t`) are independent and identically distributed (i.i.d.) over time. An extension considers the case where shocks are persistent, meaning the type in one period is correlated with the type in the next.\n\n### Data / Model Specification\n\nIn the main **i.i.d. model**, the optimal contract may feature an inefficiently low effort level (`n_0^l < n_l^{FB}`) for one period following a high-to-low state announcement. This serves as an \"implicit downsizing cost\" to deter the principal from lying.\n\nIn the **persistent shocks model**, a switch from a high to a low state is permanent. The principal's informational advantage from a lie (claiming the state is low when it is still high) extends over multiple future periods, as she knows the state will remain high with probability `q` each period.\n\n### Question\n\nSelect all statements that correctly describe the duration and nature of the inefficient effort distortion (`n_i^l < n_l^{FB}`) under different assumptions about the shock process.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe paper's main model assumes tha...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe paper's main model assumes tha...", "error": null}}
{"ID": 353, "Question": "### Background\n\n**Research Question.** This problem examines the validity of the Regression Discontinuity Design (RDD) as a tool for causal inference.\n\n**Setting / Institutional Environment.** A policy discontinuity allocates temporary work visas for South Korea to Filipino applicants based on a Korean Language Test (KLT). Applicants scoring at or above a fixed cutoff (`c=120`) become eligible to migrate. The study argues that three conditions are necessary for the validity of this quasi-experiment: (1) a large discontinuity in the treatment probability, (2) continuity of baseline traits at the cutoff, and (3) no bunching of the score density at the cutoff.\n\n---\n\n### Question\n\nBased on the principles of Regression Discontinuity Design (RDD) as applied in this study, select all statements that correctly describe a threat to the design's internal validity.", "Answer": ["C", "D"], "qid": "353", "question": "### Background\n\n**Research Question.** This problem examines the validity of the Regression Discontinuity Design (RDD) as a tool for causal inference.\n\n**Setting / Institutional Environment.** A policy discontinuity allocates temporary work visas for South Korea to Filipino applicants based on a Korean Language Test (KLT). Applicants scoring at or above a fixed cutoff (`c=120`) become eligible to migrate. The study argues that three conditions are necessary for the validity of this quasi-experiment: (1) a large discontinuity in the treatment probability, (2) continuity of baseline traits at the cutoff, and (3) no bunching of the score density at the cutoff.\n\n---\n\n### Question\n\nBased on the principles of Regression Discontinuity Design (RDD) as applied in this study, select all statements that correctly describe a threat to the design's internal validity.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 354, "Question": "### Background\n\n**Research Question.** This problem demonstrates the value of the Regression Discontinuity Design (RDD) by contrasting its findings with those from non-experimental methods like Ordinary Least Squares (OLS).\n\n**Setting / Institutional Environment.** A policy discontinuity allocates temporary work visas based on a Korean Language Test (KLT) score. The study compares the causal estimate of migration's effect on non-migrant labor supply from the RDD with estimates from OLS. The OLS method uses treated households from the study sample and a control group of non-migrant households from a national survey.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the starkly different results from the two methods.\n\n**Table 1. Comparison of RDD and OLS ITT Estimates**\n| Outcome | RDD ($\\beta_{RDD}$) | SE | OLS ($\\beta_{OLS}$) | SE |\n| :--- | :---: | :---: | :---: | :---: |\n| Non-applicant: Working? | -0.047* | (0.027) | -0.141*** | (0.016) |\n\n*Note: *** p<0.01, * p<0.10. “Working?” is an indicator for whether a non-applicant adult was employed in the past six months.*\n\n---\n\n### Question\n\nThe study finds that OLS estimates of the effect of migration on non-applicant labor supply are substantially more negative than the RDD estimate (see Table 1). An analyst tries to explain this discrepancy using the omitted variable bias framework. Select all statements that represent an **incorrect** application of this framework or a misinterpretation of the results.", "Answer": ["B", "C", "D"], "qid": "354", "question": "### Background\n\n**Research Question.** This problem demonstrates the value of the Regression Discontinuity Design (RDD) by contrasting its findings with those from non-experimental methods like Ordinary Least Squares (OLS).\n\n**Setting / Institutional Environment.** A policy discontinuity allocates temporary work visas based on a Korean Language Test (KLT) score. The study compares the causal estimate of migration's effect on non-migrant labor supply from the RDD with estimates from OLS. The OLS method uses treated households from the study sample and a control group of non-migrant households from a national survey.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the starkly different results from the two methods.\n\n**Table 1. Comparison of RDD and OLS ITT Estimates**\n| Outcome | RDD ($\\beta_{RDD}$) | SE | OLS ($\\beta_{OLS}$) | SE |\n| :--- | :---: | :---: | :---: | :---: |\n| Non-applicant: Working? | -0.047* | (0.027) | -0.141*** | (0.016) |\n\n*Note: *** p<0.01, * p<0.10. “Working?” is an indicator for whether a non-applicant adult was employed in the past six months.*\n\n---\n\n### Question\n\nThe study finds that OLS estimates of the effect of migration on non-applicant labor supply are substantially more negative than the RDD estimate (see Table 1). An analyst tries to explain this discrepancy using the omitted variable bias framework. Select all statements that represent an **incorrect** application of this framework or a misinterpretation of the results.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 355, "Question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Question\n\nThe paper argues that even with prohibitively high mobility costs for existing workers, the economy has other adjustment margins that keep relative prices stable. According to the paper, which of the following are mechanisms that serve to moderate the impact of shocks on skill rental prices? Select all that apply.", "Answer": ["B", "C", "D"], "qid": "355", "question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Question\n\nThe paper argues that even with prohibitively high mobility costs for existing workers, the economy has other adjustment margins that keep relative prices stable. According to the paper, which of the following are mechanisms that serve to moderate the impact of shocks on skill rental prices? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 356, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental tension between individual self-interest and collective benefit in a public goods game with asymmetric access. It requires the derivation of the game's key theoretical benchmarks: the non-cooperative Nash Equilibrium (NE) and the socially optimal Cooperative Equilibrium.\n\n**Setting and Sample.** The setting is a 2-player game. Player A (upstream) and Player B (downstream) are each endowed with 10 tokens. They simultaneously choose an investment `y_i` in a public good (bandwidth). The total investment `y` produces bandwidth according to a non-linear production function. Player A has priority access to the bandwidth. Players are assumed to be rational and selfish for the NE derivation.\n\n### Data / Model Specification\n\nThe production of the common resource and the payoffs from extraction are governed by the non-linear functions in **Table 1** and **Table 2**, respectively. The total net earnings for the group is calculated as `(20 - y) + Total Tokens Earned`.\n\n**Rule:** It takes 2.5 kbs of bandwidth to download one file in the 100-second period.\n\n**Table 1: Common Resource (Bandwidth) vs. Total Investment**\n| Total Investment (y) | Common Resource | Total Investment (y) | Common Resource |\n| :--- | :--- | :--- | :--- |\n| 0-7 | 0 | 14 | 33 |\n| 8 | 1 | 15 | 36 |\n| 9 | 2 | 16 | 38 |\n| 10 | 6 | 17 | 39 |\n| 11 | 12 | 18 | 39 |\n| 12 | 20 | 19 | 40 |\n| 13 | 28 | 20 | 40 |\n\n**Table 2: Tokens Earned vs. Files Downloaded**\n| Files Downloaded | Tokens Earned | Files Downloaded | Tokens Earned |\n| :--- | :--- | :--- | :--- |\n| 0-2 | 0 | 7 | 18 |\n| 3 | 1 | 8 | 19 |\n| 4 | 4 | 9 | 19 |\n| 5 | 10 | 10 | 20 |\n| 6 | 15 | | |\n\n### Question\n\nBased on the provided game structure and payoff tables for rational, selfish players, which of the following statements accurately describe the game's theoretical benchmarks? Select all that apply.", "Answer": ["C", "D"], "qid": "356", "question": "### Background\n\n**Research Question.** This problem explores the fundamental tension between individual self-interest and collective benefit in a public goods game with asymmetric access. It requires the derivation of the game's key theoretical benchmarks: the non-cooperative Nash Equilibrium (NE) and the socially optimal Cooperative Equilibrium.\n\n**Setting and Sample.** The setting is a 2-player game. Player A (upstream) and Player B (downstream) are each endowed with 10 tokens. They simultaneously choose an investment `y_i` in a public good (bandwidth). The total investment `y` produces bandwidth according to a non-linear production function. Player A has priority access to the bandwidth. Players are assumed to be rational and selfish for the NE derivation.\n\n### Data / Model Specification\n\nThe production of the common resource and the payoffs from extraction are governed by the non-linear functions in **Table 1** and **Table 2**, respectively. The total net earnings for the group is calculated as `(20 - y) + Total Tokens Earned`.\n\n**Rule:** It takes 2.5 kbs of bandwidth to download one file in the 100-second period.\n\n**Table 1: Common Resource (Bandwidth) vs. Total Investment**\n| Total Investment (y) | Common Resource | Total Investment (y) | Common Resource |\n| :--- | :--- | :--- | :--- |\n| 0-7 | 0 | 14 | 33 |\n| 8 | 1 | 15 | 36 |\n| 9 | 2 | 16 | 38 |\n| 10 | 6 | 17 | 39 |\n| 11 | 12 | 18 | 39 |\n| 12 | 20 | 19 | 40 |\n| 13 | 28 | 20 | 40 |\n\n**Table 2: Tokens Earned vs. Files Downloaded**\n| Files Downloaded | Tokens Earned | Files Downloaded | Tokens Earned |\n| :--- | :--- | :--- | :--- |\n| 0-2 | 0 | 7 | 18 |\n| 3 | 1 | 8 | 19 |\n| 4 | 4 | 9 | 19 |\n| 5 | 10 | 10 | 20 |\n| 6 | 15 | | |\n\n### Question\n\nBased on the provided game structure and payoff tables for rational, selfish players, which of the following statements accurately describe the game's theoretical benchmarks? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 357, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental tension between individual self-interest and collective benefit in a public goods game with asymmetric access. It requires the derivation of the game's key theoretical benchmarks: the non-cooperative Nash Equilibrium (NE) and the socially optimal Cooperative Equilibrium.\n\n**Setting and Sample.** The setting is a 2-player game. Player A (upstream) and Player B (downstream) are each endowed with 10 tokens. They simultaneously choose an investment `y_i` in a public good (bandwidth). The total investment `y` produces bandwidth according to a non-linear production function. Player A has priority access to the bandwidth. Players are assumed to be rational and selfish for the NE derivation.\n\n### Data / Model Specification\n\nThe production of the common resource and the payoffs from extraction are governed by the non-linear functions in **Table 1** and **Table 2**, respectively. The total net earnings for the group is calculated as `(20 - y) + Total Tokens Earned`.\n\n**Rule:** It takes 2.5 kbs of bandwidth to download one file in the 100-second period.\n\n**Table 1: Common Resource (Bandwidth) vs. Total Investment**\n| Total Investment (y) | Common Resource | Total Investment (y) | Common Resource |\n| :--- | :--- | :--- | :--- |\n| 0-7 | 0 | 14 | 33 |\n| 8 | 1 | 15 | 36 |\n| 9 | 2 | 16 | 38 |\n| 10 | 6 | 17 | 39 |\n| 11 | 12 | 18 | 39 |\n| 12 | 20 | 19 | 40 |\n| 13 | 28 | 20 | 40 |\n\n**Table 2: Tokens Earned vs. Files Downloaded**\n| Files Downloaded | Tokens Earned | Files Downloaded | Tokens Earned |\n| :--- | :--- | :--- | :--- |\n| 0-2 | 0 | 7 | 18 |\n| 3 | 1 | 8 | 19 |\n| 4 | 4 | 9 | 19 |\n| 5 | 10 | 10 | 20 |\n| 6 | 15 | | |\n\n### Question\n\nThe game's design creates a significant tension between individual rationality and collective benefit. Select all statements that correctly explain the sources of this tension or the rationale for the authors' experimental hypotheses.", "Answer": ["A", "C"], "qid": "357", "question": "### Background\n\n**Research Question.** This problem explores the fundamental tension between individual self-interest and collective benefit in a public goods game with asymmetric access. It requires the derivation of the game's key theoretical benchmarks: the non-cooperative Nash Equilibrium (NE) and the socially optimal Cooperative Equilibrium.\n\n**Setting and Sample.** The setting is a 2-player game. Player A (upstream) and Player B (downstream) are each endowed with 10 tokens. They simultaneously choose an investment `y_i` in a public good (bandwidth). The total investment `y` produces bandwidth according to a non-linear production function. Player A has priority access to the bandwidth. Players are assumed to be rational and selfish for the NE derivation.\n\n### Data / Model Specification\n\nThe production of the common resource and the payoffs from extraction are governed by the non-linear functions in **Table 1** and **Table 2**, respectively. The total net earnings for the group is calculated as `(20 - y) + Total Tokens Earned`.\n\n**Rule:** It takes 2.5 kbs of bandwidth to download one file in the 100-second period.\n\n**Table 1: Common Resource (Bandwidth) vs. Total Investment**\n| Total Investment (y) | Common Resource | Total Investment (y) | Common Resource |\n| :--- | :--- | :--- | :--- |\n| 0-7 | 0 | 14 | 33 |\n| 8 | 1 | 15 | 36 |\n| 9 | 2 | 16 | 38 |\n| 10 | 6 | 17 | 39 |\n| 11 | 12 | 18 | 39 |\n| 12 | 20 | 19 | 40 |\n| 13 | 28 | 20 | 40 |\n\n**Table 2: Tokens Earned vs. Files Downloaded**\n| Files Downloaded | Tokens Earned | Files Downloaded | Tokens Earned |\n| :--- | :--- | :--- | :--- |\n| 0-2 | 0 | 7 | 18 |\n| 3 | 1 | 8 | 19 |\n| 4 | 4 | 9 | 19 |\n| 5 | 10 | 10 | 20 |\n| 6 | 15 | | |\n\n### Question\n\nThe game's design creates a significant tension between individual rationality and collective benefit. Select all statements that correctly explain the sources of this tension or the rationale for the authors' experimental hypotheses.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 358, "Question": "### Background\n\n**Research Question.** This problem analyzes the prospective gains in identifying power from a policy intervention designed to increase survey response rates, particularly in a challenging joint censoring context.\n\n**Setting / Institutional Environment.** A survey is administered in two stages. Stage 'a' achieves an overall response rate of `π_a`. Stage 'b' is a costly follow-up effort to interview non-respondents from stage 'a', which successfully increases the cumulative response rate to `π_b > π_a`. The goal is to quantify the improvement in identification for a parameter `E[g(y)|A]` before the stage 'b' follow-up is actually conducted.\n\n**Variables & Parameters.**\n- `A`: The conditioning event defining the subpopulation of interest.\n- `z_a, z_b`: Indicators for responding by stage 'a' and stage 'b', respectively.\n- `π_a, π_b`: Overall response rates, `P(z_a=1)` and `P(z_b=1)`.\n- `π_{ac}(A), π_{bc}(A)`: Effective response rates for group `A` after stage 'a' and stage 'b'.\n- `K_0, K_1`: The infimum and supremum of `g(y)` over its domain `Y`.\n\n---\n\n### Data / Model Specification\n\nUnder joint censoring, the identification bounds for `E[g(y)|A]` are:\n\n  \n\\mathbb{E}[g(y)|A,z=1] \\cdot \\pi_{c}(A) + K_0 \\cdot (1-\\pi_{c}(A)) \\le \\mathbb{E}[g(y)|A] \\le \\mathbb{E}[g(y)|A,z=1] \\cdot \\pi_{c}(A) + K_1 \\cdot (1-\\pi_{c}(A)) \\quad \\text{(Eq. (1))}\n \n\nThe width of the bound is `(K_1 - K_0)(1 - π_c(A))`. The effective response rate after stage 'a' is `\\pi_{ac}(A) = \\frac{\\mathsf{P}(A, z_a=1)}{\\mathsf{P}(A, z_a=1) + (1-\\pi_a)}`.\n\nThe effective response rate after stage 'b', `π_{bc}(A)`, is given by:\n\n  \n\\pi_{bc}(A) = \\frac{\\mathsf{P}(A, z_b=1)}{\\mathsf{P}(A, z_b=1) + (1-\\pi_b)} \\quad \\text{(Eq. (2))}\n \n\nBefore stage 'b' is conducted, the term `P(A, z_b=1)` is unknown. It can be decomposed as:\n\n  \n\\mathsf{P}(A, z_b=1) = \\mathsf{P}(A, z_a=1) + \\mathsf{P}(A | z_a=0, z_b=1)(\\pi_b - \\pi_a) \\quad \\text{(Eq. (3))}\n \n\nwhere `P(A | z_a=0, z_b=1)` is the unknown fraction of newly converted respondents who belong to group `A`.\n\n---\n\n### Question\n\nBased on the provided information, a survey agency plans to increase the overall response rate from `π_a` to `π_b > π_a`. Before conducting the follow-up, the agency wants to understand the guaranteed improvement in identification. Select all of the following statements that are correct.", "Answer": ["A", "B"], "qid": "358", "question": "### Background\n\n**Research Question.** This problem analyzes the prospective gains in identifying power from a policy intervention designed to increase survey response rates, particularly in a challenging joint censoring context.\n\n**Setting / Institutional Environment.** A survey is administered in two stages. Stage 'a' achieves an overall response rate of `π_a`. Stage 'b' is a costly follow-up effort to interview non-respondents from stage 'a', which successfully increases the cumulative response rate to `π_b > π_a`. The goal is to quantify the improvement in identification for a parameter `E[g(y)|A]` before the stage 'b' follow-up is actually conducted.\n\n**Variables & Parameters.**\n- `A`: The conditioning event defining the subpopulation of interest.\n- `z_a, z_b`: Indicators for responding by stage 'a' and stage 'b', respectively.\n- `π_a, π_b`: Overall response rates, `P(z_a=1)` and `P(z_b=1)`.\n- `π_{ac}(A), π_{bc}(A)`: Effective response rates for group `A` after stage 'a' and stage 'b'.\n- `K_0, K_1`: The infimum and supremum of `g(y)` over its domain `Y`.\n\n---\n\n### Data / Model Specification\n\nUnder joint censoring, the identification bounds for `E[g(y)|A]` are:\n\n  \n\\mathbb{E}[g(y)|A,z=1] \\cdot \\pi_{c}(A) + K_0 \\cdot (1-\\pi_{c}(A)) \\le \\mathbb{E}[g(y)|A] \\le \\mathbb{E}[g(y)|A,z=1] \\cdot \\pi_{c}(A) + K_1 \\cdot (1-\\pi_{c}(A)) \\quad \\text{(Eq. (1))}\n \n\nThe width of the bound is `(K_1 - K_0)(1 - π_c(A))`. The effective response rate after stage 'a' is `\\pi_{ac}(A) = \\frac{\\mathsf{P}(A, z_a=1)}{\\mathsf{P}(A, z_a=1) + (1-\\pi_a)}`.\n\nThe effective response rate after stage 'b', `π_{bc}(A)`, is given by:\n\n  \n\\pi_{bc}(A) = \\frac{\\mathsf{P}(A, z_b=1)}{\\mathsf{P}(A, z_b=1) + (1-\\pi_b)} \\quad \\text{(Eq. (2))}\n \n\nBefore stage 'b' is conducted, the term `P(A, z_b=1)` is unknown. It can be decomposed as:\n\n  \n\\mathsf{P}(A, z_b=1) = \\mathsf{P}(A, z_a=1) + \\mathsf{P}(A | z_a=0, z_b=1)(\\pi_b - \\pi_a) \\quad \\text{(Eq. (3))}\n \n\nwhere `P(A | z_a=0, z_b=1)` is the unknown fraction of newly converted respondents who belong to group `A`.\n\n---\n\n### Question\n\nBased on the provided information, a survey agency plans to increase the overall response rate from `π_a` to `π_b > π_a`. Before conducting the follow-up, the agency wants to understand the guaranteed improvement in identification. Select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 359, "Question": "### Background\n\n**Research Question.** This problem analyzes the prospective gains in identifying power from a policy intervention designed to increase survey response rates, particularly in a challenging joint censoring context.\n\n**Setting / Institutional Environment.** A survey is administered in two stages. Stage 'a' achieves an overall response rate of `π_a`. Stage 'b' is a costly follow-up effort to interview non-respondents from stage 'a', which successfully increases the cumulative response rate to `π_b > π_a`. The goal is to quantify the improvement in identification for a parameter `E[g(y)|A]` before the stage 'b' follow-up is actually conducted.\n\n**Variables & Parameters.**\n- `A`: The conditioning event defining the subpopulation of interest.\n- `z_a, z_b`: Indicators for responding by stage 'a' and stage 'b', respectively.\n- `π_a, π_b`: Overall response rates, `P(z_a=1)` and `P(z_b=1)`.\n- `w`: Covariates known for all individuals in the sampling frame.\n\n---\n\n### Data / Model Specification\n\nThe effective response rate after stage 'a', `π_{ac}(A)`, can be improved to `π_{bc}(A)` by increasing the overall response rate to `π_b`. The magnitude of this improvement depends on which non-respondents are converted. The worst-case (minimum) value of `π_{bc}(A)` is guaranteed to be higher than `π_{ac}(A)`, but the actual value depends on `P(A | z_a=0, z_b=1)`, the proportion of newly converted respondents who are in group `A`.\n\n---\n\n### Question\n\nA manager, noting that the *worst-case* improvement in identification is guaranteed regardless of who is converted, argues that follow-up efforts should not be targeted. The agency has a fixed budget and can observe covariates `w` for all non-respondents. Select all statements that represent flawed reasoning or ineffective strategies in this context.", "Answer": ["A", "D"], "qid": "359", "question": "### Background\n\n**Research Question.** This problem analyzes the prospective gains in identifying power from a policy intervention designed to increase survey response rates, particularly in a challenging joint censoring context.\n\n**Setting / Institutional Environment.** A survey is administered in two stages. Stage 'a' achieves an overall response rate of `π_a`. Stage 'b' is a costly follow-up effort to interview non-respondents from stage 'a', which successfully increases the cumulative response rate to `π_b > π_a`. The goal is to quantify the improvement in identification for a parameter `E[g(y)|A]` before the stage 'b' follow-up is actually conducted.\n\n**Variables & Parameters.**\n- `A`: The conditioning event defining the subpopulation of interest.\n- `z_a, z_b`: Indicators for responding by stage 'a' and stage 'b', respectively.\n- `π_a, π_b`: Overall response rates, `P(z_a=1)` and `P(z_b=1)`.\n- `w`: Covariates known for all individuals in the sampling frame.\n\n---\n\n### Data / Model Specification\n\nThe effective response rate after stage 'a', `π_{ac}(A)`, can be improved to `π_{bc}(A)` by increasing the overall response rate to `π_b`. The magnitude of this improvement depends on which non-respondents are converted. The worst-case (minimum) value of `π_{bc}(A)` is guaranteed to be higher than `π_{ac}(A)`, but the actual value depends on `P(A | z_a=0, z_b=1)`, the proportion of newly converted respondents who are in group `A`.\n\n---\n\n### Question\n\nA manager, noting that the *worst-case* improvement in identification is guaranteed regardless of who is converted, argues that follow-up efforts should not be targeted. The agency has a fixed budget and can observe covariates `w` for all non-respondents. Select all statements that represent flawed reasoning or ineffective strategies in this context.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 360, "Question": "### Background\n\n**Research Question.** This problem examines the fundamental identification challenge posed by outcome censoring and evaluates two common practitioner approaches—weighting and imputation—against the formal bounds derived from a conservative, worst-case analysis.\n\n**Setting / Institutional Environment.** A random sample is drawn from a population. For all individuals, covariates `x` are observed. However, for a subset of the sample, the outcome `y` is missing. An analyst wishes to estimate the conditional expectation `E[g(y)|A]` and considers using either standard survey weights or an imputation procedure to handle the missing data.\n\n**Variables & Parameters.**\n- `y`: An outcome variable of interest with domain `Y`.\n- `A`: A specified subset of the covariate domain `X`.\n- `z`: A binary nonresponse indicator. `z=1` if `y` is observed; `z=0` if `y` is missing.\n- `g(y)`: A real-valued function of the outcome.\n- `K_0, K_1`: The infimum and supremum of `g(y)` over its domain `Y`.\n\n---\n\n### Data / Model Specification\n\n**1. Fundamental Bounds:** Under outcome censoring, the true value of `E[g(y)|A]` is not point-identified but is bounded by:\n\n  \n\\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + K_{0} \\cdot \\mathsf{P}(z=0|A) \\leqslant \\mathbb{E}[g(y)|A] \\leqslant \\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + K_{1} \\cdot \\mathsf{P}(z=0|A) \\quad \\text{(Eq. (1))}\n \n\n**2. Weighting:** A common weighted-average estimator uses weights `s(w)` based on covariates `w` that are observable for all units. Standard public-use weights often take the form `s(w) = \\mathsf{P}(z=1) / \\mathsf{P}(z=1|w)`.\n\n**3. Imputation:** In this approach, a value `y*` is assigned for each missing `y`. The probability limit of the resulting estimator is:\n\n  \n\\theta_{imp} = \\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + \\mathbb{E}[g(y^{*})|A,z=0] \\cdot \\mathsf{P}(z=0|A) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the provided framework for outcome censoring, select all of the following statements that are correct.", "Answer": ["B", "C"], "qid": "360", "question": "### Background\n\n**Research Question.** This problem examines the fundamental identification challenge posed by outcome censoring and evaluates two common practitioner approaches—weighting and imputation—against the formal bounds derived from a conservative, worst-case analysis.\n\n**Setting / Institutional Environment.** A random sample is drawn from a population. For all individuals, covariates `x` are observed. However, for a subset of the sample, the outcome `y` is missing. An analyst wishes to estimate the conditional expectation `E[g(y)|A]` and considers using either standard survey weights or an imputation procedure to handle the missing data.\n\n**Variables & Parameters.**\n- `y`: An outcome variable of interest with domain `Y`.\n- `A`: A specified subset of the covariate domain `X`.\n- `z`: A binary nonresponse indicator. `z=1` if `y` is observed; `z=0` if `y` is missing.\n- `g(y)`: A real-valued function of the outcome.\n- `K_0, K_1`: The infimum and supremum of `g(y)` over its domain `Y`.\n\n---\n\n### Data / Model Specification\n\n**1. Fundamental Bounds:** Under outcome censoring, the true value of `E[g(y)|A]` is not point-identified but is bounded by:\n\n  \n\\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + K_{0} \\cdot \\mathsf{P}(z=0|A) \\leqslant \\mathbb{E}[g(y)|A] \\leqslant \\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + K_{1} \\cdot \\mathsf{P}(z=0|A) \\quad \\text{(Eq. (1))}\n \n\n**2. Weighting:** A common weighted-average estimator uses weights `s(w)` based on covariates `w` that are observable for all units. Standard public-use weights often take the form `s(w) = \\mathsf{P}(z=1) / \\mathsf{P}(z=1|w)`.\n\n**3. Imputation:** In this approach, a value `y*` is assigned for each missing `y`. The probability limit of the resulting estimator is:\n\n  \n\\theta_{imp} = \\mathbb{E}[g(y)|A,z=1] \\cdot \\mathsf{P}(z=1|A) + \\mathbb{E}[g(y^{*})|A,z=0] \\cdot \\mathsf{P}(z=0|A) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the provided framework for outcome censoring, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 361, "Question": "In the model, individual agents' decisions to visit the hospital can lead to congestion, which is socially costly. The ex-post total welfare, given public belief `\\tilde{s}` and true severity `s`, is:\n  \nU(\\tilde{s},s) \\equiv -s\\mathbb{E}q - c n(\\tilde{s}) + s\\operatorname*{min}\\{n(\\tilde{s}),\\bar{n}\\}\\mathbb{E}(q|q\\geq\\beta(\\tilde{s})) \n \nwhere `c` is the visit cost, `n(\\tilde{s})` is the mass of visitors, and `\\bar{n}` is hospital capacity.\n\nCongestion occurs when `n(\\tilde{s}) > \\bar{n}`, and visitors are admitted randomly. Which of the following are distinct sources of welfare loss created specifically by this congestion? (Select all that apply.)", "Answer": ["B", "C"], "qid": "361", "question": "In the model, individual agents' decisions to visit the hospital can lead to congestion, which is socially costly. The ex-post total welfare, given public belief `\\tilde{s}` and true severity `s`, is:\n  \nU(\\tilde{s},s) \\equiv -s\\mathbb{E}q - c n(\\tilde{s}) + s\\operatorname*{min}\\{n(\\tilde{s}),\\bar{n}\\}\\mathbb{E}(q|q\\geq\\beta(\\tilde{s})) \n \nwhere `c` is the visit cost, `n(\\tilde{s})` is the mass of visitors, and `\\bar{n}` is hospital capacity.\n\nCongestion occurs when `n(\\tilde{s}) > \\bar{n}`, and visitors are admitted randomly. Which of the following are distinct sources of welfare loss created specifically by this congestion? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about In the model, individual agents' decisions to visi...", "model_answer": "This is a detailed response to the question about In the model, individual agents' decisions to visi...", "error": null}}
{"ID": 362, "Question": "### Background\n\n**Research Question.** This problem examines the empirical strategy for testing how the degree of inter-group interaction (`κ`) causally affects population polarization in a hawk-dove game.\n\n**Setting.** The theoretical model predicts a bifurcation in behavior based on `κ`. To test this, an experiment is conducted using different values of `κ` as treatments. The primary outcome measure is a 'separation index,' `Δs(κ)`, which quantifies the degree of polarization between the two populations. The experiment uses a within-subjects design across 6 sessions.\n\n**Variables and Parameters.**\n- `κ`: The coupling parameter; the experimental treatment variable, taking values in `{0, 0.2, 0.4, 0.6, 0.8, 1}`.\n- `Δs(κ)`: The separation index, defined as the share of hawks in the more hawkish population minus the share of hawks in the more dovish population.\n- `v`, `c`: The valuation of a contested good (`v=12`) and the cost of conflict (`c=18`).\n- `κ_m^*`, `κ_p^*`: Critical thresholds of `κ` predicted by the theory.\n\n---\n\n### Data / Model Specification\n\nThe theory predicts three regimes based on the thresholds `κ_m^* = 1/2` and `κ_p^* = max{v/c, 1-v/c}`:\n- **Mixed:** For `κ < κ_m^*`, the stable equilibrium is symmetric mixed `(x=y=v/c)`.\n- **Pure:** For `κ > κ_p^*`, stable equilibria are asymmetric pure `(1,0)` or `(0,1)`.\n- **Hybrid:** For `κ_m^* ≤ κ ≤ κ_p^*`, stable equilibria are hybrid (e.g., `(1, y^*)`).\n\nThe sequence of treatments for each of the six sessions is given in Table 1.\n\n**Table 1:** Sequence of Treatments (`κ`) in Each Session\n| Period | Session 1 | Session 2 | Session 3 | Session 4 | Session 5 | Session 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0.8 | 0.2 | 1.0 | 0.8 | 0.4 | 1.0 |\n| 2 | 0.2 | 1.0 | 0.4 | 0.0 | 0.8 | 0.6 |\n| 3 | 0.0 | 0.6 | 0.6 | 0.4 | 0.2 | 0.2 |\n| 4 | 0.6 | 0.0 | 0.0 | 0.6 | 1.0 | 0.4 |\n| 5 | 0.4 | 0.8 | 0.2 | 1.0 | 0.0 | 0.8 |\n| 6 | 1.0 | 0.4 | 0.8 | 0.2 | 0.6 | 0.0 |\n\n---\n\n### Question\n\nSelect all statements that are correct regarding the paper's theoretical predictions and experimental design.", "Answer": ["A", "B"], "qid": "362", "question": "### Background\n\n**Research Question.** This problem examines the empirical strategy for testing how the degree of inter-group interaction (`κ`) causally affects population polarization in a hawk-dove game.\n\n**Setting.** The theoretical model predicts a bifurcation in behavior based on `κ`. To test this, an experiment is conducted using different values of `κ` as treatments. The primary outcome measure is a 'separation index,' `Δs(κ)`, which quantifies the degree of polarization between the two populations. The experiment uses a within-subjects design across 6 sessions.\n\n**Variables and Parameters.**\n- `κ`: The coupling parameter; the experimental treatment variable, taking values in `{0, 0.2, 0.4, 0.6, 0.8, 1}`.\n- `Δs(κ)`: The separation index, defined as the share of hawks in the more hawkish population minus the share of hawks in the more dovish population.\n- `v`, `c`: The valuation of a contested good (`v=12`) and the cost of conflict (`c=18`).\n- `κ_m^*`, `κ_p^*`: Critical thresholds of `κ` predicted by the theory.\n\n---\n\n### Data / Model Specification\n\nThe theory predicts three regimes based on the thresholds `κ_m^* = 1/2` and `κ_p^* = max{v/c, 1-v/c}`:\n- **Mixed:** For `κ < κ_m^*`, the stable equilibrium is symmetric mixed `(x=y=v/c)`.\n- **Pure:** For `κ > κ_p^*`, stable equilibria are asymmetric pure `(1,0)` or `(0,1)`.\n- **Hybrid:** For `κ_m^* ≤ κ ≤ κ_p^*`, stable equilibria are hybrid (e.g., `(1, y^*)`).\n\nThe sequence of treatments for each of the six sessions is given in Table 1.\n\n**Table 1:** Sequence of Treatments (`κ`) in Each Session\n| Period | Session 1 | Session 2 | Session 3 | Session 4 | Session 5 | Session 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0.8 | 0.2 | 1.0 | 0.8 | 0.4 | 1.0 |\n| 2 | 0.2 | 1.0 | 0.4 | 0.0 | 0.8 | 0.6 |\n| 3 | 0.0 | 0.6 | 0.6 | 0.4 | 0.2 | 0.2 |\n| 4 | 0.6 | 0.0 | 0.0 | 0.6 | 1.0 | 0.4 |\n| 5 | 0.4 | 0.8 | 0.2 | 1.0 | 0.0 | 0.8 |\n| 6 | 1.0 | 0.4 | 0.8 | 0.2 | 0.6 | 0.0 |\n\n---\n\n### Question\n\nSelect all statements that are correct regarding the paper's theoretical predictions and experimental design.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 363, "Question": "### Background\n\n**Research Question.** This problem analyzes the relationship between the theoretical parameterization of a hawk-dove game and its experimental implementation, and explores the welfare implications of the resulting evolutionary equilibria.\n\n**Setting.** A two-population evolutionary game theory experiment is conducted using a specific hawk-dove payoff matrix. The behavior of agents in this game is predicted to lead to different stable equilibria depending on the inter-group coupling parameter, `κ`.\n\n**Variables and Parameters.**\n- `Π`: The 2x2 payoff matrix for the hawk-dove game.\n- `a`: A player's endowment (`a > 0`).\n- `v`: The valuation of a contested good (`v > 0`).\n- `c`: The cost of conflict (`c > v`).\n- `x`, `y`: The share of the hawk strategy in populations X and Y, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general parameterization of the hawk-dove game is given by:\n  \nΠ = \n\\begin{pmatrix} \n a + \\frac{1}{2}(v-c) & a+v \\\\ \n a & a \n\\end{pmatrix}\n \\quad \\text{(Eq. (1))}\n \nThe experiment uses the parameters `a=3`, `v=12`, and `c=18`, resulting in the specific matrix:\n  \nΠ_{exp} = \n\\begin{pmatrix} \n 0 & 15 \\\\ \n 3 & 9 \n\\end{pmatrix}\n \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nSelect all correct statements regarding the payoff structure and welfare properties of the hawk-dove game used in the experiment.", "Answer": ["A", "C"], "qid": "363", "question": "### Background\n\n**Research Question.** This problem analyzes the relationship between the theoretical parameterization of a hawk-dove game and its experimental implementation, and explores the welfare implications of the resulting evolutionary equilibria.\n\n**Setting.** A two-population evolutionary game theory experiment is conducted using a specific hawk-dove payoff matrix. The behavior of agents in this game is predicted to lead to different stable equilibria depending on the inter-group coupling parameter, `κ`.\n\n**Variables and Parameters.**\n- `Π`: The 2x2 payoff matrix for the hawk-dove game.\n- `a`: A player's endowment (`a > 0`).\n- `v`: The valuation of a contested good (`v > 0`).\n- `c`: The cost of conflict (`c > v`).\n- `x`, `y`: The share of the hawk strategy in populations X and Y, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general parameterization of the hawk-dove game is given by:\n  \nΠ = \n\\begin{pmatrix} \n a + \\frac{1}{2}(v-c) & a+v \\\\ \n a & a \n\\end{pmatrix}\n \\quad \\text{(Eq. (1))}\n \nThe experiment uses the parameters `a=3`, `v=12`, and `c=18`, resulting in the specific matrix:\n  \nΠ_{exp} = \n\\begin{pmatrix} \n 0 & 15 \\\\ \n 3 & 9 \n\\end{pmatrix}\n \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nSelect all correct statements regarding the payoff structure and welfare properties of the hawk-dove game used in the experiment.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 364, "Question": "### Background\n\n**Research Question.** This problem requires an interpretation of experimental results concerning how inter-group interaction (`κ`) affects population polarization, focusing on discrepancies between theoretical predictions and observed behavior.\n\n**Setting.** The experiment tests a model of evolutionary game theory where two populations play a hawk-dove game. The key treatment variable is `κ`, the intensity of inter-group matching. The primary outcome is the separation index, `Δs(κ)`.\n\n---\n\n### Data / Model Specification\n\nThe theoretical model predicts that for `κ > κ_p^*`, the stable equilibria are asymmetric pure profiles, implying `Δs(0.8) = Δs(1) = 1`. However, the statistical results show `Δs(0.8) < Δs(1)`. The paper also reports that for `κ ≥ 0.8`, most deviation from perfect separation is driven by dovish populations, where the share of hawk play can reach almost 20% instead of the predicted 0%.\n\nThe replicator dynamic for the dovish population (Y) when the hawkish population (X) plays pure hawk (`x=1`) is:\n  \n\\dot{y} = y(1-y)\\cdot\\frac{1}{2}[v-c(y+κ(1-y))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided model and results, select all statements that are INCORRECT or NOT supported by the paper's analysis.", "Answer": ["A", "B"], "qid": "364", "question": "### Background\n\n**Research Question.** This problem requires an interpretation of experimental results concerning how inter-group interaction (`κ`) affects population polarization, focusing on discrepancies between theoretical predictions and observed behavior.\n\n**Setting.** The experiment tests a model of evolutionary game theory where two populations play a hawk-dove game. The key treatment variable is `κ`, the intensity of inter-group matching. The primary outcome is the separation index, `Δs(κ)`.\n\n---\n\n### Data / Model Specification\n\nThe theoretical model predicts that for `κ > κ_p^*`, the stable equilibria are asymmetric pure profiles, implying `Δs(0.8) = Δs(1) = 1`. However, the statistical results show `Δs(0.8) < Δs(1)`. The paper also reports that for `κ ≥ 0.8`, most deviation from perfect separation is driven by dovish populations, where the share of hawk play can reach almost 20% instead of the predicted 0%.\n\nThe replicator dynamic for the dovish population (Y) when the hawkish population (X) plays pure hawk (`x=1`) is:\n  \n\\dot{y} = y(1-y)\\cdot\\frac{1}{2}[v-c(y+κ(1-y))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided model and results, select all statements that are INCORRECT or NOT supported by the paper's analysis.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 365, "Question": "### Background\n\n**Research Question:** This problem addresses the practical challenges of estimating PELVE from serially dependent data, such as financial time series, and understanding the statistical properties of the estimator.\n\n**Setting / Institutional Environment:** The analysis is based on a sample from a strictly stationary and `α`-mixing sequence of losses. This data structure is more realistic for financial returns than the i.i.d. assumption, as it allows for phenomena like volatility clustering.\n\n**Variables & Parameters:**\n- `ĉ_n`: The empirical PELVE estimator.\n- `m`: The block length for a block bootstrap.\n\n### Data / Model Specification\n\nThe empirical PELVE estimator, `ĉ_n`, is defined by `EŜ_{1-cε} = VaR̂_{1-ε}`. When estimating the asymptotic variance of `ĉ_n` from serially dependent (`α`-mixing) data, a block bootstrap with block length `m` is used.\n\n### Question\n\nWhich of the following statements about the block bootstrap procedure for estimating the variance of `ĉ_n` are correct? (Select all that apply)", "Answer": ["A", "C"], "qid": "365", "question": "### Background\n\n**Research Question:** This problem addresses the practical challenges of estimating PELVE from serially dependent data, such as financial time series, and understanding the statistical properties of the estimator.\n\n**Setting / Institutional Environment:** The analysis is based on a sample from a strictly stationary and `α`-mixing sequence of losses. This data structure is more realistic for financial returns than the i.i.d. assumption, as it allows for phenomena like volatility clustering.\n\n**Variables & Parameters:**\n- `ĉ_n`: The empirical PELVE estimator.\n- `m`: The block length for a block bootstrap.\n\n### Data / Model Specification\n\nThe empirical PELVE estimator, `ĉ_n`, is defined by `EŜ_{1-cε} = VaR̂_{1-ε}`. When estimating the asymptotic variance of `ĉ_n` from serially dependent (`α`-mixing) data, a block bootstrap with block length `m` is used.\n\n### Question\n\nWhich of the following statements about the block bootstrap procedure for estimating the variance of `ĉ_n` are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question:** This proble...", "error": null}}
{"ID": 366, "Question": "### Background\n\n**Research Question.** This problem addresses the macroeconomic implications of a national minimum wage (NMW), focusing on its projected effect on the aggregate wage bill, the overall price level, and the potential for sustained inflation.\n\n**Setting / Institutional Environment.** The Monetary Policy Committee (MPC) of the Bank of England was tasked with assessing the impact of the incoming NMW on its inflation target. The NMW is analyzed as a one-off supply-side shock to the economy. The MPC's central projection was that the NMW would cause a net increase in the aggregate wage bill of 0.5% and a one-time increase in the price level of 0.4%. The paper notes that at the time, private sector unionization was only around 20%, and collective bargaining was decentralized to the company level.\n\n### Question\n\nThe paper analyzes how the macroeconomy would absorb the NMW's cost and assesses its inflationary risk. Based on the provided information, select all statements that are consistent with the paper's analysis.", "Answer": ["B", "D"], "qid": "366", "question": "### Background\n\n**Research Question.** This problem addresses the macroeconomic implications of a national minimum wage (NMW), focusing on its projected effect on the aggregate wage bill, the overall price level, and the potential for sustained inflation.\n\n**Setting / Institutional Environment.** The Monetary Policy Committee (MPC) of the Bank of England was tasked with assessing the impact of the incoming NMW on its inflation target. The NMW is analyzed as a one-off supply-side shock to the economy. The MPC's central projection was that the NMW would cause a net increase in the aggregate wage bill of 0.5% and a one-time increase in the price level of 0.4%. The paper notes that at the time, private sector unionization was only around 20%, and collective bargaining was decentralized to the company level.\n\n### Question\n\nThe paper analyzes how the macroeconomy would absorb the NMW's cost and assesses its inflationary risk. Based on the provided information, select all statements that are consistent with the paper's analysis.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 367, "Question": "### Background\n\n**Research Question.** This problem addresses the macroeconomic implications of a national minimum wage (NMW), focusing on its projected effect on the aggregate wage bill and the overall price level.\n\n**Setting / Institutional Environment.** The Monetary Policy Committee (MPC) of the Bank of England's central projection was that the NMW would cause a net increase in the aggregate wage bill of 0.5% and a one-time increase in the price level of 0.4%.\n\n### Data / Model Specification\n\nConsider a simple macroeconomic model where the aggregate production function is Cobb-Douglas, and firms in a perfectly competitive market set price `P` equal to marginal cost. The marginal cost is the wage `W` divided by the marginal product of labor `MPL`. In this model, the pass-through elasticity from wages to prices is equal to the labor share of income:\n\n  \n\\frac{d(\\ln P)}{d(\\ln W)} = \\text{Labor Share}\n \n\n### Question\n\nUsing the provided model and the MPC's projections, select all of the following conclusions that are mathematically and economically sound.", "Answer": ["A", "C"], "qid": "367", "question": "### Background\n\n**Research Question.** This problem addresses the macroeconomic implications of a national minimum wage (NMW), focusing on its projected effect on the aggregate wage bill and the overall price level.\n\n**Setting / Institutional Environment.** The Monetary Policy Committee (MPC) of the Bank of England's central projection was that the NMW would cause a net increase in the aggregate wage bill of 0.5% and a one-time increase in the price level of 0.4%.\n\n### Data / Model Specification\n\nConsider a simple macroeconomic model where the aggregate production function is Cobb-Douglas, and firms in a perfectly competitive market set price `P` equal to marginal cost. The marginal cost is the wage `W` divided by the marginal product of labor `MPL`. In this model, the pass-through elasticity from wages to prices is equal to the labor share of income:\n\n  \n\\frac{d(\\ln P)}{d(\\ln W)} = \\text{Labor Share}\n \n\n### Question\n\nUsing the provided model and the MPC's projections, select all of the following conclusions that are mathematically and economically sound.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 368, "Question": "### Background\n\n**Research Question.** This problem evaluates the conceptual validity of Paglin's (1975) method for adjusting income inequality time series, based on the paper's findings regarding the drivers of the UK inequality trend from 1965-1980.\n\n**Key Empirical Findings (assumed as given):**\n1.  The overall trend in aggregate inequality (`ΔI`) was strongly positive.\n2.  This trend was driven primarily by the change in the 'between-group' or 'age effect' component (`ΔB`).\n3.  The change in the age effect (`ΔB`) was itself driven almost entirely by shifts in the **age-income profile** (i.e., changes in the relative mean incomes of different age groups), not by shifts in the population's age structure.\n\n**Variables and Parameters.**\n- `I`: A measure of aggregate inequality.\n- `W`: The 'within-group' component of inequality.\n- `B`: The 'between-group' or 'age effect' component of inequality, such that `I = W + B`.\n- `I_Paglin`: Paglin's adjusted measure of inequality.\n\n---\n\n### Data / Model Specification\n\nPaglin's proposed adjustment to the level of inequality is to subtract the 'age effect':\n\n  \nI_{Paglin} = I - B = W \\quad \\text{(Eq. (1))}\n \n\nThis implies that the change in Paglin-adjusted inequality over time is `ΔI_Paglin = ΔW`.\n\n---\n\n### Question\n\nBased on the authors' analysis and the provided findings, select all statements that accurately represent the critique of Paglin's adjustment to the inequality trend.", "Answer": ["A", "B"], "qid": "368", "question": "### Background\n\n**Research Question.** This problem evaluates the conceptual validity of Paglin's (1975) method for adjusting income inequality time series, based on the paper's findings regarding the drivers of the UK inequality trend from 1965-1980.\n\n**Key Empirical Findings (assumed as given):**\n1.  The overall trend in aggregate inequality (`ΔI`) was strongly positive.\n2.  This trend was driven primarily by the change in the 'between-group' or 'age effect' component (`ΔB`).\n3.  The change in the age effect (`ΔB`) was itself driven almost entirely by shifts in the **age-income profile** (i.e., changes in the relative mean incomes of different age groups), not by shifts in the population's age structure.\n\n**Variables and Parameters.**\n- `I`: A measure of aggregate inequality.\n- `W`: The 'within-group' component of inequality.\n- `B`: The 'between-group' or 'age effect' component of inequality, such that `I = W + B`.\n- `I_Paglin`: Paglin's adjusted measure of inequality.\n\n---\n\n### Data / Model Specification\n\nPaglin's proposed adjustment to the level of inequality is to subtract the 'age effect':\n\n  \nI_{Paglin} = I - B = W \\quad \\text{(Eq. (1))}\n \n\nThis implies that the change in Paglin-adjusted inequality over time is `ΔI_Paglin = ΔW`.\n\n---\n\n### Question\n\nBased on the authors' analysis and the provided findings, select all statements that accurately represent the critique of Paglin's adjustment to the inequality trend.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 369, "Question": "### Background\n\n**Research Question.** This problem examines the methodological choice of an inequality measure for decomposition analysis, contrasting the properties of the Gini coefficient with the Generalised Entropy (GE) class of indices.\n\n**Setting.** A population is partitioned into subgroups (e.g., by age). The goal is to decompose total inequality into 'within-group' and 'between-group' components.\n\n**Variables and Parameters.**\n- `ν_k`: Population share of subgroup `k`.\n- `λ_k`: Relative mean income of subgroup `k`.\n- `G`: Gini coefficient for the total population.\n- `G^k`: Gini coefficient within subgroup `k`.\n- `R`: Residual 'interaction effect' in the Gini decomposition.\n\n---\n\n### Data / Model Specification\n\nThe decomposition of the Gini coefficient for overlapping subgroup income distributions is given by:\n\n  \nG = \\underbrace{\\sum_{k} \\nu_{k}^{2} \\lambda_{k} G^{k}}_{\\text{Within-group}} + \\underbrace{\\frac{1}{2} \\sum_{k} \\sum_{h} \\nu_{k} \\nu_{h} |\\lambda_{k} - \\lambda_{h}|}_{\\text{Between-group}} + R \\quad \\text{(Eq. (1))}\n \n\nThe decomposition for a Generalised Entropy index `I_0` is clean and additive, without a residual term:\n\n  \nI_0 = \\sum_{k} \\nu_{k} I_{0}^{k} + \\sum_{k} \\nu_{k} \\log(1/\\lambda_{k}) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nA government policy successfully reduces income inequality *within* every age group (`G^k` decreases for all `k`), while leaving population shares (`ν_k`) and relative mean incomes (`λ_k`) unchanged. Based on the provided decomposition formula for the Gini coefficient (Eq. (1)), select all conclusions that are **NOT** guaranteed to be true.", "Answer": ["B", "C"], "qid": "369", "question": "### Background\n\n**Research Question.** This problem examines the methodological choice of an inequality measure for decomposition analysis, contrasting the properties of the Gini coefficient with the Generalised Entropy (GE) class of indices.\n\n**Setting.** A population is partitioned into subgroups (e.g., by age). The goal is to decompose total inequality into 'within-group' and 'between-group' components.\n\n**Variables and Parameters.**\n- `ν_k`: Population share of subgroup `k`.\n- `λ_k`: Relative mean income of subgroup `k`.\n- `G`: Gini coefficient for the total population.\n- `G^k`: Gini coefficient within subgroup `k`.\n- `R`: Residual 'interaction effect' in the Gini decomposition.\n\n---\n\n### Data / Model Specification\n\nThe decomposition of the Gini coefficient for overlapping subgroup income distributions is given by:\n\n  \nG = \\underbrace{\\sum_{k} \\nu_{k}^{2} \\lambda_{k} G^{k}}_{\\text{Within-group}} + \\underbrace{\\frac{1}{2} \\sum_{k} \\sum_{h} \\nu_{k} \\nu_{h} |\\lambda_{k} - \\lambda_{h}|}_{\\text{Between-group}} + R \\quad \\text{(Eq. (1))}\n \n\nThe decomposition for a Generalised Entropy index `I_0` is clean and additive, without a residual term:\n\n  \nI_0 = \\sum_{k} \\nu_{k} I_{0}^{k} + \\sum_{k} \\nu_{k} \\log(1/\\lambda_{k}) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nA government policy successfully reduces income inequality *within* every age group (`G^k` decreases for all `k`), while leaving population shares (`ν_k`) and relative mean incomes (`λ_k`) unchanged. Based on the provided decomposition formula for the Gini coefficient (Eq. (1)), select all conclusions that are **NOT** guaranteed to be true.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 370, "Question": "### Background\n\n**Research Question.** This problem investigates the net effect of school desegregation on the educational attainment of Black students in Louisiana, focusing on the trade-off between increased school resources and altered peer environments.\n\n**Setting / Institutional Environment.** Following the *Brown v. Board of Education* decision, Louisiana schools underwent desegregation. This policy simultaneously altered two major inputs for Black students: school resources (e.g., funding, student-teacher ratios) and peer environment (e.g., exposure to white students). A key feature of this setting is that districts with a higher initial share of Black students (`1960fractionblack`) experienced larger increases in resources but smaller increases in exposure to white students.\n\n### Data / Model Specification\n\nThe analysis uses the reduced-form relationship between the change in an outcome and the initial Black enrollment share (`1960fractionblack`). The paper provides the following regression results and summary statistics:\n\n**Table 1: Reduced-Form Estimates of Desegregation's Effect on School Inputs**\n\n| Dependent Variable (Change from 1960-65 to 1970-75) | Coefficient on `1960fractionblack` | (Std. Error) |\n| :--- | :--- | :--- |\n| Per-Pupil Current Expenditure for Blacks | 2.174 | (0.371) |\n| Black Student-Teacher Ratio | -7.434 | (0.444) |\n\n*Note: The expenditure coefficient of 2.174 implies that a 10 percentage point increase in `1960fractionblack` is associated with an additional increase in per-pupil current expenditure of about $200 (in 2007 dollars).*\n\n**Table 2: Reduced-Form Estimates of Desegregation's Effect on Educational Attainment**\n\n| Dependent Variable (Change from 1960-65 to 1970-75) | Coefficient on `1960fractionblack` | (Std. Error) |\n| :--- | :--- | :--- |\n| HS Graduation Rate | 0.182 | (0.076) |\n\n**Summary Statistics (Pre-Desegregation Period, 1960-65):**\n- Average per-pupil current expenditure: $2,400 (in 2007 dollars).\n- Average high school graduation rate for pre-desegregation cohorts: 68 percent (0.68).\n\n### Question\n\nBased on the provided data, a researcher performs a back-of-the-envelope calculation to estimate the elasticity of the Black high school graduation rate with respect to per-pupil current expenditure, evaluated at the pre-desegregation means. The calculation assumes that the effect of `1960fractionblack` on attainment operates entirely through changes in spending.\n\nSelect all of the following statements that are **CORRECT** components or results of this calculation.", "Answer": ["A", "B", "C"], "qid": "370", "question": "### Background\n\n**Research Question.** This problem investigates the net effect of school desegregation on the educational attainment of Black students in Louisiana, focusing on the trade-off between increased school resources and altered peer environments.\n\n**Setting / Institutional Environment.** Following the *Brown v. Board of Education* decision, Louisiana schools underwent desegregation. This policy simultaneously altered two major inputs for Black students: school resources (e.g., funding, student-teacher ratios) and peer environment (e.g., exposure to white students). A key feature of this setting is that districts with a higher initial share of Black students (`1960fractionblack`) experienced larger increases in resources but smaller increases in exposure to white students.\n\n### Data / Model Specification\n\nThe analysis uses the reduced-form relationship between the change in an outcome and the initial Black enrollment share (`1960fractionblack`). The paper provides the following regression results and summary statistics:\n\n**Table 1: Reduced-Form Estimates of Desegregation's Effect on School Inputs**\n\n| Dependent Variable (Change from 1960-65 to 1970-75) | Coefficient on `1960fractionblack` | (Std. Error) |\n| :--- | :--- | :--- |\n| Per-Pupil Current Expenditure for Blacks | 2.174 | (0.371) |\n| Black Student-Teacher Ratio | -7.434 | (0.444) |\n\n*Note: The expenditure coefficient of 2.174 implies that a 10 percentage point increase in `1960fractionblack` is associated with an additional increase in per-pupil current expenditure of about $200 (in 2007 dollars).*\n\n**Table 2: Reduced-Form Estimates of Desegregation's Effect on Educational Attainment**\n\n| Dependent Variable (Change from 1960-65 to 1970-75) | Coefficient on `1960fractionblack` | (Std. Error) |\n| :--- | :--- | :--- |\n| HS Graduation Rate | 0.182 | (0.076) |\n\n**Summary Statistics (Pre-Desegregation Period, 1960-65):**\n- Average per-pupil current expenditure: $2,400 (in 2007 dollars).\n- Average high school graduation rate for pre-desegregation cohorts: 68 percent (0.68).\n\n### Question\n\nBased on the provided data, a researcher performs a back-of-the-envelope calculation to estimate the elasticity of the Black high school graduation rate with respect to per-pupil current expenditure, evaluated at the pre-desegregation means. The calculation assumes that the effect of `1960fractionblack` on attainment operates entirely through changes in spending.\n\nSelect all of the following statements that are **CORRECT** components or results of this calculation.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 371, "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to disentangle the competing effects of school desegregation on the educational attainment of Black students in Louisiana.\n\n**Setting / Institutional Environment.** Following the *Brown v. Board of Education* decision, Louisiana schools underwent desegregation. This policy simultaneously altered two major inputs for Black students: school resources (e.g., funding) and peer environment (e.g., exposure to white students). Districts with a higher initial share of Black students (`1960fractionblack`) experienced larger increases in resources but smaller increases in exposure to white students.\n\n### Data / Model Specification\n\nThe ideal structural model for the change in educational outcomes is:\n\n  \n\\Delta outcome_{c} = \\alpha_{0} + \\alpha_{1} \\Delta resources_{c} + \\alpha_{2} \\Delta peers_{c} + \\nu_{c} \\quad \\text{(Eq. 1)}\n \n\nHowever, because `Δresources` and `Δpeers` are both strongly correlated with `1960fractionblack`, this model suffers from severe multicollinearity. Instead, the author estimates the reduced-form model:\n\n  \n\\Delta outcome_{c} = \\beta_{0} + \\beta_{1} \\cdot 1960fractionblack_{c} + \\varepsilon_{c} \\quad \\text{(Eq. 2)}\n \n\nThe reduced-form coefficient `β₁` can be expressed in terms of the structural parameters (`α₁`, `α₂`) and the first-stage relationships linking `1960fractionblack` to the two channels (`π_{r1}` for resources, `π_{p1}` for peers):\n\n  \n\\beta_1 = \\alpha_1 \\pi_{r1} + \\alpha_2 \\pi_{p1}\n \n\nThe paper empirically finds that `β₁ > 0` for educational attainment.\n\n### Question\n\nGiven the institutional context and the models above, select all of the following statements that are **VALID** inferences or necessary assumptions for the paper's identification strategy and conclusion.", "Answer": ["A", "B", "C", "D"], "qid": "371", "question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to disentangle the competing effects of school desegregation on the educational attainment of Black students in Louisiana.\n\n**Setting / Institutional Environment.** Following the *Brown v. Board of Education* decision, Louisiana schools underwent desegregation. This policy simultaneously altered two major inputs for Black students: school resources (e.g., funding) and peer environment (e.g., exposure to white students). Districts with a higher initial share of Black students (`1960fractionblack`) experienced larger increases in resources but smaller increases in exposure to white students.\n\n### Data / Model Specification\n\nThe ideal structural model for the change in educational outcomes is:\n\n  \n\\Delta outcome_{c} = \\alpha_{0} + \\alpha_{1} \\Delta resources_{c} + \\alpha_{2} \\Delta peers_{c} + \\nu_{c} \\quad \\text{(Eq. 1)}\n \n\nHowever, because `Δresources` and `Δpeers` are both strongly correlated with `1960fractionblack`, this model suffers from severe multicollinearity. Instead, the author estimates the reduced-form model:\n\n  \n\\Delta outcome_{c} = \\beta_{0} + \\beta_{1} \\cdot 1960fractionblack_{c} + \\varepsilon_{c} \\quad \\text{(Eq. 2)}\n \n\nThe reduced-form coefficient `β₁` can be expressed in terms of the structural parameters (`α₁`, `α₂`) and the first-stage relationships linking `1960fractionblack` to the two channels (`π_{r1}` for resources, `π_{p1}` for peers):\n\n  \n\\beta_1 = \\alpha_1 \\pi_{r1} + \\alpha_2 \\pi_{p1}\n \n\nThe paper empirically finds that `β₁ > 0` for educational attainment.\n\n### Question\n\nGiven the institutional context and the models above, select all of the following statements that are **VALID** inferences or necessary assumptions for the paper's identification strategy and conclusion.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 372, "Question": "### Background\n\n**Research Question.** This problem investigates the unique asymptotic properties of GMM estimators for the panel AR(1) model precisely at the unit root (`ρ=1`), focusing on the concepts of superconsistency and superefficiency.\n\n**Setting / Institutional Environment.** The analysis is set in the inclusive panel AR(1) model, specialized to the case where the true autoregressive parameter `ρ` is exactly one. The properties of the estimators depend critically on the behavior of the initial observations, `y_{i,1}`.\n\n**Variables & Parameters.**\n- `y_{i,t}`: The outcome variable for individual `i` at time `t`.\n- `ρ`: The autoregressive parameter, with true value `ρ=1`.\n- `ε_{i,t}`: An idiosyncratic error with variance `σ²`.\n- `N`: The number of individuals in the panel.\n- `σ₁²`: The variance of the initial observation, `Var(y_{i,1})`.\n- `b`: A parameter (`0 ≤ b ≤ 1/2`) governing the relationship between `σ₁²` and `N`.\n\n---\n\n### Data / Model Specification\n\nConsider the panel AR(1) model for `T=3`:\n  \ny_{i,t} = \\rho y_{i,t-1} + v_{i,t}\n \nThe paper identifies a specific linear combination of moment conditions, `p_4(ρ)`, which is pivotal for the analysis at the unit root:\n  \np_4(\\rho) = (1-\\rho)(y_{i,2}^2 - y_{i,1}^2) \\quad \\text{(Eq. (1))}\n \nThe paper assumes that the variance of the initial observation in the unit root case is related to the sample size `N`:\n  \n\\sigma_1^2 = E(y_{i,1}^2) = \\bar{\\sigma}_1^2 N^{2b} + o(N^{2b}) \\quad \\text{(Eq. (2))}\n \nwhere `b` is a constant between 0 and 1/2. The Cramér-Rao (CR) lower bound for `ρ=1` under normality is given by:\n  \nCR = \\lim_{N\\to\\infty} \\frac{2\\sigma^2}{(T-1)((T-2)\\sigma^2 + 2\\sigma_1^2)} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the provided model and theoretical results, select all statements that are correct regarding the properties of GMM estimation when the true parameter `ρ=1`.", "Answer": ["A", "D"], "qid": "372", "question": "### Background\n\n**Research Question.** This problem investigates the unique asymptotic properties of GMM estimators for the panel AR(1) model precisely at the unit root (`ρ=1`), focusing on the concepts of superconsistency and superefficiency.\n\n**Setting / Institutional Environment.** The analysis is set in the inclusive panel AR(1) model, specialized to the case where the true autoregressive parameter `ρ` is exactly one. The properties of the estimators depend critically on the behavior of the initial observations, `y_{i,1}`.\n\n**Variables & Parameters.**\n- `y_{i,t}`: The outcome variable for individual `i` at time `t`.\n- `ρ`: The autoregressive parameter, with true value `ρ=1`.\n- `ε_{i,t}`: An idiosyncratic error with variance `σ²`.\n- `N`: The number of individuals in the panel.\n- `σ₁²`: The variance of the initial observation, `Var(y_{i,1})`.\n- `b`: A parameter (`0 ≤ b ≤ 1/2`) governing the relationship between `σ₁²` and `N`.\n\n---\n\n### Data / Model Specification\n\nConsider the panel AR(1) model for `T=3`:\n  \ny_{i,t} = \\rho y_{i,t-1} + v_{i,t}\n \nThe paper identifies a specific linear combination of moment conditions, `p_4(ρ)`, which is pivotal for the analysis at the unit root:\n  \np_4(\\rho) = (1-\\rho)(y_{i,2}^2 - y_{i,1}^2) \\quad \\text{(Eq. (1))}\n \nThe paper assumes that the variance of the initial observation in the unit root case is related to the sample size `N`:\n  \n\\sigma_1^2 = E(y_{i,1}^2) = \\bar{\\sigma}_1^2 N^{2b} + o(N^{2b}) \\quad \\text{(Eq. (2))}\n \nwhere `b` is a constant between 0 and 1/2. The Cramér-Rao (CR) lower bound for `ρ=1` under normality is given by:\n  \nCR = \\lim_{N\\to\\infty} \\frac{2\\sigma^2}{(T-1)((T-2)\\sigma^2 + 2\\sigma_1^2)} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the provided model and theoretical results, select all statements that are correct regarding the properties of GMM estimation when the true parameter `ρ=1`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 373, "Question": "### Background\n\n**Research Question.** This problem examines the key identification strategies for the covariance stationary (`|ρ|<1`) panel AR(1) model with unobserved individual effects, culminating in the fully efficient GMM estimator.\n\n**Setting / Institutional Environment.** The analysis is based on a panel dataset with a large number of individuals (`N`) and a fixed number of time periods (`T`). The data generating process follows a first-order autoregression with an individual-specific component, which creates an endogeneity problem.\n\n**Variables & Parameters.**\n- `y_{i,t}`: The outcome variable for individual `i` at time `t`.\n- `ρ`: The autoregressive parameter, `|ρ|<1`.\n- `μᵢ`: A time-invariant, individual-specific random effect.\n- `v_{i,t}`: The composite error term.\n- `ε_{i,t}`: An idiosyncratic, serially uncorrelated error.\n\n---\n\n### Data / Model Specification\n\nThe panel AR(1) model with random effects is given by:\n  \ny_{i,t} = \\rho y_{i,t-1} + v_{i,t} \\quad \\text{where} \\quad v_{i,t} = (1-\\rho)\\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \nStandard assumptions include `E(ε_{i,t})=0` and `E(ε_{i,s}ε_{i,t})=0` for `s ≠ t`.\n\nTwo sets of moment conditions are standard for GMM estimation:\n1.  **Arellano-Bond (for differenced equation):** These conditions are based on `E(y_{i,t-s} \\Delta v_{i,t}) = 0` for `s ≥ 2`.\n2.  **System GMM (for levels equation):** These additional conditions, `E(v_{i,t} \\Delta y_{i,t-1}) = 0`, are valid under the assumption of mean-stationarity.\n\nThe paper introduces a new linear moment condition, also derived from covariance stationarity, which, when added to the standard set, yields the Optimal Linear GMM (OLGMM) estimator. The paper proves this OLGMM estimator is asymptotically efficient.\n\n---\n\n### Question\n\nBased on the provided model specification for the covariance stationary case (`|ρ|<1`), select all statements that correctly describe the identification and efficiency of the GMM estimators discussed.", "Answer": ["B", "D"], "qid": "373", "question": "### Background\n\n**Research Question.** This problem examines the key identification strategies for the covariance stationary (`|ρ|<1`) panel AR(1) model with unobserved individual effects, culminating in the fully efficient GMM estimator.\n\n**Setting / Institutional Environment.** The analysis is based on a panel dataset with a large number of individuals (`N`) and a fixed number of time periods (`T`). The data generating process follows a first-order autoregression with an individual-specific component, which creates an endogeneity problem.\n\n**Variables & Parameters.**\n- `y_{i,t}`: The outcome variable for individual `i` at time `t`.\n- `ρ`: The autoregressive parameter, `|ρ|<1`.\n- `μᵢ`: A time-invariant, individual-specific random effect.\n- `v_{i,t}`: The composite error term.\n- `ε_{i,t}`: An idiosyncratic, serially uncorrelated error.\n\n---\n\n### Data / Model Specification\n\nThe panel AR(1) model with random effects is given by:\n  \ny_{i,t} = \\rho y_{i,t-1} + v_{i,t} \\quad \\text{where} \\quad v_{i,t} = (1-\\rho)\\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \nStandard assumptions include `E(ε_{i,t})=0` and `E(ε_{i,s}ε_{i,t})=0` for `s ≠ t`.\n\nTwo sets of moment conditions are standard for GMM estimation:\n1.  **Arellano-Bond (for differenced equation):** These conditions are based on `E(y_{i,t-s} \\Delta v_{i,t}) = 0` for `s ≥ 2`.\n2.  **System GMM (for levels equation):** These additional conditions, `E(v_{i,t} \\Delta y_{i,t-1}) = 0`, are valid under the assumption of mean-stationarity.\n\nThe paper introduces a new linear moment condition, also derived from covariance stationarity, which, when added to the standard set, yields the Optimal Linear GMM (OLGMM) estimator. The paper proves this OLGMM estimator is asymptotically efficient.\n\n---\n\n### Question\n\nBased on the provided model specification for the covariance stationary case (`|ρ|<1`), select all statements that correctly describe the identification and efficiency of the GMM estimators discussed.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 374, "Question": "### Background\n\n**Research Question.** This problem interrogates the potential for omitted variable bias in early estimates of the effect of oil prices on aggregate output, which were based on simple aggregate production functions. The author critiques this approach as a motivation for the more sophisticated model used in the paper.\n\n**Setting / Institutional Environment.** The analysis is motivated by the 1973-74 oil price shock, which coincided with major restrictive monetary shocks in the United States and other developed economies. The standard approach critiqued here treats imported oil as a third factor of production.\n\n### Data / Model Specification\n\nThe standard aggregate production function approach to estimating the impact of oil prices is specified as:\n\n  \nlog(y_t) = \\beta_0 + \\beta_1 log(l_t) + \\beta_2 log(k_t) + \\beta_3 log(\\theta_t) + \\beta_4 t + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n\nwhere `y_t` is real GNP, `l_t` is labor, `k_t` is capital, and `θ_t` is the real price of oil. The author argues that because restrictive monetary shocks occurred concurrently with the rise in `θ_t`, the OLS estimate of `β₃` is likely biased.\n\n### Question\n\nBased on the author's critique, select all statements that correctly describe the components and direction of the omitted variable bias on the estimate of `β₃` in Eq. (1).\n", "Answer": ["A", "B"], "qid": "374", "question": "### Background\n\n**Research Question.** This problem interrogates the potential for omitted variable bias in early estimates of the effect of oil prices on aggregate output, which were based on simple aggregate production functions. The author critiques this approach as a motivation for the more sophisticated model used in the paper.\n\n**Setting / Institutional Environment.** The analysis is motivated by the 1973-74 oil price shock, which coincided with major restrictive monetary shocks in the United States and other developed economies. The standard approach critiqued here treats imported oil as a third factor of production.\n\n### Data / Model Specification\n\nThe standard aggregate production function approach to estimating the impact of oil prices is specified as:\n\n  \nlog(y_t) = \\beta_0 + \\beta_1 log(l_t) + \\beta_2 log(k_t) + \\beta_3 log(\\theta_t) + \\beta_4 t + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n\nwhere `y_t` is real GNP, `l_t` is labor, `k_t` is capital, and `θ_t` is the real price of oil. The author argues that because restrictive monetary shocks occurred concurrently with the rise in `θ_t`, the OLS estimate of `β₃` is likely biased.\n\n### Question\n\nBased on the author's critique, select all statements that correctly describe the components and direction of the omitted variable bias on the estimate of `β₃` in Eq. (1).\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 375, "Question": "### Background\n\n**Research Question.** This problem interrogates the potential for omitted variable bias in early estimates of the effect of oil prices on aggregate output, which were based on simple aggregate production functions. The author critiques this approach as a motivation for the more sophisticated model used in the paper.\n\n**Setting / Institutional Environment.** The analysis is motivated by the 1973-74 oil price shock, which coincided with major restrictive monetary shocks in the United States and other developed economies. The standard approach critiqued here treats imported oil as a third factor of production.\n\n### Data / Model Specification\n\nThe standard aggregate production function approach to estimating the impact of oil prices is specified as:\n\n  \nlog(y_t) = \\beta_0 + \\beta_1 log(l_t) + \\beta_2 log(k_t) + \\beta_3 log(\\theta_t) + \\beta_4 t + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n\nwhere `y_t` is real GNP, `l_t` is labor, `k_t` is capital, and `θ_t` is the real price of oil.\n\n### Question\n\nConsider a counterfactual scenario where the 1973-74 oil price shock coincided not with restrictive monetary policy, but with a major *expansionary* monetary policy shock. Assuming this expansionary shock is omitted from Eq. (1), select all statements that correctly describe the components and direction of the resulting omitted variable bias on the estimate of `β₃`.\n", "Answer": ["B", "D"], "qid": "375", "question": "### Background\n\n**Research Question.** This problem interrogates the potential for omitted variable bias in early estimates of the effect of oil prices on aggregate output, which were based on simple aggregate production functions. The author critiques this approach as a motivation for the more sophisticated model used in the paper.\n\n**Setting / Institutional Environment.** The analysis is motivated by the 1973-74 oil price shock, which coincided with major restrictive monetary shocks in the United States and other developed economies. The standard approach critiqued here treats imported oil as a third factor of production.\n\n### Data / Model Specification\n\nThe standard aggregate production function approach to estimating the impact of oil prices is specified as:\n\n  \nlog(y_t) = \\beta_0 + \\beta_1 log(l_t) + \\beta_2 log(k_t) + \\beta_3 log(\\theta_t) + \\beta_4 t + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n\nwhere `y_t` is real GNP, `l_t` is labor, `k_t` is capital, and `θ_t` is the real price of oil.\n\n### Question\n\nConsider a counterfactual scenario where the 1973-74 oil price shock coincided not with restrictive monetary policy, but with a major *expansionary* monetary policy shock. Assuming this expansionary shock is omitted from Eq. (1), select all statements that correctly describe the components and direction of the resulting omitted variable bias on the estimate of `β₃`.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 376, "Question": "### Background\n\nThis problem analyzes the stationary subgame-perfect equilibrium (SPE) of a trade liberalization game, where players' strategic uncertainty is resolved through mixed strategies. The setting is an infinite-horizon, perfect information game between a government (player 1) and a domestic firm (player 2). In each period, the government decides to liberalize (`L`) with probability `u` or not (`NL`) with probability `1-u`. If `NL`, the firm decides to invest (`I`) with probability `v` or not (`NI`) with probability `1-v`. The analysis focuses on a stationary SPE where these probabilities are constant over time and players are risk-neutral.\n\n### Data / Model Specification\n\nThe one-period payoffs for player `h` (where `h=1` for government, `h=2` for firm) in the four possible states are:\n- `M_h`: Status Quo (Gov't chooses NL, Firm chooses NI)\n- `N_h`: Liberalization without Investment (Gov't chooses L before firm invests)\n- `P_h`: Investment without Liberalization (Firm chooses I, but gov't has not yet chosen L)\n- `Q_h`: Liberalization after Investment (Firm has a first-mover advantage)\n\nLet `d_h` be the discount factor for player `h`. In a stationary mixed-strategy SPE, both players must be indifferent between their pure actions. The equilibrium probabilities `u*` and `v*` are derived from these indifference conditions. The paper shows that `u*` is a decreasing function of `d_2`, and `v*` is a decreasing function of `d_1`.\n\n### Question\n\nBased on the properties of the stationary mixed-strategy SPE, select all of the following statements that are correct.", "Answer": ["A", "C"], "qid": "376", "question": "### Background\n\nThis problem analyzes the stationary subgame-perfect equilibrium (SPE) of a trade liberalization game, where players' strategic uncertainty is resolved through mixed strategies. The setting is an infinite-horizon, perfect information game between a government (player 1) and a domestic firm (player 2). In each period, the government decides to liberalize (`L`) with probability `u` or not (`NL`) with probability `1-u`. If `NL`, the firm decides to invest (`I`) with probability `v` or not (`NI`) with probability `1-v`. The analysis focuses on a stationary SPE where these probabilities are constant over time and players are risk-neutral.\n\n### Data / Model Specification\n\nThe one-period payoffs for player `h` (where `h=1` for government, `h=2` for firm) in the four possible states are:\n- `M_h`: Status Quo (Gov't chooses NL, Firm chooses NI)\n- `N_h`: Liberalization without Investment (Gov't chooses L before firm invests)\n- `P_h`: Investment without Liberalization (Firm chooses I, but gov't has not yet chosen L)\n- `Q_h`: Liberalization after Investment (Firm has a first-mover advantage)\n\nLet `d_h` be the discount factor for player `h`. In a stationary mixed-strategy SPE, both players must be indifferent between their pure actions. The equilibrium probabilities `u*` and `v*` are derived from these indifference conditions. The paper shows that `u*` is a decreasing function of `d_2`, and `v*` is a decreasing function of `d_1`.\n\n### Question\n\nBased on the properties of the stationary mixed-strategy SPE, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the stationa...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the stationa...", "error": null}}
{"ID": 377, "Question": "### Background\n\nThis problem analyzes the stationary subgame-perfect equilibrium (SPE) of a trade liberalization game between a government (player 1) and a domestic firm (player 2). In each period, the government decides to liberalize (`L`) with probability `u` or not (`NL`). If `NL`, the firm decides to invest (`I`) with probability `v` or not (`NI`).\n\n### Data / Model Specification\n\nThe one-period payoffs for player `h` are `M_h` (Status Quo), `N_h` (Liberalization w/o Investment), `P_h` (Investment w/o Liberalization), and `Q_h` (Liberalization w/ Investment). Let `d_h` be the discount factor for player `h`. In the stationary mixed-strategy SPE, the equilibrium probabilities `u*` and `v*` are derived from the players' indifference conditions.\n\nThe government's indifference condition, from which `v*` is derived, is:\n  \n\\frac{N_1}{1-d_1} = v \\left( P_1 + \\frac{d_1 Q_1}{1-d_1} \\right) + (1-v) \\left( M_1 + d_1 \\frac{N_1}{1-d_1} \\right)\n \n**Eq. (1)**\n\nThe firm's indifference condition, from which `u*` is derived, depends only on the firm's payoffs and its discount factor `d_2`.\n\n### Question\n\nConsider a policy where the government signs an international treaty that imposes a fixed cost `C > 0` on it for *not* liberalizing in any given period. This changes its one-period payoff in the status quo from `M_1` to `M_1 - C`. Select all of the following statements that correctly describe the consequences of this treaty.", "Answer": ["C", "D"], "qid": "377", "question": "### Background\n\nThis problem analyzes the stationary subgame-perfect equilibrium (SPE) of a trade liberalization game between a government (player 1) and a domestic firm (player 2). In each period, the government decides to liberalize (`L`) with probability `u` or not (`NL`). If `NL`, the firm decides to invest (`I`) with probability `v` or not (`NI`).\n\n### Data / Model Specification\n\nThe one-period payoffs for player `h` are `M_h` (Status Quo), `N_h` (Liberalization w/o Investment), `P_h` (Investment w/o Liberalization), and `Q_h` (Liberalization w/ Investment). Let `d_h` be the discount factor for player `h`. In the stationary mixed-strategy SPE, the equilibrium probabilities `u*` and `v*` are derived from the players' indifference conditions.\n\nThe government's indifference condition, from which `v*` is derived, is:\n  \n\\frac{N_1}{1-d_1} = v \\left( P_1 + \\frac{d_1 Q_1}{1-d_1} \\right) + (1-v) \\left( M_1 + d_1 \\frac{N_1}{1-d_1} \\right)\n \n**Eq. (1)**\n\nThe firm's indifference condition, from which `u*` is derived, depends only on the firm's payoffs and its discount factor `d_2`.\n\n### Question\n\nConsider a policy where the government signs an international treaty that imposes a fixed cost `C > 0` on it for *not* liberalizing in any given period. This changes its one-period payoff in the status quo from `M_1` to `M_1 - C`. Select all of the following statements that correctly describe the consequences of this treaty.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the stationa...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the stationa...", "error": null}}
{"ID": 378, "Question": "### Background\n\nThis problem analyzes the credibility of pure strategy equilibria in a game of trade liberalization. The government prefers to liberalize but is willing to wait if it can induce the firm to invest. The firm prefers protection but will invest if it believes liberalization is imminent.\n\n### Data / Model Specification\n\nThe government's payoff from a successful `q`-period protection is `X_1(q)`, and its payoff from immediate liberalization is `Y_1(0)`. A key parameter, `q*`, is the maximum number of periods the government is willing to wait, defined by:\n  \nX_1(q^*) > Y_1(0) > X_1(q^*+1)\n \n**Eq. (1)**\n\n**Proposition 2** in the paper shows that all pure strategy subgame-perfect equilibria (SPEs) are cyclical with period `q*+1`. The strategies for an equilibrium indexed by `k` (where `k = 0, 1, ..., q*`) are:\n- Government: `g^k(t) = L` (Liberalize) if and only if `t ≡ k+1 (mod q*+1)`\n- Firm: `f^k(t) = I` (Invest) if and only if `t ≡ k (mod q*+1)`\n\n### Question\n\nBased on the analysis of pure strategy subgame-perfect equilibria (SPEs) in the trade liberalization game, select all of the following statements that are correct.", "Answer": ["B", "D"], "qid": "378", "question": "### Background\n\nThis problem analyzes the credibility of pure strategy equilibria in a game of trade liberalization. The government prefers to liberalize but is willing to wait if it can induce the firm to invest. The firm prefers protection but will invest if it believes liberalization is imminent.\n\n### Data / Model Specification\n\nThe government's payoff from a successful `q`-period protection is `X_1(q)`, and its payoff from immediate liberalization is `Y_1(0)`. A key parameter, `q*`, is the maximum number of periods the government is willing to wait, defined by:\n  \nX_1(q^*) > Y_1(0) > X_1(q^*+1)\n \n**Eq. (1)**\n\n**Proposition 2** in the paper shows that all pure strategy subgame-perfect equilibria (SPEs) are cyclical with period `q*+1`. The strategies for an equilibrium indexed by `k` (where `k = 0, 1, ..., q*`) are:\n- Government: `g^k(t) = L` (Liberalize) if and only if `t ≡ k+1 (mod q*+1)`\n- Firm: `f^k(t) = I` (Invest) if and only if `t ≡ k (mod q*+1)`\n\n### Question\n\nBased on the analysis of pure strategy subgame-perfect equilibria (SPEs) in the trade liberalization game, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the credibil...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the credibil...", "error": null}}
{"ID": 379, "Question": "### Background\n\nThis problem analyzes the fundamental payoff structure in a game of trade liberalization between a government (player 1) and a domestic firm (player 2). The game proceeds in discrete periods and ends if the government liberalizes or the firm invests.\n\n### Data / Model Specification\n\nThe total discounted payoff for player `h` depends on the sequence of events. Let `d_h` be the player's discount factor.\n\n-   **Successful `q`-period protection:** The firm invests in period `q`. The payoff is:\n      \n    X_{h}(q) \\equiv \\frac{1-d_{h}^{q-1}}{1-d_{h}}M_{h} + d_{h}^{q-1}\\left[P_{h}+\\frac{d_{h}Q_{h}}{1-d_{h}}\\right]\n     \n    **Eq. (1)**\n\n-   **Unsuccessful `q`-period protection:** The government liberalizes at the start of period `q+1`. The payoff is:\n      \n    Y_{h}(q) \\equiv \\frac{1-d_{h}^{q}}{1-d_{h}}M_{h} + \\frac{d_{h}^{q}}{1-d_{h}}N_{h}\n     \n    **Eq. (2)**\n\nHere, `M_h`, `N_h`, `P_h`, and `Q_h` are one-period payoffs. Key assumptions include `M_1 < N_1` (government prefers liberalization to status quo) and `M_2 > N_2` (firm prefers status quo to liberalization w/o investment).\n\n### Question\n\nBased on the model's payoff structure and foundational assumptions, select all of the following statements that are correct.", "Answer": ["B", "C", "D"], "qid": "379", "question": "### Background\n\nThis problem analyzes the fundamental payoff structure in a game of trade liberalization between a government (player 1) and a domestic firm (player 2). The game proceeds in discrete periods and ends if the government liberalizes or the firm invests.\n\n### Data / Model Specification\n\nThe total discounted payoff for player `h` depends on the sequence of events. Let `d_h` be the player's discount factor.\n\n-   **Successful `q`-period protection:** The firm invests in period `q`. The payoff is:\n      \n    X_{h}(q) \\equiv \\frac{1-d_{h}^{q-1}}{1-d_{h}}M_{h} + d_{h}^{q-1}\\left[P_{h}+\\frac{d_{h}Q_{h}}{1-d_{h}}\\right]\n     \n    **Eq. (1)**\n\n-   **Unsuccessful `q`-period protection:** The government liberalizes at the start of period `q+1`. The payoff is:\n      \n    Y_{h}(q) \\equiv \\frac{1-d_{h}^{q}}{1-d_{h}}M_{h} + \\frac{d_{h}^{q}}{1-d_{h}}N_{h}\n     \n    **Eq. (2)**\n\nHere, `M_h`, `N_h`, `P_h`, and `Q_h` are one-period payoffs. Key assumptions include `M_1 < N_1` (government prefers liberalization to status quo) and `M_2 > N_2` (firm prefers status quo to liberalization w/o investment).\n\n### Question\n\nBased on the model's payoff structure and foundational assumptions, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the fundamen...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem analyzes the fundamen...", "error": null}}
{"ID": 380, "Question": "### Background\n\n**Research Question.** This problem investigates the finite-sample properties of the quadratic form specification test statistic. Specifically, it analyzes the source of its finite-sample bias under the null hypothesis and examines the construction of both a general and a simplified correction term designed to improve the test's size accuracy.\n\n**Setting / Institutional Environment.** We consider a correctly specified linear regression model, `y = X_c\\beta + u`, estimated by Ordinary Least Squares (OLS), where `X_c` is the `N x (d+1)` matrix of regressors including a constant. The error terms are assumed to be homoskedastic, `E[uu'|X] = \\sigma^2 I_N`.\n\n### Data / Model Specification\n\nThe OLS residuals are given by `\\tilde{u} = M_X u = (I - P_X)u`, where `P_X = X_c(X_c'X_c)^{-1}X_c'` is the projection matrix. The test statistic is based on the quadratic form `\\tilde{u}'W_N\\tilde{u}` where `W_N` is a symmetric weight matrix with zeros on the diagonal (`Tr(W_N)=0`).\n\nThe paper proposes a general finite-sample correction term:\n  \nFSC_{N}=\\frac{\\sum_{k=0}^{d}\\widehat{\\beta}_{k}}{\\sqrt{2}s(W_{N})} \\quad \\text{(Eq. (1))}\n \nwhere `\\hat{\\beta}_k` is the coefficient on `X_{c, \\cdot k}` in an OLS regression of the smoothed variable `W_N X_{c, \\cdot k}` on the full set of original regressors `X_c`.\n\n### Question\n\nSelect all statements that are correct regarding the finite-sample bias of the test statistic and the logic of the correction terms.", "Answer": ["A", "D"], "qid": "380", "question": "### Background\n\n**Research Question.** This problem investigates the finite-sample properties of the quadratic form specification test statistic. Specifically, it analyzes the source of its finite-sample bias under the null hypothesis and examines the construction of both a general and a simplified correction term designed to improve the test's size accuracy.\n\n**Setting / Institutional Environment.** We consider a correctly specified linear regression model, `y = X_c\\beta + u`, estimated by Ordinary Least Squares (OLS), where `X_c` is the `N x (d+1)` matrix of regressors including a constant. The error terms are assumed to be homoskedastic, `E[uu'|X] = \\sigma^2 I_N`.\n\n### Data / Model Specification\n\nThe OLS residuals are given by `\\tilde{u} = M_X u = (I - P_X)u`, where `P_X = X_c(X_c'X_c)^{-1}X_c'` is the projection matrix. The test statistic is based on the quadratic form `\\tilde{u}'W_N\\tilde{u}` where `W_N` is a symmetric weight matrix with zeros on the diagonal (`Tr(W_N)=0`).\n\nThe paper proposes a general finite-sample correction term:\n  \nFSC_{N}=\\frac{\\sum_{k=0}^{d}\\widehat{\\beta}_{k}}{\\sqrt{2}s(W_{N})} \\quad \\text{(Eq. (1))}\n \nwhere `\\hat{\\beta}_k` is the coefficient on `X_{c, \\cdot k}` in an OLS regression of the smoothed variable `W_N X_{c, \\cdot k}` on the full set of original regressors `X_c`.\n\n### Question\n\nSelect all statements that are correct regarding the finite-sample bias of the test statistic and the logic of the correction terms.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 381, "Question": "### Background\n\n**Research Question.** This problem explores the relationship between the implementation of a kernel-based specification test—specifically the choice of the smoothing parameter (bandwidth)—and the test's theoretical properties, namely its consistency against local alternatives.\n\n**Setting / Institutional Environment.** The true data generating process is `y_i = g_N(x_i) + u_i`, where `g_N(x)` is a local alternative that approaches the null model `f(x; \\alpha)` at a rate controlled by `\\xi`. The specification test is constructed using a kernel-based weight matrix `W_N` with bandwidth `h_N`.\n\n### Data / Model Specification\n\nThe test is consistent against local alternatives of order `N^{-\\xi}` if `\\xi` is smaller than a threshold `\\bar{\\xi}`. The test's asymptotic normality under the null requires the ratio `r(W_N^s)/s(W_N^s)` to converge to zero, where `r(W_N^s)` is the spectral radius (bounded) and `s(W_N^s) = O_p(h_N^{-d/2})`.\n\nThe local power threshold is given by:\n  \n\\bar{\\xi} = \\frac{1}{2} + \\frac{d}{4} \\lim_{N\\to\\infty} \\log_N(h_N)\n \n\n### Question\n\nSelect all statements that are correct regarding the choice of the kernel bandwidth `h_N` and its implications for the test's properties.", "Answer": ["C", "D"], "qid": "381", "question": "### Background\n\n**Research Question.** This problem explores the relationship between the implementation of a kernel-based specification test—specifically the choice of the smoothing parameter (bandwidth)—and the test's theoretical properties, namely its consistency against local alternatives.\n\n**Setting / Institutional Environment.** The true data generating process is `y_i = g_N(x_i) + u_i`, where `g_N(x)` is a local alternative that approaches the null model `f(x; \\alpha)` at a rate controlled by `\\xi`. The specification test is constructed using a kernel-based weight matrix `W_N` with bandwidth `h_N`.\n\n### Data / Model Specification\n\nThe test is consistent against local alternatives of order `N^{-\\xi}` if `\\xi` is smaller than a threshold `\\bar{\\xi}`. The test's asymptotic normality under the null requires the ratio `r(W_N^s)/s(W_N^s)` to converge to zero, where `r(W_N^s)` is the spectral radius (bounded) and `s(W_N^s) = O_p(h_N^{-d/2})`.\n\nThe local power threshold is given by:\n  \n\\bar{\\xi} = \\frac{1}{2} + \\frac{d}{4} \\lim_{N\\to\\infty} \\log_N(h_N)\n \n\n### Question\n\nSelect all statements that are correct regarding the choice of the kernel bandwidth `h_N` and its implications for the test's properties.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 382, "Question": "### Background\n\n**Research Question.** This problem examines the foundational theory for the class of quadratic form specification tests. It explores the motivation for the test, the mathematical structure of the test statistic, and the key condition under which its asymptotic normality is established.\n\n### Data / Model Specification\n\nThe test statistic is based on the quadratic form `Q_N = \\tilde{u}'W_N\\tilde{u}`. Its asymptotic normality relies on the spectral properties of the symmetric weight matrix `W_N`. Let `\\gamma_i` be the eigenvalues of `W_N`.\n-   **Spectral Radius:** `r(W_N) = \\max_i |\\gamma_i|`\n-   **Frobenius Norm (squared):** `s(W_N)^2 = \\sum_{i,j} w_{ij}^2 = \\sum_i \\gamma_i^2`\n\nProposition 1 in the paper establishes that a standardized version of `Q_N` converges to a standard normal distribution, provided that `r(W_N)/s(W_N) \\to_p 0`.\n\n### Question\n\nSelect all statements that are correct regarding the theoretical underpinnings of the quadratic form test.", "Answer": ["B", "D"], "qid": "382", "question": "### Background\n\n**Research Question.** This problem examines the foundational theory for the class of quadratic form specification tests. It explores the motivation for the test, the mathematical structure of the test statistic, and the key condition under which its asymptotic normality is established.\n\n### Data / Model Specification\n\nThe test statistic is based on the quadratic form `Q_N = \\tilde{u}'W_N\\tilde{u}`. Its asymptotic normality relies on the spectral properties of the symmetric weight matrix `W_N`. Let `\\gamma_i` be the eigenvalues of `W_N`.\n-   **Spectral Radius:** `r(W_N) = \\max_i |\\gamma_i|`\n-   **Frobenius Norm (squared):** `s(W_N)^2 = \\sum_{i,j} w_{ij}^2 = \\sum_i \\gamma_i^2`\n\nProposition 1 in the paper establishes that a standardized version of `Q_N` converges to a standard normal distribution, provided that `r(W_N)/s(W_N) \\to_p 0`.\n\n### Question\n\nSelect all statements that are correct regarding the theoretical underpinnings of the quadratic form test.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 383, "Question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Question\n\nTo solve the hold-up problem under Completely Private Information (CPI), a contract `(d, t)` must align each agent's private incentive to invest with the social objective of maximizing total expected surplus. According to the paper's core argument, which of the following properties are required for the contract to achieve this alignment?", "Answer": ["A", "C"], "qid": "383", "question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Question\n\nTo solve the hold-up problem under Completely Private Information (CPI), a contract `(d, t)` must align each agent's private incentive to invest with the social objective of maximizing total expected surplus. According to the paper's core argument, which of the following properties are required for the contract to achieve this alignment?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThe model involves `n` agents who ...", "model_answer": "This is a detailed response to the question about ### Background\n\nThe model involves `n` agents who ...", "error": null}}
{"ID": 384, "Question": "### Background\n\n**Research Question.** This question explores the paper's core finding: the existence and consequences of a \"negative strategic effect\" of observable investment in a First-Price Auction (FPA). An observable investment by one firm, while directly beneficial through cost reduction, may trigger a strategic reaction from competitors that erodes or even reverses the initial gain.\n\n**Setting / Institutional Environment.** An N-firm FPA for a procurement contract. One firm (the \"upgrader\") has an opportunity to make an observable investment that improves its ex-ante distribution of costs. This improvement is modeled as a \"distributional upgrade.\"\n\n### Data / Model Specification\n\nThe paper provides a numerical example to illustrate the potential magnitude of the negative strategic effect. Initially, Firm 1 is a \"laggard\" and Firm 2 is a \"leader.\" Firm 1 has an opportunity to make a costless investment to match Firm 2's strength, making the auction symmetric.\n\n**Table 1: Payoffs from a Numerical Simulation**\n\n| Configuration | Firm 1 Payoff ($\\Pi_1$) | Firm 2 Payoff ($\\Pi_2$) |\n| :--- | :--- | :--- |\n| Before Investment ($F_1=U[0,10], F_2=U[0,5]$) | 0.90445 | 1.93245 |\n| After Investment ($\\widetilde{F}_1=U[0,5], F_2=U[0,5]$) | 0.83333 | 0.83333 |\n\n### Question\n\nBased on the provided information and the numerical results in Table 1, select all of the following statements that are correct interpretations of this example.", "Answer": ["A", "C"], "qid": "384", "question": "### Background\n\n**Research Question.** This question explores the paper's core finding: the existence and consequences of a \"negative strategic effect\" of observable investment in a First-Price Auction (FPA). An observable investment by one firm, while directly beneficial through cost reduction, may trigger a strategic reaction from competitors that erodes or even reverses the initial gain.\n\n**Setting / Institutional Environment.** An N-firm FPA for a procurement contract. One firm (the \"upgrader\") has an opportunity to make an observable investment that improves its ex-ante distribution of costs. This improvement is modeled as a \"distributional upgrade.\"\n\n### Data / Model Specification\n\nThe paper provides a numerical example to illustrate the potential magnitude of the negative strategic effect. Initially, Firm 1 is a \"laggard\" and Firm 2 is a \"leader.\" Firm 1 has an opportunity to make a costless investment to match Firm 2's strength, making the auction symmetric.\n\n**Table 1: Payoffs from a Numerical Simulation**\n\n| Configuration | Firm 1 Payoff ($\\Pi_1$) | Firm 2 Payoff ($\\Pi_2$) |\n| :--- | :--- | :--- |\n| Before Investment ($F_1=U[0,10], F_2=U[0,5]$) | 0.90445 | 1.93245 |\n| After Investment ($\\widetilde{F}_1=U[0,5], F_2=U[0,5]$) | 0.83333 | 0.83333 |\n\n### Question\n\nBased on the provided information and the numerical results in Table 1, select all of the following statements that are correct interpretations of this example.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 385, "Question": "### Background\n\n**Research Question.** This question tests the understanding of the paper's main theoretical results regarding how an observable investment by one firm affects the strategic environment in a First-Price Auction (FPA).\n\n**Setting / Institutional Environment.** An N-firm FPA for a procurement contract. One firm makes an observable investment that results in a \"distributional upgrade\" of its costs.\n\n### Data / Model Specification\n\nThe paper's analysis shows that bidding strategies in an FPA are not necessarily strategic complements, meaning a firm might not always respond to more aggressive bidding by bidding more aggressively itself. The paper therefore compares equilibria directly to derive its main comparative statics results:\n\n- **Proposition 1:** Establishes the result for the $N=2$ bidder case.\n- **Proposition 2:** Generalizes the result for $N>2$ bidders under the condition that the investment causes a \"change in market leadership\" (a laggard becomes a leader).\n\n### Question\n\nAccording to the paper's theoretical analysis of the First-Price Auction (FPA), which of the following statements are correct?", "Answer": ["A", "D"], "qid": "385", "question": "### Background\n\n**Research Question.** This question tests the understanding of the paper's main theoretical results regarding how an observable investment by one firm affects the strategic environment in a First-Price Auction (FPA).\n\n**Setting / Institutional Environment.** An N-firm FPA for a procurement contract. One firm makes an observable investment that results in a \"distributional upgrade\" of its costs.\n\n### Data / Model Specification\n\nThe paper's analysis shows that bidding strategies in an FPA are not necessarily strategic complements, meaning a firm might not always respond to more aggressive bidding by bidding more aggressively itself. The paper therefore compares equilibria directly to derive its main comparative statics results:\n\n- **Proposition 1:** Establishes the result for the $N=2$ bidder case.\n- **Proposition 2:** Generalizes the result for $N>2$ bidders under the condition that the investment causes a \"change in market leadership\" (a laggard becomes a leader).\n\n### Question\n\nAccording to the paper's theoretical analysis of the First-Price Auction (FPA), which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 386, "Question": "### Background\n\n**Research Question.** This question explores the paper's normative conclusions by comparing investment incentives and social welfare outcomes between the First-Price Auction (FPA) and the Second-Price Auction (SPA).\n\n**Setting / Institutional Environment.** An N-firm procurement auction where one firm can invest to change its market position from a cost \"laggard\" to a cost \"leader.\"\n\n### Data / Model Specification\n\nThe analysis compares the FPA and SPA, where bidding in the SPA is non-strategic (bidding one's true cost is a dominant strategy). The comparison hinges on a key preliminary result:\n\n**Lemma 6:**\n1. A strong bidder (\"leader\") earns a higher expected payoff in the SPA than in the FPA.\n2. A weak bidder (\"laggard\") earns a higher expected payoff in the FPA than in the SPA.\n\nThis lemma is used to derive two main propositions:\n- **Proposition 3 (Underinvestment):** The FPA provides less incentive than the SPA for investments that involve a change of market leadership.\n- **Proposition 4 (SPA Optimality):** The SPA provides the socially optimal level of investment incentives.\n\n### Question\n\nGiven the paper's findings, particularly Lemma 6, which of the following conclusions about investment incentives and auction performance are correct?", "Answer": ["B", "C"], "qid": "386", "question": "### Background\n\n**Research Question.** This question explores the paper's normative conclusions by comparing investment incentives and social welfare outcomes between the First-Price Auction (FPA) and the Second-Price Auction (SPA).\n\n**Setting / Institutional Environment.** An N-firm procurement auction where one firm can invest to change its market position from a cost \"laggard\" to a cost \"leader.\"\n\n### Data / Model Specification\n\nThe analysis compares the FPA and SPA, where bidding in the SPA is non-strategic (bidding one's true cost is a dominant strategy). The comparison hinges on a key preliminary result:\n\n**Lemma 6:**\n1. A strong bidder (\"leader\") earns a higher expected payoff in the SPA than in the FPA.\n2. A weak bidder (\"laggard\") earns a higher expected payoff in the FPA than in the SPA.\n\nThis lemma is used to derive two main propositions:\n- **Proposition 3 (Underinvestment):** The FPA provides less incentive than the SPA for investments that involve a change of market leadership.\n- **Proposition 4 (SPA Optimality):** The SPA provides the socially optimal level of investment incentives.\n\n### Question\n\nGiven the paper's findings, particularly Lemma 6, which of the following conclusions about investment incentives and auction performance are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 387, "Question": "### Background\n\n**Research Question.** This question examines the theoretical foundations of the asymmetric First-Price Auction (FPA) model used in the paper, focusing on how investment is defined and the technical properties ensuring a well-behaved equilibrium.\n\n### Data / Model Specification\n\nThe model relies on several key components:\n\n1.  **Investment Definition:** A cost-reducing investment is modeled as a **distributional upgrade**. A new cost distribution $\\widetilde{F}$ is an upgrade of an old one $F$ if it has a uniformly higher hazard rate:\n      \n    \\frac{\\widetilde{F}'(c)}{1-\\widetilde{F}(c)} > \\frac{F'(c)}{1-F(c)} \\quad \\text{(Eq. (1))}\n     \n\n2.  **Equilibrium Condition:** The first-order condition (FOC) for firm $i$ relates its own bidding behavior to the hazard rates of its opponents' bid distributions.\n\n3.  **Technical Property:** The analysis relies on the profit function $\\pi_i(b,c)$ being strictly log-supermodular, which is a 'single-crossing' or 'sorting' condition.\n\n### Question\n\nSelect all statements that correctly describe the theoretical foundations of the FPA model presented in the paper.", "Answer": ["B", "D"], "qid": "387", "question": "### Background\n\n**Research Question.** This question examines the theoretical foundations of the asymmetric First-Price Auction (FPA) model used in the paper, focusing on how investment is defined and the technical properties ensuring a well-behaved equilibrium.\n\n### Data / Model Specification\n\nThe model relies on several key components:\n\n1.  **Investment Definition:** A cost-reducing investment is modeled as a **distributional upgrade**. A new cost distribution $\\widetilde{F}$ is an upgrade of an old one $F$ if it has a uniformly higher hazard rate:\n      \n    \\frac{\\widetilde{F}'(c)}{1-\\widetilde{F}(c)} > \\frac{F'(c)}{1-F(c)} \\quad \\text{(Eq. (1))}\n     \n\n2.  **Equilibrium Condition:** The first-order condition (FOC) for firm $i$ relates its own bidding behavior to the hazard rates of its opponents' bid distributions.\n\n3.  **Technical Property:** The analysis relies on the profit function $\\pi_i(b,c)$ being strictly log-supermodular, which is a 'single-crossing' or 'sorting' condition.\n\n### Question\n\nSelect all statements that correctly describe the theoretical foundations of the FPA model presented in the paper.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 388, "Question": "### Background\n\n**Research Question.** This problem investigates the core theoretical contribution of the paper: how to construct 'intelligible' factors for the yield curve (i.e., factors that are both interpretable and have orthogonal innovations) and why a popular alternative, the Nelson-Siegel (NS) model, fails to achieve this.\n\n**Setting.** The analysis contrasts two factor models. Both start with a set of raw factors (`θ̃` for NS, `θ` for the proposed model) and seek a transformation matrix (`B̃` or `B`) to produce intelligible factors (`φ̃` or `φ`). The key challenge is to find a model structure that allows the transformation matrix to simultaneously satisfy constraints for interpretability (defining long and short rates) and orthogonality of innovations.\n\n### Data / Model Specification\n\n**The Nelson-Siegel (NS) Model:**\nThe NS model uses three raw factors `θ̃_t` with the following limiting behavior for the yield curve `r(m)`:\n\n  \n\\lim_{m \\to \\infty} r(m) = \\tilde{\\theta}_{1t} \n \n  \n\\lim_{m \\to 0} r(m) = \\tilde{\\theta}_{1t} - \\tilde{\\theta}_{2t}\n \n\nThese definitions, combined with the goal of defining `φ̃_1` as the long rate and `φ̃_2` as the short rate, fully determine the first two rows of the transformation matrix `B̃` for the NS model, `φ̃_t = B̃θ̃_t`:\n\n  \n\\tilde{B} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ \\tilde{\\beta}_1 & \\tilde{\\beta}_2 & \\tilde{\\beta}_3 \\end{pmatrix} \\quad \\text{(Eq. (1))}\n \n\n**The Proposed Model:**\nThe paper proposes a different specification for the raw factor loadings `y(m)`. For a 3-factor model, the yield curve `r(m)` is `r_t = y(m)θ_t + ε_t`, and the limiting behavior is:\n\n  \n\\lim_{m \\to \\infty} r(m) = \\theta_{1t}\n \n  \n\\lim_{m \\to 0} r(m) = \\theta_{1t} + \\log(\\alpha_{2})\\theta_{2t} + \\log(\\alpha_{3})\\theta_{3t}\n \n\nwhere `0 < α_i < 1` are parameters governing the curvature of the loadings. Defining `φ_1` as the long rate and `φ_2` as the short rate in the transformation `φ_t = Bθ_t` yields:\n\n  \nB = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & \\log(\\alpha_2) & \\log(\\alpha_3) \\\\ \\beta_1 & \\beta_2 & \\beta_3 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n \n\n**The Orthogonality Constraint:**\nFor any model, the innovations `v_t` of a VAR on the raw factors are transformed into structural innovations `u_t = Bv_t`. Intelligibility requires `u_t` to be mutually orthogonal, meaning their covariance matrix `BΩB'` must be diagonal, where `Ω = E[v_t v_t']`.\n\n### Question\n\nBased on the model specifications provided, select all statements that correctly describe why the proposed model can achieve 'intelligibility' (interpretable factors with orthogonal innovations) while the Nelson-Siegel (NS) model cannot.", "Answer": ["A", "D"], "qid": "388", "question": "### Background\n\n**Research Question.** This problem investigates the core theoretical contribution of the paper: how to construct 'intelligible' factors for the yield curve (i.e., factors that are both interpretable and have orthogonal innovations) and why a popular alternative, the Nelson-Siegel (NS) model, fails to achieve this.\n\n**Setting.** The analysis contrasts two factor models. Both start with a set of raw factors (`θ̃` for NS, `θ` for the proposed model) and seek a transformation matrix (`B̃` or `B`) to produce intelligible factors (`φ̃` or `φ`). The key challenge is to find a model structure that allows the transformation matrix to simultaneously satisfy constraints for interpretability (defining long and short rates) and orthogonality of innovations.\n\n### Data / Model Specification\n\n**The Nelson-Siegel (NS) Model:**\nThe NS model uses three raw factors `θ̃_t` with the following limiting behavior for the yield curve `r(m)`:\n\n  \n\\lim_{m \\to \\infty} r(m) = \\tilde{\\theta}_{1t} \n \n  \n\\lim_{m \\to 0} r(m) = \\tilde{\\theta}_{1t} - \\tilde{\\theta}_{2t}\n \n\nThese definitions, combined with the goal of defining `φ̃_1` as the long rate and `φ̃_2` as the short rate, fully determine the first two rows of the transformation matrix `B̃` for the NS model, `φ̃_t = B̃θ̃_t`:\n\n  \n\\tilde{B} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ \\tilde{\\beta}_1 & \\tilde{\\beta}_2 & \\tilde{\\beta}_3 \\end{pmatrix} \\quad \\text{(Eq. (1))}\n \n\n**The Proposed Model:**\nThe paper proposes a different specification for the raw factor loadings `y(m)`. For a 3-factor model, the yield curve `r(m)` is `r_t = y(m)θ_t + ε_t`, and the limiting behavior is:\n\n  \n\\lim_{m \\to \\infty} r(m) = \\theta_{1t}\n \n  \n\\lim_{m \\to 0} r(m) = \\theta_{1t} + \\log(\\alpha_{2})\\theta_{2t} + \\log(\\alpha_{3})\\theta_{3t}\n \n\nwhere `0 < α_i < 1` are parameters governing the curvature of the loadings. Defining `φ_1` as the long rate and `φ_2` as the short rate in the transformation `φ_t = Bθ_t` yields:\n\n  \nB = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & \\log(\\alpha_2) & \\log(\\alpha_3) \\\\ \\beta_1 & \\beta_2 & \\beta_3 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n \n\n**The Orthogonality Constraint:**\nFor any model, the innovations `v_t` of a VAR on the raw factors are transformed into structural innovations `u_t = Bv_t`. Intelligibility requires `u_t` to be mutually orthogonal, meaning their covariance matrix `BΩB'` must be diagonal, where `Ω = E[v_t v_t']`.\n\n### Question\n\nBased on the model specifications provided, select all statements that correctly describe why the proposed model can achieve 'intelligibility' (interpretable factors with orthogonal innovations) while the Nelson-Siegel (NS) model cannot.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 389, "Question": "### Background\n\n**Research Question.** This problem examines the paper's main empirical finding and its profound implications for monetary policy. The analysis hinges on a variance decomposition that attributes yield curve volatility to the underlying orthogonal 'intelligible' shocks.\n\n**Setting.** The model identifies three orthogonal structural innovations: to the long factor (`u_1`), the short factor (`u_2`), and the curvature factor (`u_3`). The paper provides a monetary policy interpretation for each. The dynamics are captured by a structural VAR, which can be inverted to a Vector Moving-Average (VMA) representation to trace the impact of shocks over time.\n\n### Data / Model Specification\n\nThe model for the yield `r_t(m)` at maturity `m` is:\n  \nr_{t}(m) = k(m)\\phi_{t} + \\epsilon_{t}(m)\n \nwhere `k(m)` are the factor loadings and `φ_t` is the vector of intelligible factors.\n\nThe factors `φ_t` have a VMA representation driven by the orthogonal structural innovations `u_t`:\n  \n\\phi_{t} = \\bar{C} + \\sum_{j=0}^{\\infty} C_j u_{t-j} \\quad \\text{(Eq. (1))}\n \nwhere `C_j` are matrix coefficients.\n\n**Monetary Policy Interpretation:**\n-   **Short Factor (`u_2`) Innovations:** Surprise actions by the central bank's trading desk (unexpected policy rate changes).\n-   **Curvature Factor (`u_3`) Innovations:** Central bank communication about the intended future path of policy.\n\n**Key Empirical Finding:** Innovations to the curvature factor (`u_3`) are the main drivers of yield volatility, explaining over 90% of variance for short-term yields and over 67% for long-term yields. Innovations to the short factor (`u_2`) have an almost negligible influence.\n\n### Question\n\nGiven the model's framework and its key empirical finding that curvature factor innovations (`u₃`) are the primary drivers of yield volatility, select all statements that correctly describe the model's implications for monetary policy.", "Answer": ["C", "D"], "qid": "389", "question": "### Background\n\n**Research Question.** This problem examines the paper's main empirical finding and its profound implications for monetary policy. The analysis hinges on a variance decomposition that attributes yield curve volatility to the underlying orthogonal 'intelligible' shocks.\n\n**Setting.** The model identifies three orthogonal structural innovations: to the long factor (`u_1`), the short factor (`u_2`), and the curvature factor (`u_3`). The paper provides a monetary policy interpretation for each. The dynamics are captured by a structural VAR, which can be inverted to a Vector Moving-Average (VMA) representation to trace the impact of shocks over time.\n\n### Data / Model Specification\n\nThe model for the yield `r_t(m)` at maturity `m` is:\n  \nr_{t}(m) = k(m)\\phi_{t} + \\epsilon_{t}(m)\n \nwhere `k(m)` are the factor loadings and `φ_t` is the vector of intelligible factors.\n\nThe factors `φ_t` have a VMA representation driven by the orthogonal structural innovations `u_t`:\n  \n\\phi_{t} = \\bar{C} + \\sum_{j=0}^{\\infty} C_j u_{t-j} \\quad \\text{(Eq. (1))}\n \nwhere `C_j` are matrix coefficients.\n\n**Monetary Policy Interpretation:**\n-   **Short Factor (`u_2`) Innovations:** Surprise actions by the central bank's trading desk (unexpected policy rate changes).\n-   **Curvature Factor (`u_3`) Innovations:** Central bank communication about the intended future path of policy.\n\n**Key Empirical Finding:** Innovations to the curvature factor (`u_3`) are the main drivers of yield volatility, explaining over 90% of variance for short-term yields and over 67% for long-term yields. Innovations to the short factor (`u_2`) have an almost negligible influence.\n\n### Question\n\nGiven the model's framework and its key empirical finding that curvature factor innovations (`u₃`) are the primary drivers of yield volatility, select all statements that correctly describe the model's implications for monetary policy.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 390, "Question": "### Background\n\n**Research Question.** This problem analyzes the core concepts of the model: the suboptimal equilibrium where the government is trapped, and the average dynamics that keep it there.\n\n**Setting.** A government with a misspecified linear Phillips curve model uses a recursive least squares algorithm to update its beliefs. The analysis focuses on the deterministic, average path of these beliefs, known as the \"mean dynamics,\" and the properties of the self-confirming equilibrium (SCE) to which these dynamics converge.\n\n### Question\n\nAccording to the paper's analysis of the self-confirming equilibrium (SCE) and the mean dynamics, which of the following statements are correct? (Select all that apply)", "Answer": ["B", "C"], "qid": "390", "question": "### Background\n\n**Research Question.** This problem analyzes the core concepts of the model: the suboptimal equilibrium where the government is trapped, and the average dynamics that keep it there.\n\n**Setting.** A government with a misspecified linear Phillips curve model uses a recursive least squares algorithm to update its beliefs. The analysis focuses on the deterministic, average path of these beliefs, known as the \"mean dynamics,\" and the properties of the self-confirming equilibrium (SCE) to which these dynamics converge.\n\n### Question\n\nAccording to the paper's analysis of the self-confirming equilibrium (SCE) and the mean dynamics, which of the following statements are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 391, "Question": "### Background\n\n**Research Question.** This problem characterizes the optimal contract in an infinite-horizon Principal-Agent relationship where the Agent's output is privately observed by the Principal. This setting endogenizes punishment as the inefficient termination of a valuable relationship.\n\n**Setting / Institutional Environment.** The analysis moves to an infinitely repeated game where explicit 'money burning' is replaced by the threat of termination. The central result is that any arbitrarily complex contract is payoff-equivalent to a simple efficiency wage contract, where incentives are provided solely by the threat of being fired. The Principal's key decision is to design an optimal history-dependent termination rule.\n\n**Variables & Parameters.**\n*   `V`, `F`: The expected lifetime payoffs for the Agent and Principal, respectively.\n*   `h^t`: A history of `t` output realizations, `(y_1, ..., y_t)`.\n*   `a(h^t)`: The Principal's choice of continuation probability after history `h^t`.\n*   `(h^t, H)` and `(h^t, L)`: Histories of length `t+1` formed by appending a High or Low output to `h^t`.\n\n---\n\n### Data / Model Specification\n\nThe analysis of the infinite horizon case yields two main results.\n\nFirst, the entire class of possible contracts can be simplified without loss of generality:\n\n**Theorem 1:** Any contract is payoff-equivalent to one with three properties: (i) The Agent receives a constant wage until fired (no bonuses); (ii) The Agent exerts effort every period until fired; (iii) The Principal provides no performance feedback.\n\nSecond, an optimal termination rule must satisfy certain properties, established by variational arguments. One key property is:\n\n**Proposition 5(i):** For any history `h^t`, if `a(h^t, H) < 1`, then `a(h^t, L) = 0`.\n\n---\n\n### Question\n\nBased on the results above, select all statements that correctly characterize the optimal contract in this private monitoring environment.", "Answer": ["C", "D"], "qid": "391", "question": "### Background\n\n**Research Question.** This problem characterizes the optimal contract in an infinite-horizon Principal-Agent relationship where the Agent's output is privately observed by the Principal. This setting endogenizes punishment as the inefficient termination of a valuable relationship.\n\n**Setting / Institutional Environment.** The analysis moves to an infinitely repeated game where explicit 'money burning' is replaced by the threat of termination. The central result is that any arbitrarily complex contract is payoff-equivalent to a simple efficiency wage contract, where incentives are provided solely by the threat of being fired. The Principal's key decision is to design an optimal history-dependent termination rule.\n\n**Variables & Parameters.**\n*   `V`, `F`: The expected lifetime payoffs for the Agent and Principal, respectively.\n*   `h^t`: A history of `t` output realizations, `(y_1, ..., y_t)`.\n*   `a(h^t)`: The Principal's choice of continuation probability after history `h^t`.\n*   `(h^t, H)` and `(h^t, L)`: Histories of length `t+1` formed by appending a High or Low output to `h^t`.\n\n---\n\n### Data / Model Specification\n\nThe analysis of the infinite horizon case yields two main results.\n\nFirst, the entire class of possible contracts can be simplified without loss of generality:\n\n**Theorem 1:** Any contract is payoff-equivalent to one with three properties: (i) The Agent receives a constant wage until fired (no bonuses); (ii) The Agent exerts effort every period until fired; (iii) The Principal provides no performance feedback.\n\nSecond, an optimal termination rule must satisfy certain properties, established by variational arguments. One key property is:\n\n**Proposition 5(i):** For any history `h^t`, if `a(h^t, H) < 1`, then `a(h^t, L) = 0`.\n\n---\n\n### Question\n\nBased on the results above, select all statements that correctly characterize the optimal contract in this private monitoring environment.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 392, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental constraints of contract design in a Principal-Agent relationship where the Agent's effort is unobservable and the resulting output is privately observed by the Principal. It establishes why first-best outcomes are impossible and why simple finite-horizon relationships are doomed to fail.\n\n**Setting / Institutional Environment.** A risk-neutral Principal and Agent interact over a known, finite horizon `T`. The Agent's costly effort stochastically determines a binary output (High/Low), which only the Principal sees. The contract must be self-enforcing for both parties and must satisfy budget balance (payments from the Principal equal payments to the Agent; no 'money burning').\n\n---\n\n### Data / Model Specification\n\nIncentivizing high effort requires two conflicting conditions to be met:\n1.  **Agent's Incentive Compatibility (IC):** The Agent's continuation value must depend on output: `V_H > V_L`.\n2.  **Principal's Truth-Telling:** The Principal's continuation value must be independent of their report of the output: `F_H = F_L`.\n\n---\n\n### Question\n\nIn the finite-horizon model with budget balance, which of the following statements are **INCORRECT** descriptions of the contracting problem?", "Answer": ["B", "D"], "qid": "392", "question": "### Background\n\n**Research Question.** This problem explores the fundamental constraints of contract design in a Principal-Agent relationship where the Agent's effort is unobservable and the resulting output is privately observed by the Principal. It establishes why first-best outcomes are impossible and why simple finite-horizon relationships are doomed to fail.\n\n**Setting / Institutional Environment.** A risk-neutral Principal and Agent interact over a known, finite horizon `T`. The Agent's costly effort stochastically determines a binary output (High/Low), which only the Principal sees. The contract must be self-enforcing for both parties and must satisfy budget balance (payments from the Principal equal payments to the Agent; no 'money burning').\n\n---\n\n### Data / Model Specification\n\nIncentivizing high effort requires two conflicting conditions to be met:\n1.  **Agent's Incentive Compatibility (IC):** The Agent's continuation value must depend on output: `V_H > V_L`.\n2.  **Principal's Truth-Telling:** The Principal's continuation value must be independent of their report of the output: `F_H = F_L`.\n\n---\n\n### Question\n\nIn the finite-horizon model with budget balance, which of the following statements are **INCORRECT** descriptions of the contracting problem?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 393, "Question": "### Background\n\n**Research Question.** This problem characterizes the optimal contract in a finite-horizon setting where the impossibility of providing incentives is overcome by allowing for 'money burning'—payments made by the Principal that are not received by the Agent.\n\n**Setting / Institutional Environment.** In a finite-horizon (`T+1` periods) Principal-Agent model with private monitoring, the optimal contract pays the Agent a high wage for all output histories *except* for the single history where output is low in every period. In that specific case, an amount `Z` is burned. This structure embodies the principle of 'reusability of punishments'.\n\n---\n\n### Data / Model Specification\n\nThe size of the punishment `Z` is set to make the Agent's incentive compatibility (IC) constraint for the first period (`t=0`) bind. The formula is:\n  \nZ = \\frac{c}{\\delta^{T}(p-q)(1-p)^{T}} \\quad \\text{(Eq. (1))}\n \nThe expected present value cost of providing incentives is `C_I = \\delta^T \\times Prob(punishment) \\times Z`, which simplifies to:\n  \nC_I = \\frac{c(1-p)}{p-q} \\quad \\text{(Eq. (2))}\n \n\nAssume the following parameter values for a 2-period relationship (t=0, 1, so T=1):\n*   `c = 0.25` (cost of effort)\n*   `p = 0.7` (prob of High output with effort)\n*   `q = 0.2` (prob of High output without effort)\n*   `\\delta = 1.0` (discount factor)\n\n---\n\n### Question\n\nGiven these parameters, select all of the following statements that are true.", "Answer": ["A", "B"], "qid": "393", "question": "### Background\n\n**Research Question.** This problem characterizes the optimal contract in a finite-horizon setting where the impossibility of providing incentives is overcome by allowing for 'money burning'—payments made by the Principal that are not received by the Agent.\n\n**Setting / Institutional Environment.** In a finite-horizon (`T+1` periods) Principal-Agent model with private monitoring, the optimal contract pays the Agent a high wage for all output histories *except* for the single history where output is low in every period. In that specific case, an amount `Z` is burned. This structure embodies the principle of 'reusability of punishments'.\n\n---\n\n### Data / Model Specification\n\nThe size of the punishment `Z` is set to make the Agent's incentive compatibility (IC) constraint for the first period (`t=0`) bind. The formula is:\n  \nZ = \\frac{c}{\\delta^{T}(p-q)(1-p)^{T}} \\quad \\text{(Eq. (1))}\n \nThe expected present value cost of providing incentives is `C_I = \\delta^T \\times Prob(punishment) \\times Z`, which simplifies to:\n  \nC_I = \\frac{c(1-p)}{p-q} \\quad \\text{(Eq. (2))}\n \n\nAssume the following parameter values for a 2-period relationship (t=0, 1, so T=1):\n*   `c = 0.25` (cost of effort)\n*   `p = 0.7` (prob of High output with effort)\n*   `q = 0.2` (prob of High output without effort)\n*   `\\delta = 1.0` (discount factor)\n\n---\n\n### Question\n\nGiven these parameters, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 394, "Question": "### Background\n\n**Research Question.** This problem analyzes a tractable class of infinite-horizon contracts, known as `T`-period review contracts, to understand how incentive provision, contract value, and efficiency are related, and to explore the conditions under which near-perfect efficiency can be achieved.\n\n**Setting / Institutional Environment.** The relationship is structured into independent review periods of length `T`. At the end of each cycle, if the output was low in all `T` periods, the Agent is fired with probability `\\beta`. Otherwise, the relationship resets for another `T`-period cycle.\n\n---\n\n### Data / Model Specification\n\nThe analysis of `T`-period review contracts relies on two key results.\n\n1.  The total value of the relationship is given by:\n      \n    V = \\frac{S_T - \\frac{c(1-p)}{p-q}}{1-\\delta^T} \\quad \\text{(Eq. (1))}\n     \n    where `S_T` is the surplus generated within one `T`-period cycle.\n\n2.  The inefficiency of the contract, `\\lambda_T`, is the ratio of the fixed incentive cost to the per-cycle surplus:\n      \n    \\lambda_T = \\frac{c(1-p)}{p-q} \\frac{1}{S_T} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the analysis of T-period review contracts, select all statements that are correct.", "Answer": ["C", "D"], "qid": "394", "question": "### Background\n\n**Research Question.** This problem analyzes a tractable class of infinite-horizon contracts, known as `T`-period review contracts, to understand how incentive provision, contract value, and efficiency are related, and to explore the conditions under which near-perfect efficiency can be achieved.\n\n**Setting / Institutional Environment.** The relationship is structured into independent review periods of length `T`. At the end of each cycle, if the output was low in all `T` periods, the Agent is fired with probability `\\beta`. Otherwise, the relationship resets for another `T`-period cycle.\n\n---\n\n### Data / Model Specification\n\nThe analysis of `T`-period review contracts relies on two key results.\n\n1.  The total value of the relationship is given by:\n      \n    V = \\frac{S_T - \\frac{c(1-p)}{p-q}}{1-\\delta^T} \\quad \\text{(Eq. (1))}\n     \n    where `S_T` is the surplus generated within one `T`-period cycle.\n\n2.  The inefficiency of the contract, `\\lambda_T`, is the ratio of the fixed incentive cost to the per-cycle surplus:\n      \n    \\lambda_T = \\frac{c(1-p)}{p-q} \\frac{1}{S_T} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the analysis of T-period review contracts, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 395, "Question": "### Background\n\n**Research Question.** This problem investigates the model's approach to endogenously defining regional recession clusters based on observable state characteristics and the Bayesian statistical techniques used to estimate this relationship.\n\n**Setting / Institutional Environment.** The model posits that the probability of a state `n` belonging to an idiosyncratic recession cluster `k` is systematically related to a vector of its time-invariant economic characteristics. This relationship is estimated using a Bayesian Gibbs sampler, which is facilitated by a latent variable data augmentation technique.\n\n### Data / Model Specification\n\nThe probability of state `n`'s membership in idiosyncratic cluster `k` (`h_{nk}=1`) is modeled via a logistic function of its characteristics `\\mathbf{x}_n` and a vector of coefficients `\\boldsymbol{\\beta}_k`:\n  \np(h_{n k}=1 | \\mathbf{x}_n, \\boldsymbol{\\beta}_k) = \\frac{\\exp(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k})}{1+\\exp(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k})} \\quad \\text{(Eq. (1))}\n \nTo facilitate Bayesian estimation, this binary outcome is represented as the sign of an underlying continuous latent variable `\\xi_{nk}`:\n  \nh_{n k}= \\begin{cases} 1 & \\text{if } \\xi_{n k}>0 \\\\ 0 & \\text{otherwise } \\end{cases} \\quad \\text{(Eq. (2))}\n \nThe latent variable `\\xi_{nk}` is assumed to follow a logistic distribution with mean `\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k}` and unit scale parameter. Its cumulative distribution function (CDF) is:\n  \n\\operatorname*{Pr}(\\xi_{n k}\\leq z)=\\frac{1}{1+\\exp{(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k}-z)}} \\quad \\text{(Eq. (3))}\n \nThis structure is computationally convenient because, with further augmentation, `\\xi_{nk}` can be treated as conditionally normal, `\\xi_{nk} | ... \\sim N(\\mathbf{x}_n'\\boldsymbol{\\beta}_k, \\lambda_{nk})`, which simplifies the estimation of `\\boldsymbol{\\beta}_k`.\n\n### Question\n\nBased on the model specification, select all of the following statements that are mathematically or econometrically correct.", "Answer": ["A", "B"], "qid": "395", "question": "### Background\n\n**Research Question.** This problem investigates the model's approach to endogenously defining regional recession clusters based on observable state characteristics and the Bayesian statistical techniques used to estimate this relationship.\n\n**Setting / Institutional Environment.** The model posits that the probability of a state `n` belonging to an idiosyncratic recession cluster `k` is systematically related to a vector of its time-invariant economic characteristics. This relationship is estimated using a Bayesian Gibbs sampler, which is facilitated by a latent variable data augmentation technique.\n\n### Data / Model Specification\n\nThe probability of state `n`'s membership in idiosyncratic cluster `k` (`h_{nk}=1`) is modeled via a logistic function of its characteristics `\\mathbf{x}_n` and a vector of coefficients `\\boldsymbol{\\beta}_k`:\n  \np(h_{n k}=1 | \\mathbf{x}_n, \\boldsymbol{\\beta}_k) = \\frac{\\exp(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k})}{1+\\exp(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k})} \\quad \\text{(Eq. (1))}\n \nTo facilitate Bayesian estimation, this binary outcome is represented as the sign of an underlying continuous latent variable `\\xi_{nk}`:\n  \nh_{n k}= \\begin{cases} 1 & \\text{if } \\xi_{n k}>0 \\\\ 0 & \\text{otherwise } \\end{cases} \\quad \\text{(Eq. (2))}\n \nThe latent variable `\\xi_{nk}` is assumed to follow a logistic distribution with mean `\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k}` and unit scale parameter. Its cumulative distribution function (CDF) is:\n  \n\\operatorname*{Pr}(\\xi_{n k}\\leq z)=\\frac{1}{1+\\exp{(\\mathbf{x}_{n}^{\\prime}\\boldsymbol{\\beta}_{k}-z)}} \\quad \\text{(Eq. (3))}\n \nThis structure is computationally convenient because, with further augmentation, `\\xi_{nk}` can be treated as conditionally normal, `\\xi_{nk} | ... \\sim N(\\mathbf{x}_n'\\boldsymbol{\\beta}_k, \\lambda_{nk})`, which simplifies the estimation of `\\boldsymbol{\\beta}_k`.\n\n### Question\n\nBased on the model specification, select all of the following statements that are mathematically or econometrically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 396, "Question": "### Background\n\n**Research Question.** This problem examines the paper's core econometric framework for modeling regional business cycles, focusing on the justification for its specific structure and the critical evaluation of its key identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis begins with an ideal but computationally infeasible model where each of the `N` states can be in one of two regimes (expansion or recession), leading to `2^N` possible aggregate states. To make the problem tractable, the model simplifies this state space by assuming that recessions manifest in a small number of `K` distinct \"clusters.\"\n\n### Data / Model Specification\n\nThe baseline model for the `N x 1` vector of state employment growth rates `\\mathbf{y}_t` is:\n  \n\\mathbf{y}_{t}={\\boldsymbol{\\upmu}}_{0}+{\\boldsymbol{\\upmu}}_{1}\\odot\\mathbf{s}_{t}+\\varepsilon_{t}, \\quad \\varepsilon_{t}\\sim N(0,\\Omega) \\quad \\text{(Eq. (1))}\n \nwhere `\\mathbf{s}_t` is an `N x 1` vector of binary recession indicators, `\\boldsymbol{\\mu}_0` is the vector of expansionary growth rates, `\\boldsymbol{\\mu}_1` is the vector of recessionary growth rate differentials, and `\\odot` is the element-wise product.\n\nThis model is simplified by introducing a scalar aggregate state indicator `z_t \\in \\{1, ..., K\\}`. The distribution of `\\mathbf{y}_t` is then conditional on this state:\n  \n\\mathbf{y}_{t}|z_{t}=k \\sim N(\\mathbf{m}_{k},\\Omega)\n \nwhere the mean vector `\\mathbf{m}_k` is determined by the cluster membership vector `\\mathbf{h}_k` (an `N x 1` vector of 0s and 1s indicating which states are in cluster `k`):\n  \n\\mathbf{m}_{k}=\\boldsymbol{\\upmu}_{0}+\\boldsymbol{\\upmu}_{1}\\odot\\mathbf{h}_{k} \\quad \\text{(Eq. (2))}\n \nTwo special clusters are defined: national expansion (`\\mathbf{h}_K` is a vector of all 0s) and national recession (`\\mathbf{h}_{K-1}` is a vector of all 1s). The model makes two critical simplifying assumptions: (1) the covariance matrix `\\Omega` is diagonal, and (2) only one cluster `z_t` can be active at a time.\n\n### Question\n\nThe model employs several simplifying assumptions to achieve computational tractability. Based on the provided specification, select all statements that are **incorrect** descriptions of the model's structure or assumptions.", "Answer": ["A", "C"], "qid": "396", "question": "### Background\n\n**Research Question.** This problem examines the paper's core econometric framework for modeling regional business cycles, focusing on the justification for its specific structure and the critical evaluation of its key identifying assumptions.\n\n**Setting / Institutional Environment.** The analysis begins with an ideal but computationally infeasible model where each of the `N` states can be in one of two regimes (expansion or recession), leading to `2^N` possible aggregate states. To make the problem tractable, the model simplifies this state space by assuming that recessions manifest in a small number of `K` distinct \"clusters.\"\n\n### Data / Model Specification\n\nThe baseline model for the `N x 1` vector of state employment growth rates `\\mathbf{y}_t` is:\n  \n\\mathbf{y}_{t}={\\boldsymbol{\\upmu}}_{0}+{\\boldsymbol{\\upmu}}_{1}\\odot\\mathbf{s}_{t}+\\varepsilon_{t}, \\quad \\varepsilon_{t}\\sim N(0,\\Omega) \\quad \\text{(Eq. (1))}\n \nwhere `\\mathbf{s}_t` is an `N x 1` vector of binary recession indicators, `\\boldsymbol{\\mu}_0` is the vector of expansionary growth rates, `\\boldsymbol{\\mu}_1` is the vector of recessionary growth rate differentials, and `\\odot` is the element-wise product.\n\nThis model is simplified by introducing a scalar aggregate state indicator `z_t \\in \\{1, ..., K\\}`. The distribution of `\\mathbf{y}_t` is then conditional on this state:\n  \n\\mathbf{y}_{t}|z_{t}=k \\sim N(\\mathbf{m}_{k},\\Omega)\n \nwhere the mean vector `\\mathbf{m}_k` is determined by the cluster membership vector `\\mathbf{h}_k` (an `N x 1` vector of 0s and 1s indicating which states are in cluster `k`):\n  \n\\mathbf{m}_{k}=\\boldsymbol{\\upmu}_{0}+\\boldsymbol{\\upmu}_{1}\\odot\\mathbf{h}_{k} \\quad \\text{(Eq. (2))}\n \nTwo special clusters are defined: national expansion (`\\mathbf{h}_K` is a vector of all 0s) and national recession (`\\mathbf{h}_{K-1}` is a vector of all 1s). The model makes two critical simplifying assumptions: (1) the covariance matrix `\\Omega` is diagonal, and (2) only one cluster `z_t` can be active at a time.\n\n### Question\n\nThe model employs several simplifying assumptions to achieve computational tractability. Based on the provided specification, select all statements that are **incorrect** descriptions of the model's structure or assumptions.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 397, "Question": "### Background\n\n**Research Question.** This problem compares the statistical efficiency of three common estimators for treatment effects in panel data settings: the post-treatment only estimator (POST), the Difference-in-Differences estimator (DiD), and the Analysis of Covariance estimator (ANCOVA).\n\n**Setting / Institutional Environment.** Consider a randomized experiment with one pre-treatment survey (`m=1`) and one post-treatment survey (`r=1`). The outcome variable has a constant cross-sectional variance `σ²` and an autocorrelation `ρ` between the pre- and post-treatment periods.\n\n---\n\n### Data / Model Specification\n\nFor the `m=1, r=1` case, the variances of the three estimators are given by:\n\n  \nVar(\\hat{\\gamma}_{POST}) = \\frac{2\\sigma^2}{n} \\quad \\text{(Eq. (1))}\n \n\n  \nVar(\\hat{\\gamma}_{DiD}) = \\frac{4\\sigma^2(1-\\rho)}{n} \\quad \\text{(Eq. (2))}\n \n\n  \nVar(\\hat{\\gamma}_{ANCOVA}) = \\frac{2\\sigma^2(1-\\rho^2)}{n} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the variance formulas provided, select all statements that are correct descriptions of estimator efficiency in a randomized experiment with one baseline (`m=1`) and one follow-up (`r=1`).", "Answer": ["A", "D"], "qid": "397", "question": "### Background\n\n**Research Question.** This problem compares the statistical efficiency of three common estimators for treatment effects in panel data settings: the post-treatment only estimator (POST), the Difference-in-Differences estimator (DiD), and the Analysis of Covariance estimator (ANCOVA).\n\n**Setting / Institutional Environment.** Consider a randomized experiment with one pre-treatment survey (`m=1`) and one post-treatment survey (`r=1`). The outcome variable has a constant cross-sectional variance `σ²` and an autocorrelation `ρ` between the pre- and post-treatment periods.\n\n---\n\n### Data / Model Specification\n\nFor the `m=1, r=1` case, the variances of the three estimators are given by:\n\n  \nVar(\\hat{\\gamma}_{POST}) = \\frac{2\\sigma^2}{n} \\quad \\text{(Eq. (1))}\n \n\n  \nVar(\\hat{\\gamma}_{DiD}) = \\frac{4\\sigma^2(1-\\rho)}{n} \\quad \\text{(Eq. (2))}\n \n\n  \nVar(\\hat{\\gamma}_{ANCOVA}) = \\frac{2\\sigma^2(1-\\rho^2)}{n} \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the variance formulas provided, select all statements that are correct descriptions of estimator efficiency in a randomized experiment with one baseline (`m=1`) and one follow-up (`r=1`).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 398, "Question": "### Background\n\n**Research Question.** This problem explores the different identifying assumptions of the Difference-in-Differences (DiD) and Analysis of Covariance (ANCOVA) estimators in non-experimental (observational) studies, where treatment is not randomly assigned.\n\n**Setting / Institutional Environment.** In non-experimental settings, the choice between DiD and ANCOVA depends on which assumption about the nature of selection into treatment is more plausible.\n\n---\n\n### Question\n\nConsider a scenario evaluating a voluntary job training program where more motivated individuals, who also have higher baseline earnings growth potential, are more likely to enroll. Select all of the following statements that are correct descriptions of the estimators' validity in this context.", "Answer": ["C", "D"], "qid": "398", "question": "### Background\n\n**Research Question.** This problem explores the different identifying assumptions of the Difference-in-Differences (DiD) and Analysis of Covariance (ANCOVA) estimators in non-experimental (observational) studies, where treatment is not randomly assigned.\n\n**Setting / Institutional Environment.** In non-experimental settings, the choice between DiD and ANCOVA depends on which assumption about the nature of selection into treatment is more plausible.\n\n---\n\n### Question\n\nConsider a scenario evaluating a voluntary job training program where more motivated individuals, who also have higher baseline earnings growth potential, are more likely to enroll. Select all of the following statements that are correct descriptions of the estimators' validity in this context.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 399, "Question": "### Background\n\n**Research Question.** This problem applies the optimal rules for allocating research resources when using the ANCOVA estimator, exploring trade-offs between the number of participants (`n`) and the number of survey waves (`m`, `r`).\n\n**Setting / Institutional Environment.** A researcher using the ANCOVA estimator must make design choices based on the outcome's autocorrelation, `ρ`.\n\n---\n\n### Data / Model Specification\n\nThe paper provides two key results for optimal design with the ANCOVA estimator:\n\n- **Result 1 (Fixed Total Waves):** Given a fixed total number of survey rounds, `T = m + r`, the optimal number of post-treatment rounds, `r*`, is given by the expression: `r* = (1 + ρ(T-1)) / (2ρ)`.\n\n- **Result 2 (Fixed Follow-up Budget):** Given a single baseline survey (`m=1`) and a fixed budget for follow-up surveys (such that `nr` is constant), the optimal number of follow-up rounds, `r*`, is the nearest integer to `1/√ρ`.\n\nConsider two projects:\n- **Project A:** Measures household income, where `ρ=0.25`.\n- **Project B:** Measures student test scores, where `ρ=0.64`.\n\n---\n\n### Question\n\nBased on the provided formulas, select all of the following design choices that are optimal.", "Answer": ["A", "D"], "qid": "399", "question": "### Background\n\n**Research Question.** This problem applies the optimal rules for allocating research resources when using the ANCOVA estimator, exploring trade-offs between the number of participants (`n`) and the number of survey waves (`m`, `r`).\n\n**Setting / Institutional Environment.** A researcher using the ANCOVA estimator must make design choices based on the outcome's autocorrelation, `ρ`.\n\n---\n\n### Data / Model Specification\n\nThe paper provides two key results for optimal design with the ANCOVA estimator:\n\n- **Result 1 (Fixed Total Waves):** Given a fixed total number of survey rounds, `T = m + r`, the optimal number of post-treatment rounds, `r*`, is given by the expression: `r* = (1 + ρ(T-1)) / (2ρ)`.\n\n- **Result 2 (Fixed Follow-up Budget):** Given a single baseline survey (`m=1`) and a fixed budget for follow-up surveys (such that `nr` is constant), the optimal number of follow-up rounds, `r*`, is the nearest integer to `1/√ρ`.\n\nConsider two projects:\n- **Project A:** Measures household income, where `ρ=0.25`.\n- **Project B:** Measures student test scores, where `ρ=0.64`.\n\n---\n\n### Question\n\nBased on the provided formulas, select all of the following design choices that are optimal.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 400, "Question": "### Background\n\n**Setting.** We analyze the paper's general model of an optimal hierarchy, where employee effort `a_t` is a continuous choice between 0 and 1. This leads to a \"loss of control\" as instructions pass down the hierarchy.\n\n---\n\n### Question\n\nWhich of the following statements are correct characterizations of the firm's optimal policy in this general model? Select all that apply.", "Answer": ["A", "B"], "qid": "400", "question": "### Background\n\n**Setting.** We analyze the paper's general model of an optimal hierarchy, where employee effort `a_t` is a continuous choice between 0 and 1. This leads to a \"loss of control\" as instructions pass down the hierarchy.\n\n---\n\n### Question\n\nWhich of the following statements are correct characterizations of the firm's optimal policy in this general model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Setting.** We analyze the paper'...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Setting.** We analyze the paper'...", "error": null}}
{"ID": 401, "Question": "### Background\n\n**Research Question.** This problem covers the theoretical and econometric foundations of the paper's structural model of time allocation, from the individual's optimization problem to the full-information estimation strategy designed to correct for sample selection.\n\n**Setting / Institutional Environment.** An individual maximizes utility by allocating their total time endowment `T` between market work (`T_M`), household production (`T_H`), and leisure (`L`). The population is divided into three groups: Group I (interior solution: `T_M > 0, T_H > 0`), Group II (market work only: `T_H = 0`), and Group III (household work only: `T_M = 0`).\n\n---\n\n### Data / Model Specification\n\nThe individual's problem is to maximize utility `U(X, L)` subject to the time constraint `L = T - T_M - T_H` and the goods constraint `X = m(T_M, Y) + h(T_H, Z) + V`, where `m(·)` and `h(·)` are market and household production functions.\n\nThe marginal products (wages) are specified as:\n\n  \n\\ln w_{i} = Y_{i}\\alpha + \\varepsilon_{1i} \\quad \\text{(Eq. (1))}\n \n\n  \n\\ln w_{h i} = Z_{i}\\beta + \\gamma T_{H i} + \\varepsilon_{2i} \\quad \\text{(Eq. (2))}\n \n\nwhere `γ < 0` implies diminishing marginal returns to household work. An interior solution (Group I) requires equating the marginal returns: `ln(w_i) = ln(w_hi)`.\n\n---\n\nSelect all statements that accurately describe the key structural assumptions of the time allocation model presented and their theoretical implications.", "Answer": ["A", "B"], "qid": "401", "question": "### Background\n\n**Research Question.** This problem covers the theoretical and econometric foundations of the paper's structural model of time allocation, from the individual's optimization problem to the full-information estimation strategy designed to correct for sample selection.\n\n**Setting / Institutional Environment.** An individual maximizes utility by allocating their total time endowment `T` between market work (`T_M`), household production (`T_H`), and leisure (`L`). The population is divided into three groups: Group I (interior solution: `T_M > 0, T_H > 0`), Group II (market work only: `T_H = 0`), and Group III (household work only: `T_M = 0`).\n\n---\n\n### Data / Model Specification\n\nThe individual's problem is to maximize utility `U(X, L)` subject to the time constraint `L = T - T_M - T_H` and the goods constraint `X = m(T_M, Y) + h(T_H, Z) + V`, where `m(·)` and `h(·)` are market and household production functions.\n\nThe marginal products (wages) are specified as:\n\n  \n\\ln w_{i} = Y_{i}\\alpha + \\varepsilon_{1i} \\quad \\text{(Eq. (1))}\n \n\n  \n\\ln w_{h i} = Z_{i}\\beta + \\gamma T_{H i} + \\varepsilon_{2i} \\quad \\text{(Eq. (2))}\n \n\nwhere `γ < 0` implies diminishing marginal returns to household work. An interior solution (Group I) requires equating the marginal returns: `ln(w_i) = ln(w_hi)`.\n\n---\n\nSelect all statements that accurately describe the key structural assumptions of the time allocation model presented and their theoretical implications.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 402, "Question": "### Background\n\n**Research Question.** This problem covers the theoretical and econometric foundations of the paper's structural model of time allocation, from the individual's optimization problem to the full-information estimation strategy designed to correct for sample selection.\n\n**Setting / Institutional Environment.** An individual maximizes utility by allocating their total time endowment `T` between market work (`T_M`), household production (`T_H`), and leisure (`L`). The population is divided into three groups: Group I (interior solution: `T_M > 0, T_H > 0`), Group II (market work only: `T_H = 0`), and Group III (household work only: `T_M = 0`).\n\n---\n\n### Data / Model Specification\n\nThe marginal products (wages) are specified as:\n\n  \n\\ln w_{i} = Y_{i}\\alpha + \\varepsilon_{1i} \\quad \\text{(Eq. (1))}\n \n\n  \n\\ln w_{h i} = Z_{i}\\beta + \\gamma T_{H i} + \\varepsilon_{2i} \\quad \\text{(Eq. (2))}\n \n\nThe Full Information Maximum Likelihood (FIML) function combines information from all three groups:\n\n  \nL = \\Pi_{\\mathrm{I}}g(\\ln w_{h},T_{H i}) \\cdot \\Pi_{\\mathrm{II}}\\operatorname*{Pr}(T_{H i}^{*}\\leq0) \\cdot \\Pi_{\\mathrm{III}}\\operatorname*{Pr}(T_{H i}^{*}\\geq T-L_i) \\quad \\text{(Eq. (3))}\n \n\nwhere `g(·,·)` is the joint density for Group I, and the probabilities correspond to the corner solutions for Group II and Group III.\n\n---\n\nWhen contrasting the paper's Full Information Maximum Likelihood (FIML) approach with a standard Tobit model for market hours, which of the following statements are correct?", "Answer": ["A", "D"], "qid": "402", "question": "### Background\n\n**Research Question.** This problem covers the theoretical and econometric foundations of the paper's structural model of time allocation, from the individual's optimization problem to the full-information estimation strategy designed to correct for sample selection.\n\n**Setting / Institutional Environment.** An individual maximizes utility by allocating their total time endowment `T` between market work (`T_M`), household production (`T_H`), and leisure (`L`). The population is divided into three groups: Group I (interior solution: `T_M > 0, T_H > 0`), Group II (market work only: `T_H = 0`), and Group III (household work only: `T_M = 0`).\n\n---\n\n### Data / Model Specification\n\nThe marginal products (wages) are specified as:\n\n  \n\\ln w_{i} = Y_{i}\\alpha + \\varepsilon_{1i} \\quad \\text{(Eq. (1))}\n \n\n  \n\\ln w_{h i} = Z_{i}\\beta + \\gamma T_{H i} + \\varepsilon_{2i} \\quad \\text{(Eq. (2))}\n \n\nThe Full Information Maximum Likelihood (FIML) function combines information from all three groups:\n\n  \nL = \\Pi_{\\mathrm{I}}g(\\ln w_{h},T_{H i}) \\cdot \\Pi_{\\mathrm{II}}\\operatorname*{Pr}(T_{H i}^{*}\\leq0) \\cdot \\Pi_{\\mathrm{III}}\\operatorname*{Pr}(T_{H i}^{*}\\geq T-L_i) \\quad \\text{(Eq. (3))}\n \n\nwhere `g(·,·)` is the joint density for Group I, and the probabilities correspond to the corner solutions for Group II and Group III.\n\n---\n\nWhen contrasting the paper's Full Information Maximum Likelihood (FIML) approach with a standard Tobit model for market hours, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 403, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical core of a dynamic general equilibrium model, focusing on how the planner's intertemporal objectives are shaped by key non-linear constraints related to resource depletion and foreign borrowing.\n\n**Setting / Institutional Environment.** The model maximizes a social welfare function for the Mexican economy over a 24-year horizon. The planner's decisions are constrained by the finite nature of oil reserves and by an upward-sloping supply curve for foreign credit, implying the country has monopsony power in international capital markets.\n\n### Data / Model Specification\n\nThe planner's objective is to maximize total social welfare. The evolution of oil reserves is governed by:\n\n  \nR_{t+1} = R_t - 4 \\times DEP_t \\quad \\text{(Eq. (1))}\n \n\nwhere `R_t` is the reserve stock and `DEP_t` is reserve depletion in period `t`. The model incorporates two critical non-linearities:\n1.  **Foreign Borrowing:** The average interest rate `r` paid on debt is an increasing function of the total debt stock, `D_t`. The total interest payment is `r(D_t) \\times D_t`.\n2.  **Oil Depletion:** The shadow price on the reserve constraint (Eq. 1), denoted `\\mu_t`, represents the \"user cost\" of the oil resource. The social discount rate is `\\delta`.\n\n### Question\n\nBased on the model's structure, select all statements that correctly describe the first-order conditions governing optimal foreign borrowing and resource extraction.", "Answer": ["C", "D"], "qid": "403", "question": "### Background\n\n**Research Question.** This problem examines the theoretical core of a dynamic general equilibrium model, focusing on how the planner's intertemporal objectives are shaped by key non-linear constraints related to resource depletion and foreign borrowing.\n\n**Setting / Institutional Environment.** The model maximizes a social welfare function for the Mexican economy over a 24-year horizon. The planner's decisions are constrained by the finite nature of oil reserves and by an upward-sloping supply curve for foreign credit, implying the country has monopsony power in international capital markets.\n\n### Data / Model Specification\n\nThe planner's objective is to maximize total social welfare. The evolution of oil reserves is governed by:\n\n  \nR_{t+1} = R_t - 4 \\times DEP_t \\quad \\text{(Eq. (1))}\n \n\nwhere `R_t` is the reserve stock and `DEP_t` is reserve depletion in period `t`. The model incorporates two critical non-linearities:\n1.  **Foreign Borrowing:** The average interest rate `r` paid on debt is an increasing function of the total debt stock, `D_t`. The total interest payment is `r(D_t) \\times D_t`.\n2.  **Oil Depletion:** The shadow price on the reserve constraint (Eq. 1), denoted `\\mu_t`, represents the \"user cost\" of the oil resource. The social discount rate is `\\delta`.\n\n### Question\n\nBased on the model's structure, select all statements that correctly describe the first-order conditions governing optimal foreign borrowing and resource extraction.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 404, "Question": "### Background\n\n**Research Question.** This problem investigates an economy's capacity to adjust to simultaneous adverse external shocks and policy constraints, exploring the conditions under which a feasible macroeconomic path may cease to exist.\n\n**Setting / Institutional Environment.** The analysis considers a simulation where the Mexican economy faces two simultaneous shocks: (1) a reduction in world oil prices, and (2) the imposition of a \"debt-service constraint\" by international lenders. This constraint requires that interest payments on foreign debt not exceed a specified, and decreasing, fraction of total export revenues. Under these conditions, the model is unable to find a feasible solution.\n\n### Data / Model Specification\n\nThe debt-service constraint imposed on the economy is:\n\n  \n\\frac{\\text{Interest Payments}_t}{\\text{Total Export Revenues}_t} \\leq k_t \\quad \\text{(Eq. (1))}\n \n\nwhere `k_t` is the maximum allowable ratio in period `t`.\n\nThe model's structure includes several features that limit its flexibility:\n- Non-oil exports are projected exogenously.\n- Minimum per capita consumption levels are required for all goods.\n- The growth of government expenditure is set exogenously.\n- The model allows for discretionary 'competitive' imports of agriculture, manufacturing, and refined petroleum products.\n\n### Question\n\nSelect all statements that are **INCORRECT** explanations for why the model fails to find a feasible solution under the combined shock of lower oil prices and the new debt-service constraint.", "Answer": ["A", "D"], "qid": "404", "question": "### Background\n\n**Research Question.** This problem investigates an economy's capacity to adjust to simultaneous adverse external shocks and policy constraints, exploring the conditions under which a feasible macroeconomic path may cease to exist.\n\n**Setting / Institutional Environment.** The analysis considers a simulation where the Mexican economy faces two simultaneous shocks: (1) a reduction in world oil prices, and (2) the imposition of a \"debt-service constraint\" by international lenders. This constraint requires that interest payments on foreign debt not exceed a specified, and decreasing, fraction of total export revenues. Under these conditions, the model is unable to find a feasible solution.\n\n### Data / Model Specification\n\nThe debt-service constraint imposed on the economy is:\n\n  \n\\frac{\\text{Interest Payments}_t}{\\text{Total Export Revenues}_t} \\leq k_t \\quad \\text{(Eq. (1))}\n \n\nwhere `k_t` is the maximum allowable ratio in period `t`.\n\nThe model's structure includes several features that limit its flexibility:\n- Non-oil exports are projected exogenously.\n- Minimum per capita consumption levels are required for all goods.\n- The growth of government expenditure is set exogenously.\n- The model allows for discretionary 'competitive' imports of agriculture, manufacturing, and refined petroleum products.\n\n### Question\n\nSelect all statements that are **INCORRECT** explanations for why the model fails to find a feasible solution under the combined shock of lower oil prices and the new debt-service constraint.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 405, "Question": "### Background\n\n**Research Question.** This problem addresses the fundamental challenge of separately identifying agents' preferences, beliefs, and technological constraints in a structural model of decision-making under uncertainty.\n\n**Setting / Institutional Environment.** In a model of West African farmers, observed labor decisions are the result of three distinct, unobservable forces: (1) the household's preferences over leisure and consumption, (2) their beliefs about the riskiness of the agricultural environment, and (3) the physical production technology. Without further assumptions, these forces can be observationally equivalent, creating an identification problem.\n\n### The Question\n\nWhich of the following scenarios represent valid identification challenges where two different underlying explanations could produce the same observed behavior? Select all that apply.", "Answer": ["A", "C"], "qid": "405", "question": "### Background\n\n**Research Question.** This problem addresses the fundamental challenge of separately identifying agents' preferences, beliefs, and technological constraints in a structural model of decision-making under uncertainty.\n\n**Setting / Institutional Environment.** In a model of West African farmers, observed labor decisions are the result of three distinct, unobservable forces: (1) the household's preferences over leisure and consumption, (2) their beliefs about the riskiness of the agricultural environment, and (3) the physical production technology. Without further assumptions, these forces can be observationally equivalent, creating an identification problem.\n\n### The Question\n\nWhich of the following scenarios represent valid identification challenges where two different underlying explanations could produce the same observed behavior? Select all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 406, "Question": "### Background\n\n**Research Question.** This problem examines the microfoundations and econometric specification of the structural model used to estimate water demand under non-linear prices.\n\n**Setting / Institutional Environment.** A household's choice of water consumption `w` is modeled by maximizing a utility function subject to a piecewise-linear budget constraint. The model must account for observed characteristics `Z`, unobserved taste heterogeneity `η`, and random optimization errors `ε`.\n\n---\n\n### Data / Model Specification\n\nThe consumer's direct utility function over water `w` and a composite good `x` is:\n\n  \nU(w,x) = \\frac{\\gamma w+\\alpha}{\\gamma^{2}}\\exp\\Biggl(\\gamma\\frac{\\gamma x-w+Z\\delta+\\eta}{\\gamma w+\\alpha}\\Biggr) \\quad \\text{(Eq. 1)}\n \n\nGiven a linear budget constraint with price `P` and income `Y`, this utility function gives rise to the Marshallian demand function for water:\n\n  \n\\widetilde{w}(P,Y) = Z\\delta + \\alpha P + \\gamma Y + \\eta \\quad \\text{(Eq. 2)}\n \n\nThe econometric model for observed consumption `w_it` incorporates two distinct error terms:\n\n  \nw_{it} = w^*(P(\\cdot)) + \\varepsilon_{it} = \\begin{cases} Z_{it}\\delta + \\alpha P_{it} + \\gamma Y_{it} + \\eta_{it} + \\varepsilon_{it} & \\text{if the optimum is on a segment} \\\\ \\bar{w}_{k} + \\varepsilon_{it} & \\text{if the optimum is at kink } k \\end{cases} \\quad \\text{(Eq. 3)}\n \n\n---\n\n### Question\n\nBased on the model specification, select all of the following statements that are correct.", "Answer": ["C", "D"], "qid": "406", "question": "### Background\n\n**Research Question.** This problem examines the microfoundations and econometric specification of the structural model used to estimate water demand under non-linear prices.\n\n**Setting / Institutional Environment.** A household's choice of water consumption `w` is modeled by maximizing a utility function subject to a piecewise-linear budget constraint. The model must account for observed characteristics `Z`, unobserved taste heterogeneity `η`, and random optimization errors `ε`.\n\n---\n\n### Data / Model Specification\n\nThe consumer's direct utility function over water `w` and a composite good `x` is:\n\n  \nU(w,x) = \\frac{\\gamma w+\\alpha}{\\gamma^{2}}\\exp\\Biggl(\\gamma\\frac{\\gamma x-w+Z\\delta+\\eta}{\\gamma w+\\alpha}\\Biggr) \\quad \\text{(Eq. 1)}\n \n\nGiven a linear budget constraint with price `P` and income `Y`, this utility function gives rise to the Marshallian demand function for water:\n\n  \n\\widetilde{w}(P,Y) = Z\\delta + \\alpha P + \\gamma Y + \\eta \\quad \\text{(Eq. 2)}\n \n\nThe econometric model for observed consumption `w_it` incorporates two distinct error terms:\n\n  \nw_{it} = w^*(P(\\cdot)) + \\varepsilon_{it} = \\begin{cases} Z_{it}\\delta + \\alpha P_{it} + \\gamma Y_{it} + \\eta_{it} + \\varepsilon_{it} & \\text{if the optimum is on a segment} \\\\ \\bar{w}_{k} + \\varepsilon_{it} & \\text{if the optimum is at kink } k \\end{cases} \\quad \\text{(Eq. 3)}\n \n\n---\n\n### Question\n\nBased on the model specification, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 407, "Question": "### Background\n\nThis problem explores the axiomatic characterization of the Polluter-Pays (PP) principle in an economy where pollution causes increasing marginal damages. This convexity introduces a “negative group externality”: one agent's pollution raises the marginal damage of another's, complicating the assignment of responsibility and requiring a more sophisticated fairness framework than the constant-damage case.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Agent `i`'s emissions `e_i` generate private benefits `b_i(e_i)`. These emissions are translated into a pollution concentration `p_j` at each receptor `j` via transfer coefficients `a_{ij}`, such that `p_j = \\sum_{l \\in S_j} a_{lj} e_l`, where `S_j` is the set of sources for receptor `j`. This pollution causes damages `d_j(p_j)` that are increasing and strictly convex (`d_j' > 0`, `d_j'' > 0`), with `d_j(0)=0`.\n\nTo uniquely characterize the Polluter-Pays (PP) distribution rule under these conditions, three fairness axioms are required:\n1.  **Non-negativity:** `\\phi_i(a) \\ge 0` for all agents `i`.\n2.  **Responsibility for Pollution Impact (RPI):** `\\phi_i(a') - \\phi_i(a) = W(a') - W(a)` for a change in agent `i`'s own pollution impact.\n3.  **Single-Polluter Upper Bounds (SPUB):**\n      \n    \\phi_i(a) \\le \\max_{e_i \\ge 0} \\left[ b_i(e_i) - \\sum_{j \\in R_i} d_j(a_{ij}e_i) \right]\n     \n\n### Question\n\nSelect all statements that correctly describe the challenges and axiomatic solutions associated with increasing marginal damages.", "Answer": ["A", "B"], "qid": "407", "question": "### Background\n\nThis problem explores the axiomatic characterization of the Polluter-Pays (PP) principle in an economy where pollution causes increasing marginal damages. This convexity introduces a “negative group externality”: one agent's pollution raises the marginal damage of another's, complicating the assignment of responsibility and requiring a more sophisticated fairness framework than the constant-damage case.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Agent `i`'s emissions `e_i` generate private benefits `b_i(e_i)`. These emissions are translated into a pollution concentration `p_j` at each receptor `j` via transfer coefficients `a_{ij}`, such that `p_j = \\sum_{l \\in S_j} a_{lj} e_l`, where `S_j` is the set of sources for receptor `j`. This pollution causes damages `d_j(p_j)` that are increasing and strictly convex (`d_j' > 0`, `d_j'' > 0`), with `d_j(0)=0`.\n\nTo uniquely characterize the Polluter-Pays (PP) distribution rule under these conditions, three fairness axioms are required:\n1.  **Non-negativity:** `\\phi_i(a) \\ge 0` for all agents `i`.\n2.  **Responsibility for Pollution Impact (RPI):** `\\phi_i(a') - \\phi_i(a) = W(a') - W(a)` for a change in agent `i`'s own pollution impact.\n3.  **Single-Polluter Upper Bounds (SPUB):**\n      \n    \\phi_i(a) \\le \\max_{e_i \\ge 0} \\left[ b_i(e_i) - \\sum_{j \\in R_i} d_j(a_{ij}e_i) \right]\n     \n\n### Question\n\nSelect all statements that correctly describe the challenges and axiomatic solutions associated with increasing marginal damages.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem explores the axiomati...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem explores the axiomati...", "error": null}}
{"ID": 408, "Question": "### Background\n\nThis problem explores the axiomatic characterization of the Polluter-Pays (PP) principle in an economy where pollution causes increasing marginal damages. This convexity introduces a “negative group externality”: one agent's pollution raises the marginal damage of another's, complicating the assignment of responsibility and requiring a more sophisticated fairness framework than the constant-damage case.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Total social welfare is `W(a) = \\sum_{k \\in N} [b_k(e_k^*) - d_k(p_k^*)]`, where `e^*` is the efficient emission plan. The generalized Polluter-Pays (PP) welfare distribution `\\phi^{PP}` that satisfies the required fairness axioms is defined by an agent's incremental contribution to total welfare:\n  \n\\phi_i^{PP}(a) = W(a) - W(a^{0i}) \\quad \text{(Eq. 1)}\n \nwhere `W(a^{0i})` is the total welfare in a counterfactual economy where agent `i` does not emit (`e_i=0`), but may still suffer damages. The Single-Polluter Upper Bounds (SPUB) axiom is defined as:\n  \n\\phi_i(a) \\le \\max_{e_i \\ge 0} \\left[ b_i(e_i) - \\sum_{j \\in R_i} d_j(a_{ij}e_i) \right] \\quad \text{(Eq. 2)}\n \n\n### Question\n\nIn the model with increasing marginal damages, select all statements that correctly characterize the properties and outcomes of the generalized Polluter-Pays (PP) distribution rule.", "Answer": ["B", "C"], "qid": "408", "question": "### Background\n\nThis problem explores the axiomatic characterization of the Polluter-Pays (PP) principle in an economy where pollution causes increasing marginal damages. This convexity introduces a “negative group externality”: one agent's pollution raises the marginal damage of another's, complicating the assignment of responsibility and requiring a more sophisticated fairness framework than the constant-damage case.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Total social welfare is `W(a) = \\sum_{k \\in N} [b_k(e_k^*) - d_k(p_k^*)]`, where `e^*` is the efficient emission plan. The generalized Polluter-Pays (PP) welfare distribution `\\phi^{PP}` that satisfies the required fairness axioms is defined by an agent's incremental contribution to total welfare:\n  \n\\phi_i^{PP}(a) = W(a) - W(a^{0i}) \\quad \text{(Eq. 1)}\n \nwhere `W(a^{0i})` is the total welfare in a counterfactual economy where agent `i` does not emit (`e_i=0`), but may still suffer damages. The Single-Polluter Upper Bounds (SPUB) axiom is defined as:\n  \n\\phi_i(a) \\le \\max_{e_i \\ge 0} \\left[ b_i(e_i) - \\sum_{j \\in R_i} d_j(a_{ij}e_i) \right] \\quad \text{(Eq. 2)}\n \n\n### Question\n\nIn the model with increasing marginal damages, select all statements that correctly characterize the properties and outcomes of the generalized Polluter-Pays (PP) distribution rule.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem explores the axiomati...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem explores the axiomati...", "error": null}}
{"ID": 409, "Question": "### Background\n\nThis problem provides a comprehensive analysis of the baseline pollution model with constant marginal damages. It explores the fundamental inefficiency of the unregulated (laissez-faire) outcome and the structure of the Polluter-Pays (PP) scheme designed to correct this inefficiency.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Each agent `i` chooses an emission level `e_i \\ge 0` which provides a strictly concave benefit `b_i(e_i)`. Emissions from agent `i` cause constant marginal damage `a_{ij} \\ge 0` to each agent `j`.\n\n- The **laissez-faire** equilibrium emission level, `e_i^{lf}`, results from each agent maximizing their own welfare, `b_i(e_i) - \\sum_{j \\in S_i} a_{ji}e_j`.\n- The **socially optimal** emission level, `e_i^*`, is the level that maximizes total social welfare, which implies the first-order condition `b_i'(e_i^*) = \\sum_{j \\in R_i} a_{ij}`.\n- The **Polluter-Pays (PP) scheme** introduces a set of transfers `t_i^{PP}(e)` such that an agent's effective objective function becomes `b_i(e_i) - \\sum_{j \\in R_i} a_{ij}e_i`.\n\n### Question\n\nIn the baseline model with constant marginal damages, select all statements that correctly characterize the laissez-faire equilibrium and the impact of the Polluter-Pays (PP) scheme.", "Answer": ["A", "D"], "qid": "409", "question": "### Background\n\nThis problem provides a comprehensive analysis of the baseline pollution model with constant marginal damages. It explores the fundamental inefficiency of the unregulated (laissez-faire) outcome and the structure of the Polluter-Pays (PP) scheme designed to correct this inefficiency.\n\n### Data / Model Specification\n\nConsider an economy with a set of agents `N`. Each agent `i` chooses an emission level `e_i \\ge 0` which provides a strictly concave benefit `b_i(e_i)`. Emissions from agent `i` cause constant marginal damage `a_{ij} \\ge 0` to each agent `j`.\n\n- The **laissez-faire** equilibrium emission level, `e_i^{lf}`, results from each agent maximizing their own welfare, `b_i(e_i) - \\sum_{j \\in S_i} a_{ji}e_j`.\n- The **socially optimal** emission level, `e_i^*`, is the level that maximizes total social welfare, which implies the first-order condition `b_i'(e_i^*) = \\sum_{j \\in R_i} a_{ij}`.\n- The **Polluter-Pays (PP) scheme** introduces a set of transfers `t_i^{PP}(e)` such that an agent's effective objective function becomes `b_i(e_i) - \\sum_{j \\in R_i} a_{ij}e_i`.\n\n### Question\n\nIn the baseline model with constant marginal damages, select all statements that correctly characterize the laissez-faire equilibrium and the impact of the Polluter-Pays (PP) scheme.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem provides a comprehens...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem provides a comprehens...", "error": null}}
{"ID": 410, "Question": "### Background\n\nThis problem critically compares the Polluter-Pays (PP) scheme with the canonical Vickrey-Clarke-Groves (VCG) mechanism, specifically its pivotal implementation. While both schemes can achieve efficiency, they differ starkly in their distributional outcomes and fairness properties, particularly concerning agents who are pure victims of pollution.\n\n### Data / Model Specification\n\nConsider a pollution economy where total social welfare is `W(a)`. The efficient emission plan is `e^*`.\n\n- The **Polluter-Pays (PP)** welfare distribution for agent `i` is:\n    \n  \\phi_i^{PP}(a) = W(a) - W(a^{0i}) \\quad \text{(Eq. 1)}\n   \n  where `W(a^{0i})` is the maximum welfare in an economy where agent `i`'s emissions are fixed at zero (`e_i=0`), but their damage function `d_i` is still part of the problem.\n\n- The **Pivotal Scheme** welfare distribution for agent `i` is:\n    \n  \\phi_i^{piv}(a) = W(a) - W(a^{-i}) \\quad \text{(Eq. 2)}\n   \n  where `W(a^{-i})` is the maximum welfare in an economy without agent `i` entirely (i.e., without their benefit function `b_i` *and* their damage function `d_i`).\n\n### Question\n\nConsider a simple economy with one polluter-only agent and one victim-only agent. Select all statements that correctly compare the welfare outcomes for the victim under the Polluter-Pays (PP) scheme and the pivotal VCG scheme.", "Answer": ["C", "D"], "qid": "410", "question": "### Background\n\nThis problem critically compares the Polluter-Pays (PP) scheme with the canonical Vickrey-Clarke-Groves (VCG) mechanism, specifically its pivotal implementation. While both schemes can achieve efficiency, they differ starkly in their distributional outcomes and fairness properties, particularly concerning agents who are pure victims of pollution.\n\n### Data / Model Specification\n\nConsider a pollution economy where total social welfare is `W(a)`. The efficient emission plan is `e^*`.\n\n- The **Polluter-Pays (PP)** welfare distribution for agent `i` is:\n    \n  \\phi_i^{PP}(a) = W(a) - W(a^{0i}) \\quad \text{(Eq. 1)}\n   \n  where `W(a^{0i})` is the maximum welfare in an economy where agent `i`'s emissions are fixed at zero (`e_i=0`), but their damage function `d_i` is still part of the problem.\n\n- The **Pivotal Scheme** welfare distribution for agent `i` is:\n    \n  \\phi_i^{piv}(a) = W(a) - W(a^{-i}) \\quad \text{(Eq. 2)}\n   \n  where `W(a^{-i})` is the maximum welfare in an economy without agent `i` entirely (i.e., without their benefit function `b_i` *and* their damage function `d_i`).\n\n### Question\n\nConsider a simple economy with one polluter-only agent and one victim-only agent. Select all statements that correctly compare the welfare outcomes for the victim under the Polluter-Pays (PP) scheme and the pivotal VCG scheme.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nThis problem critically compares t...", "model_answer": "This is a detailed response to the question about ### Background\n\nThis problem critically compares t...", "error": null}}
{"ID": 411, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of a union's wage-setting policy for heterogeneous workers. It examines how a union's objectives and the institutional structure of bargaining shape the occupational wage structure.\n\n**Setting / Institutional Environment.** A single, collusive union represents two groups of production workers: skilled (`L₁`) and not-skilled (`L₂`). The union sets wage rates (`w₁`, `w₂`) to maximize the joint economic rents of its members, defined as wages earned above their constant alternative supply prices (`w̄₁`, `w̄₂`).\n\n**Variables & Parameters.**\n- `Lᵢ`: Services of labor group `i`.\n- `wᵢ`, `w̄ᵢ`: Union wage and alternative supply price for group `i`.\n- `mᵢ = (wᵢ - w̄ᵢ)/wᵢ`: Proportional wage markup for group `i`.\n- `η`: Elasticity of demand for the final product.\n- `kᵢ`: Share of total cost for factor `i`.\n- `σᵢⱼ`: Partial elasticity of substitution between factors `i` and `j`.\n\n---\n\n### Data / Model Specification\n\nA single union maximizes the sum of rents for both groups:\n\n  \nV = (w_{1}-\\bar{w}_{1})L_{1} + (w_{2}-\\bar{w}_{2})L_{2} \\quad \\text{(Eq. (1))}\n \n\nThe key result for the optimal relative wage markup, derived for a three-factor model including a non-unionized factor `L₃` (e.g., capital), is:\n\n  \n\\frac{m_1}{m_2} = \\frac{(1-k_{3})\\sigma_{12}+k_{3}\\sigma_{23}}{(1-k_{3})\\sigma_{12}+k_{3}\\sigma_{13}} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the collusive union model and the central assumption that skilled workers are less substitutable with a non-unionized factor like capital than are not-skilled workers (i.e., `σ₁₃ < σ₂₃`), select all of the following statements that are correct implications of the model.", "Answer": ["A", "C"], "qid": "411", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of a union's wage-setting policy for heterogeneous workers. It examines how a union's objectives and the institutional structure of bargaining shape the occupational wage structure.\n\n**Setting / Institutional Environment.** A single, collusive union represents two groups of production workers: skilled (`L₁`) and not-skilled (`L₂`). The union sets wage rates (`w₁`, `w₂`) to maximize the joint economic rents of its members, defined as wages earned above their constant alternative supply prices (`w̄₁`, `w̄₂`).\n\n**Variables & Parameters.**\n- `Lᵢ`: Services of labor group `i`.\n- `wᵢ`, `w̄ᵢ`: Union wage and alternative supply price for group `i`.\n- `mᵢ = (wᵢ - w̄ᵢ)/wᵢ`: Proportional wage markup for group `i`.\n- `η`: Elasticity of demand for the final product.\n- `kᵢ`: Share of total cost for factor `i`.\n- `σᵢⱼ`: Partial elasticity of substitution between factors `i` and `j`.\n\n---\n\n### Data / Model Specification\n\nA single union maximizes the sum of rents for both groups:\n\n  \nV = (w_{1}-\\bar{w}_{1})L_{1} + (w_{2}-\\bar{w}_{2})L_{2} \\quad \\text{(Eq. (1))}\n \n\nThe key result for the optimal relative wage markup, derived for a three-factor model including a non-unionized factor `L₃` (e.g., capital), is:\n\n  \n\\frac{m_1}{m_2} = \\frac{(1-k_{3})\\sigma_{12}+k_{3}\\sigma_{23}}{(1-k_{3})\\sigma_{12}+k_{3}\\sigma_{13}} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the collusive union model and the central assumption that skilled workers are less substitutable with a non-unionized factor like capital than are not-skilled workers (i.e., `σ₁₃ < σ₂₃`), select all of the following statements that are correct implications of the model.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 412, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of a union's wage-setting policy for heterogeneous workers. It examines how a union's objectives and the institutional structure of bargaining (a single monolithic union vs. multiple independent unions) shape the occupational wage structure.\n\n**Setting / Institutional Environment.** A union (or unions) represents two groups of production workers: skilled (`L₁`) and not-skilled (`L₂`). The union sets wage rates (`w₁`, `w₂`) to maximize the economic rents of its members.\n\n**Variables & Parameters.**\n- `mᵢ = (wᵢ - w̄ᵢ)/wᵢ`: Proportional wage markup for group `i`.\n- `ηᵢⱼ = (wⱼ/Lᵢ)(∂Lᵢ/∂wⱼ)`: Elasticity of demand for labor `i` with respect to wage `j`.\n- `η`: Elasticity of demand for the final product.\n\n---\n\n### Data / Model Specification\n\n**Case 1: Single Collusive Union.** A single union maximizes the sum of rents for both groups. The solution for the relative wage markup depends on own- and cross-price elasticities and is independent of scale effects from the final product demand elasticity, `η`.\n\n**Case 2: Multiple Non-Cooperative Unions.** Two separate unions non-cooperatively (in a Cournot fashion) set wages to maximize their own group's rent. The equilibrium relative wage markup is:\n\n  \n\\frac{m_1}{m_2} = \\frac{\\eta_{22}}{\\eta_{11}} \\quad \\text{(Eq. (1))}\n \n\nwhere `ηᵢᵢ` is the own-wage elasticity of demand for group `i`, which includes scale effects related to `η`.\n\n---\n\n### Question\n\nSelect all of the following statements that correctly contrast the assumptions or outcomes of the collusive (Case 1) and non-cooperative Cournot (Case 2) models of union bargaining.", "Answer": ["A", "B"], "qid": "412", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundations of a union's wage-setting policy for heterogeneous workers. It examines how a union's objectives and the institutional structure of bargaining (a single monolithic union vs. multiple independent unions) shape the occupational wage structure.\n\n**Setting / Institutional Environment.** A union (or unions) represents two groups of production workers: skilled (`L₁`) and not-skilled (`L₂`). The union sets wage rates (`w₁`, `w₂`) to maximize the economic rents of its members.\n\n**Variables & Parameters.**\n- `mᵢ = (wᵢ - w̄ᵢ)/wᵢ`: Proportional wage markup for group `i`.\n- `ηᵢⱼ = (wⱼ/Lᵢ)(∂Lᵢ/∂wⱼ)`: Elasticity of demand for labor `i` with respect to wage `j`.\n- `η`: Elasticity of demand for the final product.\n\n---\n\n### Data / Model Specification\n\n**Case 1: Single Collusive Union.** A single union maximizes the sum of rents for both groups. The solution for the relative wage markup depends on own- and cross-price elasticities and is independent of scale effects from the final product demand elasticity, `η`.\n\n**Case 2: Multiple Non-Cooperative Unions.** Two separate unions non-cooperatively (in a Cournot fashion) set wages to maximize their own group's rent. The equilibrium relative wage markup is:\n\n  \n\\frac{m_1}{m_2} = \\frac{\\eta_{22}}{\\eta_{11}} \\quad \\text{(Eq. (1))}\n \n\nwhere `ηᵢᵢ` is the own-wage elasticity of demand for group `i`, which includes scale effects related to `η`.\n\n---\n\n### Question\n\nSelect all of the following statements that correctly contrast the assumptions or outcomes of the collusive (Case 1) and non-cooperative Cournot (Case 2) models of union bargaining.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 413, "Question": "### Background\n\n**Research Question.** This problem analyzes the rational and strategic incentives in a tax evasion game where the probability of being audited is endogenous to a player's actions relative to others.\n\n**Setting.** A group of eight players simultaneously decides how much income to report. The model assumes agents are risk-neutral and aim to maximize their expected net income. The audit probability for each player depends on whether their reported income falls into the bottom four or top four reports within the group.\n\n---\n\n### Data / Model Specification\n\nThe expected net income for an individual with gross income `I` reporting `R < I` is given by Eq. (1):\n  \nENI = I - [p(tI + ft(I-R)) + (1-p)tR] \n \nThe net income from full, honest reporting (`R = I`) is given by Eq. (2):\n  \nNI = I - tI \n \nThe endogenous audit rule is as follows:\n- If a player's report is one of the four highest, their audit probability is `p = A_High = 35%`.\n- If a player's report is one of the four lowest, their audit probability is `p = A_Low = 65%`.\n- If all players report the same amount, `p = 50%` for all.\n\nThe experiment is parameterized with `t = 55%` and `f = 20%`.\n\n---\n\n### Question\n\nBased on the provided model and parameters, select all statements that are factually correct regarding the incentives for a risk-neutral agent, assuming a symmetric ex-ante audit probability of `p = 50%`.", "Answer": ["A", "B"], "qid": "413", "question": "### Background\n\n**Research Question.** This problem analyzes the rational and strategic incentives in a tax evasion game where the probability of being audited is endogenous to a player's actions relative to others.\n\n**Setting.** A group of eight players simultaneously decides how much income to report. The model assumes agents are risk-neutral and aim to maximize their expected net income. The audit probability for each player depends on whether their reported income falls into the bottom four or top four reports within the group.\n\n---\n\n### Data / Model Specification\n\nThe expected net income for an individual with gross income `I` reporting `R < I` is given by Eq. (1):\n  \nENI = I - [p(tI + ft(I-R)) + (1-p)tR] \n \nThe net income from full, honest reporting (`R = I`) is given by Eq. (2):\n  \nNI = I - tI \n \nThe endogenous audit rule is as follows:\n- If a player's report is one of the four highest, their audit probability is `p = A_High = 35%`.\n- If a player's report is one of the four lowest, their audit probability is `p = A_Low = 65%`.\n- If all players report the same amount, `p = 50%` for all.\n\nThe experiment is parameterized with `t = 55%` and `f = 20%`.\n\n---\n\n### Question\n\nBased on the provided model and parameters, select all statements that are factually correct regarding the incentives for a risk-neutral agent, assuming a symmetric ex-ante audit probability of `p = 50%`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 414, "Question": "### Background\n\n**Research Question.** This problem analyzes the rational and strategic incentives in a tax evasion game where the probability of being audited is endogenous to a player's actions relative to others.\n\n**Setting.** A group of eight players simultaneously decides how much income to report. The model assumes agents are risk-neutral and aim to maximize their expected net income. The audit probability for each player depends on whether their reported income falls into the bottom four or top four reports within the group.\n\n---\n\n### Data / Model Specification\n\nThe expected net income for an individual with gross income `I` reporting `R < I` is given by Eq. (1):\n  \nENI = I - [p(tI + ft(I-R)) + (1-p)tR] \n \nThe net income from full, honest reporting (`R = I`) is given by Eq. (2):\n  \nNI = I - tI \n \nThe endogenous audit rule is as follows:\n- If a player's report is one of the four highest, their audit probability is `p = A_High = 35%`.\n- If a player's report is one of the four lowest, their audit probability is `p = A_Low = 65%`.\n- If all players report the same amount, `p = 50%` for all.\n\nThe experiment is parameterized with `t = 55%` and `f = 20%`.\n\n---\n\n### Question\n\nIn the strategic game described, a Pure Strategy Nash Equilibrium (PSNE) does not exist. Select all statements that correctly explain the reasoning for this strategic instability.", "Answer": ["B", "D"], "qid": "414", "question": "### Background\n\n**Research Question.** This problem analyzes the rational and strategic incentives in a tax evasion game where the probability of being audited is endogenous to a player's actions relative to others.\n\n**Setting.** A group of eight players simultaneously decides how much income to report. The model assumes agents are risk-neutral and aim to maximize their expected net income. The audit probability for each player depends on whether their reported income falls into the bottom four or top four reports within the group.\n\n---\n\n### Data / Model Specification\n\nThe expected net income for an individual with gross income `I` reporting `R < I` is given by Eq. (1):\n  \nENI = I - [p(tI + ft(I-R)) + (1-p)tR] \n \nThe net income from full, honest reporting (`R = I`) is given by Eq. (2):\n  \nNI = I - tI \n \nThe endogenous audit rule is as follows:\n- If a player's report is one of the four highest, their audit probability is `p = A_High = 35%`.\n- If a player's report is one of the four lowest, their audit probability is `p = A_Low = 65%`.\n- If all players report the same amount, `p = 50%` for all.\n\nThe experiment is parameterized with `t = 55%` and `f = 20%`.\n\n---\n\n### Question\n\nIn the strategic game described, a Pure Strategy Nash Equilibrium (PSNE) does not exist. Select all statements that correctly explain the reasoning for this strategic instability.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 415, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical econometrics behind modeling the dynamic impact of a one-time shock (like a labor strike) on a non-stationary time series (like sports attendance).\n\n**Setting / Institutional Environment.** The analysis concerns annual time-series data, `Y_t`, which is often found to be integrated of order one, I(1). This means its first-difference, `y_t = Y_t - Y_{t-1}`, is stationary.\n\n---\n\n### Data / Model Specification\n\nOne can model the dynamic impact of a strike using the following AR(1) specification for the first-differenced series `y_t`:\n\n  \ny_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 z_t + \\varepsilon_t\n \n(Eq. 1)\n\nwhere `z_t` is a pulse dummy (`z_t = 1` for the strike year, `0` otherwise) and `|α_1| < 1`.\n\n---\n\n### Question\n\nBased on the AR(1) intervention model in Eq. (1), select all of the following statements that are mathematically or economically correct.", "Answer": ["C", "D"], "qid": "415", "question": "### Background\n\n**Research Question.** This problem explores the theoretical econometrics behind modeling the dynamic impact of a one-time shock (like a labor strike) on a non-stationary time series (like sports attendance).\n\n**Setting / Institutional Environment.** The analysis concerns annual time-series data, `Y_t`, which is often found to be integrated of order one, I(1). This means its first-difference, `y_t = Y_t - Y_{t-1}`, is stationary.\n\n---\n\n### Data / Model Specification\n\nOne can model the dynamic impact of a strike using the following AR(1) specification for the first-differenced series `y_t`:\n\n  \ny_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 z_t + \\varepsilon_t\n \n(Eq. 1)\n\nwhere `z_t` is a pulse dummy (`z_t = 1` for the strike year, `0` otherwise) and `|α_1| < 1`.\n\n---\n\n### Question\n\nBased on the AR(1) intervention model in Eq. (1), select all of the following statements that are mathematically or economically correct.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 416, "Question": "### Background\n\n**Research Question.** This problem explores alternative econometric strategies for modeling the impact of a shock on a non-stationary time series.\n\n**Setting / Institutional Environment.** Suppose that both league attendance (`Y_t`) and real per capita income (`Inc_t`) are non-stationary I(1) time series. An alternative to modeling their first-differences is to test if they share a stable, long-run equilibrium relationship—a property known as cointegration. If they are cointegrated, their relationship can be modeled using an Error Correction Model (ECM).\n\n---\n\n### Question\n\nSelect all statements that correctly describe the cointegration methodology (specifically, the Engle-Granger two-step procedure) and the interpretation of the resulting Error Correction Model (ECM).", "Answer": ["A", "C"], "qid": "416", "question": "### Background\n\n**Research Question.** This problem explores alternative econometric strategies for modeling the impact of a shock on a non-stationary time series.\n\n**Setting / Institutional Environment.** Suppose that both league attendance (`Y_t`) and real per capita income (`Inc_t`) are non-stationary I(1) time series. An alternative to modeling their first-differences is to test if they share a stable, long-run equilibrium relationship—a property known as cointegration. If they are cointegrated, their relationship can be modeled using an Error Correction Model (ECM).\n\n---\n\n### Question\n\nSelect all statements that correctly describe the cointegration methodology (specifically, the Engle-Granger two-step procedure) and the interpretation of the resulting Error Correction Model (ECM).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 417, "Question": "### Background\n\n**Research Question.** This question assesses understanding of the paper's central result: the non-monotonic relationship between entrepreneurial wealth and aggregate investment, driven by a switch from a pooling to a separating equilibrium in the credit market.\n\n**Setting / Institutional Environment.** In a credit market with adverse selection, the equilibrium regime—pooling (one contract for all) or separating (a menu of contracts)—is the one that yields higher profit to the Good (G) type entrepreneur. This choice depends on their level of wealth, `W`. The critical level of wealth where the G-type is indifferent between the two regimes is the switching point, `W*(r)`. \n\n### Data / Model Specification\n\nThe economy consists of Good (G) and Bad (B) type entrepreneurs, with success probabilities `p^G > p^B`. The average probability of success is `p̄`. When the average quality of the pool is sufficiently high (`p̄ > (α^B p^B) / α^G`), the investment levels at the switching point `W*(r)` are characterized by the following strict ordering:\n\n  \n\\bar{I}(r) > I^G(r, W^*(r)) > W^*(r) > I^B(r)\n\\quad \\text{(Eq. 1)}\n \nwhere `Ī(r)` is the investment level in the pooling equilibrium, and `I^G(r, W*)` and `I^B(r)` are the investment levels for G and B types, respectively, in the separating equilibrium that emerges just after the switch.\n\n### Question\n\nSelect all statements that correctly characterize the credit market equilibrium and the behavior of investment around the switching point `W*(r)`.\n", "Answer": ["C", "D"], "qid": "417", "question": "### Background\n\n**Research Question.** This question assesses understanding of the paper's central result: the non-monotonic relationship between entrepreneurial wealth and aggregate investment, driven by a switch from a pooling to a separating equilibrium in the credit market.\n\n**Setting / Institutional Environment.** In a credit market with adverse selection, the equilibrium regime—pooling (one contract for all) or separating (a menu of contracts)—is the one that yields higher profit to the Good (G) type entrepreneur. This choice depends on their level of wealth, `W`. The critical level of wealth where the G-type is indifferent between the two regimes is the switching point, `W*(r)`. \n\n### Data / Model Specification\n\nThe economy consists of Good (G) and Bad (B) type entrepreneurs, with success probabilities `p^G > p^B`. The average probability of success is `p̄`. When the average quality of the pool is sufficiently high (`p̄ > (α^B p^B) / α^G`), the investment levels at the switching point `W*(r)` are characterized by the following strict ordering:\n\n  \n\\bar{I}(r) > I^G(r, W^*(r)) > W^*(r) > I^B(r)\n\\quad \\text{(Eq. 1)}\n \nwhere `Ī(r)` is the investment level in the pooling equilibrium, and `I^G(r, W*)` and `I^B(r)` are the investment levels for G and B types, respectively, in the separating equilibrium that emerges just after the switch.\n\n### Question\n\nSelect all statements that correctly characterize the credit market equilibrium and the behavior of investment around the switching point `W*(r)`.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 418, "Question": "### Background\n\n**Research Question.** This question tests the ability to perform comparative statics on the model's equilibrium conditions, specifically analyzing how a technological shock affects the conditions for over-investment in a pooling equilibrium.\n\n**Setting / Institutional Environment.** In a pooling equilibrium, a single contract `(Ī, R̄, c̄)` is offered to both G-type and B-type entrepreneurs. In a separating equilibrium, the B-type receives their efficient investment `I^{B*}`.\n\n### Data / Model Specification\n\nThe investment level in a pooling equilibrium, `Ī`, is defined by the G-type's optimality condition:\n\n  \np^{G}\\alpha^{G}f'(\\bar{I}) = \\frac{p^{G}}{\\bar{p}}(1+r)\n\\quad \\text{(Eq. 1)}\n \nwhere `p̄` is the average success probability and `f(I)` is a concave production function (`f'' < 0`).\n\nThe B-type's efficient investment level, `I^{B*}`, is defined by:\n\n  \np^B \\alpha^B f'(I^{B*}) = 1+r\n\\quad \\text{(Eq. 2)}\n \n\nB-types are said to 'over-invest' in the pooling equilibrium if `Ī > I^{B*}`. This occurs if the average quality of the pool `p̄` is sufficiently high, specifically when `p̄ > (α^B p^B) / α^G`.\n\n### Question\n\nConsider a technological shock that increases the productivity of G-type projects, represented by an increase in the parameter `α^G`, while all other parameters (`p^G`, `p^B`, `α^B`, `r`, etc.) remain unchanged. Select all statements that are correct consequences of this shock.\n", "Answer": ["B", "C"], "qid": "418", "question": "### Background\n\n**Research Question.** This question tests the ability to perform comparative statics on the model's equilibrium conditions, specifically analyzing how a technological shock affects the conditions for over-investment in a pooling equilibrium.\n\n**Setting / Institutional Environment.** In a pooling equilibrium, a single contract `(Ī, R̄, c̄)` is offered to both G-type and B-type entrepreneurs. In a separating equilibrium, the B-type receives their efficient investment `I^{B*}`.\n\n### Data / Model Specification\n\nThe investment level in a pooling equilibrium, `Ī`, is defined by the G-type's optimality condition:\n\n  \np^{G}\\alpha^{G}f'(\\bar{I}) = \\frac{p^{G}}{\\bar{p}}(1+r)\n\\quad \\text{(Eq. 1)}\n \nwhere `p̄` is the average success probability and `f(I)` is a concave production function (`f'' < 0`).\n\nThe B-type's efficient investment level, `I^{B*}`, is defined by:\n\n  \np^B \\alpha^B f'(I^{B*}) = 1+r\n\\quad \\text{(Eq. 2)}\n \n\nB-types are said to 'over-invest' in the pooling equilibrium if `Ī > I^{B*}`. This occurs if the average quality of the pool `p̄` is sufficiently high, specifically when `p̄ > (α^B p^B) / α^G`.\n\n### Question\n\nConsider a technological shock that increases the productivity of G-type projects, represented by an increase in the parameter `α^G`, while all other parameters (`p^G`, `p^B`, `α^B`, `r`, etc.) remain unchanged. Select all statements that are correct consequences of this shock.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 419, "Question": "### Background\n\n**Research Question.** This question tests the understanding of the first-best (full information) allocation of credit, which serves as the efficient benchmark in the model. It requires applying the efficiency condition to a novel technological assumption.\n\n**Setting / Institutional Environment.** In a first-best world without informational asymmetries, competitive banks can offer type-specific contracts to entrepreneurs. The goal is to maximize total surplus, which allocates capital to its most productive use.\n\n### Data / Model Specification\n\nThe efficient (first-best) investment level `I^{j*}` for an entrepreneur of type `j` (where `j` is G or B) is determined by the condition that equates the expected marginal product of investment to the marginal cost of capital:\n\n  \np^j \\alpha^j f'(I^{j*}) = 1+r\n\\quad \\text{(Eq. 1)}\n \nwhere `f(I)` is a strictly concave production function (`f'' < 0`). The paper's baseline assumption is that G-types have a higher expected return: `p^G α^G > p^B α^B`.\n\n### Question\n\nNow, consider a hypothetical technological environment where G-type projects are safer (`p^G > p^B`) but have a **lower** expected return, i.e., `p^G α^G < p^B α^B`. Select all statements that correctly describe the first-best (full information) allocation under this new assumption.\n", "Answer": ["B", "D"], "qid": "419", "question": "### Background\n\n**Research Question.** This question tests the understanding of the first-best (full information) allocation of credit, which serves as the efficient benchmark in the model. It requires applying the efficiency condition to a novel technological assumption.\n\n**Setting / Institutional Environment.** In a first-best world without informational asymmetries, competitive banks can offer type-specific contracts to entrepreneurs. The goal is to maximize total surplus, which allocates capital to its most productive use.\n\n### Data / Model Specification\n\nThe efficient (first-best) investment level `I^{j*}` for an entrepreneur of type `j` (where `j` is G or B) is determined by the condition that equates the expected marginal product of investment to the marginal cost of capital:\n\n  \np^j \\alpha^j f'(I^{j*}) = 1+r\n\\quad \\text{(Eq. 1)}\n \nwhere `f(I)` is a strictly concave production function (`f'' < 0`). The paper's baseline assumption is that G-types have a higher expected return: `p^G α^G > p^B α^B`.\n\n### Question\n\nNow, consider a hypothetical technological environment where G-type projects are safer (`p^G > p^B`) but have a **lower** expected return, i.e., `p^G α^G < p^B α^B`. Select all statements that correctly describe the first-best (full information) allocation under this new assumption.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This questi...", "error": null}}
{"ID": 420, "Question": "### Background\n\nA dynamic model of electoral competition assumes that politicians have two latent traits: competence, `a`, and true ideology, `ρ`. While in office, a politician chooses an ideological platform, `x`, which may differ from `ρ` due to strategic moderation aimed at winning reelection. Voters observe `x` and `a` (after the first term) but not `ρ`. The econometrician observes neither `a`, `ρ`, nor `x` directly, but instead observes a vector of policies, `p`, which are noisy measures of the underlying platform and competence.\n\n### Data / Model Specification\n\nThe utility of a politician with ideology `ρ` and competence `a` who implements platform `x` is:\n\n  \n\\nu(\\rho, a, x) = -|\\rho - x| + \\lambda a + \\psi \n \n\nwhere `ψ > 0` represents the benefits from holding office. Voters' utility depends on competence via the same parameter `λ`.\n\nThe econometrician models the observed policies `p` for governor `i` using a linear factor model. For a subpopulation of governors who are known to set their platform equal to their true ideology (`x_i = ρ_i`), the system is:\n\n  \np_{ji} = \\mu_{j1}\\rho_i + \\mu_{j2}a_i + \\epsilon_{ji}, \\quad j=1, \\dots, 5 \n \n\nAn exclusion restriction is imposed: some policies (e.g., expenditures, `p_1`) are assumed to depend only on ideology, not competence (i.e., `μ_{12}=0`).\n\n### The Questions\n\nThe paper's identification strategy for the latent distribution of true ideology (`ρ`) relies on observing policies from a subpopulation of governors who are assumed to set their implemented platform (`x`) equal to their true ideology. According to the paper, which of the following groups are included in this subpopulation?", "Answer": ["A", "C"], "qid": "420", "question": "### Background\n\nA dynamic model of electoral competition assumes that politicians have two latent traits: competence, `a`, and true ideology, `ρ`. While in office, a politician chooses an ideological platform, `x`, which may differ from `ρ` due to strategic moderation aimed at winning reelection. Voters observe `x` and `a` (after the first term) but not `ρ`. The econometrician observes neither `a`, `ρ`, nor `x` directly, but instead observes a vector of policies, `p`, which are noisy measures of the underlying platform and competence.\n\n### Data / Model Specification\n\nThe utility of a politician with ideology `ρ` and competence `a` who implements platform `x` is:\n\n  \n\\nu(\\rho, a, x) = -|\\rho - x| + \\lambda a + \\psi \n \n\nwhere `ψ > 0` represents the benefits from holding office. Voters' utility depends on competence via the same parameter `λ`.\n\nThe econometrician models the observed policies `p` for governor `i` using a linear factor model. For a subpopulation of governors who are known to set their platform equal to their true ideology (`x_i = ρ_i`), the system is:\n\n  \np_{ji} = \\mu_{j1}\\rho_i + \\mu_{j2}a_i + \\epsilon_{ji}, \\quad j=1, \\dots, 5 \n \n\nAn exclusion restriction is imposed: some policies (e.g., expenditures, `p_1`) are assumed to depend only on ideology, not competence (i.e., `μ_{12}=0`).\n\n### The Questions\n\nThe paper's identification strategy for the latent distribution of true ideology (`ρ`) relies on observing policies from a subpopulation of governors who are assumed to set their implemented platform (`x`) equal to their true ideology. According to the paper, which of the following groups are included in this subpopulation?", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA dynamic model of electoral compe...", "model_answer": "This is a detailed response to the question about ### Background\n\nA dynamic model of electoral compe...", "error": null}}
{"ID": 421, "Question": "### Background\n\nA dynamic model of electoral competition assumes that politicians have two latent traits: competence, `a`, and true ideology, `ρ`. While in office, a politician chooses an ideological platform, `x`, which may differ from `ρ` due to strategic moderation aimed at winning reelection. Voters observe `x` and `a` (after the first term) but not `ρ`. The econometrician observes neither `a`, `ρ`, nor `x` directly, but instead observes a vector of policies, `p`, which are noisy measures of the underlying platform and competence.\n\n### Data / Model Specification\n\nThe utility of a politician with ideology `ρ` and competence `a` who implements platform `x` is:\n\n  \n\\nu(\\rho, a, x) = -|\\rho - x| + \\lambda a + \\psi \n \n\nwhere `ψ > 0` represents the benefits from holding office. Voters' utility depends on competence via the same parameter `λ`.\n\nThe econometrician models the observed policies `p` for governor `i` using a linear factor model. For a subpopulation of governors who are known to set their platform equal to their true ideology (`x_i = ρ_i`), the system is:\n\n  \np_{ji} = \\mu_{j1}\\rho_i + \\mu_{j2}a_i + \\epsilon_{ji}, \\quad j=1, \\dots, 5 \n \n\nAn exclusion restriction is imposed: some policies (e.g., expenditures, `p_1`) are assumed to depend only on ideology, not competence (i.e., `μ_{12}=0`).\n\n### The Questions\n\nThe paper identifies key preference parameters by linking them to specific observable data moments. Select all statements that correctly describe the identification strategy for the benefits of holding office (`ψ`) and voter preference for competence (`λ`).", "Answer": ["A", "B"], "qid": "421", "question": "### Background\n\nA dynamic model of electoral competition assumes that politicians have two latent traits: competence, `a`, and true ideology, `ρ`. While in office, a politician chooses an ideological platform, `x`, which may differ from `ρ` due to strategic moderation aimed at winning reelection. Voters observe `x` and `a` (after the first term) but not `ρ`. The econometrician observes neither `a`, `ρ`, nor `x` directly, but instead observes a vector of policies, `p`, which are noisy measures of the underlying platform and competence.\n\n### Data / Model Specification\n\nThe utility of a politician with ideology `ρ` and competence `a` who implements platform `x` is:\n\n  \n\\nu(\\rho, a, x) = -|\\rho - x| + \\lambda a + \\psi \n \n\nwhere `ψ > 0` represents the benefits from holding office. Voters' utility depends on competence via the same parameter `λ`.\n\nThe econometrician models the observed policies `p` for governor `i` using a linear factor model. For a subpopulation of governors who are known to set their platform equal to their true ideology (`x_i = ρ_i`), the system is:\n\n  \np_{ji} = \\mu_{j1}\\rho_i + \\mu_{j2}a_i + \\epsilon_{ji}, \\quad j=1, \\dots, 5 \n \n\nAn exclusion restriction is imposed: some policies (e.g., expenditures, `p_1`) are assumed to depend only on ideology, not competence (i.e., `μ_{12}=0`).\n\n### The Questions\n\nThe paper identifies key preference parameters by linking them to specific observable data moments. Select all statements that correctly describe the identification strategy for the benefits of holding office (`ψ`) and voter preference for competence (`λ`).", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\nA dynamic model of electoral compe...", "model_answer": "This is a detailed response to the question about ### Background\n\nA dynamic model of electoral compe...", "error": null}}
{"ID": 422, "Question": "### Background\n\n**Research Question.** This problem addresses the challenge of measuring the causal impact of a complex, multi-faceted legislative environment on child support outcomes. It posits that the *comprehensiveness* of a state's legal framework, rather than any single law, is the key determinant of effectiveness.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel from 1974-1988. The child support enforcement process is described as a sequence of necessary steps: 1) establishing paternity, 2) obtaining a support order, 3) locating the father, and 4) collecting money. A failure at any step undermines the entire process. Prior studies that included dummy variables for individual laws yielded inconsistent results.\n\n**Variables & Parameters.**\n- `CSL_{st}`: A state-year level index measuring the comprehensiveness of child support legislation. It is constructed from data on 13 distinct types of laws.\n- `L_{kst}`: An indicator variable equal to 1 if state `s` has law `k` (where `k`=1,...,13) in effect at year `t`, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe paper argues against estimating a model with 13 separate dummy variables for each law. Instead, it constructs a single latent variable, the Child Support Legislation (CSL) index, using a Rasch model. The text provides a key description of this model's logic:\n\n> \"The Rasch model implicitly places more weight on laws that are less commonly found across the states, on the assumption that... states with the most laws have the most advanced (that is, least common) ones, while states with the fewest laws have the least advanced (that is, most common).\"\n\nThis index is then used as a primary regressor in a model of child support receipt.\n\n---\n\n### Question\n\nBased on the paper's description of the child support enforcement process and its critique of prior research, select all of the following statements that are valid justifications for constructing and using the comprehensive Child Support Legislation (CSL) index instead of separate dummy variables for each law.", "Answer": ["B", "C"], "qid": "422", "question": "### Background\n\n**Research Question.** This problem addresses the challenge of measuring the causal impact of a complex, multi-faceted legislative environment on child support outcomes. It posits that the *comprehensiveness* of a state's legal framework, rather than any single law, is the key determinant of effectiveness.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel from 1974-1988. The child support enforcement process is described as a sequence of necessary steps: 1) establishing paternity, 2) obtaining a support order, 3) locating the father, and 4) collecting money. A failure at any step undermines the entire process. Prior studies that included dummy variables for individual laws yielded inconsistent results.\n\n**Variables & Parameters.**\n- `CSL_{st}`: A state-year level index measuring the comprehensiveness of child support legislation. It is constructed from data on 13 distinct types of laws.\n- `L_{kst}`: An indicator variable equal to 1 if state `s` has law `k` (where `k`=1,...,13) in effect at year `t`, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe paper argues against estimating a model with 13 separate dummy variables for each law. Instead, it constructs a single latent variable, the Child Support Legislation (CSL) index, using a Rasch model. The text provides a key description of this model's logic:\n\n> \"The Rasch model implicitly places more weight on laws that are less commonly found across the states, on the assumption that... states with the most laws have the most advanced (that is, least common) ones, while states with the fewest laws have the least advanced (that is, most common).\"\n\nThis index is then used as a primary regressor in a model of child support receipt.\n\n---\n\n### Question\n\nBased on the paper's description of the child support enforcement process and its critique of prior research, select all of the following statements that are valid justifications for constructing and using the comprehensive Child Support Legislation (CSL) index instead of separate dummy variables for each law.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 423, "Question": "### Background\n\n**Research Question.** This problem addresses the challenge of measuring the causal impact of a complex, multi-faceted legislative environment on child support outcomes. It posits that the *comprehensiveness* of a state's legal framework, rather than any single law, is the key determinant of effectiveness.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel from 1974-1988. The child support enforcement process is described as a sequence of necessary steps: 1) establishing paternity, 2) obtaining a support order, 3) locating the father, and 4) collecting money. A failure at any step undermines the entire process. Prior studies that included dummy variables for individual laws yielded inconsistent results.\n\n**Variables & Parameters.**\n- `CSL_{st}`: A state-year level index measuring the comprehensiveness of child support legislation.\n\n---\n\n### Data / Model Specification\n\nThe paper finds that the CSL index has a causal effect on child support receipt that materializes with a two-year lag. A potential robustness check for this claim is an event-study design, where the 'event' is a major legislative reform in a state. The model would estimate coefficients on dummy variables for years relative to the event (`k`), where `k=-1` is the omitted base year just before the reform.\n\n---\n\n### Question\n\nSelect all statements below that describe a pattern of event-study coefficients that would **CONTRADICT** the paper's hypothesis of a two-year lagged effect and/or violate the core identification assumptions of the event-study design.", "Answer": ["C", "D"], "qid": "423", "question": "### Background\n\n**Research Question.** This problem addresses the challenge of measuring the causal impact of a complex, multi-faceted legislative environment on child support outcomes. It posits that the *comprehensiveness* of a state's legal framework, rather than any single law, is the key determinant of effectiveness.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel from 1974-1988. The child support enforcement process is described as a sequence of necessary steps: 1) establishing paternity, 2) obtaining a support order, 3) locating the father, and 4) collecting money. A failure at any step undermines the entire process. Prior studies that included dummy variables for individual laws yielded inconsistent results.\n\n**Variables & Parameters.**\n- `CSL_{st}`: A state-year level index measuring the comprehensiveness of child support legislation.\n\n---\n\n### Data / Model Specification\n\nThe paper finds that the CSL index has a causal effect on child support receipt that materializes with a two-year lag. A potential robustness check for this claim is an event-study design, where the 'event' is a major legislative reform in a state. The model would estimate coefficients on dummy variables for years relative to the event (`k`), where `k=-1` is the omitted base year just before the reform.\n\n---\n\n### Question\n\nSelect all statements below that describe a pattern of event-study coefficients that would **CONTRADICT** the paper's hypothesis of a two-year lagged effect and/or violate the core identification assumptions of the event-study design.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 424, "Question": "### Background\n\n**Research Question.** This problem explores the paper's proposed solution to the computational challenges of estimating spatial probit models. The core idea is to use a Partial Maximum Likelihood Estimation (PMLE) approach based on pairs of observations, which offers a middle ground between ignoring spatial dependence entirely and attempting a computationally prohibitive full Maximum Likelihood Estimation (MLE).\n\n**Setting / Institutional Environment.** The setting is a cross-section of spatial observations where unobserved shocks are correlated. To make the PMLE approach practical, a computationally tractable error structure is needed, for which the Spatial Moving Average (SMA) model is a prime candidate as it avoids the large-scale matrix inversions required by the more common Spatial Autoregressive (SAE) model.\n\n### Data / Model Specification\n\nThe analysis considers a Spatial Moving Average (SMA) error structure defined as:\n  \n\\varepsilon_{i} = u_{i} + \\lambda \\sum_{h \\neq i} W_{ih} u_h \\quad \\text{(Eq. (1))}\n \nwhere `u_i` are i.i.d. random variables with `E[u_i]=0` and `Var(u_i)=1`, `\\lambda` is a scalar spatial parameter, and `W_{ih}` are elements of a spatial weights matrix `W`.\n\nThe alternative Spatial Autoregressive (SAE) model specifies `\\varepsilon = (I - \\lambda W)^{-1}u`, which implies a variance matrix `\\Omega = [(I - \\lambda W)'(I - \\lambda W)]^{-1}`.\n\nThe PMLE approach involves dividing the observations into pairwise groups. The joint probability of outcomes within a group depends on the variance-covariance matrix of their errors. The partial log-likelihood (PLL) for the sample is the sum of the log-likelihoods for each pair.\n\n### Question\n\nBased on the paper's methodology and rationale, select all statements that are correct.\n", "Answer": ["A", "D"], "qid": "424", "question": "### Background\n\n**Research Question.** This problem explores the paper's proposed solution to the computational challenges of estimating spatial probit models. The core idea is to use a Partial Maximum Likelihood Estimation (PMLE) approach based on pairs of observations, which offers a middle ground between ignoring spatial dependence entirely and attempting a computationally prohibitive full Maximum Likelihood Estimation (MLE).\n\n**Setting / Institutional Environment.** The setting is a cross-section of spatial observations where unobserved shocks are correlated. To make the PMLE approach practical, a computationally tractable error structure is needed, for which the Spatial Moving Average (SMA) model is a prime candidate as it avoids the large-scale matrix inversions required by the more common Spatial Autoregressive (SAE) model.\n\n### Data / Model Specification\n\nThe analysis considers a Spatial Moving Average (SMA) error structure defined as:\n  \n\\varepsilon_{i} = u_{i} + \\lambda \\sum_{h \\neq i} W_{ih} u_h \\quad \\text{(Eq. (1))}\n \nwhere `u_i` are i.i.d. random variables with `E[u_i]=0` and `Var(u_i)=1`, `\\lambda` is a scalar spatial parameter, and `W_{ih}` are elements of a spatial weights matrix `W`.\n\nThe alternative Spatial Autoregressive (SAE) model specifies `\\varepsilon = (I - \\lambda W)^{-1}u`, which implies a variance matrix `\\Omega = [(I - \\lambda W)'(I - \\lambda W)]^{-1}`.\n\nThe PMLE approach involves dividing the observations into pairwise groups. The joint probability of outcomes within a group depends on the variance-covariance matrix of their errors. The partial log-likelihood (PLL) for the sample is the sum of the log-likelihoods for each pair.\n\n### Question\n\nBased on the paper's methodology and rationale, select all statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 425, "Question": "### Background\n\n**Research Question.** This problem addresses the challenge of calculating and interpreting marginal effects in a nonlinear model where spatial dependence introduces observation-specific heterogeneity. In such models, the effect of a change in a covariate on the outcome probability is not a single parameter but varies with each observation's position in the spatial network.\n\n**Setting / Institutional Environment.** The Average Structural Function (ASF) provides a method to summarize these heterogeneous effects into a single, interpretable measure, the Average Partial Effect (APE). This allows for policy analysis and a general understanding of the model's implications.\n\n### Data / Model Specification\n\nThe model is a latent variable framework where the binary outcome is generated by `Y_i = 1[X_iβ + ε_i > 0]`. The unobserved error term is assumed to be conditionally normal with a non-constant variance that depends on the spatial structure:\n  \n\\varepsilon_{i} | X, W \\sim \\text{Normal}(0, h_{i}(W, \\lambda)) \\quad \\text{(Eq. (1))}\n \nThis leads to the following conditional response probability, a heteroskedastic probit model:\n  \nP(Y_{i}=1|X,W) = \\Phi\\left[ \\frac{X_{i}\\beta}{\\sqrt{h_{i}(W,\\lambda)}} \\right] \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF. The partial effect of a continuous covariate `X_ik` on this probability is:\n  \n\\frac{\\partial P(Y_i=1|X_i,W)}{\\partial X_{ik}} = \\phi\\left( \\frac{X_i\\beta}{\\sqrt{h_i(W, \\lambda)}} \\right) \\cdot \\frac{\\beta_k}{\\sqrt{h_i(W, \\lambda)}}\n \nwhere `φ(·)` is the standard normal PDF. The Average Partial Effect (APE) is the sample average of these individual partial effects.\n\n### Question\n\nConsider a policy change that strengthens spatial linkages, described by a new weights matrix `W*`. This policy does not alter the behavioral parameters `β` or `λ`, but it uniformly increases the error variance for all locations, such that `h_i(W*, λ) > h_i(W, λ)` for all `i`. Assume the relevant coefficient `β_k` is positive. Select all correct statements describing the consequences of this policy.\n", "Answer": ["A", "D"], "qid": "425", "question": "### Background\n\n**Research Question.** This problem addresses the challenge of calculating and interpreting marginal effects in a nonlinear model where spatial dependence introduces observation-specific heterogeneity. In such models, the effect of a change in a covariate on the outcome probability is not a single parameter but varies with each observation's position in the spatial network.\n\n**Setting / Institutional Environment.** The Average Structural Function (ASF) provides a method to summarize these heterogeneous effects into a single, interpretable measure, the Average Partial Effect (APE). This allows for policy analysis and a general understanding of the model's implications.\n\n### Data / Model Specification\n\nThe model is a latent variable framework where the binary outcome is generated by `Y_i = 1[X_iβ + ε_i > 0]`. The unobserved error term is assumed to be conditionally normal with a non-constant variance that depends on the spatial structure:\n  \n\\varepsilon_{i} | X, W \\sim \\text{Normal}(0, h_{i}(W, \\lambda)) \\quad \\text{(Eq. (1))}\n \nThis leads to the following conditional response probability, a heteroskedastic probit model:\n  \nP(Y_{i}=1|X,W) = \\Phi\\left[ \\frac{X_{i}\\beta}{\\sqrt{h_{i}(W,\\lambda)}} \\right] \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF. The partial effect of a continuous covariate `X_ik` on this probability is:\n  \n\\frac{\\partial P(Y_i=1|X_i,W)}{\\partial X_{ik}} = \\phi\\left( \\frac{X_i\\beta}{\\sqrt{h_i(W, \\lambda)}} \\right) \\cdot \\frac{\\beta_k}{\\sqrt{h_i(W, \\lambda)}}\n \nwhere `φ(·)` is the standard normal PDF. The Average Partial Effect (APE) is the sample average of these individual partial effects.\n\n### Question\n\nConsider a policy change that strengthens spatial linkages, described by a new weights matrix `W*`. This policy does not alter the behavioral parameters `β` or `λ`, but it uniformly increases the error variance for all locations, such that `h_i(W*, λ) > h_i(W, λ)` for all `i`. Assume the relevant coefficient `β_k` is positive. Select all correct statements describing the consequences of this policy.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 426, "Question": "### Background\n\n**Research Question.** This problem investigates the structure of the Spatial Autoregressive Error (SAE) model, a common method for introducing spatial dependence, and demonstrates why it leads to a significant computational barrier for full Maximum Likelihood Estimation (MLE) in discrete choice settings.\n\n**Setting / Institutional Environment.** The model applies to a cross-section of `N` spatial units where unobserved shocks are not independent but spill over between units according to a predefined network structure. The joint distribution of these shocks is critical for likelihood-based estimation.\n\n### Data / Model Specification\n\nThe reduced-form expression for the Spatial Autoregressive Error (SAE) model solves for the vector of correlated errors `ε` as a function of a vector of i.i.d. fundamental shocks `u`:\n  \n\\varepsilon = (I - \\lambda W)^{-1} u \\quad \\text{(Eq. (1))}\n \nwhere `W` is an `N x N` spatial weights matrix. The term `(I - λW)^-1` is the spatial multiplier, which can be expanded as the series `I + λW + λ^2 W^2 + ...` under certain conditions.\n\n### Question\n\nBased on the structure of the SAE model and its implications for estimation as discussed in the paper, select all statements that are correct.\n", "Answer": ["A", "C"], "qid": "426", "question": "### Background\n\n**Research Question.** This problem investigates the structure of the Spatial Autoregressive Error (SAE) model, a common method for introducing spatial dependence, and demonstrates why it leads to a significant computational barrier for full Maximum Likelihood Estimation (MLE) in discrete choice settings.\n\n**Setting / Institutional Environment.** The model applies to a cross-section of `N` spatial units where unobserved shocks are not independent but spill over between units according to a predefined network structure. The joint distribution of these shocks is critical for likelihood-based estimation.\n\n### Data / Model Specification\n\nThe reduced-form expression for the Spatial Autoregressive Error (SAE) model solves for the vector of correlated errors `ε` as a function of a vector of i.i.d. fundamental shocks `u`:\n  \n\\varepsilon = (I - \\lambda W)^{-1} u \\quad \\text{(Eq. (1))}\n \nwhere `W` is an `N x N` spatial weights matrix. The term `(I - λW)^-1` is the spatial multiplier, which can be expanded as the series `I + λW + λ^2 W^2 + ...` under certain conditions.\n\n### Question\n\nBased on the structure of the SAE model and its implications for estimation as discussed in the paper, select all statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 427, "Question": "### Background\n\n**Research Question.** This problem investigates the paper's central theoretical contribution: establishing an intuitive, non-topological characterization of the equilibrium refinement concept of `full consistency`. The analysis hinges on connecting `full consistency` to an algebraic property of beliefs called `strategic independence` under specific structural conditions on the game form.\n\n**Setting.** The analysis is for a finite `n`-player extensive-form game with perfect recall.\n\n### Data / Model Specification\n\n- A **conditional probability system (CPS)** `σ` on the strategy space `S = S₁ × ... × Sₙ` is a complete specification of beliefs for every non-empty conditioning event.\n- A CPS `σ` has the **independence property** if for any bipartition of players `{J, K}`, the conditional beliefs about group `J` are independent of information concerning only group `K`.\n- An **assessment** `(μ, π)` is **fully consistent** if it is the limit of a sequence of assessments `(μᵏ, πᵏ)` where each `πᵏ` is a strictly randomized behavioral strategy profile.\n- A **strategic extended assessment** `(σ, μ, π)` is a formalism that derives an assessment `(μ, π)` from an underlying CPS `σ`.\n- A game has **observable deviators** if for any information set `h`, the set of strategy profiles that reach it, `S(h)`, forms a Cartesian product of the constituent players' strategy subsets: `S(h) = S₁(h) × ... × Sₙ(h)`.\n\n### Question\n\nThe paper's Proposition 3.1 establishes that in a game with **observable deviators**, strategic independence is equivalent to full consistency. Consider the proof for the more complex direction: *if an assessment `(μ, π)` is derived from a CPS `σ` with the independence property, then `(μ, π)` is fully consistent.*\n\nSelect all statements that correctly describe essential steps or premises of this proof.", "Answer": ["C", "D"], "qid": "427", "question": "### Background\n\n**Research Question.** This problem investigates the paper's central theoretical contribution: establishing an intuitive, non-topological characterization of the equilibrium refinement concept of `full consistency`. The analysis hinges on connecting `full consistency` to an algebraic property of beliefs called `strategic independence` under specific structural conditions on the game form.\n\n**Setting.** The analysis is for a finite `n`-player extensive-form game with perfect recall.\n\n### Data / Model Specification\n\n- A **conditional probability system (CPS)** `σ` on the strategy space `S = S₁ × ... × Sₙ` is a complete specification of beliefs for every non-empty conditioning event.\n- A CPS `σ` has the **independence property** if for any bipartition of players `{J, K}`, the conditional beliefs about group `J` are independent of information concerning only group `K`.\n- An **assessment** `(μ, π)` is **fully consistent** if it is the limit of a sequence of assessments `(μᵏ, πᵏ)` where each `πᵏ` is a strictly randomized behavioral strategy profile.\n- A **strategic extended assessment** `(σ, μ, π)` is a formalism that derives an assessment `(μ, π)` from an underlying CPS `σ`.\n- A game has **observable deviators** if for any information set `h`, the set of strategy profiles that reach it, `S(h)`, forms a Cartesian product of the constituent players' strategy subsets: `S(h) = S₁(h) × ... × Sₙ(h)`.\n\n### Question\n\nThe paper's Proposition 3.1 establishes that in a game with **observable deviators**, strategic independence is equivalent to full consistency. Consider the proof for the more complex direction: *if an assessment `(μ, π)` is derived from a CPS `σ` with the independence property, then `(μ, π)` is fully consistent.*\n\nSelect all statements that correctly describe essential steps or premises of this proof.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 428, "Question": "### Background\n\n**Research Question.** This problem explores the axiomatic foundations of belief systems in game theory, focusing on the relationship between the abstract concept of a `fully consistent` belief system and the more intuitive `independence property`. The goal is to show that independence is a necessary condition for full consistency.\n\n**Setting.** The analysis considers conditional probability systems (CPS) on a product space of `n` players' strategies, `S = S₁ × ... × Sₙ`, within a finite extensive-form game with perfect recall.\n\n### Data / Model Specification\n\n- A **conditional probability system (CPS)** `σ` on `S` is a map `σ(A|E)` for non-empty `E ⊆ S` that specifies a probability distribution on `E` and satisfies the chain rule: `A ⊆ B ⊆ C ⇒ σ(A|C) = σ(A|B)σ(B|C)`.\n- A CPS `σ` has the **independence property** if for any bipartition of players `{J, K}` and non-empty sets `A_J, B_J ⊆ S_J` and `C_K, D_K ⊆ S_K`, the following holds:\n  \n\\sigma(A_{J}\\times C_{K}|B_{J}\\times C_{K})=\\sigma(A_{J}\\times D_{K}|B_{J}\\times D_{K}) \\quad \\text{(Eq. 1)}\n \n- A CPS `σ` is **fully consistent** if it is the limit point of a sequence of CPSs, `{σᵏ}`, where each `σᵏ` is derived via Bayes' rule from a strictly positive product probability distribution `pᵏ(s) = pᵏ₁(s₁) ⋯ pᵏₙ(sₙ)`.\n- The set of all CPSs with the independence property, `IA*(S)`, is a closed set (i.e., it contains all of its limit points).\n\n### Question\n\nThe paper proves that full consistency is a stronger condition than strategic independence (i.e., any fully consistent CPS must have the independence property). Select all statements below that are correct and essential components of this proof.", "Answer": ["A", "D"], "qid": "428", "question": "### Background\n\n**Research Question.** This problem explores the axiomatic foundations of belief systems in game theory, focusing on the relationship between the abstract concept of a `fully consistent` belief system and the more intuitive `independence property`. The goal is to show that independence is a necessary condition for full consistency.\n\n**Setting.** The analysis considers conditional probability systems (CPS) on a product space of `n` players' strategies, `S = S₁ × ... × Sₙ`, within a finite extensive-form game with perfect recall.\n\n### Data / Model Specification\n\n- A **conditional probability system (CPS)** `σ` on `S` is a map `σ(A|E)` for non-empty `E ⊆ S` that specifies a probability distribution on `E` and satisfies the chain rule: `A ⊆ B ⊆ C ⇒ σ(A|C) = σ(A|B)σ(B|C)`.\n- A CPS `σ` has the **independence property** if for any bipartition of players `{J, K}` and non-empty sets `A_J, B_J ⊆ S_J` and `C_K, D_K ⊆ S_K`, the following holds:\n  \n\\sigma(A_{J}\\times C_{K}|B_{J}\\times C_{K})=\\sigma(A_{J}\\times D_{K}|B_{J}\\times D_{K}) \\quad \\text{(Eq. 1)}\n \n- A CPS `σ` is **fully consistent** if it is the limit point of a sequence of CPSs, `{σᵏ}`, where each `σᵏ` is derived via Bayes' rule from a strictly positive product probability distribution `pᵏ(s) = pᵏ₁(s₁) ⋯ pᵏₙ(sₙ)`.\n- The set of all CPSs with the independence property, `IA*(S)`, is a closed set (i.e., it contains all of its limit points).\n\n### Question\n\nThe paper proves that full consistency is a stronger condition than strategic independence (i.e., any fully consistent CPS must have the independence property). Select all statements below that are correct and essential components of this proof.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 429, "Question": "### Background\n\n**Research Question.** This problem addresses the paper's central contribution: the development of a single test statistic for a linear trend, `z_λ`, that is robust to whether the underlying error process `u_t` is stationary (I(0)) or integrated of order one (I(1)).\n\n**Setting / Institutional Environment.** The core challenge is that the optimal test for a trend parameter `β` depends on the unknown order of integration of `u_t`. The paper proposes a data-dependent weighted average of the two optimal-but-specialized test statistics, `z_0` (for the I(0) case) and `z_1` (for the I(1) case).\n\n### Data / Model Specification\n\nThe underlying model is `y_t = α + βt + u_t`. The proposed robust test statistic is a weighted average:\n  \nz_λ := \\{1 - λ(U,S)\\}z_0 + λ(U,S)z_1 \n \n**Eq. (1)**\n\nwhere `z_0` and `z_1` are non-parametrically autocorrelation-corrected t-ratios for `β`. The data-dependent weight `λ(U,S)` is constructed from a unit root test `U` and a stationarity test `S`.\n\nThe properties of these components are governed by the following assumptions and theorems:\n\n*   **Assumption 1 & 2:** If `u_t` is I(0), then `λ(U,S) → 0`. If `u_t` is I(1), then `λ(U,S) → 1` at a rate of `1 + o_p(T^{-1/2})`.\n*   **Assumption 3:** The error process `u_t` can exhibit general forms of weak dependence (serial correlation and heteroskedasticity).\n*   **Theorem 1 (I(0) case):** If `u_t` is I(0), then (i) `z_0` converges to a standard normal distribution (under the null), and (ii) `z_1` converges in probability to 0 (`z_1 = o_p(1)`).\n*   **Theorem 2 (I(1) case):** If `u_t` is I(1), then (i) `z_0` diverges at a rate of `o_p(T^{1/2})`, and (ii) `z_1` converges to a standard normal distribution (under the null).\n\n### Question\n\nConsider the case where the error process `u_t` is I(1). Based on the provided theorems and assumptions, select all statements that correctly describe the asymptotic behavior of the components that determine the limiting distribution of `z_λ`.", "Answer": ["B", "D"], "qid": "429", "question": "### Background\n\n**Research Question.** This problem addresses the paper's central contribution: the development of a single test statistic for a linear trend, `z_λ`, that is robust to whether the underlying error process `u_t` is stationary (I(0)) or integrated of order one (I(1)).\n\n**Setting / Institutional Environment.** The core challenge is that the optimal test for a trend parameter `β` depends on the unknown order of integration of `u_t`. The paper proposes a data-dependent weighted average of the two optimal-but-specialized test statistics, `z_0` (for the I(0) case) and `z_1` (for the I(1) case).\n\n### Data / Model Specification\n\nThe underlying model is `y_t = α + βt + u_t`. The proposed robust test statistic is a weighted average:\n  \nz_λ := \\{1 - λ(U,S)\\}z_0 + λ(U,S)z_1 \n \n**Eq. (1)**\n\nwhere `z_0` and `z_1` are non-parametrically autocorrelation-corrected t-ratios for `β`. The data-dependent weight `λ(U,S)` is constructed from a unit root test `U` and a stationarity test `S`.\n\nThe properties of these components are governed by the following assumptions and theorems:\n\n*   **Assumption 1 & 2:** If `u_t` is I(0), then `λ(U,S) → 0`. If `u_t` is I(1), then `λ(U,S) → 1` at a rate of `1 + o_p(T^{-1/2})`.\n*   **Assumption 3:** The error process `u_t` can exhibit general forms of weak dependence (serial correlation and heteroskedasticity).\n*   **Theorem 1 (I(0) case):** If `u_t` is I(0), then (i) `z_0` converges to a standard normal distribution (under the null), and (ii) `z_1` converges in probability to 0 (`z_1 = o_p(1)`).\n*   **Theorem 2 (I(1) case):** If `u_t` is I(1), then (i) `z_0` diverges at a rate of `o_p(T^{1/2})`, and (ii) `z_1` converges to a standard normal distribution (under the null).\n\n### Question\n\nConsider the case where the error process `u_t` is I(1). Based on the provided theorems and assumptions, select all statements that correctly describe the asymptotic behavior of the components that determine the limiting distribution of `z_λ`.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 430, "Question": "### Background\n\n**Research Question.** This problem addresses the paper's central contribution: the development of a single test statistic for a linear trend, `z_λ`, that is robust to whether the underlying error process `u_t` is stationary (I(0)) or integrated of order one (I(1)).\n\n**Setting / Institutional Environment.** The core challenge is that the optimal test for a trend parameter `β` depends on the unknown order of integration of `u_t`. The paper proposes a data-dependent weighted average of the two optimal-but-specialized test statistics, `z_0` (for the I(0) case) and `z_1` (for the I(1) case).\n\n### Data / Model Specification\n\nThe underlying model is `y_t = α + βt + u_t`. The proposed robust test statistic is a weighted average:\n  \nz_λ := \\{1 - λ(U,S)\\}z_0 + λ(U,S)z_1 \n \n**Eq. (1)**\n\nwhere `z_0` and `z_1` are non-parametrically autocorrelation-corrected t-ratios for `β`. The data-dependent weight `λ(U,S)` is constructed from a unit root test `U` and a stationarity test `S`.\n\nThe properties of these components are governed by the following assumptions and theorems:\n\n*   **Assumption 1 & 2:** If `u_t` is I(0), then `λ(U,S) → 0`. If `u_t` is I(1), then `λ(U,S) → 1` at a rate of `1 + o_p(T^{-1/2})`.\n*   **Assumption 3:** The error process `u_t` can exhibit general forms of weak dependence (serial correlation and heteroskedasticity).\n*   **Theorem 1 (I(0) case):** If `u_t` is I(0), then (i) `z_0` converges to a standard normal distribution (under the null), and (ii) `z_1` converges in probability to 0 (`z_1 = o_p(1)`).\n*   **Theorem 2 (I(1) case):** If `u_t` is I(1), then (i) `z_0` diverges at a rate of `o_p(T^{1/2})`, and (ii) `z_1` converges to a standard normal distribution (under the null).\n\n### Question\n\nThe `z_λ` statistic is designed to provide robust inference on a linear trend, avoiding issues with traditional methods. Select all statements that are **INCORRECT** descriptions of the problems `z_λ` solves or the properties it possesses.", "Answer": ["A", "C"], "qid": "430", "question": "### Background\n\n**Research Question.** This problem addresses the paper's central contribution: the development of a single test statistic for a linear trend, `z_λ`, that is robust to whether the underlying error process `u_t` is stationary (I(0)) or integrated of order one (I(1)).\n\n**Setting / Institutional Environment.** The core challenge is that the optimal test for a trend parameter `β` depends on the unknown order of integration of `u_t`. The paper proposes a data-dependent weighted average of the two optimal-but-specialized test statistics, `z_0` (for the I(0) case) and `z_1` (for the I(1) case).\n\n### Data / Model Specification\n\nThe underlying model is `y_t = α + βt + u_t`. The proposed robust test statistic is a weighted average:\n  \nz_λ := \\{1 - λ(U,S)\\}z_0 + λ(U,S)z_1 \n \n**Eq. (1)**\n\nwhere `z_0` and `z_1` are non-parametrically autocorrelation-corrected t-ratios for `β`. The data-dependent weight `λ(U,S)` is constructed from a unit root test `U` and a stationarity test `S`.\n\nThe properties of these components are governed by the following assumptions and theorems:\n\n*   **Assumption 1 & 2:** If `u_t` is I(0), then `λ(U,S) → 0`. If `u_t` is I(1), then `λ(U,S) → 1` at a rate of `1 + o_p(T^{-1/2})`.\n*   **Assumption 3:** The error process `u_t` can exhibit general forms of weak dependence (serial correlation and heteroskedasticity).\n*   **Theorem 1 (I(0) case):** If `u_t` is I(0), then (i) `z_0` converges to a standard normal distribution (under the null), and (ii) `z_1` converges in probability to 0 (`z_1 = o_p(1)`).\n*   **Theorem 2 (I(1) case):** If `u_t` is I(1), then (i) `z_0` diverges at a rate of `o_p(T^{1/2})`, and (ii) `z_1` converges to a standard normal distribution (under the null).\n\n### Question\n\nThe `z_λ` statistic is designed to provide robust inference on a linear trend, avoiding issues with traditional methods. Select all statements that are **INCORRECT** descriptions of the problems `z_λ` solves or the properties it possesses.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 431, "Question": "### Background\n\n**Research Question.** This problem evaluates the goodness-of-fit of four competing symmetric distributions (Gaussian, Symmetric Stable, Student t, and Generalized Error Distribution) for modeling monthly U.S. stock market returns, using the extended Neyman smooth test.\n\n**Setting / Institutional Environment.** Extended Neyman smooth goodness-of-fit tests are applied to 480 monthly returns from the CRSP value-weighted index (1953-1992). The null hypothesis is varied across the four candidate distributions. The test's flexibility is controlled by `m`, the number of perturbation parameters.\n\n**Variables & Parameters.**\n- `m`: The number of perturbation parameters used in the alternative hypothesis (dimensionless integer).\n- `p-value`: The asymptotic p-value from the extended LM test.\n\n---\n\n### Data / Model Specification\n\nThe following table synthesizes the p-values from Tables 2, 4, 6, and 8 of the paper for the extended LM test using the Neyman-Legendre basis with `m=8` perturbation parameters.\n\n**Table 1. Summary of Test p-values (Neyman-Legendre Basis, m=8)**\n| Null Hypothesis                      | p-value |\n|--------------------------------------|---------|\n| Gaussian                             | 5.2e-13 |\n| Symmetric Stable                     | 0.0550  |\n| Student t                            | 0.0848  |\n| Generalized Error Distribution (GED) | 0.0104  |\n\n---\n\n### Question\n\nUsing a significance level of 5% (α=0.05), which of the following statements are valid statistical conclusions based on the evidence in Table 1?\n\nSelect all that apply.", "Answer": ["A", "B", "C", "D"], "qid": "431", "question": "### Background\n\n**Research Question.** This problem evaluates the goodness-of-fit of four competing symmetric distributions (Gaussian, Symmetric Stable, Student t, and Generalized Error Distribution) for modeling monthly U.S. stock market returns, using the extended Neyman smooth test.\n\n**Setting / Institutional Environment.** Extended Neyman smooth goodness-of-fit tests are applied to 480 monthly returns from the CRSP value-weighted index (1953-1992). The null hypothesis is varied across the four candidate distributions. The test's flexibility is controlled by `m`, the number of perturbation parameters.\n\n**Variables & Parameters.**\n- `m`: The number of perturbation parameters used in the alternative hypothesis (dimensionless integer).\n- `p-value`: The asymptotic p-value from the extended LM test.\n\n---\n\n### Data / Model Specification\n\nThe following table synthesizes the p-values from Tables 2, 4, 6, and 8 of the paper for the extended LM test using the Neyman-Legendre basis with `m=8` perturbation parameters.\n\n**Table 1. Summary of Test p-values (Neyman-Legendre Basis, m=8)**\n| Null Hypothesis                      | p-value |\n|--------------------------------------|---------|\n| Gaussian                             | 5.2e-13 |\n| Symmetric Stable                     | 0.0550  |\n| Student t                            | 0.0848  |\n| Generalized Error Distribution (GED) | 0.0104  |\n\n---\n\n### Question\n\nUsing a significance level of 5% (α=0.05), which of the following statements are valid statistical conclusions based on the evidence in Table 1?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 432, "Question": "### Background\n\n**Research Question.** This problem evaluates the goodness-of-fit of four competing symmetric distributions for modeling monthly U.S. stock market returns. The paper notes that imposing symmetry might be an incorrect assumption, as financial returns often exhibit left-skewness (e.g., from market crashes).\n\n**Setting / Institutional Environment.** An extended Neyman smooth goodness-of-fit test is performed. The test works by adding `m` perturbation functions, `φ_j(z)`, to the null density, each with a coefficient `α_j`. The test statistic measures whether these `α_j` coefficients are significantly different from zero. The choice of `φ_j(z)` determines what kind of deviation the test is sensitive to.\n\n---\n\n### Scenario\n\nSuppose the true, underlying distribution of stock returns is **left-skewed**, but a researcher incorrectly specifies a **symmetric Student t distribution** as the null hypothesis for the extended Neyman test. The test uses a basis of demeaned standard polynomials for the perturbation functions, where `φ_j(z) = z^j - 1/(j+1)`.\n\n### Question\n\nGiven this scenario of model misspecification, which of the following outcomes are plausible consequences? \n\nSelect all that apply.", "Answer": ["A", "C"], "qid": "432", "question": "### Background\n\n**Research Question.** This problem evaluates the goodness-of-fit of four competing symmetric distributions for modeling monthly U.S. stock market returns. The paper notes that imposing symmetry might be an incorrect assumption, as financial returns often exhibit left-skewness (e.g., from market crashes).\n\n**Setting / Institutional Environment.** An extended Neyman smooth goodness-of-fit test is performed. The test works by adding `m` perturbation functions, `φ_j(z)`, to the null density, each with a coefficient `α_j`. The test statistic measures whether these `α_j` coefficients are significantly different from zero. The choice of `φ_j(z)` determines what kind of deviation the test is sensitive to.\n\n---\n\n### Scenario\n\nSuppose the true, underlying distribution of stock returns is **left-skewed**, but a researcher incorrectly specifies a **symmetric Student t distribution** as the null hypothesis for the extended Neyman test. The test uses a basis of demeaned standard polynomials for the perturbation functions, where `φ_j(z) = z^j - 1/(j+1)`.\n\n### Question\n\nGiven this scenario of model misspecification, which of the following outcomes are plausible consequences? \n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** This proble...", "error": null}}
{"ID": 433, "Question": "### Background\n\n**Research Question.** How must a goodness-of-fit test be adjusted when the parameters of the null distribution are not known a priori and must be estimated from the data?\n\n**Setting / Institutional Environment.** The null hypothesis is that an i.i.d. sample `y_i` is drawn from a distribution `F(·; θ)`, where `θ` is a `k x 1` vector of unknown parameters. The parameters `θ` are estimated via Maximum Likelihood (ML) under the null, yielding `θ_hat`. The alternative hypothesis is a smooth perturbation of the null, governed by an `m x 1` parameter vector `α`.\n\n---\n\n### Data / Model Specification\n\nThe extended LM statistic, which corrects for the estimation of `θ`, is given by:\n  \n\\mathrm{LM_{ext}} = \\frac{1}{n} \\mathbf{s}_{\\alpha}' (\\mathbf{I}_{\\alpha\\alpha} - \\mathbf{I}_{\\alpha\\theta} \\mathbf{I}_{\\theta\\theta}^{-1} \\mathbf{I}_{\\theta\\alpha})^{-1} \\mathbf{s}_{\\alpha} \\quad \\text{(Eq. (1))}\n \nIn contrast, a naive (uncorrected) test would incorrectly use the simpler form:\n  \n\\mathrm{LM_{naive}} = \\frac{1}{n} \\mathbf{s}_{\\alpha}' \\mathbf{I}_{\\alpha\\alpha}^{-1} \\mathbf{s}_{\\alpha} \\quad \\text{(Eq. (2))}\n \nThe term `C = I_{αθ} I_{θθ}^{-1} I_{θα}` is a positive semi-definite matrix representing the correction.\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the consequences of estimating model parameters and the function of the correction term `C`?\n\nSelect all that apply.", "Answer": ["A", "B", "C"], "qid": "433", "question": "### Background\n\n**Research Question.** How must a goodness-of-fit test be adjusted when the parameters of the null distribution are not known a priori and must be estimated from the data?\n\n**Setting / Institutional Environment.** The null hypothesis is that an i.i.d. sample `y_i` is drawn from a distribution `F(·; θ)`, where `θ` is a `k x 1` vector of unknown parameters. The parameters `θ` are estimated via Maximum Likelihood (ML) under the null, yielding `θ_hat`. The alternative hypothesis is a smooth perturbation of the null, governed by an `m x 1` parameter vector `α`.\n\n---\n\n### Data / Model Specification\n\nThe extended LM statistic, which corrects for the estimation of `θ`, is given by:\n  \n\\mathrm{LM_{ext}} = \\frac{1}{n} \\mathbf{s}_{\\alpha}' (\\mathbf{I}_{\\alpha\\alpha} - \\mathbf{I}_{\\alpha\\theta} \\mathbf{I}_{\\theta\\theta}^{-1} \\mathbf{I}_{\\theta\\alpha})^{-1} \\mathbf{s}_{\\alpha} \\quad \\text{(Eq. (1))}\n \nIn contrast, a naive (uncorrected) test would incorrectly use the simpler form:\n  \n\\mathrm{LM_{naive}} = \\frac{1}{n} \\mathbf{s}_{\\alpha}' \\mathbf{I}_{\\alpha\\alpha}^{-1} \\mathbf{s}_{\\alpha} \\quad \\text{(Eq. (2))}\n \nThe term `C = I_{αθ} I_{θθ}^{-1} I_{θα}` is a positive semi-definite matrix representing the correction.\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the consequences of estimating model parameters and the function of the correction term `C`?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "This is a detailed response to the question about ### Background\n\n**Research Question.** How must a ...", "model_answer": "This is a detailed response to the question about ### Background\n\n**Research Question.** How must a ...", "error": null}}
